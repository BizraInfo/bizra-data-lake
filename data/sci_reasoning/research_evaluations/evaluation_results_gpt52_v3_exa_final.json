{
  "summary": {
    "model": "gpt-5.2",
    "k": 10,
    "crawl_method": "exa_ai",
    "total_papers": 77,
    "hits": 28,
    "hit_rate_percent": 36.36,
    "average_crawl_rate": 100.0,
    "average_academic_sources": 5.12,
    "runtime_minutes": 48.14,
    "cost": {
      "input_tokens": 797450,
      "output_tokens": 77368,
      "input_cost_usd": 1.59,
      "output_cost_usd": 1.08,
      "total_cost_usd": 2.68
    },
    "timestamp": "2026-01-04T22:41:12.556015"
  },
  "results": [
    {
      "paper_idx": 0,
      "paper_title": "Generalized Linear Mode Connectivity for Transformers",
      "contribution": "They develop a unified, symmetry\u2011aware reparameterization framework (permutations, semi\u2011permutations, orthogonal transforms, and general invertible maps) that uncovers low\u2011 and zero\u2011barrier linear interpolation paths between independently trained Transformers (including Vision Transformers and GPT\u20112) and across architectures of differing widths.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11001,
      "output_tokens": 920,
      "predecessor_details": [
        {
          "success": true,
          "title": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs",
          "url": "https://arxiv.org/abs/1802.10026",
          "content": "[1802.10026] Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:1802.10026\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:1802.10026**(stat)\n[Submitted on 27 Feb 2018 ([v1](https://arxiv.org/abs/1802.10026v1)), last revised 30 Oct 2018 (this version, v4)]\n# Title:Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs\nAuthors:[Timur Garipov](https://arxiv.org/search/stat?searchtype=author&amp;query=Garipov,+T),[Pavel Izmailov](https://arxiv.org/search/stat?searchtype=author&amp;query=Izmailov,+P),[Dmitrii Podoprikhin](https://arxiv.org/search/stat?searchtype=author&amp;query=Podoprikhin,+D),[Dmitry Vetrov](https://arxiv.org/search/stat?searchtype=author&amp;query=Vetrov,+D),[Andrew Gordon Wilson](https://arxiv.org/search/stat?searchtype=author&amp;query=Wilson,+A+G)\nView a PDF of the paper titled Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs, by Timur Garipov and 4 other authors\n[View PDF](https://arxiv.org/pdf/1802.10026)> > Abstract:\n> The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet. Comments:|Appears at Advances in Neural Information Processing Systems (NIPS), 2018|\nSubjects:|Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)|\nCite as:|[arXiv:1802.10026](https://arxiv.org/abs/1802.10026)[stat.ML]|\n|(or[arXiv:1802.10026v4](https://arxiv.org/abs/1802.10026v4)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.1802.10026](https://doi.org/10.48550/arXiv.1802.10026)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Andrew Wilson [[view email](https://arxiv.org/show-email/f09ca40a/1802.10026)]\n**[[v1]](https://arxiv.org/abs/1802.10026v1)**Tue, 27 Feb 2018 17:13:28 UTC (2,199 KB)\n**[[v2]](https://arxiv.org/abs/1802.10026v2)**Thu, 1 Mar 2018 13:53:29 UTC (1,997 KB)\n**[[v3]](https://arxiv.org/abs/1802.10026v3)**Tue, 20 Mar 2018 00:16:34 UTC (1,998 KB)\n**[v4]**Tue, 30 Oct 2018 11:39:49 UTC (2,219 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs, by Timur Garipov and 4 other authors\n* [View PDF](https://arxiv.org/pdf/1802.10026)\n* [TeX Source](https://arxiv.org/src/1802.10026)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1802.10026&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1802.10026&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2018-02](https://arxiv.org/list/stat.ML/2018-02)\nChange to browse by:\n[cs](https://arxiv.org/abs/1802.10026?context=cs)\n[cs.AI](https://arxiv.org/abs/1802.10026?context=cs.AI)\n[cs.LG](https://arxiv.org/abs/1802.10026?context=cs.LG)\n[stat](https://arxiv.org/abs/1802.10026?context=stat)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1802.10026)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1802.10026)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1802.10026)\n### [3 blog links](https://arxiv.org/tb/1802.10026)\n([what is this?](https://info.arxiv.org/help/trackback.html))\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1802.10026)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of Deep Neural Networks",
          "cleaned_query": "Loss Surfaces, Mode Connectivity, and Fast Ensembling of Deep Neural Networks"
        },
        {
          "success": true,
          "title": "Generalized Linear Mode Connectivity for Transformers",
          "url": "https://arxiv.org/abs/2506.22712",
          "content": "[2506.22712] Generalized Linear Mode Connectivity for Transformers\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2506.22712\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2506.22712**(cs)\n[Submitted on 28 Jun 2025 ([v1](https://arxiv.org/abs/2506.22712v1)), last revised 13 Nov 2025 (this version, v2)]\n# Title:Generalized Linear Mode Connectivity for Transformers\nAuthors:[Alexander Theus](https://arxiv.org/search/cs?searchtype=author&amp;query=Theus,+A),[Alessandro Cabodi](https://arxiv.org/search/cs?searchtype=author&amp;query=Cabodi,+A),[Sotiris Anagnostidis](https://arxiv.org/search/cs?searchtype=author&amp;query=Anagnostidis,+S),[Antonio Orvieto](https://arxiv.org/search/cs?searchtype=author&amp;query=Orvieto,+A),[Sidak Pal Singh](https://arxiv.org/search/cs?searchtype=author&amp;query=Singh,+S+P),[Valentina Boeva](https://arxiv.org/search/cs?searchtype=author&amp;query=Boeva,+V)\nView a PDF of the paper titled Generalized Linear Mode Connectivity for Transformers, by Alexander Theus and 5 other authors\n[View PDF](https://arxiv.org/pdf/2506.22712)[HTML (experimental)](https://arxiv.org/html/2506.22712v2)> > Abstract:\n> Understanding the geometry of neural network loss landscapes is a central question in deep learning, with implications for generalization and optimization. A striking phenomenon is linear mode connectivity (LMC), where independently trained models can be connected by low- or zero-loss paths despite appearing to lie in separate loss basins. However, this is often obscured by symmetries in parameter space -- such as neuron permutations -- which make functionally equivalent models appear dissimilar. Prior work has predominantly focused on neuron reordering through permutations, but such approaches are limited in scope and fail to capture the richer symmetries exhibited by modern architectures such as Transformers. In this work, we introduce a unified framework that captures four symmetry classes -- permutations, semi-permutations, orthogonal transformations, and general invertible maps -- broadening the set of valid reparameterizations and subsuming many previous approaches as special cases. Crucially, this generalization enables, for the first time, the discovery of low- and zero-barrier linear interpolation paths between independently trained Vision Transformers and GPT-2 models. Furthermore, our framework extends beyond pairwise alignment to multi-model and width-heterogeneous settings, enabling alignment across architectures of different sizes. These results reveal deeper structure in the loss landscape and underscore the importance of symmetry-aware analysis for understanding model space geometry. Comments:|Accepted to NeurIPS 2025 (Oral)|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2506.22712](https://arxiv.org/abs/2506.22712)[cs.LG]|\n|(or[arXiv:2506.22712v2](https://arxiv.org/abs/2506.22712v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2506.22712](https://doi.org/10.48550/arXiv.2506.22712)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Alexander Theus [[view email](https://arxiv.org/show-email/eb467b8c/2506.22712)]\n**[[v1]](https://arxiv.org/abs/2506.22712v1)**Sat, 28 Jun 2025 01:46:36 UTC (504 KB)\n**[v2]**Thu, 13 Nov 2025 18:21:43 UTC (5,095 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Generalized Linear Mode Connectivity for Transformers, by Alexander Theus and 5 other authors\n* [View PDF](https://arxiv.org/pdf/2506.22712)\n* [HTML (experimental)](https://arxiv.org/html/2506.22712v2)\n* [TeX Source](https://arxiv.org/src/2506.22712)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2506.22712&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2506.22712&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-06](https://arxiv.org/list/cs.LG/2025-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/2506.22712?context=cs)\n[stat](https://arxiv.org/abs/2506.22712?context=stat)\n[stat.ML](https://arxiv.org/abs/2506.22712?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2506.22712)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2506.22712)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2506.22712)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is ",
          "original_query": "Linear Mode Connectivity and its implications for neural networks",
          "cleaned_query": "Linear Mode Connectivity and its implications for neural networks"
        },
        {
          "success": true,
          "title": "Energy expenditure and obesity across the economic spectrum | PNAS",
          "url": "https://www.pnas.org/doi/10.1073/pnas.2420902122",
          "content": "Contents\n\n## Significance\n\nEconomic development is associated with increased prevalence of obesity and related health problems, but the relative importance of increased caloric intake and reduced energy expenditure remains unresolved. We show that daily energy expenditures are greater in developed populations, and activity energy expenditures are not reduced in more industrialized populations, challenging the hypothesis that decreased physical activity contributes to rises in obesity with economic development. Instead, our results suggest that dietary intake plays a far greater role than reduced expenditure in the elevated prevalence of obesity associated with economic development.\n\n## Abstract\n\nGlobal economic development has been associated with an increased prevalence of obesity and related health problems. Increased caloric intake and reduced energy expenditure are both cited as development-related contributors to the obesity crisis, but their relative importance remains unresolved. Here, we examine energy expenditure and two measures of obesity (body fat percentage and body mass index, BMI) for 4,213 adults from 34 populations across six continents and a wide range of lifestyles and economies, including hunter-gatherer, pastoralist, farming, and industrialized populations. Economic development was positively associated with greater body mass, BMI, and body fat, but also with greater total, basal, and activity energy expenditure. Body size\u2013adjusted total and basal energy expenditures both decreased approximately 6 to 11% with increasing economic development, but were highly variable among populations and did not correspond closely with lifestyle. Body size\u2013adjusted total energy expenditure was negatively, but weakly, associated with measures of obesity, accounting for roughly one-tenth of the elevated body fat percentage and BMI associated with economic development. In contrast, estimated energy intake was greater in economically developed populations, and in populations with available data (n = 25), the percentage of ultraprocessed food in the diet was associated with body fat percentage, suggesting that dietary intake plays a far greater role than reduced energy expenditure in obesity related to economic development.\n\n### Sign up for PNAS alerts.\n\nGet alerts for new articles, or get an alert when an article is cited.\n\n[Manage alerts](https://www.pnas.org/action/showPreferences?menuTab=Alerts)\n\nObesity is a leading cause of global mortality and morbidity, accounting for more than 4 million deaths and 140 million disability-adjusted life years worldwide each year ( [1](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r1)). The causes of the modern obesity crisis remain a focus of debate in public health research but appear to be related to economic development. Obesity was rare in the 1800s in the United States ( [2](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r2)), for example, and remains so in traditional farming and foraging communities today, but has become common over the past century among most industrialized populations ( [3](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r3)\u2013 [5](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r5)).\n\nFundamentally, weight gain results from consuming and absorbing more calories than are expended. Public health organizations typically attribute this imbalance to both reduced physical activity energy expenditure (AEE) and dietary changes promoting overconsumption, but assessing the relative contributions of expenditure and intake has proven challenging. Industrialized populations are much less physically active than traditional farming and foraging communities ( [4](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r4)), and daily physical activity has declined within industrialized populations over the past few decades as economic development increased ( [6](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r6), [7](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r7)). However, comparisons between populations and over time indicate that activity decline does not necessarily lead to corresponding reductions in total energy expenditure (TEE) (i.e., the total energy expended per 24 h) ( [8](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r8)\u2013 [11](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r11)). Similarly, while industrialization and economic development have irrevocably changed our environments and the foods we eat, the salient obesogenic aspects of modern lifestyles and diets and the importance of increased intake relative to declining expenditure are unclear ( [12](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r12)). Clarifying the relative importance of reduced energy expenditure and increased energy consumption and absorption in unhealthy weight gain with economic development would help inform public health efforts to prevent obesity.\n\nA major challenge in resolving the contribution of intake and expenditure is the lack of empirically measured expenditure, intake, and body composition across diverse populations. Most broad population comparisons of obesity prevalence lack empirical measures of energy expenditure ( [5](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r5)). Conversely, most studies of total daily energy expenditure (MJ/d) have examined individual nonindustrial populations ( [8](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r8), [9](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r9), [13](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r13)) or have lacked measures of adiposity (i.e., body fat percentage) ( [14](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r14)). Diet assessments across populations have relied largely on surveys or country-level consumption ( [15](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r15), [16](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r16)), which are insufficient for establishing accurate measures of energy intake ( [17](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r17)).\n\nIn this study, we investigated the relative contribution of expenditure and intake to obesity across a global, economically diverse sample of 34 populations, using empirical measures of TEE (MJ/d) and body composition for 4,213 adult individuals between the ages of 18 and 60 y ( [_SI Appendix_](http://www.pnas.org/lookup/doi/10.1073/pnas.2420902122#supplementary-materials)). These populations represent a wide spectrum of economic development, including hunter-gatherers, pastoralists, farmers, and people in industrialized countries. We used the United Nations Human Development Index (HDI) ( [18](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r18)), which integrates measures of wealth, longevity, and education, to further categorize the industrialized populations into Low-, Mid-, and High-HDI economies ( [_SI Appendix_, Table S8](http://www.pnas.org/lookup/doi/10.1073/pnas.2420902122#supplementary-materials)). Following previous work with the International Atomic Energy Agency Doubly Labelled Water Database ( [19](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r19)), TEE was determined using the doubly labeled water method. Basal energy expenditure (BEE) was measured using indirect calorimetry or, when no measures were available, estimated from body size ( [_SI Appendix_](http://www.pnas.org/lookup/doi/10.1073/pnas.2420902122#supplementary-materials)). Following previous work ( [19](https://www.pnas.org/doi/10.1073/pnas.2420902122#core-collateral-r19)), AEE was estimated as \\[0.9(TEE) \u2013 BEE\\], which assumes 10% of daily calories are expended on digesting and metabolizing food; see _Materials and Methods_ for additional detail. Body com",
          "original_query": "Weight/Neuron Matching (permutation) approaches for revealing connectivity between trained networks",
          "cleaned_query": "Weight"
        },
        {
          "success": true,
          "title": "GitHub - snimu/rebasin: Apply methods described in \"Git Re-basin\"-paper [1] to arbitrary models  ---   [1] Ainsworth et al. (https://arxiv.org/abs/2209.04836)",
          "url": "https://github.com/snimu/rebasin",
          "content": "GitHub - snimu/rebasin: Apply methods described in &quot;Git Re-basin&quot;-paper [1] to arbitrary models --- [1] Ainsworth et al. (https://arxiv.org/abs/2209.04836)\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/snimu/rebasin)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n \nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n \nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/snimu/rebasin)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=snimu/rebasin)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[snimu](https://github.com/snimu)/**[rebasin](https://github.com/snimu/rebasin)**Public\n* [Notifications](https://github.com/login?return_to=/snimu/rebasin)You must be signed in to change notification settings\n* [Fork1](https://github.com/login?return_to=/snimu/rebasin)\n* [Star15](https://github.com/login?return_to=/snimu/rebasin)\nApply methods described in \"Git Re-basin\"-paper [1] to arbitrary models --- [1] Ainsworth et al. ([https://arxiv.org/abs/2209.04836](https://arxiv.org/abs/2209.04836))\n### License\n[MIT license](https://github.com/snimu/rebasin/blob/main/LICENSE)\n[15stars](https://github.com/snimu/rebasin/stargazers)[1fork](https://github.com/snimu/rebasin/forks)[Branches](https://github.com/snimu/rebasin/branches)[Tags](https://github.com/snimu/rebasin/tags)[Activity](https://github.com/snimu/rebasin/activity)\n[Star](https://github.com/login?return_to=/snimu/rebasin)\n[Notifications](https://github.com/login?return_to=/snimu/rebasin)You must be signed in to change notification settings\n# snimu/rebasin\nmain\n[Branches](https://github.com/snimu/rebasin/branches)[Tags](https://github.com/snimu/rebasin/tags)\n[](https://github.com/snimu/rebasin/branches)[](https://github.com/snimu/rebasin/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[461 Commits](https://github.com/snimu/rebasin/commits/main/)\n[](https://github.com/snimu/rebasin/commits/main/)\n|\n[rebasin](https://github.com/snimu/rebasin/tree/main/rebasin)\n|\n[rebasin](https://github.com/snimu/rebasin/tree/main/rebasin)\n|\n|\n|\n[tests](https://github.com/snimu/rebasin/tree/main/tests)\n|\n[tests](https://github.com/snimu/rebasin/tree/main/tests)\n|\n|\n|\n[.gitignore](https://github.com/snimu/rebasin/blob/main/.gitignore)\n|\n[.gitignore](https://github.com/snimu/rebasin/blob/main/.gitignore)\n|\n|\n|\n[.pre-commit-config.yaml](https://github.com/snimu/rebasin/blob/main/.pre-commit-config.yaml)\n|\n[.pre-commit-config.yaml](https://github.com/snimu/rebasin/blob/main/.pre-commit-config.yaml)\n|\n|\n|\n[LICENSE](https://github.com/snimu/rebasin/blob/main/LICENSE)\n|\n[LICENSE](https://github.com/snimu/rebasin/blob/main/LICENSE)\n|\n|\n|\n[README.md](https://github.com/snimu/rebasin/blob/main/README.md)\n|\n[README.md](https://github.com/snimu/rebasin/blob/main/README.md)\n|\n|\n|\n[plans.md](https://github.com/snimu/rebasin/blob/main/plans.md)\n|\n[plans.md](https://github.com/snimu/rebasin/blob/main/plans.md)\n|\n|\n|\n[requirements-dev.txt](https://github.com/snimu/rebasin/blob/main/requirements-dev.txt)\n|\n[requirements-dev.txt](https://github.com/snimu/rebasin/blob/main/requirements-dev.txt)\n|\n|\n|\n[requirements.txt](https://github.com/snimu/rebasin/blob/main/requirements.txt)\n|\n[requirements.txt](https://github.com/snimu/rebasin/blob/main/requirements.txt)\n|\n|\n|\n[ruff.toml](https://github.com/snimu/rebasin/blob/main/ruff.toml)\n|\n[ruff.toml](https://github.com/snimu/rebasin/blob/main/ruff.toml)\n|\n|\n|\n[setup.cfg](https://github.com/snimu/rebasin/blob/main/setup.cfg)\n|\n[setup.cfg](https://github.com/snimu/rebasin/blob/main/setup.cfg)\n|\n|\n|\n[setup.py](https://github.com/snimu/rebasin/blob/main/setup.py)\n|\n[setup.py](https://github.com/snimu/rebasin/blob/main/setup.py)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# rebasin\n[](#rebasin)\n[![PyPI Version](https://camo.githubusercontent.com/2af74ac6d5c8f7f34aa23c09f34c1f96ecdfbd0c48470d3427c879e6a82a7e6e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7265626173696e)](https://camo.githubusercontent.com/2af74ac6d5c8f7f34aa23c09f34c1f96ecdfbd0c48470d3427c879e6a82a7e6e/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7265626173696e)[![Wheel](https://camo.githubusercontent.com/c15d656bca8c4432c8c2d3cfd0611ed3bd56cb5585f9be5b0a9b718294712726/68747470733a2f2f696d672e736869656c64732e696f2f707970692f776865656c2f7265626173696e)](https://camo.githubusercontent.com/c15d656bca8c4432c8c2d3cfd0611ed3bd56cb5585f9be5b0a9b718294712726/68747470733a2f2f696d672e736869656c64732e696f2f707970692f776865656c2f7265626173696e)[![Python 3.8+](https://camo.githubusercontent.com/5d93683edb8c07343416dd43d001407612a076960545ee5518241b30bda34a29/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f507974686f6e2d332e382b2d626c75652e737667)](https://www.python.org/downloads/release/python-370/)[![License](https://camo.githubusercontent.com/14f41dd1cd429dee39c272c8d2a3a838debc1f76a8213ff540135f00d5a299ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f736e696d752f7265626173696e)](https://camo.githubusercontent.com/14f41dd1cd429dee39c272c8d2a3a838debc1f76a8213ff540135f00d5a299ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f736e696d752f7265626173696e)\nAn implementation of methods described in[\"Git Re-basin\"-paper by Ainsworth et al.](https://arxiv.org/abs/2209.04836)\nCan be applied to**arbitrary models**, without modification.\n(Well,*almost*arbitrary models, see[Limitations](#limitations)).\n**Table of Contents**\n* [Installation](#installation)\n* [Usage](#usage)\n* [Limitations](#limitations)\n* [Results](#results)\n* [Acknowledgements](#acknowledgements)\n## Installation\n[](#installation)\nRequirements should be automatically installed, but one of them is graphviz,\nwhich you might have to install per apt / brew / ... on your device.\nThe following install instructions are taken directly from[torchview's installation instructions](https://github.com/mert-kurttutan/torchview#installation).\nDebian-based Linux distro (e.g. Ubuntu):\n```\napt-get install graphviz\n```\nWindows:\n```\nchoco install graphviz\n```\nmacOS\n```\nbrew install graphviz\n```\nsee more details[here](https://graphviz.readthedocs.io/en/stable/manual.html).\nThen, install rebasin via pip:\n```\npip install rebasin\n```\n## Usage\n[](#usage)\nCurrently, only weight-matching is implemented as a method for rebasing,\nand only a simplified form of linear interpolation is implemented.\nThe following is a minimal example. For now, the documentation lives in the docstrings,\nthough I intend to create a proper one.`PermutationCoordinateDescent`and`interpolation.LerpSimple`are the main classes, beside`MergeMany`(see below).\n```\nfromrebasinimportPermutationCoordinateDescentfromrebasinimportinterpolationmodel\\_a,model\\_b,train\\_dl=...input\\_data=next(iter(train\\_dl))[0]# Rebasinpcd=PermutationCoordinateDescent(model\\_a,model\\_b,input\\_data)# weight-matchingpcd.rebasin()# Reb",
          "original_query": "Symmetry\u2011aware rebasin / neuron alignment for deeper architectures (Ainsworth et al., 2022 and related)",
          "cleaned_query": "Symmetry\u2011aware rebasin"
        },
        {
          "success": true,
          "title": "Backward-Compatible Aligned Representations via an Orthogonal ...",
          "url": "https://www.arxiv.org/abs/2408.08793",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2408.08793** (cs)\n\n\\[Submitted on 16 Aug 2024\\]\n\n# Title:Backward-Compatible Aligned Representations via an Orthogonal Transformation Layer\n\nAuthors: [Simone Ricci](https://arxiv.org/search/cs?searchtype=author&query=Ricci,+S), [Niccol\u00f2 Biondi](https://arxiv.org/search/cs?searchtype=author&query=Biondi,+N), [Federico Pernici](https://arxiv.org/search/cs?searchtype=author&query=Pernici,+F), [Alberto Del Bimbo](https://arxiv.org/search/cs?searchtype=author&query=Del+Bimbo,+A)\n\nView a PDF of the paper titled Backward-Compatible Aligned Representations via an Orthogonal Transformation Layer, by Simone Ricci and 3 other authors\n\n[View PDF](https://www.arxiv.org/pdf/2408.08793) [HTML (experimental)](https://arxiv.org/html/2408.08793v1)\n\n> Abstract:Visual retrieval systems face significant challenges when updating models with improved representations due to misalignment between the old and new representations. The costly and resource-intensive backfilling process involves recalculating feature vectors for images in the gallery set whenever a new model is introduced. To address this, prior research has explored backward-compatible training methods that enable direct comparisons between new and old representations without backfilling. Despite these advancements, achieving a balance between backward compatibility and the performance of independently trained models remains an open problem. In this paper, we address it by expanding the representation space with additional dimensions and learning an orthogonal transformation to achieve compatibility with old models and, at the same time, integrate new information. This transformation preserves the original feature space's geometry, ensuring that our model aligns with previous versions while also learning new data. Our Orthogonal Compatible Aligned (OCA) approach eliminates the need for re-indexing during model updates and ensures that features can be compared directly across different model updates without additional mapping functions. Experimental results on CIFAR-100 and ImageNet-1k demonstrate that our method not only maintains compatibility with previous models but also achieves state-of-the-art accuracy, outperforming several existing methods.\n\n| | |\n| --- | --- |\n| Comments: | Accepted at BEW2024 Workshop at ECCV2024 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2408.08793](https://arxiv.org/abs/2408.08793) \\[cs.CV\\] |\n| | (or [arXiv:2408.08793v1](https://arxiv.org/abs/2408.08793v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2408.08793](https://doi.org/10.48550/arXiv.2408.08793) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Simone Ricci \\[ [view email](https://www.arxiv.org/show-email/6dda7ace/2408.08793)\\]\n\n**\\[v1\\]**\nFri, 16 Aug 2024 15:05:28 UTC (262 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Backward-Compatible Aligned Representations via an Orthogonal Transformation Layer, by Simone Ricci and 3 other authors\n\n- [View PDF](https://www.arxiv.org/pdf/2408.08793)\n- [HTML (experimental)](https://arxiv.org/html/2408.08793v1)\n- [TeX Source](https://www.arxiv.org/src/2408.08793)\n- [Other Formats](https://www.arxiv.org/format/2408.08793)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://www.arxiv.org/prevnext?id=2408.08793&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://www.arxiv.org/prevnext?id=2408.08793&function=next&context=cs.CV)\n\n[new](https://www.arxiv.org/list/cs.CV/new) \\| [recent](https://www.arxiv.org/list/cs.CV/recent) \\| [2024-08](https://www.arxiv.org/list/cs.CV/2024-08)\n\nChange to browse by:\n\n[cs](https://www.arxiv.org/abs/2408.08793?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2408.08793)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2408.08793)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2408.08793)\n\n[a](https://www.arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://www.arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2408.08793&description=Backward-Compatible Aligned Representations via an Orthogonal Transformation Layer) [![Reddit logo](https://www.arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2408.08793&title=Backward-Compatible Aligned Representations via an Orthogonal Transformation Layer)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://www.arxiv.org/auth/show-endorsers/2408.08793) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Representational alignment via orthogonal transformations (e.g., SVCCA / CKA / Orthogonal Procrustes literature)",
          "cleaned_query": "Representational alignment via orthogonal transformations (e.g., SVCCA"
        }
      ],
      "generated_ideas": [
        "Symmetry-Aware Fast Geometric Ensembling for Transformers\nDevelop an FGE-style training schedule for Transformers where successive \u201csnapshots\u201d are first aligned using the generalized symmetry classes (permutation/semi-permutation/orthogonal/invertible) before ensembling. Evaluate whether symmetry-aligned FGE improves ensemble diversity and accuracy vs. standard snapshot ensembles and whether it reduces the number of required snapshots to reach a target performance.",
        "Mode-Connectivity\u2013Preserving Backward Compatibility for Retrieval Models\nCombine OCA\u2019s orthogonal compatibility layer with mode-connectivity constraints so that the *entire path* from old to new model remains low-loss and retrieval-compatible (not just the endpoint). Implement training that penalizes accuracy drops along linear interpolation after orthogonal alignment, and test whether it enables \u201chot swapping\u201d model upgrades without reindexing or temporary degradation.",
        "Multi-Model \u201cRebasing Graph\u201d for Continuous Model Upgrades\nExtend rebasin-style alignment to build a graph over many model versions (nodes) with learned pairwise (or global) symmetry transforms (edges) that enable low-barrier interpolation between any two versions. Use this graph to compute minimal-change upgrade routes (sequence of interpolations) that preserve task metrics and compatibility constraints over time.",
        "Width-Heterogeneous Rebasin for Distillation-Free Model Compression\nUse the width-heterogeneous alignment capability from generalized LMC to map a large Transformer into a smaller one via learned invertible/orthogonal subspace transforms, then perform direct weight interpolation rather than distillation. Measure whether \u201caligned interpolation compression\u201d can match distillation quality while reducing compute and engineering overhead.",
        "Functional Diversity Metrics Along Connected Paths as an Ensemble Selection Tool\nPropose metrics that quantify *functional* (not parameter) diversity along mode-connected curves after symmetry alignment (e.g., disagreement on calibrated probabilities, representation CKA drift, attention pattern divergence). Use these metrics to automatically select a small set of checkpoints along a path that maximizes ensemble gains under a fixed inference budget.",
        "Orthogonal Subspace Expansion for Transformers to Enable Cross-Version Logit Compatibility\nGeneralize OCA beyond retrieval embeddings to *logit/representation compatibility* in language models by expanding hidden states with extra dimensions and learning orthogonal transforms that preserve old-model subspaces. Evaluate whether new checkpoints remain compatible with old calibrated decision thresholds (e.g., safety classifiers, tool triggers) without backfilling or re-tuning.",
        "Connectivity-Guided Federated Model Aggregation via Rebasin + Linear Interpolation\nIn federated learning, replace naive weight averaging with: (i) client model rebasing/alignment into a shared symmetry gauge, then (ii) connectivity-validated linear interpolation/merging that checks for low-loss barriers on a server-held validation proxy. Test whether this reduces accuracy collapse under non-IID data compared to FedAvg and standard permutation matching.",
        "Loss-Landscape Cartography of Diet\u2013Obesity Predictors via Mode Connectivity\nTrain multiple deep models to predict obesity metrics from dietary composition proxies (e.g., ultraprocessed food percentage) and expenditure covariates, then analyze whether high-performing solutions are connected after symmetry-aware alignment. Use the connected subspace to estimate which covariates are stable (invariant importance) across modes vs. spurious, improving interpretability and robustness of epidemiological predictors.",
        "Barrier-Aware \u201cSafe Update\u201d Criterion for Deployed Models\nDefine a deployment test that certifies a proposed model update is safe if, after optimal symmetry alignment, the linear path between old and new weights maintains bounded loss and bounded distribution shift in representations (e.g., retrieval neighbors or token probabilities). Build tooling that outputs a scalar \u201cupdate barrier score\u201d and validate whether it predicts real-world regressions better than endpoint-only A/B evaluation."
      ],
      "generated_ideas_raw": "1. **Symmetry-Aware Fast Geometric Ensembling for Transformers**\n   Develop an FGE-style training schedule for Transformers where successive \u201csnapshots\u201d are first aligned using the generalized symmetry classes (permutation/semi-permutation/orthogonal/invertible) before ensembling. Evaluate whether symmetry-aligned FGE improves ensemble diversity and accuracy vs. standard snapshot ensembles and whether it reduces the number of required snapshots to reach a target performance.\n\n2. **Mode-Connectivity\u2013Preserving Backward Compatibility for Retrieval Models**\n   Combine OCA\u2019s orthogonal compatibility layer with mode-connectivity constraints so that the *entire path* from old to new model remains low-loss and retrieval-compatible (not just the endpoint). Implement training that penalizes accuracy drops along linear interpolation after orthogonal alignment, and test whether it enables \u201chot swapping\u201d model upgrades without reindexing or temporary degradation.\n\n3. **Multi-Model \u201cRebasing Graph\u201d for Continuous Model Upgrades**\n   Extend rebasin-style alignment to build a graph over many model versions (nodes) with learned pairwise (or global) symmetry transforms (edges) that enable low-barrier interpolation between any two versions. Use this graph to compute minimal-change upgrade routes (sequence of interpolations) that preserve task metrics and compatibility constraints over time.\n\n4. **Width-Heterogeneous Rebasin for Distillation-Free Model Compression**\n   Use the width-heterogeneous alignment capability from generalized LMC to map a large Transformer into a smaller one via learned invertible/orthogonal subspace transforms, then perform direct weight interpolation rather than distillation. Measure whether \u201caligned interpolation compression\u201d can match distillation quality while reducing compute and engineering overhead.\n\n5. **Functional Diversity Metrics Along Connected Paths as an Ensemble Selection Tool**\n   Propose metrics that quantify *functional* (not parameter) diversity along mode-connected curves after symmetry alignment (e.g., disagreement on calibrated probabilities, representation CKA drift, attention pattern divergence). Use these metrics to automatically select a small set of checkpoints along a path that maximizes ensemble gains under a fixed inference budget.\n\n6. **Task-Vector Transport with Symmetry Alignment for Reliable Model Merging**\n   Study whether task vectors (fine-tune deltas) can be \u201ctransported\u201d between independently trained bases by first applying generalized symmetry alignment, then adding/interpolating deltas in the aligned coordinate system. Implement on GPT-2/Vision Transformers across multiple downstream tasks to test if alignment reduces destructive interference in model merging.\n\n7. **Orthogonal Subspace Expansion for Transformers to Enable Cross-Version Logit Compatibility**\n   Generalize OCA beyond retrieval embeddings to *logit/representation compatibility* in language models by expanding hidden states with extra dimensions and learning orthogonal transforms that preserve old-model subspaces. Evaluate whether new checkpoints remain compatible with old calibrated decision thresholds (e.g., safety classifiers, tool triggers) without backfilling or re-tuning.\n\n8. **Connectivity-Guided Federated Model Aggregation via Rebasin + Linear Interpolation**\n   In federated learning, replace naive weight averaging with: (i) client model rebasing/alignment into a shared symmetry gauge, then (ii) connectivity-validated linear interpolation/merging that checks for low-loss barriers on a server-held validation proxy. Test whether this reduces accuracy collapse under non-IID data compared to FedAvg and standard permutation matching.\n\n9. **Loss-Landscape Cartography of Diet\u2013Obesity Predictors via Mode Connectivity**\n   Train multiple deep models to predict obesity metrics from dietary composition proxies (e.g., ultraprocessed food percentage) and expenditure covariates, then analyze whether high-performing solutions are connected after symmetry-aware alignment. Use the connected subspace to estimate which covariates are stable (invariant importance) across modes vs. spurious, improving interpretability and robustness of epidemiological predictors.\n\n10. **Barrier-Aware \u201cSafe Update\u201d Criterion for Deployed Models**\n   Define a deployment test that certifies a proposed model update is safe if, after optimal symmetry alignment, the linear path between old and new weights maintains bounded loss and bounded distribution shift in representations (e.g., retrieval neighbors or token probabilities). Build tooling that outputs a scalar \u201cupdate barrier score\u201d and validate whether it predicts real-world regressions better than endpoint-only A/B evaluation.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Symmetry-Aware Fast Geometric Ensembling for Transformers\nDevelop an FGE-style training schedule for Transformers where successive \u201csnapshots\u201d are first aligned using the generalized symmetry classes ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Mode-Connectivity\u2013Preserving Backward Compatibility for Retrieval Models\nCombine OCA\u2019s orthogonal compatibility layer with mode-connectivity constraints so that the *entire path* from old to new model",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Multi-Model \u201cRebasing Graph\u201d for Continuous Model Upgrades\nExtend rebasin-style alignment to build a graph over many model versions (nodes) with learned pairwise (or global) symmetry transforms (edges",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Width-Heterogeneous Rebasin for Distillation-Free Model Compression\nUse the width-heterogeneous alignment capability from generalized LMC to map a large Transformer into a smaller one via learned inve",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Functional Diversity Metrics Along Connected Paths as an Ensemble Selection Tool\nPropose metrics that quantify *functional* (not parameter) diversity along mode-connected curves after symmetry alignme",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Orthogonal Subspace Expansion for Transformers to Enable Cross-Version Logit Compatibility\nGeneralize OCA beyond retrieval embeddings to *logit/representation compatibility* in language models by expa",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Connectivity-Guided Federated Model Aggregation via Rebasin + Linear Interpolation\nIn federated learning, replace naive weight averaging with: (i) client model rebasing/alignment into a shared symmetr",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Loss-Landscape Cartography of Diet\u2013Obesity Predictors via Mode Connectivity\nTrain multiple deep models to predict obesity metrics from dietary composition proxies (e.g., ultraprocessed food percentage",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Barrier-Aware \u201cSafe Update\u201d Criterion for Deployed Models\nDefine a deployment test that certifies a proposed model update is safe if, after optimal symmetry alignment, the linear path between old and ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 1,
      "paper_title": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation",
      "contribution": "Introduce a compositional phase-domain diffusion framework (with ACT-PAE, SPDM and TPDM) that denoises semantic and transition-aware phase latents so long multi-segment motion sequences are both semantically aligned and smoothly transitioned.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 11410,
      "output_tokens": 902,
      "predecessor_details": [
        {
          "success": true,
          "title": "periodic autoencoders for learning motion phase manifolds ...",
          "url": "https://dl.acm.org/doi/10.1145/3528223.3530178",
          "content": "DeepPhase: periodic autoencoders for learning motion phase manifolds: ACM Transactions on Graphics: Vol 41, No 4[skip to main content](#skip-to-main-content)\n[](#global-menu)\nSearch ACM Digital Library\nSearchSearch\n[Advanced Search](https://dl.acm.org/search/advanced)\n[ACM Transactions on Graphics](#)\n**## Export Citations\nSelect Citation formatBibTeXEndNoteACM Ref**\n* Please download or close your previous search result export first before starting a new bulk export.\nPreview is not available.\nBy clicking download,**a status dialog**will open to start the export process. The process may take**a few minutes**but once it finishes a file will be downloadable from your browser. You may continue to browse the DL while the export process is in progress.\n* ```\n```\n* [Download citation**](javascript:void(0))\n* [Copy citation**](javascript:void(0))\nresearch-article\nShare on\n* **\n* **\n* **\n* **\n* **\n# DeepPhase:periodic autoencoders for learning motion phase manifolds\nAuthors:[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)SebastianStarke](#artseq-00001),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)IanMason](#artseq-00002),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)TakuKomura](#artseq-00003)[Authors Info &amp; Claims](#tab-contributors)\n[ACM Transactions on Graphics (TOG),Volume41,Issue4](https://dl.acm.org/toc/tog/2022/41/4)\nArticle No.: 136, Pages1-13\n[https://doi.org/10.1145/3528223.3530178](https://doi.org/10.1145/3528223.3530178)\nPublished:22 July 2022[Publication History](#core-history)[](#)\n**120citation**6,643Downloads\nMetrics\n[\nTotal Citations120\n](#tab-citations)[\nTotal Downloads6,643\n](#tab-metrics-inner)\nLast 12 Months1,235\nLast 6 weeks125\n**Get Citation Alerts\n**## New Citation Alert added!\nThis alert has been successfully added and will be sent to:\nYou will be notified whenever a record that you have chosen has been cited.\nTo manage your alert preferences, click on the button below.\n[Manage my Alerts](https://dl.acm.org/action/showPreferences?menuTab=Alerts)\n**## New Citation Alert!\nPlease[log in to your account](https://dl.acm.org/action/showLogin?redirectUri=/doi/10.1145/3528223.3530178)\n**\n**\n[**Get Access](#core-collateral-purchase-access)\n**Contents\n## Abstract\nLearning the spatial-temporal structure of body movements is a fundamental problem for character motion synthesis. In this work, we propose a novel neural network architecture called the Periodic Autoencoder that can learn periodic features from large unstructured motion datasets in an unsupervised manner. The character movements are decomposed into multiple latent channels that capture the non-linear periodicity of different body segments while progressing forward in time. Our method extracts a multi-dimensional phase space from full-body motion data, which effectively clusters animations and produces a manifold in which computed feature distances provide a better similarity measure than in the original motion space to achieve better temporal and spatial alignment. We demonstrate that the learned periodic embedding can significantly help to improve neural motion synthesis in a number of tasks, including diverse locomotion skills, style-based movements, dance motion synthesis from music, synthesis of dribbling motions in football, and motion query for matching poses within large animation databases.\n## Supplemental Material\nMP4 File\nsupplemental material\n* [Download](https://dl.acm.org/doi/suppl/10.1145/3528223.3530178/suppl_file/136-732-supp-video.mp4)\n* 483.02 MB\nMP4 File\npresentation\n* [Download](https://dl.acm.org/doi/suppl/10.1145/3528223.3530178/suppl_file/3528223.3530178.mp4)\n* 1671.05 MB\nSRT File\npresentation\n* [Download](https://dl.acm.org/doi/suppl/10.1145/3528223.3530178/suppl_file/3528223.3530178.srt)\n* 25.40 KB\nZIP File\nsupplemental material\n* [Download](https://dl.acm.org/doi/suppl/10.1145/3528223.3530178/suppl_file/136-732-supp-mtl.zip)\n* 343.08 MB\n## References\n[1]\nOkan Arikan and David A Forsyth. 2002. Interactive motion generation from examples.*ACM Trans on Graph*21, 3 (2002), 483--490.\n[Digital Library](https://dl.acm.org/doi/10.1145/566654.566606)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/566654.566606)\n[2]\nPhilippe Beaudoin, Pierre Poulin, and Michiel van de Panne. 2007. Adapting wavelet compression to human motion capture clips. In*Proceedings of Graphics Interface 2007.*313--318.\n[Google Scholar](https://scholar.google.com/scholar?q=Philippe+Beaudoin,+Pierre+Poulin,+and+Michiel+van+de+Panne.+2007.+Adapting+wavelet+compression+to+human+motion+capture+clips.+In+Proceedings+of+Graphics+Interface+2007.+313--318.)\n[3]\nKevin Bergamin, Simon Clavet, Daniel Holden, and James Richard Forbes. 2019. DReCon: data-driven responsive control of physics-based characters.*ACM Transactions on Graphics (TOG)*38, 6 (2019), 1--11.\n[Digital Library](https://dl.acm.org/doi/10.1145/3355089.3356536)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/3355089.3356536)\n[4]\nArmin Bruderlin and Lance Williams. 1995. Motion signal processing. In*Proceedings of the 22nd annual conference on Computer graphics and interactive techniques.*97--104.\n[Digital Library](https://dl.acm.org/doi/10.1145/218380.218421)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/218380.218421)\n[5]\nKyungmin Cho, Chaelin Kim, Jungjin Park, Joonkyu Park, and Junyong Noh. 2021. Motion recommendation for online character control.*ACM Transactions on Graphics (TOG)*40, 6 (2021), 1--16.\n[Digital Library](https://dl.acm.org/doi/10.1145/3478513.3480512)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/3478513.3480512)\n[6]\nSimon Clavet. 2016. Motion matching and the road to next-gen animation. In*Proc. of GDC.*\n[Google Scholar](https://scholar.google.com/scholar?q=Simon+Clavet.+2016.+Motion+matching+and+the+road+to+next-gen+animation.+In+Proc.+of+GDC.)\n[7]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In*Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).*4171--4186.\n[Crossref](https://doi.org/10.18653/v1/N19-1423)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.18653/v1/N19-1423)\n[8]\nMilan R Dimitrijevic, Yuri Gerasimenko, and Michaela M Pinter. 1998. Evidence for a spinal central pattern generator in humans.*Annals of the New York Academy of Sciences*860, 1 (1998), 360--376.\n[Crossref](https://doi.org/10.1111/j.1749-6632.1998.tb09062.x)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1111/j.1749-6632.1998.tb09062.x)\n[9]\nLevi Fussell, Kevin Bergamin, and Daniel Holden. 2021. SuperTrack: motion tracking for physically simulated characters using supervised learning.*ACM Transactions on Graphics (TOG)*40, 6 (2021), 1--13.\n[Digital Library](https://dl.acm.org/doi/10.1145/3478513.3480527)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/3478513.3480527)\n[10]\nF\u00e9lix G Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. 2020. Robust motion in-betweening.*ACM Transactions on Graphics (TOG)*39, 4 (2020), 60--1.\n[Digital Library](https://dl.acm.org/doi/10.1145/3386569.3392480)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/3386569.3392480)\n[11]\nRachel Heck and Michael Gleicher. 2007. Parametric motion graphs. In*Proc. I3D.*129--136.\n[Digital Library](https://dl.acm.org/doi/10.1145/1230100.1230123)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/1230100.1230123)\n[12]\nGustav Eje Henter, Simon Alexanderson, and Jonas Beskow. 2020. Moglow: Probabilistic and controllable motion synthesis using normalising flows.*ACM Transactions on Graphics (TOG)*39, 6 (2020), 1--14.\n[Digital Library](https://dl.acm.org/doi",
          "original_query": "Deepphase: Periodic autoencoders for learning motion phase manifolds",
          "cleaned_query": "Deepphase: Periodic autoencoders for learning motion phase manifolds"
        },
        {
          "success": true,
          "title": "[2312.04036] DiffusionPhase: Motion Diffusion in Frequency Domain",
          "url": "https://arxiv.org/abs/2312.04036",
          "content": "[2312.04036] DiffusionPhase: Motion Diffusion in Frequency Domain\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2312.04036\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2312.04036**(cs)\n[Submitted on 7 Dec 2023]\n# Title:DiffusionPhase: Motion Diffusion in Frequency Domain\nAuthors:[Weilin Wan](https://arxiv.org/search/cs?searchtype=author&amp;query=Wan,+W),[Yiming Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+Y),[Shutong Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+S),[Taku Komura](https://arxiv.org/search/cs?searchtype=author&amp;query=Komura,+T),[Wenping Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+W),[Dinesh Jayaraman](https://arxiv.org/search/cs?searchtype=author&amp;query=Jayaraman,+D),[Lingjie Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+L)\nView a PDF of the paper titled DiffusionPhase: Motion Diffusion in Frequency Domain, by Weilin Wan and 6 other authors\n[View PDF](https://arxiv.org/pdf/2312.04036)> > Abstract:\n> In this study, we introduce a learning-based method for generating high-quality human motion sequences from text descriptions (e.g., ``A person walks forward&#34;). Existing techniques struggle with motion diversity and smooth transitions in generating arbitrary-length motion sequences, due to limited text-to-motion datasets and the pose representations used that often lack expressiveness or compactness. To address these issues, we propose the first method for text-conditioned human motion generation in the frequency domain of motions. We develop a network encoder that converts the motion space into a compact yet expressive parameterized phase space with high-frequency details encoded, capturing the local periodicity of motions in time and space with high accuracy. We also introduce a conditional diffusion model for predicting periodic motion parameters based on text descriptions and a start pose, efficiently achieving smooth transitions between motion sequences associated with different text descriptions. Experiments demonstrate that our approach outperforms current methods in generating a broader variety of high-quality motions, and synthesizing long sequences with natural transitions. Subjects:|Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)|\nCite as:|[arXiv:2312.04036](https://arxiv.org/abs/2312.04036)[cs.CV]|\n|(or[arXiv:2312.04036v1](https://arxiv.org/abs/2312.04036v1)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2312.04036](https://doi.org/10.48550/arXiv.2312.04036)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Weilin Wan [[view email](https://arxiv.org/show-email/b0fdfbc2/2312.04036)]\n**[v1]**Thu, 7 Dec 2023 04:39:22 UTC (19,221 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled DiffusionPhase: Motion Diffusion in Frequency Domain, by Weilin Wan and 6 other authors\n* [View PDF](https://arxiv.org/pdf/2312.04036)\n* [TeX Source](https://arxiv.org/src/2312.04036)\n[![license icon](https://arxiv.org/icons/licenses/by-nc-nd-4.0.png)view license](http://creativecommons.org/licenses/by-nc-nd/4.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2312.04036&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2312.04036&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2023-12](https://arxiv.org/list/cs.CV/2023-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/2312.04036?context=cs)\n[cs.LG](https://arxiv.org/abs/2312.04036?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2312.04036)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2312.04036)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2312.04036)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2312.04036)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Diffusionphase: Motion diffusion in frequency domain",
          "cleaned_query": "Diffusionphase: Motion diffusion in frequency domain"
        },
        {
          "success": true,
          "title": "[2209.14916] Human Motion Diffusion Model - arXiv",
          "url": "https://arxiv.org/abs/2209.14916",
          "content": "[2209.14916] Human Motion Diffusion Model\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2209.14916\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2209.14916**(cs)\n[Submitted on 29 Sep 2022 ([v1](https://arxiv.org/abs/2209.14916v1)), last revised 3 Oct 2022 (this version, v2)]\n# Title:Human Motion Diffusion Model\nAuthors:[Guy Tevet](https://arxiv.org/search/cs?searchtype=author&amp;query=Tevet,+G),[Sigal Raab](https://arxiv.org/search/cs?searchtype=author&amp;query=Raab,+S),[Brian Gordon](https://arxiv.org/search/cs?searchtype=author&amp;query=Gordon,+B),[Yonatan Shafir](https://arxiv.org/search/cs?searchtype=author&amp;query=Shafir,+Y),[Daniel Cohen-Or](https://arxiv.org/search/cs?searchtype=author&amp;query=Cohen-Or,+D),[Amit H. Bermano](https://arxiv.org/search/cs?searchtype=author&amp;query=Bermano,+A+H)\nView a PDF of the paper titled Human Motion Diffusion Model, by Guy Tevet and 4 other authors\n[View PDF](https://arxiv.org/pdf/2209.14916)> > Abstract:\n> Natural and expressive human motion generation is the holy grail of computer animation. It is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. Diffusion models, which have already shown remarkable generative capabilities in other domains, are promising candidates for human motion due to their many-to-many nature, but they tend to be resource hungry and hard to control. In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for the human motion domain. MDM is transformer-based, combining insights from motion generation literature. A notable design-choice is the prediction of the sample, rather than the noise, in each diffusion step. This facilitates the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion and action-to-motion. [> this https URL\n](https://guytevet.github.io/mdm-page/)> . Subjects:|Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)|\nCite as:|[arXiv:2209.14916](https://arxiv.org/abs/2209.14916)[cs.CV]|\n|(or[arXiv:2209.14916v2](https://arxiv.org/abs/2209.14916v2)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2209.14916](https://doi.org/10.48550/arXiv.2209.14916)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Guy Tevet [[view email](https://arxiv.org/show-email/1af4c8b5/2209.14916)]\n**[[v1]](https://arxiv.org/abs/2209.14916v1)**Thu, 29 Sep 2022 16:27:53 UTC (11,215 KB)\n**[v2]**Mon, 3 Oct 2022 09:17:41 UTC (11,213 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Human Motion Diffusion Model, by Guy Tevet and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2209.14916)\n* [TeX Source](https://arxiv.org/src/2209.14916)\n[![license icon](https://arxiv.org/icons/licenses/by-sa-4.0.png)view license](http://creativecommons.org/licenses/by-sa/4.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2209.14916&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2209.14916&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2022-09](https://arxiv.org/list/cs.CV/2022-09)\nChange to browse by:\n[cs](https://arxiv.org/abs/2209.14916?context=cs)\n[cs.GR](https://arxiv.org/abs/2209.14916?context=cs.GR)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2209.14916)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2209.14916)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2209.14916)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2209.14916)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Human motion diffusion model (MDM)",
          "cleaned_query": "Human motion diffusion model (MDM)"
        },
        {
          "success": true,
          "title": "[2303.01418] Human Motion Diffusion as a Generative Prior - arXiv",
          "url": "https://arxiv.org/abs/2303.01418",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2303.01418** (cs)\n\n\\[Submitted on 2 Mar 2023 ( [v1](https://arxiv.org/abs/2303.01418v1)), last revised 30 Aug 2023 (this version, v3)\\]\n\n# Title:Human Motion Diffusion as a Generative Prior\n\nAuthors: [Yonatan Shafir](https://arxiv.org/search/cs?searchtype=author&query=Shafir,+Y), [Guy Tevet](https://arxiv.org/search/cs?searchtype=author&query=Tevet,+G), [Roy Kapon](https://arxiv.org/search/cs?searchtype=author&query=Kapon,+R), [Amit H. Bermano](https://arxiv.org/search/cs?searchtype=author&query=Bermano,+A+H)\n\nView a PDF of the paper titled Human Motion Diffusion as a Generative Prior, by Yonatan Shafir and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2303.01418)\n\n> Abstract:Recent work has demonstrated the significant potential of denoising diffusion models for generating human motion, including text-to-motion capabilities. However, these methods are restricted by the paucity of annotated motion data, a focus on single-person motions, and a lack of detailed control. In this paper, we introduce three forms of composition based on diffusion priors: sequential, parallel, and model composition. Using sequential composition, we tackle the challenge of long sequence generation. We introduce DoubleTake, an inference-time method with which we generate long animations consisting of sequences of prompted intervals and their transitions, using a prior trained only for short clips. Using parallel composition, we show promising steps toward two-person generation. Beginning with two fixed priors as well as a few two-person training examples, we learn a slim communication block, ComMDM, to coordinate interaction between the two resulting motions. Lastly, using model composition, we first train individual priors to complete motions that realize a prescribed motion for a given joint. We then introduce DiffusionBlending, an interpolation mechanism to effectively blend several such models to enable flexible and efficient fine-grained joint and trajectory-level control and editing. We evaluate the composition methods using an off-the-shelf motion diffusion model, and further compare the results to dedicated models trained for these specific tasks.\n\n| | |\n| --- | --- |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR) |\n| Cite as: | [arXiv:2303.01418](https://arxiv.org/abs/2303.01418) \\[cs.CV\\] |\n| | (or [arXiv:2303.01418v3](https://arxiv.org/abs/2303.01418v3) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2303.01418](https://doi.org/10.48550/arXiv.2303.01418) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Guy Tevet \\[ [view email](https://arxiv.org/show-email/a2d593de/2303.01418)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2303.01418v1)**\nThu, 2 Mar 2023 17:09:27 UTC (19,793 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2303.01418v2)**\nThu, 17 Aug 2023 00:24:41 UTC (19,761 KB)\n\n**\\[v3\\]**\nWed, 30 Aug 2023 04:41:10 UTC (1,801 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Human Motion Diffusion as a Generative Prior, by Yonatan Shafir and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2303.01418)\n- [TeX Source](https://arxiv.org/src/2303.01418)\n- [Other Formats](https://arxiv.org/format/2303.01418)\n\n[![license icon](https://arxiv.org/icons/licenses/by-sa-4.0.png)view license](http://creativecommons.org/licenses/by-sa/4.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2303.01418&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2303.01418&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2023-03](https://arxiv.org/list/cs.CV/2023-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2303.01418?context=cs)\n\n[cs.GR](https://arxiv.org/abs/2303.01418?context=cs.GR)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2303.01418)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2303.01418)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2303.01418)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2303.01418&description=Human Motion Diffusion as a Generative Prior) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2303.01418&title=Human Motion Diffusion as a Generative Prior)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2303.01418) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Human motion diffusion as a generative prior (priorMDM)",
          "cleaned_query": "Human motion diffusion as a generative prior (priorMDM)"
        },
        {
          "success": true,
          "title": "[2209.04066] TEACH: Temporal Action Composition for 3D Humans",
          "url": "https://arxiv.org/abs/2209.04066",
          "content": "[2209.04066] TEACH: Temporal Action Composition for 3D Humans\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2209.04066\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2209.04066**(cs)\n[Submitted on 9 Sep 2022 ([v1](https://arxiv.org/abs/2209.04066v1)), last revised 12 Sep 2022 (this version, v2)]\n# Title:TEACH: Temporal Action Composition for 3D Humans\nAuthors:[Nikos Athanasiou](https://arxiv.org/search/cs?searchtype=author&amp;query=Athanasiou,+N),[Mathis Petrovich](https://arxiv.org/search/cs?searchtype=author&amp;query=Petrovich,+M),[Michael J. Black](https://arxiv.org/search/cs?searchtype=author&amp;query=Black,+M+J),[G\u00fcl Varol](https://arxiv.org/search/cs?searchtype=author&amp;query=Varol,+G)\nView a PDF of the paper titled TEACH: Temporal Action Composition for 3D Humans, by Nikos Athanasiou and 3 other authors\n[View PDF](https://arxiv.org/pdf/2209.04066)> > Abstract:\n> Given a series of natural language descriptions, our task is to generate 3D human motions that correspond semantically to the text, and follow the temporal order of the instructions. In particular, our goal is to enable the synthesis of a series of actions, which we refer to as temporal action composition. The current state of the art in text-conditioned motion synthesis only takes a single action or a single sentence as input. This is partially due to lack of suitable training data containing action sequences, but also due to the computational complexity of their non-autoregressive model formulation, which does not scale well to long sequences. In this work, we address both issues. First, we exploit the recent BABEL motion-text collection, which has a wide range of labeled actions, many of which occur in a sequence with transitions between them. Next, we design a Transformer-based approach that operates non-autoregressively within an action, but autoregressively within the sequence of actions. This hierarchical formulation proves effective in our experiments when compared with multiple baselines. Our approach, called TEACH for &#34;TEmporal Action Compositions for Human motions&#34;, produces realistic human motions for a wide variety of actions and temporal compositions from language descriptions. To encourage work on this new task, we make our code available for research purposes at our $\\href{\n[> this http URL\n](http://teach.is.tue.mpg.de)> }{\\text{website}}$. Comments:|3DV 2022 Camera Ready, Affiliations corrected|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:2209.04066](https://arxiv.org/abs/2209.04066)[cs.CV]|\n|(or[arXiv:2209.04066v2](https://arxiv.org/abs/2209.04066v2)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2209.04066](https://doi.org/10.48550/arXiv.2209.04066)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Nikos Athanasiou [[view email](https://arxiv.org/show-email/98e1d5d5/2209.04066)]\n**[[v1]](https://arxiv.org/abs/2209.04066v1)**Fri, 9 Sep 2022 00:33:40 UTC (36,260 KB)\n**[v2]**Mon, 12 Sep 2022 16:34:20 UTC (665 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled TEACH: Temporal Action Composition for 3D Humans, by Nikos Athanasiou and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2209.04066)\n* [TeX Source](https://arxiv.org/src/2209.04066)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2209.04066&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2209.04066&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2022-09](https://arxiv.org/list/cs.CV/2022-09)\nChange to browse by:\n[cs](https://arxiv.org/abs/2209.04066?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2209.04066)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2209.04066)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2209.04066)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2209.04066)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "TEACH: Temporal action composition for 3d humans (BABEL-TEACH)",
          "cleaned_query": "TEACH: Temporal action composition for 3d humans (BABEL-TEACH)"
        },
        {
          "success": true,
          "title": "[2308.12751] Motion In-Betweening with Phase Manifolds - arXiv",
          "url": "https://arxiv.org/abs/2308.12751",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "Motion in-betweening with phase manifolds (PhaseBetweener / RSMT)",
          "cleaned_query": "Motion in-betweening with phase manifolds (PhaseBetweener"
        }
      ],
      "generated_ideas": [
        "Phase-Consistent Diffusion for Foot-Contact and Gait Stability\nBuild a text-conditioned diffusion model (MDM-style) that generates in motion space but is regularized by a DeepPhase/DiffusionPhase latent phase manifold. Add a phase-consistency loss that aligns predicted foot-contact events with phase progression (e.g., stance/swing regions), reducing foot skating and improving long-horizon stability without extra labels.",
        "Hierarchical \u201cAction-Phase\u201d Tokens for Temporal Action Composition\nExtend TEACH by inserting an intermediate representation: discrete action tokens plus continuous phase tokens learned via periodic autoencoders. Train a hierarchical transformer where the high level predicts action order and target phase trajectories, while the low level (diffusion or decoder) realizes motion that matches both semantics and phase evolution, yielding cleaner transitions between heterogeneous actions.",
        "Two-Person Interaction via Coupled Phase Manifolds\nGeneralize ComMDM by learning a *shared* interaction phase space: two periodic autoencoders whose latents are coupled through cross-attention and a mutual phase-locking objective. Use this coupled representation as the diffusion generation space so that synchronized behaviors (handshakes, dancing, passing a ball) emerge from phase alignment rather than requiring many paired examples.",
        "Phase-Manifold Motion In-Betweening with Diffusion Refinement\nCombine \u201cMotion In-Betweening with Phase Manifolds\u201d with a lightweight diffusion refiner: first interpolate in the phase manifold to produce a coarse in-between sequence, then run a short denoising schedule conditioned on endpoints and phase trajectory. This targets high-frequency detail recovery (hands/feet) while preserving phase-aligned timing, improving in-betweening quality under large time gaps.",
        "Frequency-Domain Long-Sequence Generation with Global Phase Drift Correction\nExtend DiffusionPhase to arbitrarily long motions by explicitly modeling and correcting cumulative phase drift across windows. Implement an overlap-add generation scheme where each new window predicts both Fourier coefficients and a phase-offset correction term, optimized to minimize discontinuities in joint velocities and contact phases at boundaries.",
        "Text-to-Motion Editing by \u201cPhase-Slot\u201d Replacement\nCreate an editing tool that localizes which body-part phase channels (DeepPhase multi-channel latents) correspond to a textual attribute (e.g., \u201cswing arms more,\u201d \u201cfaster steps\u201d). At inference, freeze non-target phase slots and resample only selected channels with a conditional diffusion model, enabling controllable edits that preserve identity, trajectory, and unaffected limbs.",
        "Unsupervised Style Discovery as Phase-Manifold Geometry\nLearn style factors by analyzing curvature/clustering structure in the learned phase manifold (DeepPhase) across large unstructured datasets, then use these discovered style codes as conditioning for diffusion generation. The key contribution is a label-free pipeline that produces style-controllable text-to-motion (or action-to-motion) by tying \u201cstyle\u201d to consistent phase-space deformations.",
        "Phase-Aware Retrieval-Augmented Diffusion for Data-Scarce Actions\nBuild a retrieval module that finds phase-aligned motion snippets from a database using DeepPhase distances, then condition a diffusion model on retrieved latent trajectories (instead of raw poses). This directly addresses limited text-to-motion data by grounding generation in nearest-neighbor phase exemplars while still allowing diffusion to diversify and smooth transitions.",
        "Physics-Ready Phase Latents for Sim-to-Real Character Control\nAdapt periodic autoencoder latents into control variables for physics-based controllers (e.g., tracking policies), where the policy predicts phase increments and residual pose offsets rather than absolute poses. Evaluate on diverse locomotion and skill transitions, testing whether phase-latent control improves robustness to perturbations and reduces tracking artifacts compared to pose-space diffusion outputs.",
        "Cross-Modal Phase Alignment Between Music and Motion Diffusion\nExtend dance generation by learning a shared frequency/phase representation between audio (beat/tempo spectra) and motion (DiffusionPhase latents). Train a diffusion model that conditions on audio-phase features plus text (optional) and enforces beat-synchronous phase constraints, producing longer dance sequences with consistent musical alignment and fewer timing jitters."
      ],
      "generated_ideas_raw": "1. **Phase-Consistent Diffusion for Foot-Contact and Gait Stability**\n   Build a text-conditioned diffusion model (MDM-style) that generates in motion space but is regularized by a DeepPhase/DiffusionPhase latent phase manifold. Add a phase-consistency loss that aligns predicted foot-contact events with phase progression (e.g., stance/swing regions), reducing foot skating and improving long-horizon stability without extra labels.\n\n2. **Hierarchical \u201cAction-Phase\u201d Tokens for Temporal Action Composition**\n   Extend TEACH by inserting an intermediate representation: discrete action tokens plus continuous phase tokens learned via periodic autoencoders. Train a hierarchical transformer where the high level predicts action order and target phase trajectories, while the low level (diffusion or decoder) realizes motion that matches both semantics and phase evolution, yielding cleaner transitions between heterogeneous actions.\n\n3. **Two-Person Interaction via Coupled Phase Manifolds**\n   Generalize ComMDM by learning a *shared* interaction phase space: two periodic autoencoders whose latents are coupled through cross-attention and a mutual phase-locking objective. Use this coupled representation as the diffusion generation space so that synchronized behaviors (handshakes, dancing, passing a ball) emerge from phase alignment rather than requiring many paired examples.\n\n4. **Phase-Manifold Motion In-Betweening with Diffusion Refinement**\n   Combine \u201cMotion In-Betweening with Phase Manifolds\u201d with a lightweight diffusion refiner: first interpolate in the phase manifold to produce a coarse in-between sequence, then run a short denoising schedule conditioned on endpoints and phase trajectory. This targets high-frequency detail recovery (hands/feet) while preserving phase-aligned timing, improving in-betweening quality under large time gaps.\n\n5. **Frequency-Domain Long-Sequence Generation with Global Phase Drift Correction**\n   Extend DiffusionPhase to arbitrarily long motions by explicitly modeling and correcting cumulative phase drift across windows. Implement an overlap-add generation scheme where each new window predicts both Fourier coefficients and a phase-offset correction term, optimized to minimize discontinuities in joint velocities and contact phases at boundaries.\n\n6. **Text-to-Motion Editing by \u201cPhase-Slot\u201d Replacement**\n   Create an editing tool that localizes which body-part phase channels (DeepPhase multi-channel latents) correspond to a textual attribute (e.g., \u201cswing arms more,\u201d \u201cfaster steps\u201d). At inference, freeze non-target phase slots and resample only selected channels with a conditional diffusion model, enabling controllable edits that preserve identity, trajectory, and unaffected limbs.\n\n7. **Unsupervised Style Discovery as Phase-Manifold Geometry**\n   Learn style factors by analyzing curvature/clustering structure in the learned phase manifold (DeepPhase) across large unstructured datasets, then use these discovered style codes as conditioning for diffusion generation. The key contribution is a label-free pipeline that produces style-controllable text-to-motion (or action-to-motion) by tying \u201cstyle\u201d to consistent phase-space deformations.\n\n8. **Phase-Aware Retrieval-Augmented Diffusion for Data-Scarce Actions**\n   Build a retrieval module that finds phase-aligned motion snippets from a database using DeepPhase distances, then condition a diffusion model on retrieved latent trajectories (instead of raw poses). This directly addresses limited text-to-motion data by grounding generation in nearest-neighbor phase exemplars while still allowing diffusion to diversify and smooth transitions.\n\n9. **Physics-Ready Phase Latents for Sim-to-Real Character Control**\n   Adapt periodic autoencoder latents into control variables for physics-based controllers (e.g., tracking policies), where the policy predicts phase increments and residual pose offsets rather than absolute poses. Evaluate on diverse locomotion and skill transitions, testing whether phase-latent control improves robustness to perturbations and reduces tracking artifacts compared to pose-space diffusion outputs.\n\n10. **Cross-Modal Phase Alignment Between Music and Motion Diffusion**\n   Extend dance generation by learning a shared frequency/phase representation between audio (beat/tempo spectra) and motion (DiffusionPhase latents). Train a diffusion model that conditions on audio-phase features plus text (optional) and enforces beat-synchronous phase constraints, producing longer dance sequences with consistent musical alignment and fewer timing jitters.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Phase-Consistent Diffusion for Foot-Contact and Gait Stability\nBuild a text-conditioned diffusion model (MDM-style) that generates in motion space but is regularized by a DeepPhase/DiffusionPhase late",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hierarchical \u201cAction-Phase\u201d Tokens for Temporal Action Composition\nExtend TEACH by inserting an intermediate representation: discrete action tokens plus continuous phase tokens learned via periodic au",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Two-Person Interaction via Coupled Phase Manifolds\nGeneralize ComMDM by learning a *shared* interaction phase space: two periodic autoencoders whose latents are coupled through cross-attention and a m",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Phase-Manifold Motion In-Betweening with Diffusion Refinement\nCombine \u201cMotion In-Betweening with Phase Manifolds\u201d with a lightweight diffusion refiner: first interpolate in the phase manifold to produ",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Frequency-Domain Long-Sequence Generation with Global Phase Drift Correction\nExtend DiffusionPhase to arbitrarily long motions by explicitly modeling and correcting cumulative phase drift across windo",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Text-to-Motion Editing by \u201cPhase-Slot\u201d Replacement\nCreate an editing tool that localizes which body-part phase channels (DeepPhase multi-channel latents) correspond to a textual attribute (e.g., \u201cswin",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Unsupervised Style Discovery as Phase-Manifold Geometry\nLearn style factors by analyzing curvature/clustering structure in the learned phase manifold (DeepPhase) across large unstructured datasets, th",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Phase-Aware Retrieval-Augmented Diffusion for Data-Scarce Actions\nBuild a retrieval module that finds phase-aligned motion snippets from a database using DeepPhase distances, then condition a diffusio",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Physics-Ready Phase Latents for Sim-to-Real Character Control\nAdapt periodic autoencoder latents into control variables for physics-based controllers (e.g., tracking policies), where the policy predic",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Cross-Modal Phase Alignment Between Music and Motion Diffusion\nExtend dance generation by learning a shared frequency/phase representation between audio (beat/tempo spectra) and motion (DiffusionPhase",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 2,
      "paper_title": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability",
      "contribution": "Introduce an exemplar-based global GNN explainer that selects representative nodes in embedding space via a coverage-maximization over reverse k-nearest neighbors and converts their neighborhoods into concise natural-language rules using an LLM self-refinement prompting strategy, yielding scalable, high-fidelity, and human-interpretable class-level explanations.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11204,
      "output_tokens": 1050,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Context theory of classification learning. - Semantic Scholar",
          "url": "https://www.semanticscholar.org/paper/Context-theory-of-classification-learning.-Medin-Schaffer/2cd7154ef2e19d4733bfac30f04ed708f01b42d1",
          "content": "On the Interaction of Prior Knowledge and Stimulus Structure in Category Learning Psychology The Quarterly journal of experimental psychology\u2026 1995 This work reports on the results of three studies of category learning using complex schematic drawings to show that the contrast set defined by one's initial encounters with category exemplars can be a source of individual differences in feature salience assignments and more robust prior hypotheses interact with category structure to either enhance learning or impede it. Concepts and Categorization E. Wisniewski Psychology, Mathematics 2002 This chapter describes research addressing the sufficiency of this approach, which suggests that similarity is a dynamic process involving the comparison of complex mental representations, and that category learning involves the integration of empirical learning with background knowledge. Features of Similarity A. Tversky Psychology 1977 The metric and dimensional assumptions that underlie the geometric representation of similarity are questioned on both theoretical and empirical grounds and a set of qualitative assumptions are shown to imply the contrast model, which expresses the similarity between objects as a linear combination of the measures of their common and distinctive features.",
          "original_query": "Context theory of classification learning",
          "cleaned_query": "Context theory of classification learning"
        },
        {
          "success": true,
          "title": "Inductive Representation Learning on Large Graphs - arXiv",
          "url": "https://arxiv.org/abs/1706.02216",
          "content": "[1706.02216] Inductive Representation Learning on Large Graphs\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1706.02216\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Social and Information Networks\n**arXiv:1706.02216**(cs)\n[Submitted on 7 Jun 2017 ([v1](https://arxiv.org/abs/1706.02216v1)), last revised 10 Sep 2018 (this version, v4)]\n# Title:Inductive Representation Learning on Large Graphs\nAuthors:[William L. Hamilton](https://arxiv.org/search/cs?searchtype=author&amp;query=Hamilton,+W+L),[Rex Ying](https://arxiv.org/search/cs?searchtype=author&amp;query=Ying,+R),[Jure Leskovec](https://arxiv.org/search/cs?searchtype=author&amp;query=Leskovec,+J)\nView a PDF of the paper titled Inductive Representation Learning on Large Graphs, by William L. Hamilton and 2 other authors\n[View PDF](https://arxiv.org/pdf/1706.02216)> > Abstract:\n> Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node&#39;s local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions. Comments:|Published in NIPS 2017; version with full appendix and minor corrections|\nSubjects:|Social and Information Networks (cs.SI); Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:1706.02216](https://arxiv.org/abs/1706.02216)[cs.SI]|\n|(or[arXiv:1706.02216v4](https://arxiv.org/abs/1706.02216v4)[cs.SI]for this version)|\n|[https://doi.org/10.48550/arXiv.1706.02216](https://doi.org/10.48550/arXiv.1706.02216)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: William L Hamilton [[view email](https://arxiv.org/show-email/b7f310ee/1706.02216)]\n**[[v1]](https://arxiv.org/abs/1706.02216v1)**Wed, 7 Jun 2017 14:51:05 UTC (906 KB)\n**[[v2]](https://arxiv.org/abs/1706.02216v2)**Wed, 8 Nov 2017 01:45:25 UTC (1,125 KB)\n**[[v3]](https://arxiv.org/abs/1706.02216v3)**Tue, 10 Apr 2018 15:40:00 UTC (1,125 KB)\n**[v4]**Mon, 10 Sep 2018 14:26:58 UTC (1,148 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Inductive Representation Learning on Large Graphs, by William L. Hamilton and 2 other authors\n* [View PDF](https://arxiv.org/pdf/1706.02216)\n* [TeX Source](https://arxiv.org/src/1706.02216)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.SI\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1706.02216&amp;function=prev&amp;context=cs.SI) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1706.02216&amp;function=next&amp;context=cs.SI)\n[new](https://arxiv.org/list/cs.SI/new)|[recent](https://arxiv.org/list/cs.SI/recent)|[2017-06](https://arxiv.org/list/cs.SI/2017-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1706.02216?context=cs)\n[cs.LG](https://arxiv.org/abs/1706.02216?context=cs.LG)\n[stat](https://arxiv.org/abs/1706.02216?context=stat)\n[stat.ML](https://arxiv.org/abs/1706.02216?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1706.02216)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1706.02216)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1706.02216)\n### [3 blog links](https://arxiv.org/tb/1706.02216)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1706.html#HamiltonYL17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/HamiltonYL17)\n[William L. Hamilton]()\n[Rex Ying]()\n[Jure Leskovec]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/lab",
          "original_query": "Inductive representation learning on large graphs (GraphSAGE)",
          "cleaned_query": "Inductive representation learning on large graphs (GraphSAGE)"
        },
        {
          "success": true,
          "title": "GNNExplainer: Generating Explanations for Graph Neural Networks",
          "url": "https://arxiv.org/abs/1903.03894",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1903.03894** (cs)\n\n\\[Submitted on 10 Mar 2019 ( [v1](https://arxiv.org/abs/1903.03894v1)), last revised 13 Nov 2019 (this version, v4)\\]\n\n# Title:GNNExplainer: Generating Explanations for Graph Neural Networks\n\nAuthors: [Rex Ying](https://arxiv.org/search/cs?searchtype=author&query=Ying,+R), [Dylan Bourgeois](https://arxiv.org/search/cs?searchtype=author&query=Bourgeois,+D), [Jiaxuan You](https://arxiv.org/search/cs?searchtype=author&query=You,+J), [Marinka Zitnik](https://arxiv.org/search/cs?searchtype=author&query=Zitnik,+M), [Jure Leskovec](https://arxiv.org/search/cs?searchtype=author&query=Leskovec,+J)\n\nView a PDF of the paper titled GNNExplainer: Generating Explanations for Graph Neural Networks, by Rex Ying and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/1903.03894)\n\n> Abstract:Graph Neural Networks (GNNs) are a powerful tool for machine learning on [this http URL](http://graphs.GNNs) combine node feature information with the graph structure by recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models, and explaining predictions made by GNNs remains unsolved. Here we propose GNNExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GNNExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction. Further, GNNExplainer can generate consistent and concise explanations for an entire class of instances. We formulate GNNExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms baselines by 17.1% on average. GNNExplainer provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1903.03894](https://arxiv.org/abs/1903.03894) \\[cs.LG\\] |\n| | (or [arXiv:1903.03894v4](https://arxiv.org/abs/1903.03894v4) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1903.03894](https://doi.org/10.48550/arXiv.1903.03894) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Rex Ying \\[ [view email](https://arxiv.org/show-email/d1801065/1903.03894)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1903.03894v1)**\nSun, 10 Mar 2019 00:56:26 UTC (3,349 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1903.03894v2)**\nThu, 12 Sep 2019 22:53:52 UTC (7,761 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/1903.03894v3)**\nFri, 8 Nov 2019 19:08:14 UTC (5,468 KB)\n\n**\\[v4\\]**\nWed, 13 Nov 2019 22:36:57 UTC (5,468 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled GNNExplainer: Generating Explanations for Graph Neural Networks, by Rex Ying and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/1903.03894)\n- [TeX Source](https://arxiv.org/src/1903.03894)\n- [Other Formats](https://arxiv.org/format/1903.03894)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1903.03894&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1903.03894&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2019-03](https://arxiv.org/list/cs.LG/2019-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1903.03894?context=cs)\n\n[stat](https://arxiv.org/abs/1903.03894?context=stat)\n\n[stat.ML](https://arxiv.org/abs/1903.03894?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1903.03894)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1903.03894)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1903.03894)\n\n### [1 blog link](https://arxiv.org/tb/1903.03894)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1903.html#abs-1903-03894) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1903-03894)\n\n[Rex Ying](https://dblp.uni-trier.de/search/author?author=Rex%20Ying)\n\n[Dylan Bourgeois](https://dblp.uni-trier.de/search/author?author=Dylan%20Bourgeois)\n\n[Jiaxuan You](https://dblp.uni-trier.de/search/author?author=Jiaxuan%20You)\n\n[Marinka Zitnik](https://dblp.uni-trier.de/search/author?author=Marinka%20Zitnik)\n\n[Jure Leskovec](https://dblp.uni-trier.de/search/author?author=Jure%20Leskovec)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1903.03894&description=GNNExplainer: Generating Explanations for Graph Neural Networks) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1903.03894&title=GNNExplainer: Generating Explanations for Graph Neural Networks)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs",
          "original_query": "GNNExplainer: Generating explanations for graph neural networks",
          "cleaned_query": "GNNExplainer: Generating explanations for graph neural networks"
        },
        {
          "success": true,
          "title": "GraphTrail: Translating GNN Predictions into Human-Interpretable...",
          "url": "https://openreview.net/forum?id=fzlMza6dRZ&referrer=%5Bthe%20profile%20of%20Sayan%20Ranu%5D(%2Fprofile%3Fid%3D~Sayan_Ranu2)",
          "content": "GraphTrail: Translating GNN Predictions into Human-Interpretable Logical Rules | OpenReview\n[![back arrow](https://openreview.net/images/arrow_left.svg)Back to**the profile of Sayan Ranu**](https://openreview.net/profile?id=~Sayan_Ranu2)\n## GraphTrail: Translating GNN Predictions into Human-Interpretable Logical Rules\n[![Download PDF](https://openreview.net/images/pdf_icon_blue.svg)](https://openreview.net/pdf?id=fzlMza6dRZ)\n### [Burouj Armgaan](https://openreview.net/profile?id=~Burouj_Armgaan1),[Manthan Dalmia](https://openreview.net/profile?id=~Manthan_Dalmia1),[Sourav Medya](https://openreview.net/profile?id=~Sourav_Medya1),[Sayan Ranu](https://openreview.net/profile?id=~Sayan_Ranu2)\nPublished: 25 Sept 2024, Last Modified: 06 Nov 2024NeurIPS 2024 posterEveryone[Revisions](https://openreview.net/revisions?id=fzlMza6dRZ)[BibTeX](#)[CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/)\n**Keywords:**Graph Neural Network, Explainability, Global Factual Explanation, Symbolic Regression, Computation Trees\n**TL;DR:**We generate formula based global explainations of graph neural networks using symbolic regression over computation trees identified through Shapley values.\n**Abstract:**Instance-level explanation of graph neural networks (GNNs) is a well-studied area. These explainers, however, only explain an instance (e.g., a graph) and fail to uncover the combinatorial reasoning learned by a GNN from the training data towards making its predictions. In this work, we introduce GraphTrail, the first end-to-end, global, post-hoc GNN explainer that translates the functioning of a black-box GNN model to a boolean formula over the (sub)graph level concepts without relying on local explainers. GraphTrail is unique in automatically mining the discriminative subgraph-level concepts using Shapley values. Subsequently, the GNN predictions are mapped to a human-interpretable boolean formula over these concepts through symbolic regression. Extensive experiments across diverse datasets and GNN architectures demonstrate significant improvement over existing global explainers in mapping GNN predictions to faithful logical formulae. The robust and accurate performance of GraphTrail makes it invaluable for improving GNNs and facilitates adoption in domains with strict transparency requirements.\n**Primary Area:**Graph neural networks\n**Submission Number:**12856\nLoading\n[OpenReview](https://openreview.net/about)is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the[OpenReview Sponsors](https://openreview.net/sponsors). \u00a92025OpenReview",
          "original_query": "Graphtrail: Translating gnn predictions into human-interpretable logical rules",
          "cleaned_query": "Graphtrail: Translating gnn predictions into human-interpretable logical rules"
        },
        {
          "success": true,
          "title": "Global Explainability of GNNs via Logic Combination of Learned ...",
          "url": "https://arxiv.org/abs/2210.07147",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2210.07147** (cs)\n\n\\[Submitted on 13 Oct 2022 ( [v1](https://arxiv.org/abs/2210.07147v1)), last revised 11 Apr 2023 (this version, v3)\\]\n\n# Title:Global Explainability of GNNs via Logic Combination of Learned Concepts\n\nAuthors: [Steve Azzolin](https://arxiv.org/search/cs?searchtype=author&query=Azzolin,+S), [Antonio Longa](https://arxiv.org/search/cs?searchtype=author&query=Longa,+A), [Pietro Barbiero](https://arxiv.org/search/cs?searchtype=author&query=Barbiero,+P), [Pietro Li\u00f2](https://arxiv.org/search/cs?searchtype=author&query=Li%C3%B2,+P), [Andrea Passerini](https://arxiv.org/search/cs?searchtype=author&query=Passerini,+A)\n\nView a PDF of the paper titled Global Explainability of GNNs via Logic Combination of Learned Concepts, by Steve Azzolin and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2210.07147)\n\n> Abstract:While instance-level explanation of GNN is a well-studied problem with plenty of approaches being developed, providing a global explanation for the behaviour of a GNN is much less explored, despite its potential in interpretability and debugging. Existing solutions either simply list local explanations for a given class, or generate a synthetic prototypical graph with maximal score for a given class, completely missing any combinatorial aspect that the GNN could have learned. In this work, we propose GLGExplainer (Global Logic-based GNN Explainer), the first Global Explainer capable of generating explanations as arbitrary Boolean combinations of learned graphical concepts. GLGExplainer is a fully differentiable architecture that takes local explanations as inputs and combines them into a logic formula over graphical concepts, represented as clusters of local explanations. Contrary to existing solutions, GLGExplainer provides accurate and human-interpretable global explanations that are perfectly aligned with ground-truth explanations (on synthetic data) or match existing domain knowledge (on real-world data). Extracted formulas are faithful to the model predictions, to the point of providing insights into some occasionally incorrect rules learned by the model, making GLGExplainer a promising diagnostic tool for learned GNNs.\n\n| | |\n| --- | --- |\n| Comments: | Camera ready version for ICLR2023 publication |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO) |\n| Cite as: | [arXiv:2210.07147](https://arxiv.org/abs/2210.07147) \\[cs.LG\\] |\n| | (or [arXiv:2210.07147v3](https://arxiv.org/abs/2210.07147v3) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2210.07147](https://doi.org/10.48550/arXiv.2210.07147) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Steve Azzolin \\[ [view email](https://arxiv.org/show-email/3b65eec6/2210.07147)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2210.07147v1)**\nThu, 13 Oct 2022 16:30:03 UTC (1,292 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2210.07147v2)**\nFri, 14 Oct 2022 16:47:12 UTC (1,292 KB)\n\n**\\[v3\\]**\nTue, 11 Apr 2023 18:15:20 UTC (1,463 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Global Explainability of GNNs via Logic Combination of Learned Concepts, by Steve Azzolin and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2210.07147)\n- [TeX Source](https://arxiv.org/src/2210.07147)\n- [Other Formats](https://arxiv.org/format/2210.07147)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2210.07147&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2210.07147&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2022-10](https://arxiv.org/list/cs.LG/2022-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2210.07147?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2210.07147?context=cs.AI)\n\n[cs.LO](https://arxiv.org/abs/2210.07147?context=cs.LO)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2210.07147)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2210.07147)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2210.07147)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2210.07147&description=Global Explainability of GNNs via Logic Combination of Learned Concepts) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2210.07147&title=Global Explainability of GNNs via Logic Combination of Learned Concepts)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2210.07147) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Global explainability of GNNs via logic combination of learned concepts (GLGExplainer)",
          "cleaned_query": "Global explainability of GNNs via logic combination of learned concepts (GLGExplainer)"
        },
        {
          "success": true,
          "title": "Self-Refine: Iterative Refinement with Self-Feedback",
          "url": "https://arxiv.org/abs/2303.17651",
          "content": "[2303.17651] Self-Refine: Iterative Refinement with Self-Feedback\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2303.17651\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2303.17651**(cs)\n[Submitted on 30 Mar 2023 ([v1](https://arxiv.org/abs/2303.17651v1)), last revised 25 May 2023 (this version, v2)]\n# Title:Self-Refine: Iterative Refinement with Self-Feedback\nAuthors:[Aman Madaan](https://arxiv.org/search/cs?searchtype=author&amp;query=Madaan,+A),[Niket Tandon](https://arxiv.org/search/cs?searchtype=author&amp;query=Tandon,+N),[Prakhar Gupta](https://arxiv.org/search/cs?searchtype=author&amp;query=Gupta,+P),[Skyler Hallinan](https://arxiv.org/search/cs?searchtype=author&amp;query=Hallinan,+S),[Luyu Gao](https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+L),[Sarah Wiegreffe](https://arxiv.org/search/cs?searchtype=author&amp;query=Wiegreffe,+S),[Uri Alon](https://arxiv.org/search/cs?searchtype=author&amp;query=Alon,+U),[Nouha Dziri](https://arxiv.org/search/cs?searchtype=author&amp;query=Dziri,+N),[Shrimai Prabhumoye](https://arxiv.org/search/cs?searchtype=author&amp;query=Prabhumoye,+S),[Yiming Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+Y),[Shashank Gupta](https://arxiv.org/search/cs?searchtype=author&amp;query=Gupta,+S),[Bodhisattwa Prasad Majumder](https://arxiv.org/search/cs?searchtype=author&amp;query=Majumder,+B+P),[Katherine Hermann](https://arxiv.org/search/cs?searchtype=author&amp;query=Hermann,+K),[Sean Welleck](https://arxiv.org/search/cs?searchtype=author&amp;query=Welleck,+S),[Amir Yazdanbakhsh](https://arxiv.org/search/cs?searchtype=author&amp;query=Yazdanbakhsh,+A),[Peter Clark](https://arxiv.org/search/cs?searchtype=author&amp;query=Clark,+P)\nView a PDF of the paper titled Self-Refine: Iterative Refinement with Self-Feedback, by Aman Madaan and 15 other authors\n[View PDF](https://arxiv.org/pdf/2303.17651)> > Abstract:\n> Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach. Comments:|Code, data, and demo at[this https URL](https://selfrefine.info/)|\nSubjects:|Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)|\nCite as:|[arXiv:2303.17651](https://arxiv.org/abs/2303.17651)[cs.CL]|\n|(or[arXiv:2303.17651v2](https://arxiv.org/abs/2303.17651v2)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2303.17651](https://doi.org/10.48550/arXiv.2303.17651)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Uri Alon [[view email](https://arxiv.org/show-email/21392df0/2303.17651)]\n**[[v1]](https://arxiv.org/abs/2303.17651v1)**Thu, 30 Mar 2023 18:30:01 UTC (15,993 KB)\n**[v2]**Thu, 25 May 2023 19:13:47 UTC (1,505 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Self-Refine: Iterative Refinement with Self-Feedback, by Aman Madaan and 15 other authors\n* [View PDF](https://arxiv.org/pdf/2303.17651)\n* [TeX Source](https://arxiv.org/src/2303.17651)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2303.17651&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2303.17651&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2023-03](https://arxiv.org/list/cs.CL/2023-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2303.17651?context=cs)\n[cs.AI](https://arxiv.org/abs/2303.17651?context=cs.AI)\n[cs.LG](https://arxiv.org/abs/2303.17651?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2303.17651)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2303.17651)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2303.17651)\n### [1 blog link](https://arxiv.org/tb/2303.17651)\n([what is this?](https://info.arxiv.org/help/trackback.html))\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nC",
          "original_query": "Self-refine: Iterative refinement with self-feedback (LLM prompting technique)",
          "cleaned_query": "Self-refine: Iterative refinement with self-feedback (LLM prompting technique)"
        }
      ],
      "generated_ideas": [
        "Context-Aware GraphSAGE via Contrast-Set Initialization\nBuild an inductive GNN that explicitly models the \u201ccontrast set\u201d effect from context theory by learning a per-node (or per-batch) salience vector conditioned on the first \\(k\\) encountered neighborhood samples during training. Implement this as a meta-network that reweights GraphSAGE aggregations based on early exemplar neighborhoods, then test whether it explains/induces systematic individual-difference-like variation in learned feature importance across random data orders.",
        "Tversky-Style Contrast Similarity as an Attention Mechanism for Message Passing\nReplace standard attention with a differentiable Tversky contrast module that scores neighbor contributions as a linear combination of \u201ccommon\u201d vs \u201cdistinctive\u201d feature sets between center and neighbor nodes (e.g., via learned masks over feature dimensions). Evaluate whether this improves inductive generalization on heterophilic graphs and whether extracted \u201cdistinctive-feature\u201d masks align with human-interpretable rationales.",
        "Self-Refining GNNExplanations with Mutual-Information Feedback Loops\nCreate a Self-Refine-inspired pipeline where an explainer proposes a subgraph+feature mask (like GNNExplainer), then a critique step checks faithfulness/stability (e.g., prediction invariance under perturbations; MI objective gaps), and iteratively refines the mask. The contribution is a test-time iterative explainer that converges to smaller, more stable explanations without retraining the base GNN.",
        "Global Logic Rules that Generalize Inductively to Unseen Nodes/Graphs\nExtend GraphTrail/GLGExplainer to the inductive regime by learning concepts and rule templates on training graphs, then instantiating them on entirely unseen graphs (GraphSAGE-style) with distribution shift. Concretely, learn concept detectors as neighborhood-aggregation prototypes and compile them into Boolean formulas; evaluate cross-graph transfer fidelity and rule stability under domain shift.",
        "Shapley-Guided Concept Mining for GraphSAGE Neighborhood Functions\nAdapt GraphTrail\u2019s Shapley-based computation-tree extraction to GraphSAGE\u2019s sampled neighborhood aggregators to identify which sampled hops/features drive predictions under inductive sampling noise. The key contribution is a concept-mining method that is robust to neighbor sampling variance, yielding global rules that remain consistent across different sampling seeds.",
        "Prior-Knowledge Injection into Global Rule Learning via Soft Logical Constraints\nIncorporate domain priors (from ontologies or expert-provided \u201cif substructure then class\u201d hints) as soft constraints during symbolic regression (GraphTrail) or differentiable logic composition (GLGExplainer). The actionable piece is a constrained rule-learning objective that trades off fidelity to the GNN with adherence to prior rules, enabling controlled debugging of \u201cincorrect rules learned by the model.\u201d",
        "Order-of-Exposure Experiments for GNNs: Measuring Feature-Salience Path Dependence\nDesign a systematic benchmark where training data order and early exemplar neighborhoods are manipulated (mirroring context theory experiments) and measure resulting shifts in GNN feature/subgraph salience via GNNExplainer/GraphTrail. The contribution is an empirical protocol plus metrics (salience divergence, rule drift) that quantify \u201clearning history effects\u201d in GNNs and connect them to explanation instability.",
        "Counterfactual Rule Editing: Use Global Logical Explanations to Patch GNN Behavior\nUse extracted global Boolean formulas (GraphTrail/GLGExplainer) as editable surrogates: propose minimal rule edits that would correct systematic errors (e.g., flip a conjunction to a disjunction), then translate edits back into targeted finetuning signals for the GNN (constraint loss on concept activations). This yields a concrete \u201cexplain-to-edit-to-improve\u201d loop where explanations are not just descriptive but operational for model repair.",
        "Multi-Objective Explanations Balancing Compactness, Stability, and Distinctiveness\nExtend the GNNExplainer MI formulation with explicit penalties inspired by contrast theory: encourage explanations that maximize distinctiveness w.r.t. competing classes (contrast set) while remaining compact and stable across perturbations. Implement as a three-term objective (MI + stability regularizer + contrastive margin against alternative-class predictions) and evaluate whether it reduces spurious subgraph selections.",
        "Human-in-the-Loop Self-Refinement of Graph Logic Explanations\nBuild an interactive system where an LLM applies Self-Refine to improve natural-language and logical-rule explanations produced by GraphTrail/GLGExplainer, using automated checks (faithfulness tests) plus optional expert feedback on concept semantics. The research contribution is a reproducible protocol that measures how iterative critique/refinement changes (i) rule fidelity to the GNN, (ii) alignment with domain knowledge, and (iii) human comprehension time/accuracy."
      ],
      "generated_ideas_raw": "1. **Context-Aware GraphSAGE via Contrast-Set Initialization**\n   Build an inductive GNN that explicitly models the \u201ccontrast set\u201d effect from context theory by learning a per-node (or per-batch) salience vector conditioned on the first \\(k\\) encountered neighborhood samples during training. Implement this as a meta-network that reweights GraphSAGE aggregations based on early exemplar neighborhoods, then test whether it explains/induces systematic individual-difference-like variation in learned feature importance across random data orders.\n\n2. **Tversky-Style Contrast Similarity as an Attention Mechanism for Message Passing**\n   Replace standard attention with a differentiable Tversky contrast module that scores neighbor contributions as a linear combination of \u201ccommon\u201d vs \u201cdistinctive\u201d feature sets between center and neighbor nodes (e.g., via learned masks over feature dimensions). Evaluate whether this improves inductive generalization on heterophilic graphs and whether extracted \u201cdistinctive-feature\u201d masks align with human-interpretable rationales.\n\n3. **Self-Refining GNNExplanations with Mutual-Information Feedback Loops**\n   Create a Self-Refine-inspired pipeline where an explainer proposes a subgraph+feature mask (like GNNExplainer), then a critique step checks faithfulness/stability (e.g., prediction invariance under perturbations; MI objective gaps), and iteratively refines the mask. The contribution is a test-time iterative explainer that converges to smaller, more stable explanations without retraining the base GNN.\n\n4. **Global Logic Rules that Generalize Inductively to Unseen Nodes/Graphs**\n   Extend GraphTrail/GLGExplainer to the inductive regime by learning concepts and rule templates on training graphs, then instantiating them on entirely unseen graphs (GraphSAGE-style) with distribution shift. Concretely, learn concept detectors as neighborhood-aggregation prototypes and compile them into Boolean formulas; evaluate cross-graph transfer fidelity and rule stability under domain shift.\n\n5. **Shapley-Guided Concept Mining for GraphSAGE Neighborhood Functions**\n   Adapt GraphTrail\u2019s Shapley-based computation-tree extraction to GraphSAGE\u2019s sampled neighborhood aggregators to identify which sampled hops/features drive predictions under inductive sampling noise. The key contribution is a concept-mining method that is robust to neighbor sampling variance, yielding global rules that remain consistent across different sampling seeds.\n\n6. **Prior-Knowledge Injection into Global Rule Learning via Soft Logical Constraints**\n   Incorporate domain priors (from ontologies or expert-provided \u201cif substructure then class\u201d hints) as soft constraints during symbolic regression (GraphTrail) or differentiable logic composition (GLGExplainer). The actionable piece is a constrained rule-learning objective that trades off fidelity to the GNN with adherence to prior rules, enabling controlled debugging of \u201cincorrect rules learned by the model.\u201d\n\n7. **Order-of-Exposure Experiments for GNNs: Measuring Feature-Salience Path Dependence**\n   Design a systematic benchmark where training data order and early exemplar neighborhoods are manipulated (mirroring context theory experiments) and measure resulting shifts in GNN feature/subgraph salience via GNNExplainer/GraphTrail. The contribution is an empirical protocol plus metrics (salience divergence, rule drift) that quantify \u201clearning history effects\u201d in GNNs and connect them to explanation instability.\n\n8. **Counterfactual Rule Editing: Use Global Logical Explanations to Patch GNN Behavior**\n   Use extracted global Boolean formulas (GraphTrail/GLGExplainer) as editable surrogates: propose minimal rule edits that would correct systematic errors (e.g., flip a conjunction to a disjunction), then translate edits back into targeted finetuning signals for the GNN (constraint loss on concept activations). This yields a concrete \u201cexplain-to-edit-to-improve\u201d loop where explanations are not just descriptive but operational for model repair.\n\n9. **Multi-Objective Explanations Balancing Compactness, Stability, and Distinctiveness**\n   Extend the GNNExplainer MI formulation with explicit penalties inspired by contrast theory: encourage explanations that maximize distinctiveness w.r.t. competing classes (contrast set) while remaining compact and stable across perturbations. Implement as a three-term objective (MI + stability regularizer + contrastive margin against alternative-class predictions) and evaluate whether it reduces spurious subgraph selections.\n\n10. **Human-in-the-Loop Self-Refinement of Graph Logic Explanations**\n   Build an interactive system where an LLM applies Self-Refine to improve natural-language and logical-rule explanations produced by GraphTrail/GLGExplainer, using automated checks (faithfulness tests) plus optional expert feedback on concept semantics. The research contribution is a reproducible protocol that measures how iterative critique/refinement changes (i) rule fidelity to the GNN, (ii) alignment with domain knowledge, and (iii) human comprehension time/accuracy.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Context-Aware GraphSAGE via Contrast-Set Initialization\nBuild an inductive GNN that explicitly models the \u201ccontrast set\u201d effect from context theory by learning a per-node (or per-batch) salience vecto",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Tversky-Style Contrast Similarity as an Attention Mechanism for Message Passing\nReplace standard attention with a differentiable Tversky contrast module that scores neighbor contributions as a linear ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Self-Refining GNNExplanations with Mutual-Information Feedback Loops\nCreate a Self-Refine-inspired pipeline where an explainer proposes a subgraph+feature mask (like GNNExplainer), then a critique ste",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Global Logic Rules that Generalize Inductively to Unseen Nodes/Graphs\nExtend GraphTrail/GLGExplainer to the inductive regime by learning concepts and rule templates on training graphs, then instantiat",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Shapley-Guided Concept Mining for GraphSAGE Neighborhood Functions\nAdapt GraphTrail\u2019s Shapley-based computation-tree extraction to GraphSAGE\u2019s sampled neighborhood aggregators to identify which sample",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Prior-Knowledge Injection into Global Rule Learning via Soft Logical Constraints\nIncorporate domain priors (from ontologies or expert-provided \u201cif substructure then class\u201d hints) as soft constraints d",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Order-of-Exposure Experiments for GNNs: Measuring Feature-Salience Path Dependence\nDesign a systematic benchmark where training data order and early exemplar neighborhoods are manipulated (mirroring c",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Counterfactual Rule Editing: Use Global Logical Explanations to Patch GNN Behavior\nUse extracted global Boolean formulas (GraphTrail/GLGExplainer) as editable surrogates: propose minimal rule edits th",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Multi-Objective Explanations Balancing Compactness, Stability, and Distinctiveness\nExtend the GNNExplainer MI formulation with explicit penalties inspired by contrast theory: encourage explanations th",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Human-in-the-Loop Self-Refinement of Graph Logic Explanations\nBuild an interactive system where an LLM applies Self-Refine to improve natural-language and logical-rule explanations produced by GraphTr",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 3,
      "paper_title": "RAG4GFM: Bridging Knowledge Gaps in Graph Foundation Models through Graph Retrieval Augmented Generation",
      "contribution": "Introduce RAG4GFM, an end-to-end retrieval-augmented generation framework that adapts the RAG paradigm to graph corpora via hierarchical multi-level graph indexing, task-aware retrieval, and graph-fusion enhancement to enable fast knowledge updating and more faithful reasoning for Graph Foundation Models.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 8452,
      "output_tokens": 952,
      "predecessor_details": [
        {
          "success": true,
          "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
          "url": "https://arxiv.org/html/2503.10677v1",
          "content": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
          "original_query": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
          "cleaned_query": "Retrieval-augmented generation for knowledge-intensive nlp tasks"
        },
        {
          "success": true,
          "title": "Inductive Representation Learning on Large Graphs - arXiv",
          "url": "https://arxiv.org/abs/1706.02216",
          "content": "[1706.02216] Inductive Representation Learning on Large Graphs\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1706.02216\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Social and Information Networks\n**arXiv:1706.02216**(cs)\n[Submitted on 7 Jun 2017 ([v1](https://arxiv.org/abs/1706.02216v1)), last revised 10 Sep 2018 (this version, v4)]\n# Title:Inductive Representation Learning on Large Graphs\nAuthors:[William L. Hamilton](https://arxiv.org/search/cs?searchtype=author&amp;query=Hamilton,+W+L),[Rex Ying](https://arxiv.org/search/cs?searchtype=author&amp;query=Ying,+R),[Jure Leskovec](https://arxiv.org/search/cs?searchtype=author&amp;query=Leskovec,+J)\nView a PDF of the paper titled Inductive Representation Learning on Large Graphs, by William L. Hamilton and 2 other authors\n[View PDF](https://arxiv.org/pdf/1706.02216)> > Abstract:\n> Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node&#39;s local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions. Comments:|Published in NIPS 2017; version with full appendix and minor corrections|\nSubjects:|Social and Information Networks (cs.SI); Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:1706.02216](https://arxiv.org/abs/1706.02216)[cs.SI]|\n|(or[arXiv:1706.02216v4](https://arxiv.org/abs/1706.02216v4)[cs.SI]for this version)|\n|[https://doi.org/10.48550/arXiv.1706.02216](https://doi.org/10.48550/arXiv.1706.02216)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: William L Hamilton [[view email](https://arxiv.org/show-email/b7f310ee/1706.02216)]\n**[[v1]](https://arxiv.org/abs/1706.02216v1)**Wed, 7 Jun 2017 14:51:05 UTC (906 KB)\n**[[v2]](https://arxiv.org/abs/1706.02216v2)**Wed, 8 Nov 2017 01:45:25 UTC (1,125 KB)\n**[[v3]](https://arxiv.org/abs/1706.02216v3)**Tue, 10 Apr 2018 15:40:00 UTC (1,125 KB)\n**[v4]**Mon, 10 Sep 2018 14:26:58 UTC (1,148 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Inductive Representation Learning on Large Graphs, by William L. Hamilton and 2 other authors\n* [View PDF](https://arxiv.org/pdf/1706.02216)\n* [TeX Source](https://arxiv.org/src/1706.02216)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.SI\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1706.02216&amp;function=prev&amp;context=cs.SI) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1706.02216&amp;function=next&amp;context=cs.SI)\n[new](https://arxiv.org/list/cs.SI/new)|[recent](https://arxiv.org/list/cs.SI/recent)|[2017-06](https://arxiv.org/list/cs.SI/2017-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1706.02216?context=cs)\n[cs.LG](https://arxiv.org/abs/1706.02216?context=cs.LG)\n[stat](https://arxiv.org/abs/1706.02216?context=stat)\n[stat.ML](https://arxiv.org/abs/1706.02216?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1706.02216)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1706.02216)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1706.02216)\n### [3 blog links](https://arxiv.org/tb/1706.02216)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1706.html#HamiltonYL17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/HamiltonYL17)\n[William L. Hamilton]()\n[Rex Ying]()\n[Jure Leskovec]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/lab",
          "original_query": "Inductive representation learning on large graphs (GraphSAGE)",
          "cleaned_query": "Inductive representation learning on large graphs (GraphSAGE)"
        },
        {
          "success": true,
          "title": "Towards Graph Foundation Models: A Survey and Beyond - arXiv",
          "url": "https://arxiv.org/html/2310.11829v3",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "Towards graph foundation models: A survey and beyond",
          "cleaned_query": "Towards graph foundation models: A survey and beyond"
        },
        {
          "success": true,
          "title": "Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete ...",
          "url": "https://arxiv.org/abs/2404.14741",
          "content": "[2404.14741] Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2404.14741\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2404.14741**(cs)\n[Submitted on 23 Apr 2024 ([v1](https://arxiv.org/abs/2404.14741v1)), last revised 6 Oct 2024 (this version, v3)]\n# Title:Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering\nAuthors:[Yao Xu](https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Y),[Shizhu He](https://arxiv.org/search/cs?searchtype=author&amp;query=He,+S),[Jiabei Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+J),[Zihao Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z),[Yangqiu Song](https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+Y),[Hanghang Tong](https://arxiv.org/search/cs?searchtype=author&amp;query=Tong,+H),[Guang Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+G),[Kang Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+K),[Jun Zhao](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+J)\nView a PDF of the paper titled Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering, by Yao Xu and 8 other authors\n[View PDF](https://arxiv.org/pdf/2404.14741)[HTML (experimental)](https://arxiv.org/html/2404.14741v3)> > Abstract:\n> To address the issues of insufficient knowledge and hallucination in Large Language Models (LLMs), numerous studies have explored integrating LLMs with Knowledge Graphs (KGs). However, these methods are typically evaluated on conventional Knowledge Graph Question Answering (KGQA) with complete KGs, where all factual triples required for each question are entirely covered by the given KG. In such cases, LLMs primarily act as an agent to find answer entities within the KG, rather than effectively integrating the internal knowledge of LLMs and external knowledge sources such as KGs. In fact, KGs are often incomplete to cover all the knowledge required to answer questions. To simulate these real-world scenarios and evaluate the ability of LLMs to integrate internal and external knowledge, we propose leveraging LLMs for QA under Incomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the factual triples for each question, and construct corresponding datasets. To handle IKGQA, we propose a training-free method called Generate-on-Graph (GoG), which can generate new factual triples while exploring KGs. Specifically, GoG performs reasoning through a Thinking-Searching-Generating framework, which treats LLM as both Agent and KG in IKGQA. Experimental results on two datasets demonstrate that our GoG outperforms all previous methods. Comments:|Accepted by EMNLP 2024 Main|\nSubjects:|Computation and Language (cs.CL); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2404.14741](https://arxiv.org/abs/2404.14741)[cs.CL]|\n|(or[arXiv:2404.14741v3](https://arxiv.org/abs/2404.14741v3)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2404.14741](https://doi.org/10.48550/arXiv.2404.14741)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Yao Xu [[view email](https://arxiv.org/show-email/034ff145/2404.14741)]\n**[[v1]](https://arxiv.org/abs/2404.14741v1)**Tue, 23 Apr 2024 04:47:22 UTC (786 KB)\n**[[v2]](https://arxiv.org/abs/2404.14741v2)**Thu, 3 Oct 2024 15:44:59 UTC (1,316 KB)\n**[v3]**Sun, 6 Oct 2024 10:55:23 UTC (1,316 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering, by Yao Xu and 8 other authors\n* [View PDF](https://arxiv.org/pdf/2404.14741)\n* [HTML (experimental)](https://arxiv.org/html/2404.14741v3)\n* [TeX Source](https://arxiv.org/src/2404.14741)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2404.14741&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2404.14741&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2024-04](https://arxiv.org/list/cs.CL/2024-04)\nChange to browse by:\n[cs](https://arxiv.org/abs/2404.14741?context=cs)\n[cs.AI](https://arxiv.org/abs/2404.14741?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2404.14741)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2404.14741)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2404.14741)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth i",
          "original_query": "Generate-on-graph: Treat llm as both agent and kg in incomplete knowledge graph question answering",
          "cleaned_query": "Generate-on-graph: Treat llm as both agent and kg in incomplete knowledge graph question answering"
        },
        {
          "success": true,
          "title": "GraphEdit: Large Language Models for Graph Structure Learning",
          "url": "https://arxiv.org/abs/2402.15183",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2402.15183** (cs)\n\n\\[Submitted on 23 Feb 2024 ( [v1](https://arxiv.org/abs/2402.15183v1)), last revised 10 Mar 2025 (this version, v5)\\]\n\n# Title:GraphEdit: Large Language Models for Graph Structure Learning\n\nAuthors: [Zirui Guo](https://arxiv.org/search/cs?searchtype=author&query=Guo,+Z), [Lianghao Xia](https://arxiv.org/search/cs?searchtype=author&query=Xia,+L), [Yanhua Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+Y), [Yuling Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+Y), [Kangkang Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu,+K), [Zhiyong Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+Z), [Chao Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+C)\n\nView a PDF of the paper titled GraphEdit: Large Language Models for Graph Structure Learning, by Zirui Guo and 6 other authors\n\n[View PDF](https://arxiv.org/pdf/2402.15183) [HTML (experimental)](https://arxiv.org/html/2402.15183v5)\n\n> Abstract:Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy connections but also identifies node-wise dependencies from a global perspective, providing a comprehensive understanding of the graph structure. We conduct extensive experiments on multiple benchmark datasets to demonstrate the effectiveness and robustness of GraphEdit across various settings. We have made our model implementation available at: [this https URL](https://github.com/HKUDS/GraphEdit).\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2402.15183](https://arxiv.org/abs/2402.15183) \\[cs.LG\\] |\n| (or [arXiv:2402.15183v5](https://arxiv.org/abs/2402.15183v5) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2402.15183](https://doi.org/10.48550/arXiv.2402.15183) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Zirui Guo \\[ [view email](https://arxiv.org/show-email/d08867fa/2402.15183)\\] **[\\[v1\\]](https://arxiv.org/abs/2402.15183v1)**\nFri, 23 Feb 2024 08:29:42 UTC (959 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2402.15183v2)**\nTue, 27 Feb 2024 08:22:11 UTC (959 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2402.15183v3)**\nThu, 29 Feb 2024 04:15:44 UTC (960 KB)\n**[\\[v4\\]](https://arxiv.org/abs/2402.15183v4)**\nTue, 5 Mar 2024 05:22:00 UTC (959 KB)\n**\\[v5\\]**\nMon, 10 Mar 2025 14:04:39 UTC (408 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled GraphEdit: Large Language Models for Graph Structure Learning, by Zirui Guo and 6 other authors\n\n- [View PDF](https://arxiv.org/pdf/2402.15183)\n- [HTML (experimental)](https://arxiv.org/html/2402.15183v5)\n- [TeX Source](https://arxiv.org/src/2402.15183)\n- [Other Formats](https://arxiv.org/format/2402.15183)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2402.15183&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2402.15183&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2024-02](https://arxiv.org/list/cs.LG/2024-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2402.15183?context=cs) [cs.AI](https://arxiv.org/abs/2402.15183?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2402.15183)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2402.15183)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2402.15183)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2402.15183) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Graphedit: Large language models for graph structure learning",
          "cleaned_query": "Graphedit: Large language models for graph structure learning"
        }
      ],
      "generated_ideas": [
        "GraphSAGE-Conditioned Retrieval for Knowledge-Oriented RAG\nTrain an inductive retriever that uses GraphSAGE embeddings over an entity/document graph to select evidence for RAG, enabling generalization to newly added nodes/documents without retraining the entire retriever. Compare against dense retrieval by measuring answer accuracy, retrieval recall, and latency under graph growth and node cold-start.",
        "Uncertainty-Gated \u201cGenerate-on-Graph\u201d for Verified Triple Expansion\nExtend GoG by having the LLM generate candidate missing triples *with calibrated uncertainty* (e.g., self-consistency + verbalized confidence), then gate insertion into the explored KG only when corroborated by retrieved text evidence (knowledge-oriented RAG) or structural plausibility scores. The contribution is a concrete pipeline that reduces hallucinated KG expansions while improving incomplete-KG QA.",
        "GraphEdit-Driven Retrieval Index Repair for RAG Systems\nUse GraphEdit to denoise and rewire the underlying retrieval graph (entities \u2194 passages \u2194 citations) that RAG relies on, treating retrieval failures as supervision signals for structure edits. Evaluate whether edited graphs improve multi-hop retrieval and downstream generation faithfulness compared to static indexing and standard re-ranking.",
        "Foundation-Model Pretraining on \u201cRAG Interaction Graphs\u201d\nBuild a graph foundation model pretraining dataset where nodes are queries, retrieved documents, generated claims, and KG entities, with edges representing retrieval, citation, entailment, and correction events. Pretrain a graph model (per graph foundation model survey directions) to predict which evidence paths lead to faithful answers, then plug it in as a controller for RAG and KGQA.",
        "Inductive IKGQA Benchmark with Evolving Graph Snapshots\nCreate an evaluation suite for incomplete-KG QA where the KG evolves over time (new entities/relations appear), requiring inductive generalization akin to GraphSAGE rather than transductive memorization. Provide standardized splits (time-based + unseen-entity) and metrics that separate (i) retrieval success, (ii) triple-generation correctness, and (iii) final answer faithfulness.",
        "Dual-Memory KGQA: Internal LLM Memory vs External KG Completion\nDesign a method that explicitly decomposes answers into parts supported by (a) existing KG paths, (b) retrieved text, and (c) the LLM\u2019s internal parametric knowledge, then uses GoG-style triple generation only for the unsupported remainder. The key contribution is an actionable attribution and routing mechanism that improves robustness on incomplete KGs while making \u201cwhat came from where\u201d measurable.",
        "Structure-Aware Negative Sampling for Graph-Augmented RAG Training\nLeverage GraphEdit to propose \u201chard negatives\u201d (plausible but incorrect edges/triples) and train a RAG model to resist them via contrastive objectives at both retrieval and generation stages. This directly targets hallucination and spurious correlations by teaching the system to discriminate near-miss evidence chains, not just irrelevant documents.",
        "Query-Time Graph Reasoning Planner for Multi-Hop Retrieval\nImplement an LLM planner that converts a natural-language question into a graph traversal policy over a heterogeneous retrieval+KG graph (entities, passages, relations), using sampled neighborhood aggregation signals (GraphSAGE-style) to choose next hops. The contribution is a concrete hybrid of symbolic planning and inductive graph embeddings that optimizes multi-hop evidence discovery under budget constraints.",
        "Faithfulness Certificates via Graph Provenance Subgraphs\nFor each generated answer, output a minimal provenance subgraph (documents + KG triples + generated triples) that is sufficient to entail the response, and train/validate it with automated entailment checks and ablations. This creates a standardized \u201ccertificate\u201d interface for knowledge-oriented RAG and IKGQA, enabling apples-to-apples faithfulness evaluation beyond final-answer accuracy.",
        "Active Learning Loop for KG Repair: Ask-to-Edit with GraphEdit + GoG\nBuild an active learning system that identifies high-impact uncertain regions of the KG (using graph model uncertainty) and prompts an LLM to either (i) propose edits (GraphEdit) or (ii) generate missing triples (GoG), then verifies via retrieval and human-in-the-loop only for the most uncertain cases. The key contribution is an end-to-end, cost-aware KG maintenance protocol that continuously improves downstream KGQA/RAG performance."
      ],
      "generated_ideas_raw": "1. **GraphSAGE-Conditioned Retrieval for Knowledge-Oriented RAG**\n   Train an inductive retriever that uses GraphSAGE embeddings over an entity/document graph to select evidence for RAG, enabling generalization to newly added nodes/documents without retraining the entire retriever. Compare against dense retrieval by measuring answer accuracy, retrieval recall, and latency under graph growth and node cold-start.\n\n2. **Uncertainty-Gated \u201cGenerate-on-Graph\u201d for Verified Triple Expansion**\n   Extend GoG by having the LLM generate candidate missing triples *with calibrated uncertainty* (e.g., self-consistency + verbalized confidence), then gate insertion into the explored KG only when corroborated by retrieved text evidence (knowledge-oriented RAG) or structural plausibility scores. The contribution is a concrete pipeline that reduces hallucinated KG expansions while improving incomplete-KG QA.\n\n3. **GraphEdit-Driven Retrieval Index Repair for RAG Systems**\n   Use GraphEdit to denoise and rewire the underlying retrieval graph (entities \u2194 passages \u2194 citations) that RAG relies on, treating retrieval failures as supervision signals for structure edits. Evaluate whether edited graphs improve multi-hop retrieval and downstream generation faithfulness compared to static indexing and standard re-ranking.\n\n4. **Foundation-Model Pretraining on \u201cRAG Interaction Graphs\u201d**\n   Build a graph foundation model pretraining dataset where nodes are queries, retrieved documents, generated claims, and KG entities, with edges representing retrieval, citation, entailment, and correction events. Pretrain a graph model (per graph foundation model survey directions) to predict which evidence paths lead to faithful answers, then plug it in as a controller for RAG and KGQA.\n\n5. **Inductive IKGQA Benchmark with Evolving Graph Snapshots**\n   Create an evaluation suite for incomplete-KG QA where the KG evolves over time (new entities/relations appear), requiring inductive generalization akin to GraphSAGE rather than transductive memorization. Provide standardized splits (time-based + unseen-entity) and metrics that separate (i) retrieval success, (ii) triple-generation correctness, and (iii) final answer faithfulness.\n\n6. **Dual-Memory KGQA: Internal LLM Memory vs External KG Completion**\n   Design a method that explicitly decomposes answers into parts supported by (a) existing KG paths, (b) retrieved text, and (c) the LLM\u2019s internal parametric knowledge, then uses GoG-style triple generation only for the unsupported remainder. The key contribution is an actionable attribution and routing mechanism that improves robustness on incomplete KGs while making \u201cwhat came from where\u201d measurable.\n\n7. **Structure-Aware Negative Sampling for Graph-Augmented RAG Training**\n   Leverage GraphEdit to propose \u201chard negatives\u201d (plausible but incorrect edges/triples) and train a RAG model to resist them via contrastive objectives at both retrieval and generation stages. This directly targets hallucination and spurious correlations by teaching the system to discriminate near-miss evidence chains, not just irrelevant documents.\n\n8. **Query-Time Graph Reasoning Planner for Multi-Hop Retrieval**\n   Implement an LLM planner that converts a natural-language question into a graph traversal policy over a heterogeneous retrieval+KG graph (entities, passages, relations), using sampled neighborhood aggregation signals (GraphSAGE-style) to choose next hops. The contribution is a concrete hybrid of symbolic planning and inductive graph embeddings that optimizes multi-hop evidence discovery under budget constraints.\n\n9. **Faithfulness Certificates via Graph Provenance Subgraphs**\n   For each generated answer, output a minimal provenance subgraph (documents + KG triples + generated triples) that is sufficient to entail the response, and train/validate it with automated entailment checks and ablations. This creates a standardized \u201ccertificate\u201d interface for knowledge-oriented RAG and IKGQA, enabling apples-to-apples faithfulness evaluation beyond final-answer accuracy.\n\n10. **Active Learning Loop for KG Repair: Ask-to-Edit with GraphEdit + GoG**\n   Build an active learning system that identifies high-impact uncertain regions of the KG (using graph model uncertainty) and prompts an LLM to either (i) propose edits (GraphEdit) or (ii) generate missing triples (GoG), then verifies via retrieval and human-in-the-loop only for the most uncertain cases. The key contribution is an end-to-end, cost-aware KG maintenance protocol that continuously improves downstream KGQA/RAG performance.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "GraphSAGE-Conditioned Retrieval for Knowledge-Oriented RAG\nTrain an inductive retriever that uses GraphSAGE embeddings over an entity/document graph to select evidence for RAG, enabling generalization",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Uncertainty-Gated \u201cGenerate-on-Graph\u201d for Verified Triple Expansion\nExtend GoG by having the LLM generate candidate missing triples *with calibrated uncertainty* (e.g., self-consistency + verbalized c",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "GraphEdit-Driven Retrieval Index Repair for RAG Systems\nUse GraphEdit to denoise and rewire the underlying retrieval graph (entities \u2194 passages \u2194 citations) that RAG relies on, treating retrieval fail",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Foundation-Model Pretraining on \u201cRAG Interaction Graphs\u201d\nBuild a graph foundation model pretraining dataset where nodes are queries, retrieved documents, generated claims, and KG entities, with edges ",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Inductive IKGQA Benchmark with Evolving Graph Snapshots\nCreate an evaluation suite for incomplete-KG QA where the KG evolves over time (new entities/relations appear), requiring inductive generalizati",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Dual-Memory KGQA: Internal LLM Memory vs External KG Completion\nDesign a method that explicitly decomposes answers into parts supported by (a) existing KG paths, (b) retrieved text, and (c) the LLM\u2019s ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Structure-Aware Negative Sampling for Graph-Augmented RAG Training\nLeverage GraphEdit to propose \u201chard negatives\u201d (plausible but incorrect edges/triples) and train a RAG model to resist them via contr",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Query-Time Graph Reasoning Planner for Multi-Hop Retrieval\nImplement an LLM planner that converts a natural-language question into a graph traversal policy over a heterogeneous retrieval+KG graph (ent",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Faithfulness Certificates via Graph Provenance Subgraphs\nFor each generated answer, output a minimal provenance subgraph (documents + KG triples + generated triples) that is sufficient to entail the r",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Active Learning Loop for KG Repair: Ask-to-Edit with GraphEdit + GoG\nBuild an active learning system that identifies high-impact uncertain regions of the KG (using graph model uncertainty) and prompts",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 4,
      "paper_title": "Agnostic Active Learning Is Always Better Than Passive Learning",
      "contribution": "A new agnostic active learning algorithm and analysis that give a sharp, instance-independent first-order query complexity for all concept classes whose leading term is always strictly smaller than passive sample complexity, eliminating disagreement-coefficient-type factors from the leading term.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 4,
      "crawl_rate": 1.0,
      "ideas_generated": 1,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 4844,
      "output_tokens": 939,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Agnostic Active Learning ? - CMU School of Computer Science",
          "url": "http://www.cs.cmu.edu/~ninamf/papers/a2.pdf",
          "content": "Agnostic Active Learning ?\nMaria-Florina Balcan\nCarnegie Mellon University, Pittsburgh, PA 15213\nAlina Beygelzimer\nIBM T. J. Watson Research Center, Hawthorne, NY 10532\nJohn Langford\nYahoo! Research, New York, NY 10011\nAbstract\nWe state and analyze the first active learning algorithm that finds an \u000f-optimal\nhypothesis in any hypothesis class, when the underlying distribution has arbitrary\nforms of noise. The algorithm, A2(for Agnostic Active), relies only upon the as\u0002sumption that it has access to a stream of unlabeled examples drawn i.i.d. from\na fixed distribution. We show that A2 achieves an exponential improvement (i.e.,\nrequires only O\n\nln 1\n\u000f\n\u0001\nsamples to find an \u000f-optimal classifier) over the usual sample\ncomplexity of supervised learning, for several settings considered before in the real\u0002izable case. These include learning threshold classifiers and learning homogeneous\nlinear separators with respect to an input distribution which is uniform over the\nunit sphere.\nKey words: Active Learning, Agnostic Setting, Sample Complexity, Linear\nSeparators.\n1 Introduction\nTraditionally, machine learning has focused on the problem of learning a\ntask from labeled examples only. In many applications, however, labeling\n? A preliminary version of this paper appeared in the 23rd International Conference\non Machine Learning, ICML 2006.\nEmail addresses: ninamf@cs.cmu.edu (Maria-Florina Balcan),\nbeygel@us.ibm.com (Alina Beygelzimer), jl@yahoo-inc.com (John Langford).\nPreprint submitted to Elsevier Science 13 June 2008\nis expensive while unlabeled data is usually ample. This observation moti\u0002vated substantial work on properly using unlabeled data to benefit learning\n[4,10,11,30,34,28,33], and there are many examples showing that unlabeled\ndata can significantly help [9,32].\nThere are two main frameworks for incorporating unlabeled data into the\nlearning process. The first framework is semi-supervised learning [18], where\nin addition to a set of labeled examples, the learning algorithm can also use\na (usually larger) set of unlabeled examples drawn at random from the same\nunderlying data distribution. In this setting, unlabeled data becomes useful\nunder additional assumptions and beliefs about the learning problem. For\nexample, transductive SVM learning [28] assumes that the target function cuts\nthrough low density regions of the space, while co-training [11] assumes that\nthe target should be self-consistent in some way. Unlabeled data is potentially\nuseful in this setting because it allows one to reduce the search space to a set\nwhich is a-priori reasonable with respect to the underlying distribution.\nThe second setting, which is the main focus of this paper, is active learn\u0002ing [19,22]. Here the learning algorithm is allowed to draw random unlabeled\nexamples from the underlying distribution and ask for the labels of any of these\nexamples. The hope is that a good classifier can be learned with significantly\nfewer labels by actively directing the queries to informative examples.\nAs in passive supervised learning, but unlike in semi-supervised learning, the\nonly prior belief about the learning problem here is that the target function\n(or a good approximation of it) belongs to a given concept class. For some\nconcept classes such as thresholds on the line, one can achieve an exponential\nimprovement over the usual sample complexity of supervised learning, under\nno additional assumptions about the learning problem [19,22]. In general, the\nspeedups achievable in active learning depend on the match between the data\ndistribution and the hypothesis class, and therefore on the target hypothesis\nin the class. The most noteworthy non-trivial example of improvement is the\ncase of homogeneous (i.e., through the origin) linear separators, when the data\nis linearly separable and distributed uniformly over the unit sphere [25,24,22].\nThere are also simple examples where active learning does not help at all, even\nin the realizable case [22].\nMost of the previous work on active learning has focused on the realizable\ncase. In fact, many of the existing active learning strategies are noise seeking\non natural learning problems, because the process of actively finding an opti\u0002mal separation between one class and another often involves label queries for\nexamples close to the decision boundary, and such examples often used a large\nconditional noise rate (e.g., due to a mismatch between the hypothesis class\nand the data distribution). Thus the most informative examples are also the\nones that are typically the most noise-prone.\n2\nConsider an active learning algorithm which searches for the optimal threshold\non an interval using binary search. This example is often used to demonstrate\nthe potential of active learning in the noise-free case when there is a perfect\nthreshold separating the classes [19]. Binary search needs O(ln 1\n\u000f\n) labeled ex\u0002amples to learn a threshold with error less than \u000f, while learning passively\nrequires O\n\u0010\n1\n\u000f\n\u0011\nlabels. A fundamental drawback of this algorithm is that a\nsmall amount of adversarial noise can force the algorithm to behave badly. Is\nthis extreme brittleness to small amounts of noise essential? Can an exponen\u0002tial decrease in sample complexity be achieved? Can assumptions about the\nmechanism producing noise be avoided? These are the questions addressed\nhere.\nPrevious Work on Active Learning There has been substantial work\non active learning under additional assumptions. For example, the Query by\nCommittee analysis [25] assumes realizability (i.e., existence of a perfect clas\u0002sifier in a known set), and a correct Bayesian prior on the set of hypotheses.\nDasgupta [22] has identified sufficient conditions (which are also necessary\nagainst an adversarially chosen distribution) for active learning given only\nthe additional realizability assumption. There are several other papers that\nassume only realizability [21,24]. If there exists a perfect separator amongst\nhypotheses, any informative querying strategy can direct the learning process\nwithout the need to worry about the distribution it induces\u2014any inconsis\u0002tent hypothesis can be eliminated based on a single query, regardless of which\ndistribution this query comes from. In the agnostic case, however, a hypoth\u0002esis that performs badly on the query distribution may well be the optimal\nhypothesis with respect to the input distribution. This is the main challenge\nin agnostic active learning that is not present in the non-agnostic case. Bur\u0002nashev and Zigangirov [15] allow noise, but require a correct Bayesian prior\non threshold functions. Some papers require specific noise models such as a\nconstant noise rate everywhere [17] or Tsybakov noise conditions [5,16].\nThe membership-query setting [1,2,14,27] is similar to active learning con\u0002sidered here, except that no unlabeled data is given. Instead, the learning\nalgorithm is allowed to query examples of its own choice. This is problematic\nin several applications because natural oracles, such as hired humans, have dif\u0002ficulty labeling synthetic examples [8]. Ulam\u2019s Problem (quoted in [20]), where\nthe goal is find a distinguished element in a set by asking subset membership\nqueries, is also related. The quantity of interest is the smallest number of such\nqueries required to find the element, given a bound on the number of queries\nthat can be answered incorrectly. But both types of results do not apply here\nsince an active learning strategy can only buy labels of the examples it ob\u0002serves. For example, a membership query algorithm can be used to quickly\nfind a separating hyperplane in a high-dimensional space. An active learning\nalgorithm can not do so when the data distribution does not support queries\nclose to the decision boundary.\n3\nOur Contributions This paper presents the first agnostic active learning\nalgorithm, A2. The only necessary assumption is that the algorithm has ac\u0002cess to a stream of examples drawn i.i.d. from some fixed distribution. No\nadditional ",
          "original_query": "A2: Agnostic Active Learning (Balcan, Beygelzimer, and Langford; 2005/2006/2009)",
          "cleaned_query": "A2: Agnostic Active Learning",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "The Label Complexity of Active Learning from Observational Data",
          "url": "https://arxiv.org/abs/1905.12791",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: Counterfactual learning from observational data involves learning a\nclassifier on an entire population based on data that is observed conditioned\non a selection policy. This work considers this problem in an active setting,\nwhere the learner additionally has access to unlabeled examples and can choose\nto get a subset of these labeled by an oracle.\n Prior work on this problem uses disagreement-based active learning, along\nwith an importance weighted loss estimator to account for counterfactuals,\nwhich leads to a high label complexity. We show how to instead incorporate a\nmore efficient counterfactual risk minimizer into the active learning\nalgorithm. This requires us to modify both the counterfactual risk to make it\namenable to active learning, as well as the active learning process to make it\namenable to the risk. We provably demonstrate that the result of this is an\nalgorithm which is statistically consistent as well as more label-efficient\nthan prior work.\n \n \n \n \n Submission history From: Songbai Yan [ view email]\n \n [v1] \n Wed, 29 May 2019 23:48:16 UTC (30 KB) [v2] \nMon, 28 Oct 2019 03:03:17 UTC (97 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > stat > arXiv:1905.12791\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Statistics > Machine Learning\n\n arXiv:1905.12791 (stat)\n [Submitted on 29 May 2019 (v1), last revised 28 Oct 2019 (this version, v2)]\n\n Title: The Label Complexity of Active Learning from Observational Data\n\n Authors: Songbai Yan, Kamalika Chaudhuri, Tara Javidi\n Download PDF\n Abstract: Counterfactual learning from observational data involves learning a classifier on an entire population based on data that is observed conditioned on a selection policy. This work considers this problem in an active setting, where the learner additionally has access to unlabeled examples and can choose to get a subset of these labeled by an oracle.\n Prior work on this problem uses disagreement-based active learning, along with an importance weighted loss estimator to account for counterfactuals, which leads to a high label complexity. We show how to instead incorporate a more efficient counterfactual risk minimizer into the active learning algorithm. This requires us to modify both the counterfactual risk to make it amenable to active learning, as well as the active learning process to make it amenable to the risk. We provably demonstrate that the result of this is an algorithm which is statistically consistent as well as more label-efficient than prior work.\n Comments: NeurIPS 2019 \n Subjects: Machine Learning (stat.ML) ; Machine Learning (cs.LG)\n Cite as: arXiv:1905.12791 [stat.ML] \n (or arXiv:1905.12791v2 [stat.ML] for this version) \n https://doi.org/10.48550/arXiv.1905.12791 \n Focus to learn more \n arXiv-issued DOI via DataCite \n \n\n Submission history\n\n From: Songbai Yan [view email]\n [v1] Wed, 29 May 2019 23:48:16 UTC (30 KB)\n [v2] Mon, 28 Oct 2019 03:03:17 UTC (97 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * Other formats\n (license)\n Current browse context:\n stat.ML\n < prev | next >\n new | recent | 1905\n Change to browse by:\n cs\n cs.LG\n stat\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs and how to get involved.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Label Complexity of Active Learning (Hanneke; 2007b)",
          "cleaned_query": "Label Complexity of Active Learning"
        },
        {
          "success": true,
          "title": "Active Learning in the Non-realizable Case - Springer Link",
          "url": "https://link.springer.com/chapter/10.1007/11894841_9",
          "content": "\n \n \n Abstract Most of the existing active learning algorithms are based on the realizability assumption: The learner\u2019s hypothesis class is assumed to contain a target function that perfectly classifies all training and test examples. This assumption can hardly ever be justified in practice. In this paper, we study how relaxing the realizability assumption affects the sample complexity of active learning. First, we extend existing results on query learning to show that any active learning algorithm for the realizable case can be transformed to tolerate random bounded rate class noise. Thus, bounded rate class noise adds little extra complications to active learning, and in particular exponential label complexity savings over passive learning are still possible. However, it is questionable whether this noise model is any more realistic in practice than assuming no noise at all. Our second result shows that if we move to the truly non-realizable model of statistical learning theory, then the label complexity of active learning has the same dependence \u03a9(1/ \u03b5 \n 2) on the accuracy parameter \u03b5 as the passive learning label complexity. More specifically, we show that under the assumption that the best classifier in the learner\u2019s hypothesis class has generalization error at most \u03b2 &gt;0, the label complexity of active learning is \u03a9( \u03b2 \n 2 / \u03b5 \n 2 log(1/ \u03b4)), where the accuracy parameter \u03b5 measures how close to optimal within the hypothesis class the active learner has to get and \u03b4 is the confidence parameter. The implication of this lower bound is that exponential savings should not be expected in realistic models of active learning, and thus the label complexity goals in active learning should be refined. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Preview \n \n Unable to display preview.\u00a0 Download preview PDF. \n \n \n \n \n \n \n \n References Freund, Y., Seung, H.S., Shamir, E., Tishby, N.: Selective sampling using the query by committee algorithm. Machine Learning\u00a028(2-3), 133\u2013168 (1997) CrossRef \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Dasgupta, S., Kalai, A.T., Monteleoni, C.: Analysis of perceptron-based active learning. In: Auer, P., Meir, R. (eds.) COLT 2005. LNCS, vol.\u00a03559, pp. 249\u2013263. Springer, Heidelberg (2005) CrossRef \u00a0\n \n Google Scholar \u00a0\n Dasgupta, S.: Coarse sample complexity bounds for active learning. In: NIPS 2005 (2005) \n Google Scholar \u00a0\n Balcan, N., Beygelzimer, A., Langford, J.: Agnostic active learning. In: ICML (accepted, 2006) \n Google Scholar \u00a0\n Tong, S., Koller, D.: Support vector machine active learning with applications to text classification. Journal of Machine Learning Research\u00a02, 45\u201366 (2002) CrossRef \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Angluin, D., Laird, P.: Learning from noisy examples. Machine Learning\u00a02(4), 343\u2013370 (1987) \n Google Scholar \u00a0\n Sakakibara, Y.: On learning from queries and counterexamples in the presence of noise. Information Processing Letters\u00a037(5), 279\u2013284 (1991) CrossRef \u00a0\n MATH \u00a0\n MathSciNet \u00a0\n \n Google Scholar \u00a0\n Vapnik, V.N.: Estimation of Dependencies Based on Empirical Data. Springer, Heidelberg (1982) \n Google Scholar \u00a0\n Gentile, C., Helmbold, D.P.: Improved lower bounds for learning from noisy examples: an information-theoretic approach. In: COLT 1998, pp. 104\u2013115. ACM Press, New York (1998) CrossRef \u00a0\n \n Google Scholar \u00a0\n Domingo, C., Gavald\u00e1, R., Watanabe, O.: Adaptive sampling methods for scaling up knowledge discovery algorithms. In: Arikawa, S., Furukawa, K. (eds.) DS 1999. LNCS, vol.\u00a01721, pp. 172\u2013183. Springer, Heidelberg (1999) CrossRef \u00a0\n \n Google Scholar \u00a0\n Castro, R.: Personal communication (March 2006) \n Google Scholar \u00a0\n Silvey, S.D.: Optimal Design. Chapman and Hall, London (1980) MATH \u00a0\n \n Google Scholar \u00a0\n Elfving, G.: Selection of nonrepeatable observations for estimation. In: Proceedings of the 3rd Berkeley Symposium on Mathematical Statistics and Probability, vol.\u00a01, pp. 69\u201375 (1956) \n Google Scholar \u00a0\n Canetti, R., Even, G., Goldreich, O.: Lower bounds for sampling algorithms for estimating the average. Information Processing Letters\u00a053(1), 17\u201325 (1995) CrossRef \u00a0\n MATH \u00a0\n MathSciNet \u00a0\n \n Google Scholar \u00a0\n Download references Author information Authors and Affiliations Department of Computer Science, University of Helsinki, \u00a0 Matti K\u00e4\u00e4ri\u00e4inen Authors Matti K\u00e4\u00e4ri\u00e4inen You can also search for this author in\n PubMed \u00a0 Google Scholar Editor information Editors and Affiliations Departament de Llenguatges i Sistemes Inform\u00e0tics Laboratori d\u2019Algor\u00edsmica Relacional, Complexitat i Aprenentatge, Universitat Polit\u00e8cnica de Catalunya, Barcelona, \u00a0 Jos\u00e9 L. Balc\u00e1zar Google, 1600 Amphitheatre Parkway, 94043, Mountain View, CA, USA Philip M. Long Department of Computer Science and Department of Mathematics, National University of Singapore, 117543, Singapore, Republic of Singapore Frank Stephan Rights and permissions Copyright information \u00a9 2006 Springer-Verlag Berlin Heidelberg About this paper Cite this paper K\u00e4\u00e4ri\u00e4inen, M. (2006). Active Learning in the Non-realizable Case.\n In: Balc\u00e1zar, J.L., Long, P.M., Stephan, F. (eds) Algorithmic Learning Theory. ALT 2006. Lecture Notes in Computer Science(), vol 4264. Springer, Berlin, Heidelberg. https://doi.org/10.1007/11894841_9 Download citation.RIS.ENW.BIB DOI: https://doi.org/10.1007/11894841_9 \n Publisher Name: Springer, Berlin, Heidelberg \n Print ISBN: 978-3-540-46649-9 \n Online ISBN: 978-3-540-46650-5 eBook Packages: Computer Science Computer Science (R0) \n \n \n \n ||||I|||| Skip to main content\n\n Advertisement\n\n Search\n Go to cart\n * Log in\n Search SpringerLink\n Search\n\n International Conference on Algorithmic Learning Theory\n\n ALT 2006: Algorithmic Learning Theory pp 63\u201377Cite as\n\n Active Learning in the Non-realizable Case\n\n * Matti K\u00e4\u00e4ri\u00e4inen21\n * Conference paper\n\n * 826 Accesses\n\n * 26 Citations\n\n Part of the Lecture Notes in Computer Science book series (LNAI,volume 4264)\n\n Abstract\n\n Most of the existing active learning algorithms are based on the realizability assumption: The learner\u2019s hypothesis class is assumed to contain a target function that perfectly classifies all training and test examples. This assumption can hardly ever be justified in practice. In this paper, we study how relaxing the realizability assumption affects the sample complexity of active learning. First, we extend existing results on query learning to show that any active learning algorithm for the realizable case can be transformed to tolerate random bounded rate class noise. Thus, bounded rate class noise adds little extra complications to active learning, and in particular exponential label complexity savings over passive learning are still possible. However, it is questionable whether this noise model is any more realistic in practice than assuming no noise at all.\n\n Our second result shows that if we move to the truly non-realizable model of statistical learning theory, then the label complexity of active learning has the same dependence \u03a9(1/\u03b5 2) on the accuracy parameter \u03b5 as the passive learning label complexity. More specifically, we show that under the assumption that the best classifier in the learner\u2019s hypothesis class has generalization error at most \u03b2>0, the label complexity of active learning is \u03a9(\u03b2 2/\u03b5 2log(1/\u03b4)), where the accuracy parameter \u03b5 measures how close to optimal within the hypothesis class the active learner has to get and \u03b4 is the confidence parameter. The implication of this lower bound is that exponential savings should not be expected in realistic models of active learning, and thus the label complexity goals in active learning should be refined.\n\n This is a preview of subscription content, access via your institution.\n\n Buying options\n\n Chapter\n USD 29.95\n Price excludes VAT (USA)\n * DOI: 10.1007/11894841_9\n * Chapter length: 15 pages\n * Instant PDF download\n * Readable on all devices\n * Own it forever\n * Exclusive offer for individuals only\n * Tax calculation will be finalised during checkout\n Buy Chapter\n eBook USD 84.99 Price excludes VAT (USA)\n * ISBN: 9",
          "original_query": "Active Learning in the Non-Realizable Case / Lower Bounds (K\u00e4\u00e4ri\u00e4inen; 2005/2006)",
          "cleaned_query": "Active Learning in the Non-Realizable Case"
        },
        {
          "success": true,
          "title": "[0812.4952] Importance Weighted Active Learning - arXiv",
          "url": "https://arxiv.org/abs/0812.4952",
          "content": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
          "original_query": "Importance-Weighted Active Learning and Strengthened Lower Bounds (Beygelzimer, Dasgupta, and Langford; 2009)",
          "cleaned_query": "Importance-Weighted Active Learning and Strengthened Lower Bounds"
        },
        {
          "success": true,
          "title": "Coarse sample complexity bounds for active learning",
          "url": "https://papers.nips.cc/paper/2943-coarse-sample-complexity-bounds-for-active-learning",
          "content": "Coarse sample complexity bounds for active learning\n#### Coarse sample complexity bounds for active learning\nPart of[Advances in Neural Information Processing Systems 18 (NIPS 2005)](https://papers.nips.cc/paper_files/paper/2005)\n[Bibtex](https://papers.nips.cc/paper_files/paper/2005/file/6e82873a32b95af115de1c414a1849cb-Bibtex.bib)[Metadata](https://papers.nips.cc/paper_files/paper/2005/file/6e82873a32b95af115de1c414a1849cb-Metadata.json)[Paper](https://papers.nips.cc/paper_files/paper/2005/file/6e82873a32b95af115de1c414a1849cb-Paper.pdf)\n#### Authors\n*Sanjoy Dasgupta*\n#### Abstract\nWe characterize the sample complexity of active learning problems in terms of a parameter which takes into account the distribution over the input space, the specific target hypothesis, and the desired accuracy.\n#### Name Change Policy\n&times;\nRequests for name changes in the electronic proceedings will be accepted with no questions asked. However name changes may cause bibliographic tracking issues. Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings.\nUse the \"Report an Issue\" link to request a name change.\nDo not remove: This comment is monitored to verify that the site is working properly",
          "original_query": "Coarse Sample Complexity Bounds for Active Learning (Dasgupta; 2005)",
          "cleaned_query": "Coarse Sample Complexity Bounds for Active Learning"
        }
      ],
      "generated_ideas": [
        "Active Learning with Unknown Selection Policy: Joint Propensity Estimation and Querying\nDevelop an algorithm for observational active learning when propensities are unknown or partially observed by jointly estimating the selection policy and learning the classifier. Use active queries to reduce propensity uncertainty in regions that dominate counterfactual risk, and prove consistency plus label complexity bounds under mild identifiability/overlap conditions."
      ],
      "generated_ideas_raw": "1. **A2 with Counterfactual Risk for Biased Observational Data**\n   Develop an agnostic active learning algorithm that replaces A2\u2019s empirical error tests with the modified counterfactual risk minimizer from observational-data active learning (Paper 2), enabling selective querying under selection bias. Prove label-complexity bounds in terms of disagreement coefficient (Paper 5) and propensity overlap, and identify regimes where it improves over importance-weighted disagreement-based baselines.\n\n2. **Variance-Controlled Importance-Weighted Active Learning via Adaptive Clipping**\n   Design an importance-weighted active learner (Paper 4) that adaptively clips/normalizes weights based on online variance estimates to prevent label complexity blowups caused by rare actions/low propensities (Paper 2). Provide finite-sample guarantees trading bias and variance, and derive an optimal clipping schedule that minimizes excess risk under agnostic noise (Paper 1).\n\n3. **Noise-Aware Query Policies that Avoid Boundary-Seeking in Agnostic Settings**\n   Create an active learning strategy that explicitly models increased conditional noise near decision boundaries (motivated by Paper 1\u2019s discussion) and penalizes querying in high-noise regions unless uncertainty is sufficiently informative. Analyze its label complexity under Tsybakov-style noise/margin conditions and compare to standard disagreement-based querying (Paper 5) that can be \u201cnoise-seeking.\u201d\n\n4. **Instance-Dependent Disagreement Coefficients for Observational Active Learning**\n   Extend Dasgupta\u2019s distribution/target-dependent complexity measure (Paper 5) to an *instance-dependent* version that incorporates selection bias and propensities (Paper 2). Use it to drive a new query rule that prioritizes regions with both high disagreement and high effective sample size (propensity support), with matching upper/lower bounds on label complexity.\n\n5. **Active Learning with Unknown Selection Policy: Joint Propensity Estimation and Querying**\n   Develop an algorithm for observational active learning when propensities are unknown or partially observed by jointly estimating the selection policy and learning the classifier. Use active queries to reduce propensity uncertainty in regions that dominate counterfactual risk, and prove consistency plus label complexity bounds under mild identifiability/overlap conditions.\n\n6. **Robust Agnostic Active Learning under Adversarially Corrupted Labels (Beyond Bounded-Rate Noise)**\n   Building on the contrast between bounded-rate noise tolerance and pessimistic non-realizable lower bounds (Paper 3), propose an active learner that is robust to a small fraction of adversarial label corruptions using median-of-means or filtering. Show improved label efficiency compared to passive robust learning when corruption is sparse and disagreement regions are small.\n\n7. **Actionable Lower Bounds: Tight Characterization of When Observational Active Learning Helps**\n   Derive lower bounds on label complexity for active learning from observational data (Paper 2) that mirror K\u00e4\u00e4ri\u00e4inen\u2019s non-realizable limitations (Paper 3), but parameterized by propensity overlap and a counterfactual disagreement coefficient. This would precisely delineate when active querying can beat passive learning and when selection bias forces \u03a9(1/\u03b5\u00b2)-type behavior.\n\n8. **Pool-Based Active Learning with Counterfactual Objectives for Multiclass/Structured Decisions**\n   Extend the counterfactual active learning framework (Paper 2) from binary classification to multiclass and structured prediction (e.g., ranking, sequence labeling) using importance-weighted surrogates and a generalized disagreement notion. Provide an implementable algorithm (e.g., reduction to cost-sensitive classification) and prove label complexity in terms of structured hypothesis class complexity and overlap.\n\n9. **Unlabeled-Data-Driven Region Partitioning for A2-Style Version Space Pruning**\n   Use unlabeled samples to learn a data-dependent partition (e.g., via density estimation or clustering) and run A2-like pruning (Paper 1) separately within regions to tighten error tests and reduce redundant queries. Analyze gains under mixture distributions where standard global disagreement regions are large but local disagreement is small, connecting to distribution-sensitive measures (Paper 5).\n\n10. **Active Off-Policy Evaluation + Learning: Dual Objective Querying**\n   Propose a unified algorithm that actively queries labels to simultaneously (i) learn a strong classifier and (ii) produce a tight off-policy evaluation/confidence interval for a target policy, leveraging importance-weighted estimators (Paper 4) and counterfactual risk minimization (Paper 2). Develop a query criterion that optimizes a weighted sum of predicted excess risk and estimator variance, with end-to-end guarantees on both learning performance and evaluation error.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Active Learning with Unknown Selection Policy: Joint Propensity Estimation and Querying\nDevelop an algorithm for observational active learning when propensities are unknown or partially observed by jo",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 5,
      "paper_title": "Learning Linear Attention in Polynomial Time",
      "contribution": "Shows that multi\u2011head linear attention can be learned in polynomial time by recasting the model as learning a rank\u2011H kernel predictor in an RKHS, and provides an algorithm that both finds near\u2011optimal MHLA parameters and certifies when all empirical best\u2011fits implement the same computation.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12204,
      "output_tokens": 955,
      "predecessor_details": [
        {
          "success": true,
          "title": "Fast Autoregressive Transformers with Linear Attention",
          "url": "https://arxiv.org/abs/2006.16236",
          "content": "[2006.16236] Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention[![close this message](/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](/IgnoreMe)\n[![arxiv logo](/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](/)&gt;[cs](/list/cs/recent)&gt;arXiv:2006.16236\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2006.16236**(cs)\n[Submitted on 29 Jun 2020 ([v1](https://arxiv.org/abs/2006.16236v1)), last revised 31 Aug 2020 (this version, v3)]\n# Title:Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\nAuthors:[Angelos Katharopoulos](https://arxiv.org/search/cs?searchtype=author&amp;query=Katharopoulos,+A),[Apoorv Vyas](https://arxiv.org/search/cs?searchtype=author&amp;query=Vyas,+A),[Nikolaos Pappas](https://arxiv.org/search/cs?searchtype=author&amp;query=Pappas,+N),[Fran\u00e7ois Fleuret](https://arxiv.org/search/cs?searchtype=author&amp;query=Fleuret,+F)\nView a PDF of the paper titled Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention, by Angelos Katharopoulos and 2 other authors\n[View PDF](/pdf/2006.16236)> > Abstract:\n> Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input&#39;s length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences. Comments:|ICML 2020, project at[this https URL](https://linear-transformers.com/)|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2006.16236](https://arxiv.org/abs/2006.16236)[cs.LG]|\n|(or[arXiv:2006.16236v3](https://arxiv.org/abs/2006.16236v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2006.16236](https://doi.org/10.48550/arXiv.2006.16236)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Angelos Katharopoulos [[view email](/show-email/cacdb53f/2006.16236)]\n**[[v1]](/abs/2006.16236v1)**Mon, 29 Jun 2020 17:55:38 UTC (1,626 KB)\n**[[v2]](/abs/2006.16236v2)**Tue, 30 Jun 2020 11:26:55 UTC (1,626 KB)\n**[v3]**Mon, 31 Aug 2020 11:09:32 UTC (1,626 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention, by Angelos Katharopoulos and 2 other authors\n* [View PDF](/pdf/2006.16236)\n* [TeX Source](/src/2006.16236)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](/prevnext?id=2006.16236&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](/prevnext?id=2006.16236&amp;function=next&amp;context=cs.LG)\n[new](/list/cs.LG/new)|[recent](/list/cs.LG/recent)|[2020-06](/list/cs.LG/2020-06)\nChange to browse by:\n[cs](/abs/2006.16236?context=cs)\n[stat](/abs/2006.16236?context=stat)\n[stat.ML](/abs/2006.16236?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2006.16236)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2006.16236)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2006.16236)\n### [2 blog links](/tb/2006.16236)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2006.html#abs-2006-16236)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2006-16236)\n[Angelos Katharopoulos]()\n[Apoorv Vyas]()\n[Nikolaos Pappas]()\n[Fran\u00e7ois Fleuret]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](/auth/show-endorsers/2006.16236)|[Disable MathJax](javascript:setMathjaxCookie())([What is Ma",
          "original_query": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Katharopoulos et al., 2020)",
          "cleaned_query": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"
        },
        {
          "success": true,
          "title": "[2009.14794] Rethinking Attention with Performers - arXiv",
          "url": "https://arxiv.org/abs/2009.14794",
          "content": "[2009.14794] Rethinking Attention with Performers\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2009.14794\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2009.14794**(cs)\n[Submitted on 30 Sep 2020 ([v1](https://arxiv.org/abs/2009.14794v1)), last revised 19 Nov 2022 (this version, v4)]\n# Title:Rethinking Attention with Performers\nAuthors:[Krzysztof Choromanski](https://arxiv.org/search/cs?searchtype=author&amp;query=Choromanski,+K),[Valerii Likhosherstov](https://arxiv.org/search/cs?searchtype=author&amp;query=Likhosherstov,+V),[David Dohan](https://arxiv.org/search/cs?searchtype=author&amp;query=Dohan,+D),[Xingyou Song](https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+X),[Andreea Gane](https://arxiv.org/search/cs?searchtype=author&amp;query=Gane,+A),[Tamas Sarlos](https://arxiv.org/search/cs?searchtype=author&amp;query=Sarlos,+T),[Peter Hawkins](https://arxiv.org/search/cs?searchtype=author&amp;query=Hawkins,+P),[Jared Davis](https://arxiv.org/search/cs?searchtype=author&amp;query=Davis,+J),[Afroz Mohiuddin](https://arxiv.org/search/cs?searchtype=author&amp;query=Mohiuddin,+A),[Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaiser,+L),[David Belanger](https://arxiv.org/search/cs?searchtype=author&amp;query=Belanger,+D),[Lucy Colwell](https://arxiv.org/search/cs?searchtype=author&amp;query=Colwell,+L),[Adrian Weller](https://arxiv.org/search/cs?searchtype=author&amp;query=Weller,+A)\nView a PDF of the paper titled Rethinking Attention with Performers, by Krzysztof Choromanski and 12 other authors\n[View PDF](https://arxiv.org/pdf/2009.14794)> > Abstract:\n> We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers. Comments:|Published as a conference paper + oral presentation at ICLR 2021. 38 pages. See[this https URL](https://github.com/google-research/google-research/tree/master/protein_lm)for protein language model code, and[this https URL](https://github.com/google-research/google-research/tree/master/performer)for Performer code. See[this https URL](https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html)for Google AI Blog|\nSubjects:|Machine Learning (cs.LG); Computation and Language (cs.CL); Machine Learning (stat.ML)|\nCite as:|[arXiv:2009.14794](https://arxiv.org/abs/2009.14794)[cs.LG]|\n|(or[arXiv:2009.14794v4](https://arxiv.org/abs/2009.14794v4)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2009.14794](https://doi.org/10.48550/arXiv.2009.14794)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Valerii Likhosherstov [[view email](https://arxiv.org/show-email/e755a50c/2009.14794)]\n**[[v1]](https://arxiv.org/abs/2009.14794v1)**Wed, 30 Sep 2020 17:09:09 UTC (10,282 KB)\n**[[v2]](https://arxiv.org/abs/2009.14794v2)**Tue, 16 Feb 2021 21:40:24 UTC (13,996 KB)\n**[[v3]](https://arxiv.org/abs/2009.14794v3)**Tue, 9 Mar 2021 16:26:47 UTC (13,996 KB)\n**[v4]**Sat, 19 Nov 2022 12:45:21 UTC (27,987 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Rethinking Attention with Performers, by Krzysztof Choromanski and 12 other authors\n* [View PDF](https://arxiv.org/pdf/2009.14794)\n* [TeX Source](https://arxiv.org/src/2009.14794)\n* [Other Formats](https://arxiv.org/format/2009.14794)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2009.14794&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2009.14794&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2020-09](https://arxiv.org/list/cs.LG/2020-09)\nChange to browse by:\n[cs](https://arxiv.org/abs/2009.14794?context=cs)\n[cs.CL](https://arxiv.org/abs/2009.14794?context=cs.CL)\n[stat](https://arxiv.org/abs/2009.14794?context=stat)\n[stat.ML](https://arxiv.org/abs/2009.14794?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2009.14794)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2009.14794)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2009.14794)\n### [5 blog links](https://arxiv.org/tb/2009.14794)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2009.html#abs-2009-14794)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2009-14794)\n[Krzysztof Choromanski]()\n[Valerii Likhosherstov]()\n[David Dohan]()\n[Xingyou Song]()\n[Andreea Gane]()\n&hellip;\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css)export BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https",
          "original_query": "Rethinking Attention with Performers (Choromanski et al., 2021)",
          "cleaned_query": "Rethinking Attention with Performers"
        },
        {
          "success": true,
          "title": "Learning with Kernels: Support Vector Machines ...",
          "url": "https://dl.acm.org/doi/10.5555/559923",
          "content": "![logo](data:,)\n\n## This website uses cookies\n\nWe occasionally run membership recruitment campaigns on social media channels and use cookies to track post-clicks. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you\u2019ve provided to them or that they\u2019ve collected from your use of their services. Use the check boxes below to choose the types of cookies you consent to have stored on your device.\n\nDo not sell or share my personal information\n\n[Use necessary cookies only](https://dl.acm.org/doi/10.5555/559923) [Allow all cookies](https://dl.acm.org/doi/10.5555/559923) [Show details](https://dl.acm.org/doi/10.5555/559923)\n\n[OK](https://dl.acm.org/doi/10.5555/559923)\n\n[Use necessary cookies only](https://dl.acm.org/doi/10.5555/559923) [Allow selected cookies](https://dl.acm.org/doi/10.5555/559923) [Allow all cookies](https://dl.acm.org/doi/10.5555/559923)\n\nNecessary\n\nPreferences\n\nStatistics\n\nMarketing\n\n[Show details](https://dl.acm.org/doi/10.5555/559923)\n\n[Cookie declaration](https://dl.acm.org/doi/10.5555/559923) [\\[#IABV2SETTINGS#\\]](https://dl.acm.org/doi/10.5555/559923) [About](https://dl.acm.org/doi/10.5555/559923)\n\n[Necessary (9)](https://dl.acm.org/doi/10.5555/559923) [Preferences (5)](https://dl.acm.org/doi/10.5555/559923) [Statistics (14)](https://dl.acm.org/doi/10.5555/559923) [Marketing (25)](https://dl.acm.org/doi/10.5555/559923) [Unclassified (21)](https://dl.acm.org/doi/10.5555/559923)\n\nNecessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies. These cookies do not gather information about you that could be used for marketing purposes and do not remember where you have been on the internet.\n\n| Name | Provider | Purpose | Maximum Storage Duration | Type |\n| --- | --- | --- | --- | --- |\n| \\_\\_cf\\_bm\u00a0\\[x2\\] | [ACM](https://www.acm.org/privacy-policy) | This cookie is used to distinguish between humans and bots. This is beneficial for the website, in order to make valid reports on the use of their website. | 1 day | HTTP Cookie |\n| \\_\\_jid | c.disquscdn.com | Used to add comments to the website and remember the user's Disqus login credentials across websites that use said service. | Session | HTTP Cookie |\n| disqusauth | c.disquscdn.com | Registers whether the user is logged in. This allows the website owner to make parts of the website inaccessible, based on the user's log-in status. | Session | HTTP Cookie |\n| \\_cfuvid | [ACM](https://www.acm.org/privacy-policy) | This cookie is a part of the services provided by Cloudflare - Including load-balancing, deliverance of website content and serving DNS connection for website operators. | Session | HTTP Cookie |\n| CookieConsent | [Cookiebot](https://www.cookiebot.com/goto/privacy-policy/) | Stores the user's cookie consent state for the current domain | 1 year | HTTP Cookie |\n| JSESSIONID | [ACM](https://www.acm.org/privacy-policy) | Preserves users states across page requests. | Session | HTTP Cookie |\n| \\_gh\\_sess | [Github](https://help.github.com/en/articles/github-privacy-statement) | Preserves users states across page requests. | Session | HTTP Cookie |\n| logged\\_in | [Github](https://help.github.com/en/articles/github-privacy-statement) | Registers whether the user is logged in. This allows the website owner to make parts of the website inaccessible, based on the user's log-in status. | 1 year | HTTP Cookie |\n\nPreference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\n\n| Name | Provider | Purpose | Maximum Storage Duration | Type |\n| --- | --- | --- | --- | --- |\n| aet-dismiss | c.disquscdn.com | Necessary for the functionality of the website's comment-system. | Persistent | HTML Local Storage |\n| drafts.queue | c.disquscdn.com | Necessary for the functionality of the website's comment-system. | Persistent | HTML Local Storage |\n| submitted\\_posts\\_cache | c.disquscdn.com | Necessary for the functionality of the website's comment-system. | Persistent | HTML Local Storage |\n| mopDeploy | [Mopinion](https://mopinion.com/privacy/) | Pending | Session | HTML Local Storage |\n| MACHINE\\_LAST\\_SEEN | [ACM](https://www.acm.org/privacy-policy) | Pending | 300 days | HTTP Cookie |\n\nStatistic cookies help website owners understand how visitors interact with websites by collecting and reporting information anonymously.\n\n| Name | Provider | Purpose | Maximum Storage Duration | Type |\n| --- | --- | --- | --- | --- |\n| \\_ga | [Google](https://business.safety.google/privacy/) | Registers a unique ID that is used to generate statistical data on how the visitor uses the website. | 2 years | HTTP Cookie |\n| \\_ga\\_# | [Google](https://business.safety.google/privacy/) | Used by Google Analytics to collect data on the number of times a user has visited the website as well as dates for the first and most recent visit. | 2 years | HTTP Cookie |\n| \\_gat | [Google](https://business.safety.google/privacy/) | Used by Google Analytics to throttle request rate | 1 day | HTTP Cookie |\n| \\_gid | [Google](https://business.safety.google/privacy/) | Registers a unique ID that is used to generate statistical data on how the visitor uses the website. | 1 day | HTTP Cookie |\n| \\_hjSession\\_# | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Collects statistics on the visitor's visits to the website, such as the number of visits, average time spent on the website and what pages have been read. | 1 day | HTTP Cookie |\n| \\_hjSessionUser\\_# | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Collects statistics on the visitor's visits to the website, such as the number of visits, average time spent on the website and what pages have been read. | 1 year | HTTP Cookie |\n| \\_hjTLDTest | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Registers statistical data on users' behaviour on the website. Used for internal analytics by the website operator. | Session | HTTP Cookie |\n| \\_hp2\\_# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 1 day | HTTP Cookie |\n| \\_hp2\\_hld#.# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 1 day | HTTP Cookie |\n| \\_hp2\\_id.# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 13 months | HTTP Cookie |\n| \\_hp2\\_ses\\_props.# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 1 day | HTTP Cookie |\n| disqus\\_unique | c.disquscdn.com | Collects statistics related to the user's visits to the website, such as number of visits, average time spent on the website and loaded pages. | Session | HTTP Cookie |\n| \\_octo | [Github](https://help.github.com/en/articles/github-privacy-statement) | Pending | 1 year | HTTP Cookie |\n| collect | [Google](https://business.safety.google/privacy/) | Used to send data to Google Analytics about the visitor's device and behavior. Tracks the visitor across devices and marketing channels. | Session | Pixel Tracker |\n\nMarketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers.\n\n| Name | Provider | Purpose | Maximum Storage Duration | Type |\n| --- | --- | --- | --- | --- |\n| badge",
          "original_query": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Sch\u00f6lkopf & Smola, 2002)",
          "cleaned_query": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond"
        },
        {
          "success": true,
          "title": "The approximation of one matrix by another of lower rank",
          "url": "https://link.springer.com/article/10.1007/BF02288367",
          "content": "The approximation of one matrix by another of lower rank | Psychometrika\n[Skip to main content](#main)\nAdvertisement\n[![Springer Nature Link](https://link.springer.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1007/BF02288367?)\n# The approximation of one matrix by another of lower rank\n* Published:September 1936\n* Volume\u00a01,\u00a0pages 211\u2013218, (1936)\n* [Cite this article](#citeas)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/11336?as=webp)Psychometrika](https://link.springer.com/journal/11336)[Aims and scope](https://link.springer.com/journal/11336/aims-and-scope)\n* [Carl Eckart](#auth-Carl-Eckart-Aff1)[1](#Aff1)&amp;\n* [Gale Young](#auth-Gale-Young-Aff1)[1](#Aff1)\n* 6421Accesses\n* 13Altmetric\n* [Explore all metrics](https://link.springer.com/article/10.1007/BF02288367/metrics)\n## Abstract\nThe mathematical problem of approximating one matrix by another of lower rank is closely related to the fundamental postulate of factor-theory. When formulated as a least-squares problem, the normal equations cannot be immediately written down, since the elements of the approximate matrix are not independent of one another. The solution of the problem is simplified by first expressing the matrices in a canonic form. It is found that the problem always has a solution which is usually unique. Several conclusions can be drawn from the form of this solution.\nA hypothetical interpretation of the canonic components of a score matrix is discussed.\nThis is a preview of subscription content,[log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/article/10.1007/BF02288367?error=cookies_not_supported&code=9981b19e-6625-47a2-9c4f-f1ec10812bc1)to check access.\n## Access this article\n[Log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/article/10.1007/BF02288367?error=cookies_not_supported&code=9981b19e-6625-47a2-9c4f-f1ec10812bc1)\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1134%2FS0965542522050141/MediaObjects/11470_2022_1655_Fig1_HTML.png)\n### [On the Best Approximation Algorithm by Low-Rank Matrices in Chebyshev\u2019s Norm](https://link.springer.com/10.1134/S0965542522050141?fromPaywallRec=true)\nArticle01 May 2022\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1134%2FS0965542519110149/MediaObjects/11470_2019_1247_Fig1_HTML.gif)\n### [Application of Matrix Decompositions for Matrix Canonization](https://link.springer.com/10.1134/S0965542519110149?fromPaywallRec=true)\nArticle01 November 2019\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1186%2Fs13660-018-1931-4/MediaObjects/13660_2018_1931_Fig1_HTML.png)\n### [A new method based on the manifold-alternative approximating for low-rank matrix completion](https://link.springer.com/10.1186/s13660-018-1931-4?fromPaywallRec=true)\nArticleOpen access11 December 2018\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Algebra](https://link.springer.com/subjects/algebra)\n* [Approximations and Expansions](https://link.springer.com/subjects/approximations-and-expansions)\n* [Computational Mathematics and Numerical Analysis](https://link.springer.com/subjects/computational-mathematics-and-numerical-analysis)\n* [Linear Algebra](https://link.springer.com/subjects/linear-algebra)\n* [Mathematics](https://link.springer.com/subjects/mathematics)\n* [Matrix Theory](https://link.springer.com/subjects/matrix-theory)\n## Author information\n### Authors and Affiliations\n1. University of Chicago, Chicago, Illinois\nCarl Eckart\u00a0&amp;\u00a0Gale Young\nAuthors\n1. Carl Eckart\n[View author publications](https://link.springer.com/search?sortBy=newestFirst&amp;dc.creator=Carl%20Eckart)\nSearch author on:[PubMed]()[Google Scholar]()\n2. Gale Young\n[View author publications](https://link.springer.com/search?sortBy=newestFirst&amp;dc.creator=Gale%20Young)\nSearch author on:[PubMed]()[Google Scholar]()\n## Rights and permissions\n[Reprints and permissions]()\n## About this article\n### Cite this article\nEckart, C., Young, G. The approximation of one matrix by another of lower rank.*Psychometrika***1**, 211\u2013218 (1936). https://doi.org/10.1007/BF02288367\n[Download citation](https://citation-needed.springer.com/v2/references/10.1007/BF02288367?format=refman&amp;flavour=citation)\n* Issue date:September 1936\n* DOI:https://doi.org/10.1007/BF02288367\n### Share this article\nAnyone you share the following link with will be able to read this content:\nGet shareable link\nSorry, a shareable link is not currently available for this article.\nCopy shareable link to clipboard\nProvided by the Springer Nature SharedIt content-sharing initiative\n### Keywords\n* [Public Policy](https://link.springer.com/search?query=Public%20Policy&amp;facet-discipline=&#34;Psychology&#34;)\n* [Statistical Theory](https://link.springer.com/search?query=Statistical%20Theory&amp;facet-discipline=&#34;Psychology&#34;)\n* [Lower Rank](https://link.springer.com/search?query=Lower%20Rank&amp;facet-discipline=&#34;Psychology&#34;)\n* [Canonic Form](https://link.springer.com/search?query=Canonic%20Form&amp;facet-discipline=&#34;Psychology&#34;)\n* [Mathematical Problem](https://link.springer.com/search?query=Mathematical%20Problem&amp;facet-discipline=&#34;Psychology&#34;)\n## Access this article\n[Log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/article/10.1007/BF02288367?error=cookies_not_supported&code=9981b19e-6625-47a2-9c4f-f1ec10812bc1)\nAdvertisement",
          "original_query": "The approximation of one matrix by another of lower rank (Eckart & Young, 1936)",
          "cleaned_query": "The approximation of one matrix by another of lower rank"
        },
        {
          "success": true,
          "title": "[PDF] Toward efficient agnostic learning - CIS UPenn",
          "url": "https://www.cis.upenn.edu/~mkearns/papers/agnostic-journal.pdf",
          "content": "Machine Learning, 17, 115-141 (1994) \n@ 1994 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands. \nToward Efficient Agnostic Learning \nMICHAEL J. KEARNS \nROBERT E. SCHAPIRE \nAT&T Bell Laboratories, 600 Mountain Avenue, Murray Hill, NJ 07974-0636 \nLINDA M. SELLIE \nDepartment of Computer Science, University of Chicago, Chicago, IL 60637 \nmkearns @ research.att.com \nschapire@ research.att.eom \nsellie@research.aU.com \nEditor: Lisa Hellerstein \nAbstract. In this paper we initiate an investigation of generalizations of the Probably Approximately Correct \n(PAC) learning model that attempt to significantly weaken the target function assumptions. The ultimate goal \nin this direction is informally termed agnostic learning, in which we make virtually no assumptions on the \ntarget function. The name derives from the fact that as designers of learning algorithms, we give up the belief \nthat Nature (as represented by the target function) has a simple or succinct explanation. We give a number \nof positive and negative results that provide an initial outline of the possibilities for agnostic learning. Our \nresults include hardness results for the most obvious generalization of the PAC model to an agnostic setting, \nan efficient and general agnostic learning method based on dynamic programming, relationships between loss \nfunctions for agnostic learning, and an algorithm for a learning problem that involves hidden variables. \nKeywords: machine learning, agnostic learning, PAC learning, computational learning theory \n1. Introduction \nOne of the major limitations of the Probably Approximately Correct (or PAC) learn\u0002ing model (Valiant, 1984) (and related models) is the strong assumptions placed on the \nso-called target function that the learning algorithm is attempting to approximate from \nexamples. While such restrictions have permitted a rigorous study of the computational \ncomplexity of learning as a function of the representational complexity of the target \nfunction, the PAC family of models diverges from the setting typically encountered in \npractice and in empirical machine learning research. Empirical approaches often make \nfew or no assumptions on the target function, but search a limited space of hypothe\u0002sis functions in an attempt to find the \"best\" approximation to the target function; in \ncases where the target function is too complex, even this best approximation may incur \nsignificant error. \nIn this paper we initiate an investigation of generalizations of the PAC model in an \nattempt to significantly weaken the target function assumptions whenever possible. Our \nultimate goal is informally termed agnostic learning, 1 in which we make virtually no \nassumptions on the target function. We use the word \"agnostic\" -- whose root means \nliterally \"not known\" -- to emphasize the fact that as designers of learning algorithms, we \nmay have no prior knowledge about the target function. It is important to note that in this \npaper we make no attempt to remove the assumption of statistical independence between \nthe examples seen by a learning algorithm, another worthwhile research direction that \n116 M.J. KEARNS, R.E. SCHAPIRE AND L.M. SELLIE \nhas been pursued by a number of authors (Aldous & Vazirani, 1990; Helmbold & Long, \n1994). \nThis paper describes a preliminary study of the possibilities and limitations for efficient \nagnostic learning. As such, we do not claim to have a definitive model but instead use \na rather general model (based on the work of Haussler (1992)) that allows easy con\u0002sideration of many natural modifications. Perhaps not surprisingly in light of evidence \nfrom the standard PAC model, efficient agnostic learning in its purest form (no assump\u0002tions on target function or distribution) is hard to come by, as some of our results will \ndemonstrate. Thus, we will consider several variations of these perhaps overly ambitious \ncriteria in an attempt to find positive results with target assumptions that are at least \nsignificantly weakened over the standard PAC setting. \nThere are several prior studies of weakened target assumptions for PAC learning that \nare relevant to our work. The first is due to Haussler (1992) who describes a powerful \ngeneralization of the standard PAC model based on decision theory and uniform con\u0002vergence results. Haussler's results are of central importance to much of the research \ndescribed here. Indeed, the agnostic model that we describe is quite similar to Haus\u0002sler's, differing only in the introduction of a \"touchstone\" class (see Section 2). However, \nwhile Haussler's concern is exclusively on the information-theoretic and statistical issues \nin agnostic learning, we are here concerned almost exclusively with efficient computa\u0002tion. Also relevant is the large body of research on nonparametric density estimation in \nthe field of statistics (see, for instance, Izenman's (1991) excellent survey). \nAnother relevant investigation is the work on probabilistic concepts of Kearns and \nSchapire (1990), as well as the work of Yamanishi (1992a) on stochastic rules. Here, \nthe target function is a conditional probability distribution, typically on a discrete range \nspace, such as {0, 1}. A significant portion of the research described in this paper extends \nthis work. Some of the results presented are also closely related to the work of Pitt and \nValiant on heuristic learning (Pitt & Valiant, 1988; Valiant, 1985), which can be viewed \nas a variant of our agnostic PAC model. \nThe following is a brief overview of the paper: in Section 2 we motivate and develop \nin detail the general learning fi'amework we will use. In Section 3 we consider the \nrestriction of this general model to the case of agnostic PAC learning and give strong \nevidence for the intractability of even rather simple learning problems in this model. \nIn Section 4 we discuss the empirical minimization of loss and give a general method \nfor agnostic learning of \"piecewise\" functions that is based on dynamic programming. \nSection 5 gives a useful relationship in the agnostic setting between two common loss \nfunctions, the quadratic and prediction loss, and gives applications of this relationship. \nIn Section 6 we investigate a compromise between agnostic learning and the strong target \nassumptions of the standard PAC model by providing an efficient learning algorithm in \na model for learning problems involving hidden variables. Finally, in Section 7, we list \na few of the many problems that remain open in this area. \nTOWARD EFFICIENT AONOSTIC LEARNING 117 \n2. Definitions and models \nIn this section we define our notation and the generalized framework we will use in \nour attempt to weaken the target function assumptions needed for efficient learning. \nOur approach is strongly influenced by the decision-theoretic learning model that was \nintroduced to the computational learning theory community by Haussler (1992). In giving \nour definitions, we err on the side of formality -- in order to lay the groundwork for \nfuture research on agnostic learning, we wish to give a model that is both precise and \nquite general. For most of the paper, however, we will be using various restrictions of \nthis general model that will be locally specified using less cumbersome notation. \nLet X be a set called the domain; we refer to points in X as instances, and we \nintuitively think of instances as the inputs to a \"black box\" whose behavior we wish \nto learn or to model. Let Y~ be a set called the range, and let Y be a set called the \nobserved range. We think of Y~ as the space of possible values that might be output \nby the black box; however, we introduce Y because we may not have direct access to \nthe output value, but only to some quantity derived from it. In general, we make no \nassumptions about the relationship between Y and Y~. We call a pair (x, y) E X x Y \nan observation. \n2.1. The assumption class \nThe assumption class A is a class of probability distributions on th",
          "original_query": "Agnostic learning / Agnostic PAC learning formalism (Kearns et al., mid\u20111990s and related literature)",
          "cleaned_query": "Agnostic learning",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "Characterizing the Expressivity of Fixed-Precision Transformer ...",
          "url": "https://arxiv.org/html/2505.23623v2",
          "content": "Characterizing the Expressivity of Fixed-Precision Transformer Language Models\n# Characterizing the Expressivity of Fixed-Precision Transformer Language Models\nJiaoda Li \u2004\u2004Ryan Cotterell\n{[jiaoda.li](mailto:jiaoda.li@inf.ethz.ch),[ryan.cotterell](mailto:ryan.cotterell@inf.ethz.ch)}@inf.ethz.ch\n![[Uncaptioned image]](figs/ethz.png)\n###### Abstract\nTransformer-based language models (LMs) have achieved widespread empirical success, but their theoretical expressive power remains only partially understood.\nIn this work, we analyze a restricted idealization of fixed-precision transformers with strict future masking, soft attention, and no positional encodings.\nWe establish that this class of models is exactly as expressive as a specific fragment of linear temporal logic that contains only a single temporal operator: thepastoperator.\nWe further connect this fragment to established classes in formal language theory, automata theory, and algebra, yielding a unified framework for understanding transformer expressivity under this idealization.\nFinally, we present empirical results that align closely with our theory: transformers trained on languages within their characterized expressive capacity generalize reliably across sequence lengths, while they consistently fail to generalize on languages beyond it.111Code available at[GitHub repository](https://github.com/rycolab/expressivity-of-fixed-precision-transformers).\n## 1Introduction\nTransformer-based language models (LMs) have demonstrated remarkable empirical success> [\n[> 46\n](https://arxiv.org/html/2505.23623v2#bib.bib46)> , [> 36\n](https://arxiv.org/html/2505.23623v2#bib.bib36)> , [> 12\n](https://arxiv.org/html/2505.23623v2#bib.bib12)> ]\non a wide variety of natural language tasks> [\n[> 47\n](https://arxiv.org/html/2505.23623v2#bib.bib47)> , [> 19\n](https://arxiv.org/html/2505.23623v2#bib.bib19)> , [> 41\n](https://arxiv.org/html/2505.23623v2#bib.bib41)> , > inter alia\n> ]\n.\nThis success has sparked growing interest in understanding the theoretical expressive power of transformers, i.e., what languages they can and cannot recognize, and, by extension, what tasks they can and cannot perform.\nA significant body of work approaches this question by relating transformers to well-established frameworks such as formal languages, logic, and circuit complexity> [\n[> 18\n](https://arxiv.org/html/2505.23623v2#bib.bib18)> , [> 31\n](https://arxiv.org/html/2505.23623v2#bib.bib31)> , [> 50\n](https://arxiv.org/html/2505.23623v2#bib.bib50)> , [> 42\n](https://arxiv.org/html/2505.23623v2#bib.bib42)> ]\n.\nTo facilitate their theoretical analysis, theoreticians often propose idealizations of transformers.\nFor instance, while practical implementations of transformers operate under fixed precision, e.g., single (32-bit) or half (16-bit) precision, many authors assume arbitrary> [\n[> 38\n](https://arxiv.org/html/2505.23623v2#bib.bib38)> , [> 18\n](https://arxiv.org/html/2505.23623v2#bib.bib18)> , [> 34\n](https://arxiv.org/html/2505.23623v2#bib.bib34)> ]\nor length-dependent precision> [\n[> 32\n](https://arxiv.org/html/2505.23623v2#bib.bib32)> , [> 7\n](https://arxiv.org/html/2505.23623v2#bib.bib7)> ]\n.\nAlthough such idealizations capture key aspects of transformers, they tend to overestimate their expressive power> [\n[> 38\n](https://arxiv.org/html/2505.23623v2#bib.bib38)> ]\n.\nA recent step toward a more faithful theoretical understanding of the expressive power of transformers comes from> Yang et al. [\n[> 50\n](https://arxiv.org/html/2505.23623v2#bib.bib50)> ]\n, who show that fixed-precision transformers with strict future masking and unique hard attention (UHA) are exactly as expressive as linear temporal logicLTL\u200b[P,F,S,U]{\\\\textup{{LTL}}[{\\\\mathrel{\\\\textup{{P}}}},{\\\\mathrel{\\\\textup{{F}}}},{\\\\mathrel{\\\\textup{{S}}}},{\\\\mathrel{\\\\textup{{U}}}}]}, which includes four temporal operators:P{\\\\mathrel{\\\\textup{{P}}}}(past),F{\\\\mathrel{\\\\textup{{F}}}}(future),S{\\\\mathrel{\\\\textup{{S}}}}(since), andU{\\\\mathrel{\\\\textup{{U}}}}(until).\nHowever, UHA still deviates from the soft attention used in practice. To address this gap,> Yang and Chiang [\n[> 49\n](https://arxiv.org/html/2505.23623v2#bib.bib49)> ]\nanalyze fixed-precision transformers with strict future masking and soft attention, an idealization that most closely reflects the models deployed in real-world applications.> Yang and Chiang [\n[> 49\n](https://arxiv.org/html/2505.23623v2#bib.bib49)> ]\nshow that such models are upper bounded by C-RASP, a counting-based programming language, though a precise characterization of these models\u2019 expressivity remains open.\nIn this paper, we close this gap by providing an exact characterization of the expressive power of fixed-precision transformers with soft attention, strict masking, and no positional encodings (NoPE).\nWe show they are precisely characterized byLTL\u200b[P]{\\\\textup{{LTL}}[{\\\\mathrel{\\\\textup{{P}}}}]}, a restricted fragment ofLTL\u200b[P,F,S,U]{\\\\textup{{LTL}}[{\\\\mathrel{\\\\textup{{P}}}},{\\\\mathrel{\\\\textup{{F}}}},{\\\\mathrel{\\\\textup{{S}}}},{\\\\mathrel{\\\\textup{{U}}}}]}that uses only thepastoperator (P{\\\\mathrel{\\\\textup{{P}}}}). We further demonstrate thatLTL\u200b[P]{\\\\textup{{LTL}}[{\\\\mathrel{\\\\textup{{P}}}}]}is equivalent in expressivity to partially ordered deterministic finite automata (PODFAs), which are characterized by\u211b{{{\\\\mathcal{R}}}}-trivial monoids and recognize left-deterministic polynomials.\nThese results offer a detailed and principled characterization of the expressive power of this idealization, delineating its strengths and limitations.\nCrucially, our findings imply that many simple languages, e.g., bounded Dyck languages, which have been shown to be recognizable under more permissive idealizations, are beyond the reach of the models we study.\nWe also extend our theoretical results to transformer LMs, showing that their expressivity matches that of transformer recognizers. A visual overview of the theoretical landscape is provided in[Fig.\u02dc1](https://arxiv.org/html/2505.23623v2#S1.F1).\nLTL\u200b[P]{\\\\textup{{LTL}}[{\\\\mathrel{\\\\textup{{P}}}}]}PFO2\u200b[&lt;]{\\\\textup{{PFO}}^{2}[\\\\mathord{&lt;&lt;}]}PODFA\u211b{{{\\\\mathcal{R}}}}-trivialmonoidleft-deterministicpolynomialtransformerlanguage modeltransformer[Theorem\u02dc2.1](https://arxiv.org/html/2505.23623v2#S2.Thmtheorem1)[Theorem\u02dc5.2](https://arxiv.org/html/2505.23623v2#S5.Thmtheorem2)[Theorem\u02dc5.1](https://arxiv.org/html/2505.23623v2#S5.Thmtheorem1)[Theorem\u02dc3.3](https://arxiv.org/html/2505.23623v2#S3.Thmtheorem3)[Theorem\u02dc3.2](https://arxiv.org/html/2505.23623v2#S3.Thmtheorem2)[Lemma\u02dcC.1](https://arxiv.org/html/2505.23623v2#A3.Thmtheorem1)[Lemma\u02dcC.4](https://arxiv.org/html/2505.23623v2#A3.Thmtheorem4)> [\n[> 4\n](https://arxiv.org/html/2505.23623v2#bib.bib4)> ]\nFigure 1:Roadmap of the paper.Red arrowsindicate novel results.\nTo arrive at a compelling theory, it is essential to show that it faithfully reflects the behavior of models trained under standard machine learning paradigms. To this end, we provide empirical evidence using the length generalization framework, a widely used method for gauging neural network expressivity> [\n[> 10\n](https://arxiv.org/html/2505.23623v2#bib.bib10)> , [> 6\n](https://arxiv.org/html/2505.23623v2#bib.bib6)> , [> 22\n](https://arxiv.org/html/2505.23623v2#bib.bib22)> ]\n. We construct a suite of languages spanning a fine-grained hierarchy of formal language classes. Our results ([Tab.\u02dc1](https://arxiv.org/html/2505.23623v2#S5.T1)) exhibit strong alignment between theory and practice: for all languages that transformers are predicted to recognize, the models generalize perfectly over lengths (100%100\\\\%accuracy); for languages beyond their theoretical capacity, they consistently make generalization errors, regardless of learning rates or random seeds.\n## 2Background\nIn this section, we present the necessary background knowledge that underpins our analysis.\n### 2.1Strings and Languages\nAnalphabet, denoted as\u03a3{{\\\\Sigma}}, is a finite, non-empty set of symbols.\nAstringover\u03a3{{\\\\Sigma}}",
          "original_query": "Expressivity results for Transformers (representative works: P\u00e9rez et al., 2019 and related literature)",
          "cleaned_query": "Expressivity results for Transformers"
        }
      ],
      "generated_ideas": [
        "Feature-Dimension\u2013Accuracy\u2013Language Tradeoffs for FAVOR+\nDerive explicit bounds linking FAVOR+ random feature count to (i) uniform approximation error of softmax attention and (ii) ability to recognize specific regular language subclasses (e.g., dot-depth / polynomial hierarchies). Construct a benchmark suite where success provably requires a minimum approximation fidelity, giving actionable guidance for selecting Performer feature counts under compute budgets.",
        "Low-Rank + Random-Feature Hybrid Attention with Provable Error Guarantees\nCombine Eckart\u2013Young optimal low-rank approximations with Performer-style random features by learning a small-rank \u201cbackbone\u201d attention plus a random-feature residual estimator. Provide theory showing improved approximation (or variance reduction) at fixed compute, and implement an architecture that adaptively allocates rank vs. random features per head/layer during training.",
        "Streaming Agnostic Learning Guarantees for Autoregressive Linear Transformers\nUse the \u201cTransformers are RNNs\u201d iterative formulation to study online/streaming training under agnostic learning (no realizability), yielding regret or excess-risk bounds for next-token prediction with linear attention. Concretely, design a dynamic-programming-inspired or mirror-descent style optimizer that leverages the associative state update to train on unbounded sequences with bounded memory.",
        "Kernel Selection for Attention as an Agnostic Model-Selection Problem\nTreat the attention kernel (softmax, Laplacian, polynomial, etc.) as a hypothesis choice and apply agnostic learning principles to select kernels that minimize empirical loss with generalization control. Implement a \u201cmulti-kernel Performer\u201d that mixes several kernelizable attentions with learned weights, and evaluate whether different kernels systematically align with different formal-language/operator needs.",
        "Positional Encodings vs. Linear Attention: A Logic-Level Characterization\nStarting from the NoPE expressivity characterization (LTL[P]), analyze how adding specific positional encodings (absolute, relative, rotary) changes the recognized language class *when attention is linearized*. Aim for a crisp separation result (e.g., which encodings lift the model beyond PODFA-recognizable languages) and corroborate with length-generalization experiments.",
        "Variance-Controlled Random Features for Stable Long-Context Autoregression\nDesign new positive random feature constructions tailored to the *autoregressive iterative* linear-attention update to minimize accumulation of approximation error over long horizons. Provide an end-to-end stability analysis (error growth with sequence length) and a practical algorithm (e.g., refresh/reseed schedules or control variates) that improves perplexity on extremely long contexts.",
        "Certified Attention Approximation: Per-Token Error Certificates for Linearized Softmax\nBuild a system that, during inference, outputs a computable certificate bounding the deviation between approximated attention (FAVOR+/feature maps) and true softmax attention for each token. This would enable runtime adaptivity: increase features or switch to exact attention only when the certificate exceeds a threshold, giving predictable quality/latency tradeoffs on long sequences.",
        "SVM-Inspired Margin Theory for Kernelized Attention Representations\nReinterpret the attention mechanism as constructing kernel features over token histories and develop margin-based generalization bounds analogous to classical kernel methods/SVMs. Concretely, define a notion of \u201cattention margin\u201d for next-token classification and test whether maximizing it (via regularizers on feature maps/queries-keys) improves robustness and length generalization.",
        "Minimal Counterexample Synthesis for Diagnosing Expressivity Failures\nCombine the logic/automata characterization with targeted search to automatically generate *short counterexamples* that distinguish what a given fixed-precision (linearized) transformer can vs. cannot compute. Implement a tool that, given a trained model and a formal-language spec, returns minimal failing strings and maps them to specific attention/feature-map limitations\u2014turning theory into a practical debugging workflow."
      ],
      "generated_ideas_raw": "1. **Expressivity of Fixed-Precision *Linear* Attention Transformers**\n   Extend Li & Cotterell\u2019s fixed-precision, masked, soft-attention analysis to *kernelized linear attention* (as in Linear Transformers and Performers) by formalizing the feature-map approximation as part of the model class. Prove an exact (or tight) logical/automata characterization as a function of feature dimension and kernel choice, and validate with synthetic formal languages that separate softmax vs. FAVOR+ vs. deterministic feature maps.\n\n2. **Feature-Dimension\u2013Accuracy\u2013Language Tradeoffs for FAVOR+**\n   Derive explicit bounds linking FAVOR+ random feature count to (i) uniform approximation error of softmax attention and (ii) ability to recognize specific regular language subclasses (e.g., dot-depth / polynomial hierarchies). Construct a benchmark suite where success provably requires a minimum approximation fidelity, giving actionable guidance for selecting Performer feature counts under compute budgets.\n\n3. **Low-Rank + Random-Feature Hybrid Attention with Provable Error Guarantees**\n   Combine Eckart\u2013Young optimal low-rank approximations with Performer-style random features by learning a small-rank \u201cbackbone\u201d attention plus a random-feature residual estimator. Provide theory showing improved approximation (or variance reduction) at fixed compute, and implement an architecture that adaptively allocates rank vs. random features per head/layer during training.\n\n4. **Streaming Agnostic Learning Guarantees for Autoregressive Linear Transformers**\n   Use the \u201cTransformers are RNNs\u201d iterative formulation to study online/streaming training under agnostic learning (no realizability), yielding regret or excess-risk bounds for next-token prediction with linear attention. Concretely, design a dynamic-programming-inspired or mirror-descent style optimizer that leverages the associative state update to train on unbounded sequences with bounded memory.\n\n5. **Kernel Selection for Attention as an Agnostic Model-Selection Problem**\n   Treat the attention kernel (softmax, Laplacian, polynomial, etc.) as a hypothesis choice and apply agnostic learning principles to select kernels that minimize empirical loss with generalization control. Implement a \u201cmulti-kernel Performer\u201d that mixes several kernelizable attentions with learned weights, and evaluate whether different kernels systematically align with different formal-language/operator needs.\n\n6. **Positional Encodings vs. Linear Attention: A Logic-Level Characterization**\n   Starting from the NoPE expressivity characterization (LTL[P]), analyze how adding specific positional encodings (absolute, relative, rotary) changes the recognized language class *when attention is linearized*. Aim for a crisp separation result (e.g., which encodings lift the model beyond PODFA-recognizable languages) and corroborate with length-generalization experiments.\n\n7. **Variance-Controlled Random Features for Stable Long-Context Autoregression**\n   Design new positive random feature constructions tailored to the *autoregressive iterative* linear-attention update to minimize accumulation of approximation error over long horizons. Provide an end-to-end stability analysis (error growth with sequence length) and a practical algorithm (e.g., refresh/reseed schedules or control variates) that improves perplexity on extremely long contexts.\n\n8. **Certified Attention Approximation: Per-Token Error Certificates for Linearized Softmax**\n   Build a system that, during inference, outputs a computable certificate bounding the deviation between approximated attention (FAVOR+/feature maps) and true softmax attention for each token. This would enable runtime adaptivity: increase features or switch to exact attention only when the certificate exceeds a threshold, giving predictable quality/latency tradeoffs on long sequences.\n\n9. **SVM-Inspired Margin Theory for Kernelized Attention Representations**\n   Reinterpret the attention mechanism as constructing kernel features over token histories and develop margin-based generalization bounds analogous to classical kernel methods/SVMs. Concretely, define a notion of \u201cattention margin\u201d for next-token classification and test whether maximizing it (via regularizers on feature maps/queries-keys) improves robustness and length generalization.\n\n10. **Minimal Counterexample Synthesis for Diagnosing Expressivity Failures**\n   Combine the logic/automata characterization with targeted search to automatically generate *short counterexamples* that distinguish what a given fixed-precision (linearized) transformer can vs. cannot compute. Implement a tool that, given a trained model and a formal-language spec, returns minimal failing strings and maps them to specific attention/feature-map limitations\u2014turning theory into a practical debugging workflow.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Feature-Dimension\u2013Accuracy\u2013Language Tradeoffs for FAVOR+\nDerive explicit bounds linking FAVOR+ random feature count to (i) uniform approximation error of softmax attention and (ii) ability to recogniz",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Low-Rank + Random-Feature Hybrid Attention with Provable Error Guarantees\nCombine Eckart\u2013Young optimal low-rank approximations with Performer-style random features by learning a small-rank \u201cbackbone\u201d ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Streaming Agnostic Learning Guarantees for Autoregressive Linear Transformers\nUse the \u201cTransformers are RNNs\u201d iterative formulation to study online/streaming training under agnostic learning (no reali",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Kernel Selection for Attention as an Agnostic Model-Selection Problem\nTreat the attention kernel (softmax, Laplacian, polynomial, etc.) as a hypothesis choice and apply agnostic learning principles to",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Positional Encodings vs. Linear Attention: A Logic-Level Characterization\nStarting from the NoPE expressivity characterization (LTL[P]), analyze how adding specific positional encodings (absolute, rel",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Variance-Controlled Random Features for Stable Long-Context Autoregression\nDesign new positive random feature constructions tailored to the *autoregressive iterative* linear-attention update to minimi",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Certified Attention Approximation: Per-Token Error Certificates for Linearized Softmax\nBuild a system that, during inference, outputs a computable certificate bounding the deviation between approximat",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "SVM-Inspired Margin Theory for Kernelized Attention Representations\nReinterpret the attention mechanism as constructing kernel features over token histories and develop margin-based generalization bou",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Minimal Counterexample Synthesis for Diagnosing Expressivity Failures\nCombine the logic/automata characterization with targeted search to automatically generate *short counterexamples* that distinguis",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 6,
      "paper_title": "Optimal Mistake Bounds for Transductive Online Learning",
      "contribution": "Shows that the transductive online mistake bound is \u0398(\u221ad) (giving an \u2126(\u221ad) lower bound and a matching O(\u221ad) upper bound), thereby establishing a quadratic separation from the standard online bound of \u0398(d).",
      "num_predecessors": 4,
      "predecessors_crawled": 4,
      "academic_sources": 4,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 8500,
      "output_tokens": 1011,
      "predecessor_details": [
        {
          "success": true,
          "title": "Nick Littlestone's research works | Harvard University and ...",
          "url": "https://www.researchgate.net/scientific-contributions/Nick-Littlestone-7647712",
          "content": "Nick Littlestone's research works | Harvard University and other places\n# Nick Littlestone\u2019s research while affiliated with Harvard University and other places\n## What is this page?\nThis page lists works of an author who doesn't have a ResearchGate profile or hasn't added the works to their profile yet. It is automatically generated from public (personal) data to further our legitimate goal of comprehensive and accurate scientific recordkeeping. If you are this author and want this page removed, please[let us know](https://help.researchgate.net/hc/en-us/requests/new?tf_38727008724625=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOjAsImlhdCI6MTc2NTIwOTE3NCwiaXNzIjoiaHR0cHM6Ly93d3cucmVzZWFyY2hnYXRlLm5ldC8ifQ.uW5T2z8I0pO8wA5HeGjnPyF8OGU3IHDY_NSpnbTc5bA&amp;ticket_form_id=13146494812305).\n## Publications (20)\n[Relating Data Compression and Learnability](https://www.researchgate.net/publication/2808486_Relating_Data_Compression_and_Learnability)\n* [Article](publication/2808486_Relating_Data_Compression_and_Learnability)\nOctober 2000\n\u00b7443 Reads\n\u00b7269 Citations\n[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Nick Littlestone](scientific-contributions/Nick-Littlestone-7647712)\n\u00b7[![](https://c5.rgstatic.net/m/4671872220764/images/template/default/profile/profile_default_m.jpg)Manfred Warmuth](profile/Manfred-Warmuth)\nWe explore the learnability of two-valued functions from samples using the paradigm of Data Compression. A rst algorithm (compression) choses a small subset of the sample which is called the kernel. A second algorithm predicts future values of the function from the kernel, i.e. the algorithm acts as an hypothesis for the function to be learned. The second algorithm must be able to reconstruct the correct function values when given a point of the original sample. We demonstrate that the existence of a suitable data compression scheme is sucient to ensure learnability. We express the probability that the hypothesis predicts the function correctly on a random sample point as a function of the sample and kernel sizes. No assumptions are made on the probability distributions according to which the sample points are generated. This approach provides an alternative to that of [BEHW86], which uses the Vapnik-Chervonenkis dimension to classify learnable geometric concepts. Our bo...\nRead more\n[On-Line Learning with Linear Loss Constraints.](https://www.researchgate.net/publication/220246538_On-Line_Learning_with_Linear_Loss_Constraints)\n* [Article](publication/220246538_On-Line_Learning_with_Linear_Loss_Constraints)\nSeptember 2000\n\u00b757 Reads\n\u00b72 Citations\nInformation and Computation\n[![](https://i1.rgstatic.net/ii/profile.image/272706960621583-1442029814255_Q64/David-Helmbold.jpg)David P. Helmbold](profile/David-Helmbold)\n\u00b7[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Nick Littlestone](scientific-contributions/Nick-Littlestone-7647712)\n\u00b7[![](https://i1.rgstatic.net/ii/profile.image/279224091987970-1443583619545_Q64/Philip-Long-2.jpg)Philip Long](profile/Philip-Long-2)\nWe consider a generalization of the mistake-bound model (for learning {0, 1}-valued functions) in which the learner must satisfy a general constraint on the number M+ of incorrect 1 predictions and the number M- of incorrect 0 predictions. We describe a general-purpose optimal algorithm for our formulation of this problem, We describe several applications of our general results, involving situations in which the learner wishes to satisfy linear inequalities in M+ and M-. (C) 2000 Academic Press.\nRead more\n[Apple Tasting.](https://www.researchgate.net/publication/220248281_Apple_Tasting)\n* [Article](publication/220248281_Apple_Tasting)\nSeptember 2000\n\u00b786 Reads\n\u00b724 Citations\nInformation and Computation\n[![](https://i1.rgstatic.net/ii/profile.image/272706960621583-1442029814255_Q64/David-Helmbold.jpg)David P. Helmbold](profile/David-Helmbold)\n\u00b7[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Nick Littlestone](scientific-contributions/Nick-Littlestone-7647712)\n\u00b7[![](https://i1.rgstatic.net/ii/profile.image/279224091987970-1443583619545_Q64/Philip-Long-2.jpg)Philip Long](profile/Philip-Long-2)\nIn the standard on-line model the learning algorithm tries to minimizethe total number of mistakes made in a series of trials. On each trial the learner sees an instance, makes a prediction of its classification, then finds out the correct classification. We define a natural variant of this model (\u201capple tasting\u201d) whereu\u2022 the classes are interpreted as the good and bad instances,\u2022 the prediction is interpreted as accepting or rejecting the instance,and\u2022 the learner gets feedback only when the instance is accepted.We use two transformations to relate the apple tasting model to an enhanced standard model where false acceptances are counted separately from false rejections. We apply our results to obtain a good general-purpose apple tasting algorithm as well as nearly optimal apple tasting algorithms for a variety of standard classes, such as conjunctions and disjunctions of n boolean variables. We also present and analyze a simpler transformation useful when the instances are drawn at random rather than selected by an adversary.\nRead more\n[The Weighted Majority Algorithm](https://www.researchgate.net/publication/2803045_The_Weighted_Majority_Algorithm)\n* [Article](publication/2803045_The_Weighted_Majority_Algorithm)\nMay 2000\n\u00b7707 Reads\n\u00b71,469 Citations\nInformation and Computation\n[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Nick Littlestone](scientific-contributions/Nick-Littlestone-7647712)\n\u00b7[![](https://c5.rgstatic.net/m/4671872220764/images/template/default/profile/profile_default_m.jpg)Manfred Warmuth](profile/Manfred-Warmuth)\nWe study the construction of prediction algorithms in a situation in which a learner faces a sequence of trials, with a prediction to be made in each, and the goal of the learner is to make few mistakes. We are interested in the case that the learner has reason to believe that one of some pool of known algorithms will perform well, but the learner does not know which one. A simple and effective method, based on weighted voting, is introduced for constructing a compound algorithm in such a circumstance. We call this method the Weighted Majority Algorithm. We show that this algorithm is robust in the presence of errors in the data. We discuss various versions of the Weighted Majority Algorithm and prove mistake bounds for them that are closely related to the mistake bounds of the best algorithms of the pool. For example, given a sequence of trials, if there is an algorithm in the pool A that makes at most m mistakes then the Weighted Majority Algorithm will make at most c(log jAj + m) mi...\nRead more\n[The Robustness of the](https://www.researchgate.net/publication/221497223_The_Robustness_of_the)\n* [Conference Paper](publication/221497223_The_Robustness_of_the)\nJanuary 1999\n\u00b710 Reads\n\u00b75 Citations\n[![](https://i1.rgstatic.net/ii/profile.image/272437031993365-1441965458467_Q64/Claudio-Gentile-3.jpg)Claudio Gentile](profile/Claudio-Gentile-3)\n\u00b7[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Nick Littlestone](scientific-contributions/Nick-Littlestone-7647712)\n[Learning in the Presence of Finitely or Infinitely Many Irrelevant](https://www.researchgate.net/publication/2858346_Learning_in_the_Presence_of_Finitely_or_Infinitely_Many_Irrelevant)\n* [Article](publication/2858346_Learning_in_the_Presence_of_Finitely_or_Infinitely_Many_Irrelevant)\nFebruary 1998\n\u00b77 Reads\n\u00b75 Citations\n[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Avrim Blum](scientific-contributions/Avrim-Blum-73372623)\n\u00b7[![](https://c5.rgstatic.net/m/435982309481010/images/template/default/author/author_default_m.jpg)Lisa Hell",
          "original_query": "Nick Littlestone (1987) \u2014 \"Learning Quickly When Irrelevant Attributes Abound\"",
          "cleaned_query": "Nick Littlestone"
        },
        {
          "success": true,
          "title": "a case study for Vietnamese real estate data - Springer Link",
          "url": "https://link.springer.com/article/10.1007/s10489-025-06238-2",
          "content": "\n 1 Introduction With the popularity of e-commerce, online advertisements and marketplaces have been used widely by both sellers and buyers. These e-commerce marketplaces are growing rapidly, supporting the sale and purchase of different products such as real estates, household goods, labor-intensive equipments, etc. With the increased access to these marketplaces, there is also an increase in the number of fake advertisements on these platforms that target vulnerable buyers to obtain their personal information or to trick them into buying fake products\u00a0[ 1]. These fake advertisements severely affect both the genuine sellers and buyers, and potentially undermine the creditability of the e-commerce websites if left undetected. In this work, we will develop a novel end-to-end machine learning system, called Fake Advertisements Detection using Automated Multimodal Learning (FADAML), to detect and filter out fake online advertisements. Our system combines techniques in multimodal machine learning\u00a0[ 2, 3] and automated machine learning (AutoML)\u00a0[ 4, 5] to improve the system\u2019s detection rate. To our knowledge, this is the first work that combines multimodal and automated machine learning for fake advertisement detection. By utilizing multimodal learning, we can leverage both free-form text-based inputs as well as specialized handcrafted features to improve the performance of our system. Additionally, by incorporating AutoML, we can train several powerful machine learning models and combine them to further enhance the performance. In general, our FADAML system consists of three main components: a data crawler and preprocessor, a multimodal feature extractor, and an AutoML system. The first two components extract and transform raw advertisement texts into refined multimodal features through several processing steps. These refined features are then fed into the AutoML component to train and select the best model for the data, which would be used to filter new advertisements. One distinctive feature of our approach when compared with previous work on multimodal fake news detection\u00a0[ 6, 7, 8, 9] is that we use AutoML to train our model. This simplifies the implementation of our method and makes it highly flexible and applicable. Furthermore, in our work, instead of using generic multimodal information, we work with domain experts to select highly usable multimodal features for our model. As a case study, we apply our system to detect fake advertisements on Vietnamese real estate websites. The residential real estate market in Vietnam is valued about $7.3B in 2022 with the majority of real estate agents relying on e-commerce websites to increase transactions\u00a0[ 10]. Furthermore, Vietnamese is a low-resource language that is usually very challenging to work with due to (1) the relatively smaller corpus, (2) the difference in word structures compared to English, and (3) the difference between the corpus used to pre-train language models (i.e., Vietnamese Wikipedia\u2019s articles) and that of the downstream tasks (e.g., real estate advertisements)\u00a0[ 11]. In this paper, we show that our system can work well for the Vietnamese language. We implement and test our FADAML system thoroughly using a real-world data set collected from five popular Vietnamese real estate websites. Our experiments show that FADAML can achieve 91.5% accuracy when detecting fake advertisements, significantly outperforming three state-of-the-art fake news detection baseline methods. To gain a better understanding into our system, we inspect the individual models trained by AutoML and also investigate the feature importance scores of our final model. Finally, we conduct an ablation study to explore the effects of different multimodal feature sets on the performance of our system. In summary, our paper makes the following contributions. (1) We develop FADAML, a novel end-to-end machine learning system to detect and filter out fake online advertisements. Our system combines techniques in multimodal machine learning and automated machine learning to perform this task. (2) We show how our system can be applied to detect fake advertisements on popular Vietnamese real estate websites. We also empirically show that the system can achieve good detection accuracy for this problem. 2 Related work 2.1 Fake advertisement detection Online fake advertisement detection\u00a0[ 12, 13, 14, 15] is an important research topic whose solutions can potentially help protect e-commerce websites\u2019 users from potential scams. Previous methods to detect such fake advertisements range from using simple classifiers\u00a0[ 12] and data mining\u00a0[ 13] to heuristic statistics\u00a0[ 15] and combinations of available online services\u00a0[ 14]. These previous works, however, focused on the English language and did not use the state-of-the-art advancements in machine learning, such as deep learning, multimodal machine learning, or automated machine learning. In this work, we approach this problem with a solution that combines multimodal and automated machine learning, the two useful frameworks for this type of data. Additionally, our work focuses on advertisements in the Vietnamese language, which received no prior attention. The fake advertisement detection problem here is also related to fake news detection\u00a0[ 16] and misinformation detection\u00a0[ 17] on online social networks. An approach for this problem was proposed in\u00a0[ 18] that uses contextualized word embeddings from a pre-trained ELMo model\u00a0[ 19] to input into a Bi-LSTM with an attention layer and a softmax classifier for final predictions. Another solution was proposed in\u00a0[ 20] that introduced FNDNet, which leverages parallel convolutional layers with varying kernel sizes and word embeddings to extract semantic relationships. Both of these methods can achieve remarkable performance on fake news detection; thus, we use them as baselines in this paper to compare with our system. Fake news detection in non-English languages was also considered in some previous work. In\u00a0[ 21], three language groups (i.e., Germanic, Latin, and Slavic) were considered and a text-feature method was suggested for identifying fake news. Another work\u00a0[ 22] created a corpus of both fake and real news in Spanish and detected fake news by using linguistically motivated features. For German, [ 23] developed FANG-COVID, a large-scale benchmark dataset for fake news detection related to the COVID-19 pandemic. Using an explainable textual and social context-based model, they proposed a method that can achieve comparable results to the black box model solely relying on BERT-embeddings. For the Portuguese language, [ 24] investigated the linguistic characteristics of fake news and applied machine learning techniques to achieve significant results. However, unlike our approach, none of these work employs AutoML in their solutions. 2.2 Vietnamese natural language processing Our work is related to natural language processing for the Vietnamese language. In recent years, there have been several works investigating different tasks for this language, such as part-of-speech tagging [ 25, 26], punctuation prediction\u00a0[ 27, 28], named entity recognition\u00a0[ 29, 30, 31, 32], and dependency parsing\u00a0[ 25, 33]. For Vietnamese, a robust pre-trained transformer-based language model, called PhoBERT\u00a0[ 11], has also been developed and applied to the part-of-speech tagging, named entity recognition, and dependency parsing problems above. In this paper, we will use PhoBERT to obtain the embeddings for the named entity recognition module of our system. 2.3 Real estate data analysis Real estate data analysis has gained much attention recently with the availability of data and analysis tools\u00a0[ 34, 35, 36]. Among these analyses, property price estimation is an important problem and several approaches have been proposed for this problem that include vision-based approach\u00a0[ 37], eager learning\u00a0[ 38], graph convolutional neural network\u00a0[ 39], etc. In terms of Vie",
          "original_query": "Ben\u2011David, Kushilevitz, and Mansour (1995, 1997) \u2014 work on transductive online learning",
          "cleaned_query": "Ben\u2011David, Kushilevitz, and Mansour"
        },
        {
          "success": true,
          "title": "Fine-Grained Distribution-Dependent Learning Curves",
          "url": "https://proceedings.mlr.press/v195/bousquet23a.html",
          "content": "\n [ edit] \n \n \n Proceedings of Thirty Sixth Conference on Learning Theory,\u00a0PMLR 195:5890-5924,\u00a02023.\n \n \n Abstract \n \n Learning curves plot the expected error of a learning algorithm as a function of the number of labeled samples it receives from a target distribution. They are widely used as a measure of an algorithm\u2019s performance, but classic PAC learning theory cannot explain their behavior. As observed by Antos and Lugosi (1996, 1998), the classic \u2018No Free Lunch\u2019 lower bounds only trace the upper envelope above all learning curves of specific target distributions. For a concept class with VC dimension $d$ the classic bound decays like $d/n$, yet it is possible that the learning curve for \\emph{every} specific distribution decays exponentially. In this case, for each $n$ there exists a different \u2018hard\u2019 distribution requiring $d/n$ samples. Antos and Lugosi asked which concept classes admit a \u2018strong minimax lower bound\u2019 \u2013 a lower bound of $d\u2019/n$ that holds for a fixed distribution for infinitely many $n$.We solve this problem in a principled manner, by introducing a combinatorial dimension called VCL that characterizes the best $d\u2019$ for which $d\u2019/n$ is a strong minimax lower bound. Conceptually, the VCL dimension determines the asymptotic rate of decay of the minimax learning curve, which we call the \u2018distribution-free trail\u2019 of the class. Our characterization strengthens the lower bounds of Bousquet, Hanneke, Moran, van Handel, and Yehudayoff (2021), and it refines their analysis of learning curves, by showing that for classes with finite VCL the learning rate can be decomposed into a linear component that depends only on the hypothesis class and a faster (e.g., exponential) component that depends also on the target distribution. As a corollary, we recover the lower bound of Antos and Lugosi (1996, 1998) for half-spaces in $\\mathbb{R}^d$.Finally, to provide another viewpoint on our work and how it compares to traditional PAC learning bounds, we also present an alternative formulation of our results in a language that is closer to the PAC setting.\n \n Cite this Paper \n \n \n \n BibTeX\n \n \n \n@InProceedings{pmlr-v195-bousquet23a,\n title = {Fine-Grained Distribution-Dependent Learning Curves},\n author = {Bousquet, Olivier and Hanneke, Steve and Moran, Shay and Shafer, Jonathan and Tolstikhin, Ilya},\n booktitle = {Proceedings of Thirty Sixth Conference on Learning Theory},\n pages = {5890--5924},\n year = {2023},\n editor = {Neu, Gergely and Rosasco, Lorenzo},\n volume = {195},\n series = {Proceedings of Machine Learning Research},\n month = {12--15 Jul},\n publisher = {PMLR},\n pdf = {https://proceedings.mlr.press/v195/bousquet23a/bousquet23a.pdf},\n url = {https://proceedings.mlr.press/v195/bousquet23a.html},\n abstract = {Learning curves plot the expected error of a learning algorithm as a function of the number of labeled samples it receives from a target distribution. They are widely used as a measure of an algorithm\u2019s performance, but classic PAC learning theory cannot explain their behavior. As observed by Antos and Lugosi (1996, 1998), the classic \u2018No Free Lunch\u2019 lower bounds only trace the upper envelope above all learning curves of specific target distributions. For a concept class with VC dimension $d$ the classic bound decays like $d/n$, yet it is possible that the learning curve for \\emph{every} specific distribution decays exponentially. In this case, for each $n$ there exists a different \u2018hard\u2019 distribution requiring $d/n$ samples. Antos and Lugosi asked which concept classes admit a \u2018strong minimax lower bound\u2019 \u2013 a lower bound of $d\u2019/n$ that holds for a fixed distribution for infinitely many $n$.We solve this problem in a principled manner, by introducing a combinatorial dimension called VCL that characterizes the best $d\u2019$ for which $d\u2019/n$ is a strong minimax lower bound. Conceptually, the VCL dimension determines the asymptotic rate of decay of the minimax learning curve, which we call the \u2018distribution-free trail\u2019 of the class. Our characterization strengthens the lower bounds of Bousquet, Hanneke, Moran, van Handel, and Yehudayoff (2021), and it refines their analysis of learning curves, by showing that for classes with finite VCL the learning rate can be decomposed into a linear component that depends only on the hypothesis class and a faster (e.g., exponential) component that depends also on the target distribution. As a corollary, we recover the lower bound of Antos and Lugosi (1996, 1998) for half-spaces in $\\mathbb{R}^d$.Finally, to provide another viewpoint on our work and how it compares to traditional PAC learning bounds, we also present an alternative formulation of our results in a language that is closer to the PAC setting.}\n}\n \n \n \n \n \n \n Endnote\n \n \n %0 Conference Paper\n%T Fine-Grained Distribution-Dependent Learning Curves\n%A Olivier Bousquet\n%A Steve Hanneke\n%A Shay Moran\n%A Jonathan Shafer\n%A Ilya Tolstikhin\n%B Proceedings of Thirty Sixth Conference on Learning Theory\n%C Proceedings of Machine Learning Research\n%D 2023\n%E Gergely Neu\n%E Lorenzo Rosasco \n%F pmlr-v195-bousquet23a\n%I PMLR\n%P 5890--5924\n%U https://proceedings.mlr.press/v195/bousquet23a.html\n%V 195\n%X Learning curves plot the expected error of a learning algorithm as a function of the number of labeled samples it receives from a target distribution. They are widely used as a measure of an algorithm\u2019s performance, but classic PAC learning theory cannot explain their behavior. As observed by Antos and Lugosi (1996, 1998), the classic \u2018No Free Lunch\u2019 lower bounds only trace the upper envelope above all learning curves of specific target distributions. For a concept class with VC dimension $d$ the classic bound decays like $d/n$, yet it is possible that the learning curve for \\emph{every} specific distribution decays exponentially. In this case, for each $n$ there exists a different \u2018hard\u2019 distribution requiring $d/n$ samples. Antos and Lugosi asked which concept classes admit a \u2018strong minimax lower bound\u2019 \u2013 a lower bound of $d\u2019/n$ that holds for a fixed distribution for infinitely many $n$.We solve this problem in a principled manner, by introducing a combinatorial dimension called VCL that characterizes the best $d\u2019$ for which $d\u2019/n$ is a strong minimax lower bound. Conceptually, the VCL dimension determines the asymptotic rate of decay of the minimax learning curve, which we call the \u2018distribution-free trail\u2019 of the class. Our characterization strengthens the lower bounds of Bousquet, Hanneke, Moran, van Handel, and Yehudayoff (2021), and it refines their analysis of learning curves, by showing that for classes with finite VCL the learning rate can be decomposed into a linear component that depends only on the hypothesis class and a faster (e.g., exponential) component that depends also on the target distribution. As a corollary, we recover the lower bound of Antos and Lugosi (1996, 1998) for half-spaces in $\\mathbb{R}^d$.Finally, to provide another viewpoint on our work and how it compares to traditional PAC learning bounds, we also present an alternative formulation of our results in a language that is closer to the PAC setting.\n \n \n \n \n \n \n APA\n \n \n \nBousquet, O., Hanneke, S., Moran, S., Shafer, J. &amp; Tolstikhin, I.. (2023). Fine-Grained Distribution-Dependent Learning Curves. Proceedings of Thirty Sixth Conference on Learning Theory, in Proceedings of Machine Learning Research 195:5890-5924 Available from https://proceedings.mlr.press/v195/bousquet23a.html.\n \n \n \n \n \n \n Related Material \n \n \n \n",
          "original_query": "Hanneke, Moran, and Shafer (2023)",
          "cleaned_query": "Hanneke, Moran, and Shafer"
        },
        {
          "success": true,
          "title": "Support-vector networks | Machine Learning - Springer Link",
          "url": "https://link.springer.com/article/10.1007/BF00994018",
          "content": "Advertisement\n\n# Support-vector networks\n\n- Published: September 1995\n\n- Volume\u00a020,\u00a0pages 273\u2013297, (1995)\n- [Cite this article](https://link.springer.com/link.springer.com#citeas)\n\n[Download PDF](https://link.springer.com/content/pdf/10.1007/BF00994018.pdf)\n\n[Machine Learning](https://link.springer.com/journal/10994) [Aims and scope](https://link.springer.com/journal/10994/aims-and-scope) [Submit manuscript](https://submission.springernature.com/new-submission/10994/3)\n\nSupport-vector networks\n\n[Download PDF](https://link.springer.com/content/pdf/10.1007/BF00994018.pdf)\n\n## Abstract\n\nThe _support-vector network_ is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.\n\nHigh generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.\n\n## Article PDF\n\n[Download](https://link.springer.com/content/pdf/10.1007/BF00994018.pdf) to read the full article text\n\n### Similar content being viewed by others\n\n### [Performance and Evaluation of Support Vector Machine and Artificial Neural Network over Heterogeneous Data](https://link.springer.com/10.1007/978-981-13-9181-1_51?fromPaywallRec=false)\n\nChapter\u00a9 2019\n\n### [Support Vector Machines](https://link.springer.com/10.1007/978-3-031-12282-8_8?fromPaywallRec=false)\n\nChapter\u00a9 2023\n\n### [Linear Classification of Data with Support Vector Machines and Generalized Support Vector Machines](https://link.springer.com/10.1007/978-3-319-42105-6_17?fromPaywallRec=false)\n\nChapter\u00a9 2016\n\n### Explore related subjects\n\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n\n- [Bayesian Network](https://link.springer.com/subjects/bayesian-network)\n- [Categorization](https://link.springer.com/subjects/categorization)\n- [Computational Intelligence](https://link.springer.com/subjects/computational-intelligence)\n- [Learning algorithms](https://link.springer.com/subjects/learning-algorithms)\n- [Machine Learning](https://link.springer.com/subjects/machine-learning)\n- [Artificial Intelligence](https://link.springer.com/subjects/artificial-intelligence)\n\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=10994)\n\nAvoid common mistakes on your manuscript.\n\n## References\n\n- Aizerman, M., Braverman, E., & Rozonoer, L. (1964). Theoretical foundations of the potential function method in pattern recognition learning. _Automation and Remote Control_, 25:821\u2013837.\n\n[Google Scholar](http://scholar.google.com/scholar_lookup?&title=Theoretical%20foundations%20of%20the%20potential%20function%20method%20in%20pattern%20recognition%20learning&journal=Automation%20and%20Remote%20Control&volume=25&pages=821-837&publication_year=1964&author=Aizerman%2CM.&author=Braverman%2CE.&author=Rozonoer%2CL.)\n\n- Anderson, T.W., & Bahadur, R.R. (1966). Classification into two multivariate normal distributions with different covariance matrices. _Ann. Math. Stat._, 33:420\u2013431.\n\n[Google Scholar](http://scholar.google.com/scholar_lookup?&title=Classification%20into%20two%20multivariate%20normal%20distributions%20with%20different%20covariance%20matrices&journal=Ann.%20Math.%20Stat.&volume=33&pages=420-431&publication_year=1966&author=Anderson%2CT.W.&author=Bahadur%2CR.R.)\n\n- Boser, B.E., Guyon, I., & Vapnik, V.N. (1992). A training algorithm for optimal margin classifiers. In _Proceedings of the Fifth Annual Workshop of Computational Learning Theory_, 5, 144\u2013152. Pittsburgh, ACM.\n\n[Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20training%20algorithm%20for%20optimal%20margin%20classifiers&journal=Proceedings%20of%20the%20Fifth%20Annual%20Workshop%20of%20Computational%20Learning%20Theory&volume=5&pages=144-152&publication_year=1992&author=Boser%2CB.E.&author=Guyon%2CI.&author=Vapnik%2CV.N.)\n\n- Bottou, L., Cortes, C., Denker, J.S., Drucker, H., Guyon, I., Jackel, L.D., LeCun, Y., Sackinger, E., Simard, P., Vapnik, V., & Miller, U.A. (1994). Comparison of classifier methods: A case study in handwritten digit recognition. _Proceedings of 12th International Conference on Pattern Recognition and Neural Network_.\n\n- Bromley, J., & Sackinger, E. (1991). Neural-network and _k_-nearest-neighbor classifiers. Technical Report 11359-910819-16TM, AT&T.\n\n- Cournant, R., & Hilbert, D. (1953). _Methods of Mathematical Physics_, Interscience, New York.\n\n[Google Scholar](http://scholar.google.com/scholar_lookup?&title=Methods%20of%20Mathematical%20Physics&publication_year=1953&author=Cournant%2CR.&author=Hilbert%2CD.)\n\n- Fisher, R.A. (1936). The use of multiple measurements in taxonomic problems. _Ann. Eugenics_, 7:111\u2013132.\n\n[Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20use%20of%20multiple%20measurements%20in%20taxonomic%20problems&journal=Ann.%20Eugenics&volume=7&pages=111-132&publication_year=1936&author=Fisher%2CR.A.)\n\n- LeCun, Y. (1985). Une procedure d'apprentissage pour reseau a seuil assymetrique. _Cognitiva 85: A la Frontiere de l'Intelligence Artificielle des Sciences de la Connaissance des Neurosciences_, 599\u2013604, Paris.\n\n- LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., & Jackel, L.D. (1990). Handwritten digit recognition with a back-propagation network. _Advances in Neural Information Processing Systems_, 2, 396\u2013404, Morgan Kaufman.\n\n[Google Scholar](http://scholar.google.com/scholar_lookup?&title=Handwritten%20digit%20recognition%20with%20a%20back-propagation%20network&journal=Advances%20in%20Neural%20Information%20Processing%20Systems&volume=2&pages=396-404&publication_year=1990&author=LeCun%2CY.&author=Boser%2CB.&author=Denker%2CJ.S.&author=Henderson%2CD.&author=Howard%2CR.E.&author=Hubbard%2CW.&author=Jackel%2CL.D.)\n\n- Parker, D.B. (1985). Learning logic. Technical Report TR-47, Center for Computational Research in Economics and Management Science, Massachusetts Institute of Technology, Cambridge, MA.\n\n- Rosenblatt, F. (1962). _Principles of Neurodynamics_, Spartan Books, New York.\n\n[Google Scholar](http://scholar.google.com/scholar_lookup?&title=Principles%20of%20Neurodynamics&publication_year=1962&author=Rosenblatt%2CF.)\n\n- Rumelhart, D.E., Hinton, G.E., & Williams, R.J. (1986). Learning internal representations by backpropagating errors. _Nature_, 323:533\u2013536.\n\n[Google Scholar](http://scholar.google.com/scholar_lookup?&title=Learning%20internal%20representations%20by%20backpropagating%20errors&journal=Nature&volume=323&pages=533-536&publication_year=1986&author=Rumelhart%2CD.E.&author=Hinton%2CG.E.&author=Williams%2CR.J.)\n\n- Rumelhart, D.E., Hinton, G.E., & Williams, R.J. (1987). Learning internal representations by error propagation. In James L. McClelland & David E. Rumelhart (Eds.), _Parallel Distributed Processing_, 1, 318\u2013362, MIT Press.\n\n[Google Scholar](http://scholar.google.com/scholar_lookup?&title=Learning%20internal%20representations%20by%20error%20propagation&journal=Parallel%20Distributed%20Processing&volume=1&pages=318-362&publication_year=1987&author=Rumelhart%2CD.E.&author=Hinton%2CG.E.&author=Williams%2CR.J.)\n\n- Vapnik, V.N. (1982). _Estimation of Dependences Based on Empirical Data_, Addendum 1, New York: Springer-Verlag.\n\n[Google Scholar](http://scholar.google.com/scholar_lookup?&title=Estimation%20of%20Dependences%20Based%20on%20Empirical%20Data%2C%20Addendum%201&",
          "original_query": "Vladimir Vapnik \u2014 transductive inference (Statistical Learning Theory)",
          "cleaned_query": "Vladimir Vapnik"
        }
      ],
      "generated_ideas": [
        "VCL-Guided Data Compression Schemes for Distribution-Dependent Learning Curves\nDevelop compression-based learners (kernel + reconstruction) whose compression size is explicitly controlled by the class\u2019s VCL dimension, then prove resulting distribution-dependent learning curves that decompose into a VCL-driven \\(d'/n\\) \u201ctrail\u201d plus a faster distribution-specific term. Validate the theory by constructing explicit schemes for halfspaces and conjunctions and measuring empirical curve shapes vs. predicted rates.",
        "Online Weighted Majority with Linear Loss Constraints under Partial Feedback (Apple-Tasting++)\nExtend the Apple Tasting model by combining Weighted Majority with Littlestone-style linear constraints on false accepts/false rejects while only observing labels on accepted items. Provide an optimal or near-optimal regret/mistake guarantee under constraints, and implement it for fake-ad filtering where \u201caccept\u201d corresponds to triggering expensive verification.",
        "Distribution-Dependent Active \u201cApple Tasting\u201d for Scam Detection Workflows\nDesign an active decision policy that chooses when to \u201ctaste\u201d (send an ad to human moderation) to minimize label cost subject to constraints on missed scams and false alarms. The key contribution is a theory/algorithm link: use distribution-dependent learning-curve estimates (VCL-style) to schedule queries adaptively, and evaluate on Vietnamese real-estate ads with simulated moderation budgets.",
        "AutoML that Optimizes Mistake-Constraints Instead of Accuracy for Fake-Ad Filtering\nModify AutoML objectives to directly optimize linear constraints on asymmetric mistakes (e.g., cap false negatives for scams while limiting false positives to preserve user experience), rather than maximizing accuracy/AUC. Implement constraint-aware model selection/stacking that outputs a feasible ensemble (e.g., calibrated thresholds + constrained voting) and benchmark against standard AutoML on FADAML-style multimodal features.",
        "Compression-Regularized Multimodal Ensembles for Low-Resource Vietnamese Text\nIntroduce a training criterion for multimodal classifiers where the selected feature subset and/or support set is explicitly compressive (small \u201ckernel\u201d), inspired by compression-to-learnability. The contribution is an actionable algorithm: learn a sparse set of prototype ads (or tokens/features) that reconstruct predictions, improving generalization under domain shift between Vietnamese Wikipedia-pretraining and real-estate ad language.",
        "Support-Vector Networks with Learned Multimodal Kernels via AutoML Search\nCreate an AutoML pipeline that searches over multimodal kernel constructions (text embeddings + handcrafted real-estate features) and SVM hyperparameters, optimizing both performance and interpretability via support vectors. The key novelty is treating the kernel design itself as a structured AutoML search space (e.g., additive/multiplicative kernels, modality-specific bandwidths) and evaluating robustness across websites.",
        "VCL Estimates for Practical Hypothesis Classes Used in Scam Detection (Trees, Linear, Kernel SVMs)\nEmpirically and theoretically estimate/upper-bound VCL for model classes commonly surfaced by AutoML (gradient-boosted trees, linear separators on embeddings, kernel SVMs). Use these estimates to predict which classes will show slow \\(d'/n\\)-type tails versus fast (near-exponential) decay on specific ad-distributions, then test predictions by plotting learning curves on progressively larger labeled sets.",
        "Hard-Distribution Construction for Multimodal Ad Data to Stress-Test AutoML\nUsing the \u201cstrong minimax\u201d perspective, construct controlled \u201chard\u201d distributions over multimodal ad features (text templates + numeric/geo metadata) where any learner must exhibit a \\(d'/n\\) tail for infinitely many \\(n\\). Release these as benchmark generators to evaluate whether AutoML systems overfit to easy distributions and to diagnose when observed gains are due to distribution-specific fast components.",
        "Constrained Online-to-Batch Conversion for Continually Changing Scam Tactics\nDevelop an online learner (Weighted Majority / constrained mistake-bound) for streaming ad data with evolving scam patterns, then convert it to a batch model periodically with guarantees that preserve linear loss constraints. The contribution is a deployable method for real marketplaces: bounded false-negative rate over time, even when labels arrive with delay and only for inspected (\u201ctasted\u201d) ads.",
        "Support-Vector Explanations via Compression Kernels for Moderator-Facing Interpretability\nExploit the link between support vectors and compression: define a \u201ccompressed explanation set\u201d consisting of a minimal subset of training ads (support vectors/prototypes) that reconstructs decisions and satisfies reconstruction constraints on the sample. Build moderator-facing explanations (\u201cthis ad is similar to these known scams\u201d) and quantify tradeoffs between explanation set size, detection accuracy, and distribution shift across different Vietnamese real-estate sites."
      ],
      "generated_ideas_raw": "1. **VCL-Guided Data Compression Schemes for Distribution-Dependent Learning Curves**  \nDevelop compression-based learners (kernel + reconstruction) whose compression size is explicitly controlled by the class\u2019s VCL dimension, then prove resulting distribution-dependent learning curves that decompose into a VCL-driven \\(d'/n\\) \u201ctrail\u201d plus a faster distribution-specific term. Validate the theory by constructing explicit schemes for halfspaces and conjunctions and measuring empirical curve shapes vs. predicted rates.\n\n2. **Online Weighted Majority with Linear Loss Constraints under Partial Feedback (Apple-Tasting++)**  \nExtend the Apple Tasting model by combining Weighted Majority with Littlestone-style linear constraints on false accepts/false rejects while only observing labels on accepted items. Provide an optimal or near-optimal regret/mistake guarantee under constraints, and implement it for fake-ad filtering where \u201caccept\u201d corresponds to triggering expensive verification.\n\n3. **Distribution-Dependent Active \u201cApple Tasting\u201d for Scam Detection Workflows**  \nDesign an active decision policy that chooses when to \u201ctaste\u201d (send an ad to human moderation) to minimize label cost subject to constraints on missed scams and false alarms. The key contribution is a theory/algorithm link: use distribution-dependent learning-curve estimates (VCL-style) to schedule queries adaptively, and evaluate on Vietnamese real-estate ads with simulated moderation budgets.\n\n4. **AutoML that Optimizes Mistake-Constraints Instead of Accuracy for Fake-Ad Filtering**  \nModify AutoML objectives to directly optimize linear constraints on asymmetric mistakes (e.g., cap false negatives for scams while limiting false positives to preserve user experience), rather than maximizing accuracy/AUC. Implement constraint-aware model selection/stacking that outputs a feasible ensemble (e.g., calibrated thresholds + constrained voting) and benchmark against standard AutoML on FADAML-style multimodal features.\n\n5. **Compression-Regularized Multimodal Ensembles for Low-Resource Vietnamese Text**  \nIntroduce a training criterion for multimodal classifiers where the selected feature subset and/or support set is explicitly compressive (small \u201ckernel\u201d), inspired by compression-to-learnability. The contribution is an actionable algorithm: learn a sparse set of prototype ads (or tokens/features) that reconstruct predictions, improving generalization under domain shift between Vietnamese Wikipedia-pretraining and real-estate ad language.\n\n6. **Support-Vector Networks with Learned Multimodal Kernels via AutoML Search**  \nCreate an AutoML pipeline that searches over multimodal kernel constructions (text embeddings + handcrafted real-estate features) and SVM hyperparameters, optimizing both performance and interpretability via support vectors. The key novelty is treating the kernel design itself as a structured AutoML search space (e.g., additive/multiplicative kernels, modality-specific bandwidths) and evaluating robustness across websites.\n\n7. **VCL Estimates for Practical Hypothesis Classes Used in Scam Detection (Trees, Linear, Kernel SVMs)**  \nEmpirically and theoretically estimate/upper-bound VCL for model classes commonly surfaced by AutoML (gradient-boosted trees, linear separators on embeddings, kernel SVMs). Use these estimates to predict which classes will show slow \\(d'/n\\)-type tails versus fast (near-exponential) decay on specific ad-distributions, then test predictions by plotting learning curves on progressively larger labeled sets.\n\n8. **Hard-Distribution Construction for Multimodal Ad Data to Stress-Test AutoML**  \nUsing the \u201cstrong minimax\u201d perspective, construct controlled \u201chard\u201d distributions over multimodal ad features (text templates + numeric/geo metadata) where any learner must exhibit a \\(d'/n\\) tail for infinitely many \\(n\\). Release these as benchmark generators to evaluate whether AutoML systems overfit to easy distributions and to diagnose when observed gains are due to distribution-specific fast components.\n\n9. **Constrained Online-to-Batch Conversion for Continually Changing Scam Tactics**  \nDevelop an online learner (Weighted Majority / constrained mistake-bound) for streaming ad data with evolving scam patterns, then convert it to a batch model periodically with guarantees that preserve linear loss constraints. The contribution is a deployable method for real marketplaces: bounded false-negative rate over time, even when labels arrive with delay and only for inspected (\u201ctasted\u201d) ads.\n\n10. **Support-Vector Explanations via Compression Kernels for Moderator-Facing Interpretability**  \nExploit the link between support vectors and compression: define a \u201ccompressed explanation set\u201d consisting of a minimal subset of training ads (support vectors/prototypes) that reconstructs decisions and satisfies reconstruction constraints on the sample. Build moderator-facing explanations (\u201cthis ad is similar to these known scams\u201d) and quantify tradeoffs between explanation set size, detection accuracy, and distribution shift across different Vietnamese real-estate sites.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "VCL-Guided Data Compression Schemes for Distribution-Dependent Learning Curves\nDevelop compression-based learners (kernel + reconstruction) whose compression size is explicitly controlled by the class",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Online Weighted Majority with Linear Loss Constraints under Partial Feedback (Apple-Tasting++)\nExtend the Apple Tasting model by combining Weighted Majority with Littlestone-style linear constraints o",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Distribution-Dependent Active \u201cApple Tasting\u201d for Scam Detection Workflows\nDesign an active decision policy that chooses when to \u201ctaste\u201d (send an ad to human moderation) to minimize label cost subject",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "AutoML that Optimizes Mistake-Constraints Instead of Accuracy for Fake-Ad Filtering\nModify AutoML objectives to directly optimize linear constraints on asymmetric mistakes (e.g., cap false negatives f",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Compression-Regularized Multimodal Ensembles for Low-Resource Vietnamese Text\nIntroduce a training criterion for multimodal classifiers where the selected feature subset and/or support set is explicit",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Support-Vector Networks with Learned Multimodal Kernels via AutoML Search\nCreate an AutoML pipeline that searches over multimodal kernel constructions (text embeddings + handcrafted real-estate featur",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "VCL Estimates for Practical Hypothesis Classes Used in Scam Detection (Trees, Linear, Kernel SVMs)\nEmpirically and theoretically estimate/upper-bound VCL for model classes commonly surfaced by AutoML ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Hard-Distribution Construction for Multimodal Ad Data to Stress-Test AutoML\nUsing the \u201cstrong minimax\u201d perspective, construct controlled \u201chard\u201d distributions over multimodal ad features (text template",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Constrained Online-to-Batch Conversion for Continually Changing Scam Tactics\nDevelop an online learner (Weighted Majority / constrained mistake-bound) for streaming ad data with evolving scam patterns",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Support-Vector Explanations via Compression Kernels for Moderator-Facing Interpretability\nExploit the link between support vectors and compression: define a \u201ccompressed explanation set\u201d consisting of ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 7,
      "paper_title": "State Entropy Regularization for Robust Reinforcement Learning",
      "contribution": "Shows that regularizing the entropy of the state-visitation distribution yields provable robustness to structured and spatially correlated perturbations (under reward and transition uncertainty), contrasts these guarantees with policy-entropy regularization, and analyzes practical sensitivities such as number of rollouts.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 6,
      "input_tokens": 9823,
      "output_tokens": 1036,
      "predecessor_details": [
        {
          "success": true,
          "title": "Twice regularized MDPs and the equivalence between robustness ...",
          "url": "https://arxiv.org/abs/2110.06267",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2110.06267** (cs)\n\n\\[Submitted on 12 Oct 2021\\]\n\n# Title:Twice regularized MDPs and the equivalence between robustness and regularization\n\nAuthors: [Esther Derman](https://arxiv.org/search/cs?searchtype=author&query=Derman,+E), [Matthieu Geist](https://arxiv.org/search/cs?searchtype=author&query=Geist,+M), [Shie Mannor](https://arxiv.org/search/cs?searchtype=author&query=Mannor,+S)\n\nView a PDF of the paper titled Twice regularized MDPs and the equivalence between robustness and regularization, by Esther Derman and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2110.06267)\n\n> Abstract:Robust Markov decision processes (MDPs) aim to handle changing or partially known system dynamics. To solve them, one typically resorts to robust optimization methods. However, this significantly increases computational complexity and limits scalability in both learning and planning. On the other hand, regularized MDPs show more stability in policy learning without impairing time complexity. Yet, they generally do not encompass uncertainty in the model dynamics. In this work, we aim to learn robust MDPs using regularization. We first show that regularized MDPs are a particular instance of robust MDPs with uncertain reward. We thus establish that policy iteration on reward-robust MDPs can have the same time complexity as on regularized MDPs. We further extend this relationship to MDPs with uncertain transitions: this leads to a regularization term with an additional dependence on the value function. We finally generalize regularized MDPs to twice regularized MDPs (R${}^2$ MDPs), i.e., MDPs with $\\\\textit{both}$ value and policy regularization. The corresponding Bellman operators enable developing policy iteration schemes with convergence and robustness guarantees. It also reduces planning and learning in robust MDPs to regularized MDPs.\n\n| | |\n| --- | --- |\n| Comments: | Accepted to NeurIPS 2021 |\n| Subjects: | Machine Learning (cs.LG); Optimization and Control (math.OC) |\n| Cite as: | [arXiv:2110.06267](https://arxiv.org/abs/2110.06267) \\[cs.LG\\] |\n| (or [arXiv:2110.06267v1](https://arxiv.org/abs/2110.06267v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2110.06267](https://doi.org/10.48550/arXiv.2110.06267) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Esther Derman \\[ [view email](https://arxiv.org/show-email/bf2713e8/2110.06267)\\] **\\[v1\\]**\nTue, 12 Oct 2021 18:33:45 UTC (407 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Twice regularized MDPs and the equivalence between robustness and regularization, by Esther Derman and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2110.06267)\n- [TeX Source](https://arxiv.org/src/2110.06267)\n- [Other Formats](https://arxiv.org/format/2110.06267)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2110.06267&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2110.06267&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-10](https://arxiv.org/list/cs.LG/2021-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2110.06267?context=cs) [math](https://arxiv.org/abs/2110.06267?context=math) [math.OC](https://arxiv.org/abs/2110.06267?context=math.OC)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2110.06267)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2110.06267)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2110.06267)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2110.html#abs-2110-06267) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2110-06267)\n\n[Esther Derman](https://dblp.uni-trier.de/search/author?author=Esther%20Derman) [Matthieu Geist](https://dblp.uni-trier.de/search/author?author=Matthieu%20Geist) [Shie Mannor](https://dblp.uni-trier.de/search/author?author=Shie%20Mannor)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2110.06267) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Twice regularized MDPs and the equivalence between robustness and regularization",
          "cleaned_query": "Twice regularized MDPs and the equivalence between robustness and regularization"
        },
        {
          "success": true,
          "title": "Maximum Entropy RL (Provably) Solves Some Robust RL Problems",
          "url": "https://arxiv.org/abs/2103.06257",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "Maximum entropy RL (provably) solves some robust RL problems",
          "cleaned_query": "Maximum entropy RL (provably) solves some robust RL problems"
        },
        {
          "success": true,
          "title": "State Entropy Maximization with Random Encoders for Efficient ...",
          "url": "https://arxiv.org/abs/2102.09430",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2102.09430** (cs)\n\n\\[Submitted on 18 Feb 2021 ( [v1](https://arxiv.org/abs/2102.09430v1)), last revised 21 Jun 2021 (this version, v4)\\]\n\n# Title:State Entropy Maximization with Random Encoders for Efficient Exploration\n\nAuthors: [Younggyo Seo](https://arxiv.org/search/cs?searchtype=author&query=Seo,+Y), [Lili Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+L), [Jinwoo Shin](https://arxiv.org/search/cs?searchtype=author&query=Shin,+J), [Honglak Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee,+H), [Pieter Abbeel](https://arxiv.org/search/cs?searchtype=author&query=Abbeel,+P), [Kimin Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee,+K)\n\nView a PDF of the paper titled State Entropy Maximization with Random Encoders for Efficient Exploration, by Younggyo Seo and 5 other authors\n\n[View PDF](https://arxiv.org/pdf/2102.09430)\n\n> Abstract:Recent exploration methods have proven to be a recipe for improving sample-efficiency in deep reinforcement learning (RL). However, efficient exploration in high-dimensional observation spaces still remains a challenge. This paper presents Random Encoders for Efficient Exploration (RE3), an exploration method that utilizes state entropy as an intrinsic reward. In order to estimate state entropy in environments with high-dimensional observations, we utilize a k-nearest neighbor entropy estimator in the low-dimensional representation space of a convolutional encoder. In particular, we find that the state entropy can be estimated in a stable and compute-efficient manner by utilizing a randomly initialized encoder, which is fixed throughout training. Our experiments show that RE3 significantly improves the sample-efficiency of both model-free and model-based RL methods on locomotion and navigation tasks from DeepMind Control Suite and MiniGrid benchmarks. We also show that RE3 allows learning diverse behaviors without extrinsic rewards, effectively improving sample-efficiency in downstream tasks. Source code and videos are available at [this https URL](https://sites.google.com/view/re3-rl).\n\n| | |\n| --- | --- |\n| Comments: | ICML 2021. First two authors contributed equally. Website: [this https URL](https://sites.google.com/view/re3-rl) Code: [this https URL](https://github.com/younggyoseo/RE3) |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2102.09430](https://arxiv.org/abs/2102.09430) \\[cs.LG\\] |\n| | (or [arXiv:2102.09430v4](https://arxiv.org/abs/2102.09430v4) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2102.09430](https://doi.org/10.48550/arXiv.2102.09430) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Younggyo Seo \\[ [view email](https://arxiv.org/show-email/be20dd08/2102.09430)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2102.09430v1)**\nThu, 18 Feb 2021 15:45:17 UTC (4,736 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2102.09430v2)**\nThu, 10 Jun 2021 14:01:37 UTC (8,757 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/2102.09430v3)**\nFri, 11 Jun 2021 01:01:23 UTC (8,770 KB)\n\n**\\[v4\\]**\nMon, 21 Jun 2021 04:54:56 UTC (8,770 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled State Entropy Maximization with Random Encoders for Efficient Exploration, by Younggyo Seo and 5 other authors\n\n- [View PDF](https://arxiv.org/pdf/2102.09430)\n- [TeX Source](https://arxiv.org/src/2102.09430)\n- [Other Formats](https://arxiv.org/format/2102.09430)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2102.09430&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2102.09430&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-02](https://arxiv.org/list/cs.LG/2021-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2102.09430?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2102.09430)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2102.09430)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2102.09430)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2102.html#abs-2102-09430) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2102-09430)\n\n[Lili Chen](https://dblp.uni-trier.de/search/author?author=Lili%20Chen)\n\n[Jinwoo Shin](https://dblp.uni-trier.de/search/author?author=Jinwoo%20Shin)\n\n[Honglak Lee](https://dblp.uni-trier.de/search/author?author=Honglak%20Lee)\n\n[Pieter Abbeel](https://dblp.uni-trier.de/search/author?author=Pieter%20Abbeel)\n\n[Kimin Lee](https://dblp.uni-trier.de/search/author?author=Kimin%20Lee)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2102.09430&description=State Entropy Maximization with Random Encoders for Efficient Exploration) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2102.09430&title=State Entropy Maximization with Random Encoders for Efficient Exploration)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorser",
          "original_query": "State entropy maximization with random encoders for efficient exploration",
          "cleaned_query": "State entropy maximization with random encoders for efficient exploration"
        },
        {
          "success": true,
          "title": "[1812.02690] Provably Efficient Maximum Entropy Exploration - arXiv",
          "url": "https://arxiv.org/abs/1812.02690",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1812.02690** (cs)\n\n\\[Submitted on 6 Dec 2018 ( [v1](https://arxiv.org/abs/1812.02690v1)), last revised 26 Jan 2019 (this version, v2)\\]\n\n# Title:Provably Efficient Maximum Entropy Exploration\n\nAuthors: [Elad Hazan](https://arxiv.org/search/cs?searchtype=author&query=Hazan,+E), [Sham M. Kakade](https://arxiv.org/search/cs?searchtype=author&query=Kakade,+S+M), [Karan Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh,+K), [Abby Van Soest](https://arxiv.org/search/cs?searchtype=author&query=Van+Soest,+A)\n\nView a PDF of the paper titled Provably Efficient Maximum Entropy Exploration, by Elad Hazan and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/1812.02690)\n\n> Abstract:Suppose an agent is in a (possibly unknown) Markov Decision Process in the absence of a reward signal, what might we hope that an agent can efficiently learn to do? This work studies a broad class of objectives that are defined solely as functions of the state-visitation frequencies that are induced by how the agent behaves. For example, one natural, intrinsically defined, objective problem is for the agent to learn a policy which induces a distribution over state space that is as uniform as possible, which can be measured in an entropic sense. We provide an efficient algorithm to optimize such such intrinsically defined objectives, when given access to a black box planning oracle (which is robust to function approximation). Furthermore, when restricted to the tabular setting where we have sample based access to the MDP, our proposed algorithm is provably efficient, both in terms of its sample and computational complexities. Key to our algorithmic methodology is utilizing the conditional gradient method (a.k.a. the Frank-Wolfe algorithm) which utilizes an approximate MDP solver.\n\n| | |\n| --- | --- |\n| Comments: | Updated experiment results; minor revisions in writing |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1812.02690](https://arxiv.org/abs/1812.02690) \\[cs.LG\\] |\n| (or [arXiv:1812.02690v2](https://arxiv.org/abs/1812.02690v2) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1812.02690](https://doi.org/10.48550/arXiv.1812.02690) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Karan Singh \\[ [view email](https://arxiv.org/show-email/57268e59/1812.02690)\\] **[\\[v1\\]](https://arxiv.org/abs/1812.02690v1)**\nThu, 6 Dec 2018 18:15:44 UTC (890 KB)\n**\\[v2\\]**\nSat, 26 Jan 2019 01:54:32 UTC (1,145 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Provably Efficient Maximum Entropy Exploration, by Elad Hazan and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/1812.02690)\n- [TeX Source](https://arxiv.org/src/1812.02690)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1812.02690&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1812.02690&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2018-12](https://arxiv.org/list/cs.LG/2018-12)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1812.02690?context=cs) [cs.AI](https://arxiv.org/abs/1812.02690?context=cs.AI) [stat](https://arxiv.org/abs/1812.02690?context=stat) [stat.ML](https://arxiv.org/abs/1812.02690?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1812.02690)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1812.02690)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1812.02690)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1812.html#abs-1812-02690) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1812-02690)\n\n[Elad Hazan](https://dblp.uni-trier.de/search/author?author=Elad%20Hazan) [Sham M. Kakade](https://dblp.uni-trier.de/search/author?author=Sham%20M.%20Kakade) [Karan Singh](https://dblp.uni-trier.de/search/author?author=Karan%20Singh) [Abby Van Soest](https://dblp.uni-trier.de/search/author?author=Abby%20Van%20Soest)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1812.02690) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Provably efficient maximum entropy exploration",
          "cleaned_query": "Provably efficient maximum entropy exploration"
        },
        {
          "success": true,
          "title": "Best-Effort Policies for Robust Markov Decision Processes",
          "url": "https://arxiv.org/html/2508.07790v1",
          "content": "Best-Effort Policies for Robust Markov Decision Processes\n# Best-Effort Policies for Robust Markov Decision Processes\nAlessandro Abate1,\nThom Badings1,\nGiuseppe De Giacomo1,2,\nFrancesco Fabiano1\n###### Abstract\nWe study the common generalization of Markov decision processes (MDPs) with sets of transition probabilities, known as robust MDPs (RMDPs). A standard goal in RMDPs is to compute a policy that maximizes the expected return under an adversarial choice of the transition probabilities. If the uncertainty in the probabilities is independent between the states, known asss-rectangularity, such optimal robust policies can be computed efficiently using robust value iteration. However, there might still be multiple optimal robust policies, which, while equivalent with respect to the worst-case, reflect different expected returns under non-adversarial choices of the transition probabilities. Hence, we propose a refined policy selection criterion for RMDPs, drawing inspiration from the notions of*dominance*and*best-effort*in game theory. Instead of seeking a policy that only maximizes the worst-case expected return, we additionally require the policy to achieve a*maximal*expected return under different (\\\\ienot fully adversarial) transition probabilities. We call such a policy an*optimal robust best-effort*(ORBE\\\\mathrm{ORBE}) policy. We prove thatORBE\\\\mathrm{ORBE}policies always exist, characterize their structure, and present an algorithm to compute them with a small overhead compared to standard robust value iteration.ORBE\\\\mathrm{ORBE}policies offer a principled tie-breaker among optimal robust policies. Numerical experiments show the feasibility of our approach.\n## 1Introduction\n*Markov decision processes*(MDPs) are the standard model for sequential decision making in stochastic environments and are ubiquitous in artificial intelligence (AI)> (Russell and Norvig [> 2010\n](https://arxiv.org/html/2508.07790v1#bib.bib30)> )\n, operations research> (Davis [> 2018\n](https://arxiv.org/html/2508.07790v1#bib.bib9)> )\n, control theory> (\u00c5str\u00f6m [> 2012\n](https://arxiv.org/html/2508.07790v1#bib.bib4)> )\n, and robotics> (Hanheide et\u00a0al. [> 2017\n](https://arxiv.org/html/2508.07790v1#bib.bib18)> )\n.\nWithin AI, MDPs are at the core of many model-based reinforcement learning methods> (Moerland et\u00a0al. [> 2023\n](https://arxiv.org/html/2508.07790v1#bib.bib24)> )\n.\nSolving an MDP amounts to computing a*policy*(or*strategy*) for the agent,\\\\iea mapping from states to actions, that maximizes a particular performance value, such as the expected (discounted) cumulative reward> (Puterman [> 1994\n](https://arxiv.org/html/2508.07790v1#bib.bib27)> )\n.\n#### Robust MDPs.\nA fundamental limitation of MDPs is the requirement to specify transition probabilities precisely.\nIn practice, accurately determining these probabilities can be challenging, especially if parameters are uncertain or if the model is learned from data> (Badings et\u00a0al. [> 2023b\n](https://arxiv.org/html/2508.07790v1#bib.bib6)> )\n.\nMoreover, optimal policies may be sensitive to small changes in the transition probabilities> (Mannor et\u00a0al. [> 2004\n](https://arxiv.org/html/2508.07790v1#bib.bib23)> )\n.\nTo address this issue,*robust MDPs*(RMDPs) generalize MDPs by allowing for*sets of transition probabilities*> (Iyengar [> 2005\n](https://arxiv.org/html/2508.07790v1#bib.bib20)> ; Nilim and Ghaoui [> 2005\n](https://arxiv.org/html/2508.07790v1#bib.bib26)> ; Wiesemann, Kuhn, and Rustem [> 2013\n](https://arxiv.org/html/2508.07790v1#bib.bib34)> )\n.\nThat is, instead of assigning precise probabilities between 0 and 1, the transitions in an RMDP are described by a set of feasible probabilities, called the*uncertainty set*of the RMDP.\nThe standard objective in an RMDP is to compute an*optimal robust policy*, defined as a policy that*maximizes*the expected return under the*minimizing*(\\\\ieworst-case) transition probabilities in the uncertainty set. Unfortunately, computing optimal robust policies under general uncertainty sets is NP-hard> (Wiesemann, Kuhn, and Rustem [> 2013\n](https://arxiv.org/html/2508.07790v1#bib.bib34)> )\n.\nTo ensure tractability, uncertainty sets are commonly assumed to be convex as well as independent between the states and/or actions of the RMDP, referred to as*rectangularity*of the uncertainty set.\nUnder these assumptions, optimal robust policies can be computed,\\\\egusing robust value iteration.\n#### The adversarial nature of RMDPs.\nWhen computing an optimal robust policy, the choice of transition probabilities is inherently adversarial.\nHowever, in many scenarios, the choice of transition probabilities is*not*actively working against the agent, making this assumption overly conservative.\nTake, for example, an autonomous drone flying through uncertain wind conditions.\nClearly, the wind conditions do not depend on the drone\u2019s control policy, so reasoning solely about the worst-case conditions might be too conservative.\nMoreover, multiple optimal robust policies may exist, even though their performance under non-adversarial conditions may differ.\nWe thus raise the vital question: can we compute a policy that is optimal in the worst case, but also \u201cis best\u201d when the environment does not act fully adversarially?\n#### Optimal robust best-effort policies.\nTo address the limitations of purely adversarial reasoning in RMDPs, we draw inspiration from advances in reactive stochastic games> (Aminof et\u00a0al. [> 2023\n](https://arxiv.org/html/2508.07790v1#bib.bib3)> ; Giacomo, Favorito, and Silo [> 2024\n](https://arxiv.org/html/2508.07790v1#bib.bib15)> )\n, where policies are evaluated based on their ability to succeed against sets of environment policies.\nIn this framework, a policy is deemed*winning*,*dominant*, or*best-effort*if it succeeds against*all*, the*maximum*subset, or a*maximal*subset of the environment policies, respectively.\nYet, these papers consider games where only the graph of the model is known and the probabilities are unconstrained, as opposed to RMDPs, where the uncertainty is captured by bounded sets of distributions.\nIn this paper, we leverage the concepts of dominance and best-effort to define a refined policy selection criterion for RMDPs, which we term*optimal robust best-effort*(ORBE\\\\mathrm{ORBE}).\nAnORBE\\\\mathrm{ORBE}policy satisfies two properties:(1)it achieves an optimal expected return under the*worst-case*transition probabilities in the uncertainty set; and(2)it is not dominated by any other policy,\\\\ieis best-effort.Here, one policy is said to dominate another if it performs at least as well across the entire uncertainty set and strictly better in at least one instance of the transition probabilities from the uncertainty set.\nThis best-effort perspective offers a principled tie-breaking criterion among optimal robust policies, favoring those that also achieve a maximal expected return under non-adversarial transition probabilities.\n#### Contributions and structure.\nWe introduce a novel class of policies for RMDPs, called*optimal robust best-effort*(ORBE\\\\mathrm{ORBE}) policies.\nThese policies combine the worst-case guarantees of standard robust policies with the refinement offered by best-effort reasoning, ensuring strong performance even when the environment is not fully adversarial.\nSpecifically, our key contributions are as follows:\n* \u2022We formalize the notions of dominant and best-effort policies within the context of RMDPs ([Sect.3](https://arxiv.org/html/2508.07790v1#S3)).\n* \u2022We provide a full characterization ofORBE\\\\mathrm{ORBE}policies and present an efficient algorithm to compute them with small overhead to standard robust value iteration ([Sect.5](https://arxiv.org/html/2508.07790v1#S5)).\n* \u2022We empirically demonstrate the feasibility of our techniques as a tie-breaker in robust value iteration ([Sect.6](https://arxiv.org/html/2508.07790v1#S6)).\nWe postpone a detailed discussion of related work to[Sect.7](https://arxiv.org/html/2508.07790v1#S7).\n## 2Preliminaries\nWe write\u27e8u,v\u27e9",
          "original_query": "Robust Markov decision processes",
          "cleaned_query": "Robust Markov decision processes"
        }
      ],
      "generated_ideas": [
        "1. **Adaptive Twice-Regularization for Domain-Shift Robustness**\n   - Build an online RL algorithm that tunes both policy- and value-regularization strengths in an R\u00b2-MDP based on detected transition uncertainty (e.g., discrepancy between predicted and observed next-state embeddings). Use the paper-1 equivalence to map transition-robustness into a value-dependent regularizer, and update it via dual ascent to maintain a target worst-case performance.",
        "**RE3-Driven Uncertainty Set Construction for Robust MDP Planning**\n   - Use RE3\u2019s kNN state-entropy estimator in random-encoder space to derive *data-dependent* (state-wise) confidence regions over transitions (e.g., smaller sets in high-density regions, larger sets in novel regions). Plug these uncertainty sets into robust value iteration / R\u00b2-MDP policy iteration, yielding a concrete pipeline from representation-space novelty to formal robust planning guarantees.",
        "**Frank\u2013Wolfe Policy Iteration for Twice-Regularized Robust MDPs**\n   - Combine Hazan et al.\u2019s conditional gradient approach (paper 4) with R\u00b2-MDP Bellman operators (paper 1) to avoid expensive inner minimizations over uncertainty sets. Concretely, represent visitation measures as the optimization variable, use a planning oracle to compute linearized steps, and prove iteration complexity improvements over standard robust dynamic programming when uncertainty sets are large but convex.",
        "**Optimal Robust Best-Effort (ORBE) Policies with Entropy Tie-Breaking**\n   - Extend ORBE policies (paper 5) by replacing the \u201cmaximal expected return under non-adversarial transitions\u201d criterion with a *maximum-entropy visitation* criterion (papers 3\u20134) among worst-case optimal policies. Develop an algorithm that first computes the robust-optimal set, then performs a second-stage maximum-entropy optimization over occupancy measures to select a unique, exploration-friendly ORBE policy.",
        "**Maximum-Entropy RL as an Implicit ORBE Selector**\n   - Formalize when max-entropy RL objectives (paper 2) select ORBE policies (paper 5) among multiple robust-optimal solutions, by viewing entropy regularization as a principled tie-breaker. Provide sufficient conditions on rectangular uncertainty sets and reward structure under which the maximum-entropy solution is worst-case optimal and simultaneously maximizes performance under a reference (non-adversarial) transition prior.",
        "**Value-Dependent Regularization for Transition Robustness in Deep RL**\n   - Implement the paper-1 transition-uncertainty-to-regularization reduction in a deep actor-critic by adding a *value-dependent penalty* term whose coefficient depends on estimated model error. Evaluate whether this achieves comparable robustness to explicit robust RL baselines while keeping the computational profile of standard regularized RL (no adversarial transition optimization at training time).",
        "**Robust Exploration: Optimizing State-Entropy Under Worst-Case Dynamics**\n   - Create a reward-free exploration algorithm that maximizes state entropy (papers 3\u20134) but under an adversarial choice of transitions within uncertainty sets (paper 1/5). This yields exploration policies that remain diverse even under model misspecification; empirically test whether robust-entropy exploration improves downstream task transfer when dynamics shift at deployment.",
        "**Learning ORBE Policies from Data via Regularized Robust Bellman Backups**\n   - Develop an offline RL method that estimates an RMDP uncertainty set from logged data, computes worst-case optimality, and then applies the ORBE refinement (paper 5) using R\u00b2-MDP-style regularization (paper 1) to stabilize learning. The actionable contribution is a practical algorithm that outputs a conservative-yet-not-overly-pessimistic policy, with ablations separating \u201crobustness,\u201d \u201cregularization,\u201d and \u201cbest-effort\u201d effects.",
        "**Representation-Space Robustness: Random Encoders for Robust Value Regularization**\n   - Investigate whether RE3\u2019s fixed random encoder (paper 3) can serve as a stable feature map for defining robustness/regularization terms in R\u00b2-MDPs (paper 1), reducing sensitivity to representation drift. Concretely, define uncertainty sets or regularizers in the random-feature space and test if this improves robustness and reproducibility compared to learned encoders that co-adapt with the policy.",
        "**Beyond Rectangularity: Approximating Non-Rectangular Robustness with Twice Regularization**\n   - Propose an approximation scheme for non-rectangular uncertainty sets (NP-hard in general) by fitting a sequence of state-wise rectangular sets whose induced robust objective matches a twice-regularized surrogate (paper 1). Validate the approximation by measuring the gap between the true non-rectangular worst-case value and the surrogate\u2019s value, and identify classes of coupled uncertainties where the surrogate remains tight."
      ],
      "generated_ideas_raw": "1. **Adaptive Twice-Regularization for Domain-Shift Robustness**\n   - Build an online RL algorithm that tunes both policy- and value-regularization strengths in an R\u00b2-MDP based on detected transition uncertainty (e.g., discrepancy between predicted and observed next-state embeddings). Use the paper-1 equivalence to map transition-robustness into a value-dependent regularizer, and update it via dual ascent to maintain a target worst-case performance.\n\n2. **RE3-Driven Uncertainty Set Construction for Robust MDP Planning**\n   - Use RE3\u2019s kNN state-entropy estimator in random-encoder space to derive *data-dependent* (state-wise) confidence regions over transitions (e.g., smaller sets in high-density regions, larger sets in novel regions). Plug these uncertainty sets into robust value iteration / R\u00b2-MDP policy iteration, yielding a concrete pipeline from representation-space novelty to formal robust planning guarantees.\n\n3. **Frank\u2013Wolfe Policy Iteration for Twice-Regularized Robust MDPs**\n   - Combine Hazan et al.\u2019s conditional gradient approach (paper 4) with R\u00b2-MDP Bellman operators (paper 1) to avoid expensive inner minimizations over uncertainty sets. Concretely, represent visitation measures as the optimization variable, use a planning oracle to compute linearized steps, and prove iteration complexity improvements over standard robust dynamic programming when uncertainty sets are large but convex.\n\n4. **Optimal Robust Best-Effort (ORBE) Policies with Entropy Tie-Breaking**\n   - Extend ORBE policies (paper 5) by replacing the \u201cmaximal expected return under non-adversarial transitions\u201d criterion with a *maximum-entropy visitation* criterion (papers 3\u20134) among worst-case optimal policies. Develop an algorithm that first computes the robust-optimal set, then performs a second-stage maximum-entropy optimization over occupancy measures to select a unique, exploration-friendly ORBE policy.\n\n5. **Maximum-Entropy RL as an Implicit ORBE Selector**\n   - Formalize when max-entropy RL objectives (paper 2) select ORBE policies (paper 5) among multiple robust-optimal solutions, by viewing entropy regularization as a principled tie-breaker. Provide sufficient conditions on rectangular uncertainty sets and reward structure under which the maximum-entropy solution is worst-case optimal and simultaneously maximizes performance under a reference (non-adversarial) transition prior.\n\n6. **Value-Dependent Regularization for Transition Robustness in Deep RL**\n   - Implement the paper-1 transition-uncertainty-to-regularization reduction in a deep actor-critic by adding a *value-dependent penalty* term whose coefficient depends on estimated model error. Evaluate whether this achieves comparable robustness to explicit robust RL baselines while keeping the computational profile of standard regularized RL (no adversarial transition optimization at training time).\n\n7. **Robust Exploration: Optimizing State-Entropy Under Worst-Case Dynamics**\n   - Create a reward-free exploration algorithm that maximizes state entropy (papers 3\u20134) but under an adversarial choice of transitions within uncertainty sets (paper 1/5). This yields exploration policies that remain diverse even under model misspecification; empirically test whether robust-entropy exploration improves downstream task transfer when dynamics shift at deployment.\n\n8. **Learning ORBE Policies from Data via Regularized Robust Bellman Backups**\n   - Develop an offline RL method that estimates an RMDP uncertainty set from logged data, computes worst-case optimality, and then applies the ORBE refinement (paper 5) using R\u00b2-MDP-style regularization (paper 1) to stabilize learning. The actionable contribution is a practical algorithm that outputs a conservative-yet-not-overly-pessimistic policy, with ablations separating \u201crobustness,\u201d \u201cregularization,\u201d and \u201cbest-effort\u201d effects.\n\n9. **Representation-Space Robustness: Random Encoders for Robust Value Regularization**\n   - Investigate whether RE3\u2019s fixed random encoder (paper 3) can serve as a stable feature map for defining robustness/regularization terms in R\u00b2-MDPs (paper 1), reducing sensitivity to representation drift. Concretely, define uncertainty sets or regularizers in the random-feature space and test if this improves robustness and reproducibility compared to learned encoders that co-adapt with the policy.\n\n10. **Beyond Rectangularity: Approximating Non-Rectangular Robustness with Twice Regularization**\n   - Propose an approximation scheme for non-rectangular uncertainty sets (NP-hard in general) by fitting a sequence of state-wise rectangular sets whose induced robust objective matches a twice-regularized surrogate (paper 1). Validate the approximation by measuring the gap between the true non-rectangular worst-case value and the surrogate\u2019s value, and identify classes of coupled uncertainties where the surrogate remains tight.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "1. **Adaptive Twice-Regularization for Domain-Shift Robustness**\n   - Build an online RL algorithm that tunes both policy- and value-regularization strengths in an R\u00b2-MDP based on detected transition ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "**RE3-Driven Uncertainty Set Construction for Robust MDP Planning**\n   - Use RE3\u2019s kNN state-entropy estimator in random-encoder space to derive *data-dependent* (state-wise) confidence regions over t",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "**Frank\u2013Wolfe Policy Iteration for Twice-Regularized Robust MDPs**\n   - Combine Hazan et al.\u2019s conditional gradient approach (paper 4) with R\u00b2-MDP Bellman operators (paper 1) to avoid expensive inner ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "**Optimal Robust Best-Effort (ORBE) Policies with Entropy Tie-Breaking**\n   - Extend ORBE policies (paper 5) by replacing the \u201cmaximal expected return under non-adversarial transitions\u201d criterion with",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "**Maximum-Entropy RL as an Implicit ORBE Selector**\n   - Formalize when max-entropy RL objectives (paper 2) select ORBE policies (paper 5) among multiple robust-optimal solutions, by viewing entropy r",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "**Value-Dependent Regularization for Transition Robustness in Deep RL**\n   - Implement the paper-1 transition-uncertainty-to-regularization reduction in a deep actor-critic by adding a *value-dependen",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "**Robust Exploration: Optimizing State-Entropy Under Worst-Case Dynamics**\n   - Create a reward-free exploration algorithm that maximizes state entropy (papers 3\u20134) but under an adversarial choice of ",
          "is_match": true
        },
        {
          "idea_idx": 7,
          "idea_text": "**Learning ORBE Policies from Data via Regularized Robust Bellman Backups**\n   - Develop an offline RL method that estimates an RMDP uncertainty set from logged data, computes worst-case optimality, a",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "**Representation-Space Robustness: Random Encoders for Robust Value Regularization**\n   - Investigate whether RE3\u2019s fixed random encoder (paper 3) can serve as a stable feature map for defining robust",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "**Beyond Rectangularity: Approximating Non-Rectangular Robustness with Twice Regularization**\n   - Propose an approximation scheme for non-rectangular uncertainty sets (NP-hard in general) by fitting ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 8,
      "paper_title": "On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity",
      "contribution": "Shows that the stochasticity of conditional targets is not the primary driver of generalization in flow matching: closed-form velocity targets match (and sometimes improve) performance, and generalization instead arises from the neural network's failure to perfectly approximate the optimal closed-form velocity field in particular time intervals.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 4,
      "crawl_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10438,
      "output_tokens": 1109,
      "predecessor_details": [
        {
          "success": true,
          "title": "[2210.02747] Flow Matching for Generative Modeling",
          "url": "https://arxiv.org/abs/2210.02747",
          "content": "[2210.02747] Flow Matching for Generative Modeling\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2210.02747\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2210.02747**(cs)\n[Submitted on 6 Oct 2022 ([v1](https://arxiv.org/abs/2210.02747v1)), last revised 8 Feb 2023 (this version, v2)]\n# Title:Flow Matching for Generative Modeling\nAuthors:[Yaron Lipman](https://arxiv.org/search/cs?searchtype=author&amp;query=Lipman,+Y),[Ricky T. Q. Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+R+T+Q),[Heli Ben-Hamu](https://arxiv.org/search/cs?searchtype=author&amp;query=Ben-Hamu,+H),[Maximilian Nickel](https://arxiv.org/search/cs?searchtype=author&amp;query=Nickel,+M),[Matt Le](https://arxiv.org/search/cs?searchtype=author&amp;query=Le,+M)\nView a PDF of the paper titled Flow Matching for Generative Modeling, by Yaron Lipman and 4 other authors\n[View PDF](https://arxiv.org/pdf/2210.02747)> > Abstract:\n> We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers. Subjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)|\nCite as:|[arXiv:2210.02747](https://arxiv.org/abs/2210.02747)[cs.LG]|\n|(or[arXiv:2210.02747v2](https://arxiv.org/abs/2210.02747v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2210.02747](https://doi.org/10.48550/arXiv.2210.02747)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Yaron Lipman [[view email](https://arxiv.org/show-email/77e7051e/2210.02747)]\n**[[v1]](https://arxiv.org/abs/2210.02747v1)**Thu, 6 Oct 2022 08:32:20 UTC (23,746 KB)\n**[v2]**Wed, 8 Feb 2023 15:46:05 UTC (37,148 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Flow Matching for Generative Modeling, by Yaron Lipman and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2210.02747)\n* [TeX Source](https://arxiv.org/src/2210.02747)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2210.02747&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2210.02747&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2022-10](https://arxiv.org/list/cs.LG/2022-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/2210.02747?context=cs)\n[cs.AI](https://arxiv.org/abs/2210.02747?context=cs.AI)\n[stat](https://arxiv.org/abs/2210.02747?context=stat)\n[stat.ML](https://arxiv.org/abs/2210.02747?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2210.02747)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2210.02747)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2210.02747)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2210.02747)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Flow Matching for Generative Modeling (Lipman et al., 2023)",
          "cleaned_query": "Flow Matching for Generative Modeling"
        },
        {
          "success": true,
          "title": "Score-based deterministic density sampling",
          "url": "https://arxiv.org/abs/2504.18130",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2504.18130** (cs)\n\n\\[Submitted on 25 Apr 2025 ( [v1](https://arxiv.org/abs/2504.18130v1)), last revised 20 Oct 2025 (this version, v3)\\]\n\n# Title:Score-based deterministic density sampling\n\nAuthors: [Vasily Ilin](https://arxiv.org/search/cs?searchtype=author&query=Ilin,+V), [Peter Sushko](https://arxiv.org/search/cs?searchtype=author&query=Sushko,+P), [Jingwei Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu,+J)\n\nView a PDF of the paper titled Score-based deterministic density sampling, by Vasily Ilin and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2504.18130) [HTML (experimental)](https://arxiv.org/html/2504.18130v3)\n\n> Abstract:We propose a deterministic sampling framework using Score-Based Transport Modeling for sampling an unnormalized target density $\\\\pi$ given only its score $\\\\nabla \\\\log \\\\pi$. Our method approximates the Wasserstein gradient flow on $\\\\mathrm{KL}(f\\_t\\\\\\|\\\\pi)$ by learning the time-varying score $\\\\nabla \\\\log f\\_t$ on the fly using score matching. While having the same marginal distribution as Langevin dynamics, our method produces smooth deterministic trajectories, resulting in monotone noise-free convergence. We prove that our method dissipates relative entropy at the same rate as the exact gradient flow, provided sufficient training. Numerical experiments validate our theoretical findings: our method converges at the optimal rate, has smooth trajectories, and is often more sample efficient than its stochastic counterpart. Experiments on high-dimensional image data show that our method produces high-quality generations in as few as 15 steps and exhibits natural exploratory behavior. The memory and runtime scale linearly in the sample size.\n\n| | |\n| --- | --- |\n| Comments: | 13 pages, 2 tables, 11 figures. Key words: Deterministic sampling; score-based transport modeling; Wasserstein gradient flow; relative entropy; Fisher information; annealing; neural network; neural tangent kernel |\n| Subjects: | Machine Learning (cs.LG); Probability (math.PR); Statistics Theory (math.ST) |\n| MSC classes: | Primary 65C05, 35Q84, 49Q22, Secondary 60H10, 68T07 |\n| Cite as: | [arXiv:2504.18130](https://arxiv.org/abs/2504.18130) \\[cs.LG\\] |\n| (or [arXiv:2504.18130v3](https://arxiv.org/abs/2504.18130v3) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2504.18130](https://doi.org/10.48550/arXiv.2504.18130) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Vasily Ilin \\[ [view email](https://arxiv.org/show-email/6c6200fc/2504.18130)\\] **[\\[v1\\]](https://arxiv.org/abs/2504.18130v1)**\nFri, 25 Apr 2025 07:33:16 UTC (1,469 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2504.18130v2)**\nSat, 17 May 2025 00:37:55 UTC (3,021 KB)\n**\\[v3\\]**\nMon, 20 Oct 2025 17:37:19 UTC (3,753 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Score-based deterministic density sampling, by Vasily Ilin and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2504.18130)\n- [HTML (experimental)](https://arxiv.org/html/2504.18130v3)\n- [TeX Source](https://arxiv.org/src/2504.18130)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2504.18130&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2504.18130&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-04](https://arxiv.org/list/cs.LG/2025-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2504.18130?context=cs) [math](https://arxiv.org/abs/2504.18130?context=math) [math.PR](https://arxiv.org/abs/2504.18130?context=math.PR) [math.ST](https://arxiv.org/abs/2504.18130?context=math.ST) [stat](https://arxiv.org/abs/2504.18130?context=stat) [stat.TH](https://arxiv.org/abs/2504.18130?context=stat.TH)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2504.18130)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2504.18130)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2504.18130)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2504.18130) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Score-Based Generative Modeling / SDE View (Song et al., 2021)",
          "cleaned_query": "Score-Based Generative Modeling"
        },
        {
          "success": true,
          "title": "Foundation Flow - Build Highly Secure, Efficient & Reliable FMs",
          "url": "https://www.foundationflow.ai/",
          "content": "[Skip to content](https://foundationflow.ai/foundationflow.ai#content)\n\nWelcome to Foundation Flow\n\n# Build Highly Secure, Efficient & Reliable FMs for your Enterprise\n\nPower your business with Closed, OpenSource or your own proprietary LLMs to leverage GenerativeAI capabilities today!\n\n[Request a Demo](https://foundationflow.ai/foundationflow.ai#demo)\n\nAbout Us\n\n## Everything AI, Streamlined with FMOps\n\nFoundationFlow is a comprehensive software and orchestration platform for enterprise generative ai applications. We abstract the complexity for a semi technical user without compromising on technical features for an advanced user to manage an end to end pipeline of foundational model training, fine tuning and enterprise focused use case agent creation. With on-prem, hybrid and cloud native options, we ensure compliance with the enterprise\u2019s data laws ensuring full data security and privacy.\n\n## Partners\n\nBud Ecosystem\n\nOUR OFFERINGS\n\n## Power Up Using State-of-art-features\n\nBoost your FMOps journey with our state-of-the-art features, trusted by industry leaders in High-Performance Computing and global brands.\n\n01.\n\n### Multi-Modal Generative & Foundational Model zoo\n\nOur platform offers a unique blend of traditional Language Learning Models (LLMs), Multi-Modal LLMs, and cutting-edge Diffusion and GAN-based models. This combination ensures that you have the best tools at your disposal, making your AI journey seamless and impactful\n\n02.\n\n### Data Management\n\nExplore our vast repository, the largest collection of curated public datasets tailored for QA, Reasoning, Tool usage, and more. But that\u2019s just the beginning. Our platform goes the extra mile with advanced data pre-processing techniques. From Toxicity Analysis and Deduplication to Enrichment, Tokenisation, and Templating, we ensure your data is primed and ready, setting the gold standard for AI-driven insights and solutions.\n\n03.\n\n### Training & Fine-Tuning\n\nOur platform is designed for those who seek the best in AI training. From the foundational stages of Pre-Training to the advanced techniques of RLHF, we leverage the prowess of optimization giants like DeepSpeed and Colossal AI. We\u2019ve fine-tuned our training processes against key metrics \u2013 Data (DEFT), Compute (PEFT), Memory (PEFT), and Bandwidth. And to top it off, our Experiment Tracking and Evaluation tools provide clarity and insights, ensuring your AI projects achieve their fullest potential.\n\n04.\n\n### Evaluation\n\nNavigate the AI landscape with confidence using our specialized evaluation benchmarks for tasks like Reasoning, FID, and QA. Safeguard your models against biases with our in-depth Toxicity Analysis. And when it comes to visual content, our Image Aesthetic score offers a unique perspective on image quality, ensuring your visuals always make an impact.\n\n05.\n\n### Compliance & Privacy management\n\nNavigate the AI realm with our state-of-the-art Model Editing capabilities. Prioritize data security and user trust with our Personal Data Detection & Removal tool. Uphold content quality standards with our Inappropriate Content Flagging & Removal feature. Boost model alignment with our RLHF-based techniques, and achieve unparalleled precision with our Custom Critics, ensuring your AI is always at its best.\n\n06.\n\n### Prompt Engineering\n\nUnlock the full potential of AI with our Retrieval Augmentation feature, seamlessly connecting to browsers, the Internet, Vector DB, and various databases. Streamline your AI tasks with our Prompt Templating system, and ensure optimal results with our innovative Prompt Template-based Routing, designed for precision and efficiency.\n\n07.\n\n### Inference\n\nHarness the power of advanced AI with our Model Pruning and Model Quantisation capabilities. Our platform is designed to support the Distillation process, and with innovative Custom Attention Mechanisms such as PagedAttention & FlashAttention, throughput is significantly enhanced. Experience the flexibility of our horizontally scalable model serving, and achieve optimal results with our Batch Inference feature, optimized for both high and low batch scenarios with tiled kernel delivery. Integration is a breeze with support for DeepSpeed MII, FastChat, and BentoML.\n\n08.\n\n### Agent Development\n\nExperience the ease of AI development with our Drag & Drop Agent development feature, designed for both novices and experts. Deploying your AI agent is just a click away with our One-Click Agent Deployment. Plus, our platform offers seamless support to external tools, memory, models, and 3rd party APIs, ensuring you have all the resources you need for a comprehensive AI solution.\n\nOur Applications\n\n## Industries\n\n### Ecommerce\n\n### Finance & Insurance\n\n### Software & Technology\n\n### Manufacturing\n\n### Education\n\n### Government\n\n### Media & Design\n\nWhy Choose Us\n\n## Why leading enterprises leverage FoundationFlow?\n\nAs enterprises increasingly adopt AI and machine learning (ML) to drive business value, there is a growing need for streamlined and efficient LLM (large language model) workflows designed with the business user in mind.\n\n### Simple Drag & Drop Model Editor\n\nUnlock the full potential of Foundational Models with the FoundationFlow FMOps platform. Designed for businesses of all sizes, our platform streamlines the end-to-end lifecycle management of these models. Even without technical expertise, users can effortlessly pre-train, fine-tune, and engage in advanced in-context learning. Plus, with our user-friendly drag-and-drop interface, deploying these models into scalable APis has never been faster or easier. Transform your Al operations from months to minutes with FoundationFlow.\n\n### Seamless Integration and Collaboration\n\nThe FMOps platform in FoundationFlow fosters collaboration between data scientists, engineers, and operational teams. With a unified platform, teams can work cohesively, ensuring that models are not only accurate but also aligned with business objectives and easily integrated into existing systems.\n\n### Full lifecycle management of foundational models\n\nFoundationFlow\u2019s FMOps platform accelerates the deployment process, enabling businesses to swiftly move from model development to production. This rapid transition ensures that innovations are implemented promptly, giving enterprises a competitive edge in the market.\n\nSTATISTICS\n\n## Everything AI, Streamlined with FMOps\n\nDeliver high performant AI systems at scale while achieving cost efficiency using FoundationFlow FMOps to propel your business ahead of your competitors!\n\nHow it works\n\n## Unparalleled Capabilities in FMOps\n\nWe provide a diverse suite of FMOps tools to empower your AI initiatives, ensuring you connect with your objectives and attain unparalleled success.\n\n01.\n\n### State-of-the-Art FMOps Functionality\n\nFoundationFlow supports a world class data management system, state of the art pre-training and fine-tuning strategies and an ever growing model zoo. It also provides support for low bandwidth optimized training to support your business demands.\n\n02.\n\n### Customisable FMOps Features\n\nWe underscore the adaptability of our FMOps platform, empowering you to tailor your AI operations and model management in alignment with your distinct objectives and aspirations.\n\n03.\n\n### Reliable FMOps Support, Day and Night\n\nOur dedicated team of AI professionals stands ready to offer tailored guidance and insights, assist in navigating FMOps challenges, and ensure your model management objectives are realised to their fullest potential.\n\nOur Team\n\n## Tech with Tech & Industry Expertise\n\n##### Shameer Thaha\n\nCEO\n\n17+ years in management and 2 exits\n\nEx-Chief Strategy Officer at Accubits\n\nSP Jain School of Global Management\n\n##### Shubhra Vatsal\n\nCOO\n\n15+ Years BFSI Product Leadership\n\nMS \u2013 Electronics & Telecommunications\n\nUniversity of Maryland\n\nFAQs\n\n## Frequently Asked Questions\n\nGet Started\n\nAre you set to revolutionize your AI operations?\n\n## Let's Get Started on Your Digital Transformation Journ",
          "original_query": "Foundational flow/transport theory for conditional velocity fields (Albergo & Vanden-Eijnden, 2023)",
          "cleaned_query": "Foundational flow",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "Provable Separations between Memorization and Generalization in ...",
          "url": "https://arxiv.org/html/2511.03202v1",
          "content": "Provable Separations between Memorization and Generalization in Diffusion Models\n# Provable Separations between Memorization and Generalization in Diffusion Models\nZeqi Ye111Department of Industrial Engineering and Management Sciences, Northwestern University.zeqiye2029@u.northwestern.edu, minshuo.chen@northwestern.eduQijie Zhu222Department of Statistics and Data Science, Northwestern University.qijiezhu2029@u.northwestern.eduMolei Tao333School of Mathematics, Georgia Institute of Technology.mtao@gatech.eduMinshuo Chen111Department of Industrial Engineering and Management Sciences, Northwestern University.zeqiye2029@u.northwestern.edu, minshuo.chen@northwestern.edu\n###### Abstract\nDiffusion models have achieved remarkable success across diverse domains, but they remain vulnerable to memorization\u2014reproducing training data rather than generating novel outputs. This not only limits their creative potential but also raises concerns about privacy and safety. While empirical studies have explored mitigation strategies, theoretical understanding of memorization remains limited. We address this gap through developing a dual-separation result via two complementary perspectives: statistical estimation and network approximation. From the estimation side, we show that the ground-truth score function does not minimize the empirical denoising loss, creating a separation that drives memorization. From the approximation side, we prove that implementing the empirical score function requires network size to scale with sample size, spelling a separation compared to the more compact network representation of the ground-truth score function. Guided by these insights, we develop a pruning-based method that reduces memorization while maintaining generation quality in diffusion transformers.\n## 1Introduction\nDiffusion models have emerged as one of the most powerful families of generative models, achieving state-of-the-art performance across a wide range of tasks> (Song &amp; Ermon, [> 2019\n](https://arxiv.org/html/2511.03202v1#bib.bib53)> ; Ho et\u00a0al., [> 2020\n](https://arxiv.org/html/2511.03202v1#bib.bib18)> ; Song et\u00a0al., [> 2020a\n](https://arxiv.org/html/2511.03202v1#bib.bib52)> , [> b\n](https://arxiv.org/html/2511.03202v1#bib.bib55)> ; Kong et\u00a0al., [> 2020\n](https://arxiv.org/html/2511.03202v1#bib.bib27)> ; Mittal et\u00a0al., [> 2021\n](https://arxiv.org/html/2511.03202v1#bib.bib36)> ; Jeong et\u00a0al., [> 2021\n](https://arxiv.org/html/2511.03202v1#bib.bib23)> ; Huang et\u00a0al., [> 2022\n](https://arxiv.org/html/2511.03202v1#bib.bib20)> ; Avrahami et\u00a0al., [> 2022\n](https://arxiv.org/html/2511.03202v1#bib.bib3)> ; Ulhaq &amp; Akhtar, [> 2022\n](https://arxiv.org/html/2511.03202v1#bib.bib58)> )\n. Applications span image synthesis> (Nichol et\u00a0al., [> 2021\n](https://arxiv.org/html/2511.03202v1#bib.bib39)> ; Yang et\u00a0al., [> 2024\n](https://arxiv.org/html/2511.03202v1#bib.bib73)> )\n, molecular design> (Weiss et\u00a0al., [> 2023\n](https://arxiv.org/html/2511.03202v1#bib.bib68)> ; Guo et\u00a0al., [> 2024\n](https://arxiv.org/html/2511.03202v1#bib.bib16)> )\n, and time-series modeling> (Tashiro et\u00a0al., [> 2021\n](https://arxiv.org/html/2511.03202v1#bib.bib57)> ; Alcaraz &amp; Strodthoff, [> 2022\n](https://arxiv.org/html/2511.03202v1#bib.bib1)> )\n, where diffusion models consistently generate samples of high fidelity. Their remarkable empirical success has established them as a leading paradigm in modern generative modeling.\nDespite these advances, diffusion models have raised critical concerns. A central one is memorization, where trained models reproduce training data instead of generating genuinely novel samples> (Gu et\u00a0al., [> 2023\n](https://arxiv.org/html/2511.03202v1#bib.bib15)> ; Stein et\u00a0al., [> 2023\n](https://arxiv.org/html/2511.03202v1#bib.bib56)> ; Webster, [> 2023\n](https://arxiv.org/html/2511.03202v1#bib.bib66)> ; Kadkhodaie et\u00a0al., [> 2023\n](https://arxiv.org/html/2511.03202v1#bib.bib25)> ; Rahman et\u00a0al., [> 2025\n](https://arxiv.org/html/2511.03202v1#bib.bib45)> ; Chen et\u00a0al., [> 2024\n](https://arxiv.org/html/2511.03202v1#bib.bib8)> )\n. Such behavior undermines the creative potential of generative modeling and threatens the promise of generalization> (Somepalli et\u00a0al., [> 2023\n](https://arxiv.org/html/2511.03202v1#bib.bib51)> ; Carlini et\u00a0al., [> 2023\n](https://arxiv.org/html/2511.03202v1#bib.bib6)> )\n. Memorization also leads to serious risks for data privacy and intellectual property, as training datasets may include copyrighted works or sensitive information> (Ghalebikesabi et\u00a0al., [> 2023\n](https://arxiv.org/html/2511.03202v1#bib.bib14)> ; Cui et\u00a0al., [> 2023\n](https://arxiv.org/html/2511.03202v1#bib.bib10)> ; Vyas et\u00a0al., [> 2023\n](https://arxiv.org/html/2511.03202v1#bib.bib63)> )\n.\nA growing body of research has attempted to characterize and mitigate memorization in diffusion models. Empirical studies have explored its correlation with data duplication, training procedure, and model architecture and capacity> (Somepalli et\u00a0al., [> 2023\n](https://arxiv.org/html/2511.03202v1#bib.bib51)> ; Gu et\u00a0al., [> 2023\n](https://arxiv.org/html/2511.03202v1#bib.bib15)> ; Stein et\u00a0al., [> 2023\n](https://arxiv.org/html/2511.03202v1#bib.bib56)> )\n, and proposed defenses such as dataset de-duplication, modified training objectives, or improved sampling strategies> (Wen et\u00a0al., [> 2024\n](https://arxiv.org/html/2511.03202v1#bib.bib69)> ; Ross et\u00a0al., [> 2024\n](https://arxiv.org/html/2511.03202v1#bib.bib49)> ; Wang et\u00a0al., [> 2024\n](https://arxiv.org/html/2511.03202v1#bib.bib64)> )\n. These methods provide valuable heuristics yet leave principles underneath their success underexplored. In parallel, theoretical investigations have begun to analyze memorization from a statistical perspective. For instance, asymptotic analyses, where both sample size and data dimension grow proportionally, have provided insights into the interplay between data availability, model complexity, and generalization> (Raya &amp; Ambrogioni, [> 2023\n](https://arxiv.org/html/2511.03202v1#bib.bib46)> ; Biroli et\u00a0al., [> 2024\n](https://arxiv.org/html/2511.03202v1#bib.bib4)> ; George et\u00a0al., [> 2025\n](https://arxiv.org/html/2511.03202v1#bib.bib13)> )\n. However, these analyses do not fully explain memorization in practical, finite-sample regimes, leaving open a fundamental question:\nCan we disentangle memorization from generalization in practical regimes and mitigate it?\nIn this work, we take a step toward addressing this question. We develop a non-asymptotic analysis that theoretically explains the emergence of memorization through the dual lenses of statistical estimation and neural function approximation. Our analysis reveals that memorization is fundamentally tied to the statistical properties of the training objective\u2014the denoising score matching loss, and the approximation capacity of score neural networks. More specifically, from the statistical estimation side, we show that the ground-truth score function does not minimize the empirical denoising score matching loss, leading to an inherent gap that drives memorization. From the approximation side, we establish results demonstrating that the empirical score function demands network size scaling with the sample size, whereas the ground-truth score admits a compact representation. Guided by these insights, we explore empirical consequences and mitigation strategies. Our experiments not only validate the theories but also introduce a pruning-based method that reduces memorization while maintaining generation quality for diffusion transformers.\nOur contributions are summarized as follows.\n\u2219\\\\bulletStatistical separation theory: We show that the denoising score matching loss admits an inherent gap between the ground-truth score function and the empirical score function (Proposition[4.1](https://arxiv.org/html/2511.03202v1#S4.Thmtheorem1)). Furthermore, for mixture models, we provide a lower bound on the gap in Theorem[4.3](https://arxiv.org/html/2511.03202v1#S4.Thmtheorem3), which provi",
          "original_query": "Empirical study of memorization vs generalization in diffusion models (Kadkhodaie et al., 2024)",
          "cleaned_query": "Empirical study of memorization vs generalization in diffusion models"
        },
        {
          "success": true,
          "title": "Enhancing Noise-Robust Losses for Large-Scale Noisy Data Learning",
          "url": "https://arxiv.org/html/2306.05497v3",
          "content": "# Enhancing Noise-Robust Losses for Large-Scale Noisy Data Learning\n\nMax Staats 1,2 ,\nMatthias Thamm 2,\nBernd Rosenow 2Staats is the corresponding author.\n\n###### Abstract\n\nLarge annotated datasets inevitably contain noisy labels, which poses a major challenge for training deep neural networks as they easily memorize the labels.\nNoise-robust loss functions have emerged as a notable strategy to counteract this issue, but it remains challenging to create a robust loss function which is not susceptible to underfitting.\nThrough a quantitative approach, this paper explores the limited overlap between the network output at initialization and regions of non-vanishing gradients of bounded loss functions in the initial learning phase.\nUsing these insights, we address underfitting of several noise robust losses with a novel method denoted as _logit bias_,\nwhich adds a real number \u03f5italic-\u03f5\\\\epsilonitalic\\_\u03f5 to the logit at the position of the correct class.\nThe _logit bias_ enables these losses to achieve state-of-the-art results, even on datasets like WebVision, consisting of over a million images from 1000 classes.\nIn addition, we demonstrate that our method can be used to determine\noptimal parameters for several loss functions\n\u2013 without having to train networks.\nRemarkably, our method determines\nthe hyperparameters based on the number of classes, resulting in loss functions which require zero dataset or noise-dependent parameters.\n\n## 1 Introduction\n\nSupervised deep learning depends on high-quality labeled data for effective pattern recognition and accurate predictions (Goodfellow, Bengio, and Courville [2016](https://arxiv.org/html/2306.05497v3#bib.bib9)).\nIn real-world datasets, however, there is often label noise - erroneous or unclear labels due to human error or incomplete annotations (Liang, Liu, and Yao [2022](https://arxiv.org/html/2306.05497v3#bib.bib20)). Such noise can drastically impair the effectiveness of deep learning models, which often operate under the assumption of pristine labels (Song et\u00a0al. [2022](https://arxiv.org/html/2306.05497v3#bib.bib27)). Therefore, it is important to develop robust deep-learning algorithms that can efficiently learn from noisy datasets.\n\nOne effective approach to navigate label noise lies in employing noise-robust loss functions. These loss functions, notable for their model-agnostic nature, seamlessly integrate with any deep learning paradigm.\nThe existing literature highlights their ability to improve the robustness and generalization ability of deep learning models under noisy conditions\n(Ghosh, Kumar, and Sastry [2017](https://arxiv.org/html/2306.05497v3#bib.bib6); Zhang and Sabuncu [2018](https://arxiv.org/html/2306.05497v3#bib.bib41); Wang et\u00a0al. [2019](https://arxiv.org/html/2306.05497v3#bib.bib32); Amid et\u00a0al. [2019](https://arxiv.org/html/2306.05497v3#bib.bib1); Ma et\u00a0al. [2020](https://arxiv.org/html/2306.05497v3#bib.bib22); Zhou et\u00a0al. [2021](https://arxiv.org/html/2306.05497v3#bib.bib43); Englesson and Azizpour [2021](https://arxiv.org/html/2306.05497v3#bib.bib4)).\n\nA majority of these loss functions are bounded to prevent the learning of mislabeled examples. From a theoretical point of view, bounded losses have a higher robustness to noise if they belong to the class of symmetric losses (Ghosh, Kumar, and Sastry [2017](https://arxiv.org/html/2306.05497v3#bib.bib6)). Nonetheless, it has been suggested that such symmetry could be overly constraining (Zhou et\u00a0al. [2021](https://arxiv.org/html/2306.05497v3#bib.bib43)), with functions like the Mean Absolute Error (MAE) leaning towards underfitting.\nReflecting this, many contemporary loss functions do not satisfy this symmetry condition (Zhou et\u00a0al. [2021](https://arxiv.org/html/2306.05497v3#bib.bib43); Englesson and Azizpour [2021](https://arxiv.org/html/2306.05497v3#bib.bib4)).\n\nIn this paper, we quantitatively explore how the vanishing derivatives of bounded loss functions impact their learning behavior.\nAccording to our findings, the cause of underfitting is the limited overlap between the output values of an initialized network and the region where the derivative of a particular bounded loss function is nonzero.\nTo counteract this, we add a real number, \u03f5italic-\u03f5\\\\epsilonitalic\\_\u03f5, to the logit corresponding to the correct class label. This subtle adjustment restores the overlap between network outputs and the region of sufficiently large derivatives of the loss, enabling e.g. the MAE loss to surpass the Cross Entropy loss on datasets like Cifar-100, even in the absence of label noise.\nImpressively, this approach requires only a single constant \u03f5italic-\u03f5\\\\epsilonitalic\\_\u03f5, which is determined by the number of classes, providing an effectively parameter-free method. Other loss functions like the generalized cross entropy (genCE) (Zhang and Sabuncu [2018](https://arxiv.org/html/2306.05497v3#bib.bib41)) and NF-MAE (a combination of normalized focal loss (NF) with MAE) (Ma et\u00a0al. [2020](https://arxiv.org/html/2306.05497v3#bib.bib22)) are also able to learn the WebVision dataset with the help of the logit bias.\n\nFurthermore, our description of the early learning phase enables us to calculate suitable hyperparameters for other loss functions like genCE and NCE-AGCE (a combination of Normalized Cross Entropy and Asymmetric Generalized Cross Entropy) (Zhou et\u00a0al. [2021](https://arxiv.org/html/2306.05497v3#bib.bib43)), allowing them to show their potential for an arbitrary number of classes\nwithout the need of tuning their parameters first, e.g., by an expensive grid search. The method we propose\nis intended as a first step towards a universal framework that is capable of noise robust learning across varied numbers of classes\nwithout requiring hyperparameter fine-tuning. The need for such a method is underscored by our observation that none of the proposed loss functions that are noise resistant on the Cifar-10 dataset are capable of learning the WebVision dataset.\n\nIn summary, our paper\n(i) quantitatively describes how the initial learning phase of a newly initialized network is contingent upon the dataset\u2019s class count; (ii) explores the limitations of bounded losses in multi-class datasets and introduces the _logit bias_ technique, enabling MAE to consistently deliver competitive or even superior results across benchmarks like Fashion-MNIST, Cifar-10, Cifar-100, and WebVision \u2013 without hyperparameters; (iii) enables other noise-robust loss functions to learn the WebVision dataset using either the logit bias or by determining their hyperparameters without training a single network.\n\nAll code for reproducing the data and creating the figures in this paper is open source and available under (Author [s](https://arxiv.org/html/2306.05497v3#bib.bib2)).\n\n## 2 Related Work\n\nLabel noise in training data is a pervasive challenge that has attracted much attention in recent years (Liang, Liu, and Yao [2022](https://arxiv.org/html/2306.05497v3#bib.bib20); Song et\u00a0al. [2022](https://arxiv.org/html/2306.05497v3#bib.bib27)). One strategy for addressing it is data cleaning, aiming to filter out mislabeled samples from the training dataset.\nTo identify noisy instances, (Xiao et\u00a0al. [2015](https://arxiv.org/html/2306.05497v3#bib.bib36)) employs a probabilistic model to capture the relationship between images, labels, and noise. Other approaches utilize an auxiliary neural network, trained on curated data, to clean the main dataset (Veit et\u00a0al. [2017](https://arxiv.org/html/2306.05497v3#bib.bib31); Lee et\u00a0al. [2018](https://arxiv.org/html/2306.05497v3#bib.bib18)). Yet, an overzealous curation can sometimes be counterproductive, as eliminating too many samples might degrade model performance (Khetan, Lipton, and Anandkumar [2017](https://arxiv.org/html/2306.05497v3#bib.bib15)), compared to retaining some corrupted instances.\n\nAnother approach is estimating the noise transition matrix, which depicts the likelihood of mislabeling across classes. This matrix can be incorporate",
          "original_query": "Noisy training-loss explanation for generalization (Vastola, 2025)",
          "cleaned_query": "Noisy training-loss explanation for generalization"
        }
      ],
      "generated_ideas": [
        "Memorization-Aware Flow Matching via Capacity-Regularized Vector Fields\nIncorporate the separation results on memorization vs. generalization into Flow Matching by adding an explicit capacity penalty on the learned vector field (e.g., NTK-norm or spectral complexity proxy) and testing whether it prevents the empirical-score \u201cpull\u201d toward training-point attractors. Implement and evaluate on diffusion-transformer backbones, comparing memorization metrics (nearest-neighbor reproduction, canary exposure) under equal FID/likelihood.",
        "Pruning-Integrated Flow Matching for CNFs (Structured Vector-Field Sparsification)\nExtend the pruning-based mitigation from diffusion transformers to Flow Matching CNFs by learning with structured sparsity constraints (channel/head pruning schedules) and then pruning mid-training while preserving path-consistency. The contribution is an actionable pipeline that jointly optimizes generation quality and provable/empirical memorization reduction, with ablations over probability paths (diffusion vs. OT displacement interpolation).",
        "Optimal-Transport Path Selection Under Privacy Constraints (OT-FM with DP Budgeting)\nInvestigate how the choice of conditional probability path (diffusion vs. OT displacement interpolation vs. learned paths) affects memorization/privacy leakage, motivated by the theory that empirical objectives can promote memorization. Concretely, define a privacy-aware objective that penalizes training-sample influence (e.g., via Jacobian-based influence or membership-inference score) and search over path families to find those that minimize leakage at fixed likelihood/FID.",
        "Logit-Bias Analogues for Score/Flow Training to Reduce Underfitting in Noisy Data Regimes\nTranslate the \u201clogit bias\u201d idea to generative training by introducing a simple, class-count- or dataset-size-calibrated bias in conditional score/flow prediction heads (e.g., adding a constant offset to conditional embeddings or to per-class guidance logits) to increase early gradient overlap. Test on class-conditional FM/diffusion with synthetic label noise and WebVision-like noisy conditioning to see if it prevents collapse/underfitting without tuning noise-dependent hyperparameters.",
        "Noise-Robust Flow Matching Objectives for Corrupted Conditioning and Weak Labels\nDesign bounded, noise-robust regression losses for vector-field matching (analogous to bounded classification losses) that downweight mislabeled or corrupted conditional information while avoiding underfitting using an FM-specific \u201cbias-to-gradient-overlap\u201d calibration. Implement on text-to-image or class-conditional ImageNet with intentionally corrupted prompts/labels, measuring robustness and sample diversity.",
        "Adaptive Probability Paths Learned from Deterministic KL Dissipation\nPropose a method that learns a family of conditional probability paths by optimizing for fast deterministic KL dissipation (from the Wasserstein gradient flow perspective) while maintaining FM trainability (simulation-free regression targets). Concretely, parameterize the interpolation path (noise schedule + drift form) and train it jointly with the vector field to minimize steps-to-target under deterministic sampling, comparing against fixed diffusion and OT paths.",
        "Enterprise-Grade \u201cCompliance-by-Design\u201d Flow Models: Automated Memorization Audits + Model Editing\nInspired by the FMOps platform framing, build an end-to-end workflow for FM/diffusion deployment that automatically runs memorization audits (canaries, membership inference, nearest-neighbor retrieval) and then applies targeted mitigations (structured pruning, selective unlearning, or vector-field editing) with tracked guarantees. The research contribution is a reproducible benchmark and toolchain showing which interventions preserve likelihood/FID while meeting explicit privacy/compliance thresholds.",
        "Trajectory Smoothness as a Control Knob: Linking Deterministic Sampling to Memorization Dynamics\nTest the hypothesis that smoother deterministic trajectories (as in deterministic density sampling) reduce training-data \u201csnapping\u201d compared to stochastic samplers by systematically controlling trajectory curvature/roughness in FM/CNF sampling (e.g., regularizing higher derivatives of the flow or using curvature-penalized solvers). Provide empirical and theoretical analysis connecting trajectory smoothness, entropy dissipation, and memorization metrics, yielding actionable sampler/training recommendations."
      ],
      "generated_ideas_raw": "1. **Memorization-Aware Flow Matching via Capacity-Regularized Vector Fields**  \n   Incorporate the separation results on memorization vs. generalization into Flow Matching by adding an explicit capacity penalty on the learned vector field (e.g., NTK-norm or spectral complexity proxy) and testing whether it prevents the empirical-score \u201cpull\u201d toward training-point attractors. Implement and evaluate on diffusion-transformer backbones, comparing memorization metrics (nearest-neighbor reproduction, canary exposure) under equal FID/likelihood.\n\n2. **Pruning-Integrated Flow Matching for CNFs (Structured Vector-Field Sparsification)**  \n   Extend the pruning-based mitigation from diffusion transformers to Flow Matching CNFs by learning with structured sparsity constraints (channel/head pruning schedules) and then pruning mid-training while preserving path-consistency. The contribution is an actionable pipeline that jointly optimizes generation quality and provable/empirical memorization reduction, with ablations over probability paths (diffusion vs. OT displacement interpolation).\n\n3. **Deterministic Wasserstein-Gradient-Flow Sampling for Flow-Matched CNFs**  \n   Combine \u201cscore-based deterministic density sampling\u201d with Flow Matching by learning \\(\\nabla \\log f_t\\) online along FM trajectories and using deterministic transport updates as an alternative ODE solver stepper. This yields a hybrid sampler that remains deterministic and monotone in KL while leveraging FM-trained vector fields, targeting high-quality generations in very few steps (e.g., 10\u201320) and benchmarking against standard CNF ODE solvers.\n\n4. **Optimal-Transport Path Selection Under Privacy Constraints (OT-FM with DP Budgeting)**  \n   Investigate how the choice of conditional probability path (diffusion vs. OT displacement interpolation vs. learned paths) affects memorization/privacy leakage, motivated by the theory that empirical objectives can promote memorization. Concretely, define a privacy-aware objective that penalizes training-sample influence (e.g., via Jacobian-based influence or membership-inference score) and search over path families to find those that minimize leakage at fixed likelihood/FID.\n\n5. **Logit-Bias Analogues for Score/Flow Training to Reduce Underfitting in Noisy Data Regimes**  \n   Translate the \u201clogit bias\u201d idea to generative training by introducing a simple, class-count- or dataset-size-calibrated bias in conditional score/flow prediction heads (e.g., adding a constant offset to conditional embeddings or to per-class guidance logits) to increase early gradient overlap. Test on class-conditional FM/diffusion with synthetic label noise and WebVision-like noisy conditioning to see if it prevents collapse/underfitting without tuning noise-dependent hyperparameters.\n\n6. **Noise-Robust Flow Matching Objectives for Corrupted Conditioning and Weak Labels**  \n   Design bounded, noise-robust regression losses for vector-field matching (analogous to bounded classification losses) that downweight mislabeled or corrupted conditional information while avoiding underfitting using an FM-specific \u201cbias-to-gradient-overlap\u201d calibration. Implement on text-to-image or class-conditional ImageNet with intentionally corrupted prompts/labels, measuring robustness and sample diversity.\n\n7. **Generalization Certificates for Flow Matching via Empirical-vs-Oracle Field Gap Estimation**  \n   Build a practical estimator of the gap between the \u201cground-truth\u201d vector field (or score) and the empirical minimizer implied by finite data\u2014mirroring the paper-4 separation argument\u2014using held-out denoising residuals and local neighborhood tests. The outcome is a diagnostic that predicts memorization risk during training and can trigger interventions (pruning, early stopping, path change) before leakage manifests.\n\n8. **Adaptive Probability Paths Learned from Deterministic KL Dissipation**  \n   Propose a method that learns a family of conditional probability paths by optimizing for fast deterministic KL dissipation (from the Wasserstein gradient flow perspective) while maintaining FM trainability (simulation-free regression targets). Concretely, parameterize the interpolation path (noise schedule + drift form) and train it jointly with the vector field to minimize steps-to-target under deterministic sampling, comparing against fixed diffusion and OT paths.\n\n9. **Enterprise-Grade \u201cCompliance-by-Design\u201d Flow Models: Automated Memorization Audits + Model Editing**  \n   Inspired by the FMOps platform framing, build an end-to-end workflow for FM/diffusion deployment that automatically runs memorization audits (canaries, membership inference, nearest-neighbor retrieval) and then applies targeted mitigations (structured pruning, selective unlearning, or vector-field editing) with tracked guarantees. The research contribution is a reproducible benchmark and toolchain showing which interventions preserve likelihood/FID while meeting explicit privacy/compliance thresholds.\n\n10. **Trajectory Smoothness as a Control Knob: Linking Deterministic Sampling to Memorization Dynamics**  \n   Test the hypothesis that smoother deterministic trajectories (as in deterministic density sampling) reduce training-data \u201csnapping\u201d compared to stochastic samplers by systematically controlling trajectory curvature/roughness in FM/CNF sampling (e.g., regularizing higher derivatives of the flow or using curvature-penalized solvers). Provide empirical and theoretical analysis connecting trajectory smoothness, entropy dissipation, and memorization metrics, yielding actionable sampler/training recommendations.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Memorization-Aware Flow Matching via Capacity-Regularized Vector Fields\nIncorporate the separation results on memorization vs. generalization into Flow Matching by adding an explicit capacity penalty ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Pruning-Integrated Flow Matching for CNFs (Structured Vector-Field Sparsification)\nExtend the pruning-based mitigation from diffusion transformers to Flow Matching CNFs by learning with structured spa",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Optimal-Transport Path Selection Under Privacy Constraints (OT-FM with DP Budgeting)\nInvestigate how the choice of conditional probability path (diffusion vs. OT displacement interpolation vs. learned",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Logit-Bias Analogues for Score/Flow Training to Reduce Underfitting in Noisy Data Regimes\nTranslate the \u201clogit bias\u201d idea to generative training by introducing a simple, class-count- or dataset-size-c",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Noise-Robust Flow Matching Objectives for Corrupted Conditioning and Weak Labels\nDesign bounded, noise-robust regression losses for vector-field matching (analogous to bounded classification losses) t",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Adaptive Probability Paths Learned from Deterministic KL Dissipation\nPropose a method that learns a family of conditional probability paths by optimizing for fast deterministic KL dissipation (from th",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Enterprise-Grade \u201cCompliance-by-Design\u201d Flow Models: Automated Memorization Audits + Model Editing\nInspired by the FMOps platform framing, build an end-to-end workflow for FM/diffusion deployment that",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Trajectory Smoothness as a Control Knob: Linking Deterministic Sampling to Memorization Dynamics\nTest the hypothesis that smoother deterministic trajectories (as in deterministic density sampling) red",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 9,
      "paper_title": "Why Diffusion Models Don\u2019t Memorize:  The Role of Implicit Dynamical Regularization in Training",
      "contribution": "The paper shows that training dynamics impose an implicit dynamical regularization in diffusion models: there are two distinct timescales (\u03c4gen and \u03c4mem) so that models generalize for a wide, growing window of training times (\u03c4 \u2208 [\u03c4gen, \u03c4mem]) because \u03c4mem scales linearly with dataset size n while \u03c4gen remains constant, explaining why memorization is avoided in practice and giving a tractable random-features theory that matches experiments.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12038,
      "output_tokens": 961,
      "predecessor_details": [
        {
          "success": true,
          "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
          "url": "https://arxiv.org/abs/1503.03585",
          "content": "[1503.03585] Deep Unsupervised Learning using Nonequilibrium Thermodynamics\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1503.03585\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1503.03585**(cs)\n[Submitted on 12 Mar 2015 ([v1](https://arxiv.org/abs/1503.03585v1)), last revised 18 Nov 2015 (this version, v8)]\n# Title:Deep Unsupervised Learning using Nonequilibrium Thermodynamics\nAuthors:[Jascha Sohl-Dickstein](https://arxiv.org/search/cs?searchtype=author&amp;query=Sohl-Dickstein,+J),[Eric A. Weiss](https://arxiv.org/search/cs?searchtype=author&amp;query=Weiss,+E+A),[Niru Maheswaranathan](https://arxiv.org/search/cs?searchtype=author&amp;query=Maheswaranathan,+N),[Surya Ganguli](https://arxiv.org/search/cs?searchtype=author&amp;query=Ganguli,+S)\nView a PDF of the paper titled Deep Unsupervised Learning using Nonequilibrium Thermodynamics, by Jascha Sohl-Dickstein and 3 other authors\n[View PDF](https://arxiv.org/pdf/1503.03585)> > Abstract:\n> A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm. Subjects:|Machine Learning (cs.LG); Disordered Systems and Neural Networks (cond-mat.dis-nn); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML)|\nCite as:|[arXiv:1503.03585](https://arxiv.org/abs/1503.03585)[cs.LG]|\n|(or[arXiv:1503.03585v8](https://arxiv.org/abs/1503.03585v8)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1503.03585](https://doi.org/10.48550/arXiv.1503.03585)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jascha Sohl-Dickstein [[view email](https://arxiv.org/show-email/33b8482c/1503.03585)]\n**[[v1]](https://arxiv.org/abs/1503.03585v1)**Thu, 12 Mar 2015 04:51:37 UTC (5,395 KB)\n**[[v2]](https://arxiv.org/abs/1503.03585v2)**Thu, 2 Apr 2015 06:48:02 UTC (5,397 KB)\n**[[v3]](https://arxiv.org/abs/1503.03585v3)**Wed, 29 Apr 2015 06:00:20 UTC (5,403 KB)\n**[[v4]](https://arxiv.org/abs/1503.03585v4)**Wed, 13 May 2015 01:57:49 UTC (5,409 KB)\n**[[v5]](https://arxiv.org/abs/1503.03585v5)**Wed, 20 May 2015 03:19:10 UTC (4,586 KB)\n**[[v6]](https://arxiv.org/abs/1503.03585v6)**Thu, 9 Jul 2015 16:16:33 UTC (6,085 KB)\n**[[v7]](https://arxiv.org/abs/1503.03585v7)**Tue, 21 Jul 2015 19:44:20 UTC (6,092 KB)\n**[v8]**Wed, 18 Nov 2015 21:50:51 UTC (6,095 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Deep Unsupervised Learning using Nonequilibrium Thermodynamics, by Jascha Sohl-Dickstein and 3 other authors\n* [View PDF](https://arxiv.org/pdf/1503.03585)\n* [TeX Source](https://arxiv.org/src/1503.03585)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1503.03585&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1503.03585&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2015-03](https://arxiv.org/list/cs.LG/2015-03)\nChange to browse by:\n[cond-mat](https://arxiv.org/abs/1503.03585?context=cond-mat)\n[cond-mat.dis-nn](https://arxiv.org/abs/1503.03585?context=cond-mat.dis-nn)\n[cs](https://arxiv.org/abs/1503.03585?context=cs)\n[q-bio](https://arxiv.org/abs/1503.03585?context=q-bio)\n[q-bio.NC](https://arxiv.org/abs/1503.03585?context=q-bio.NC)\n[stat](https://arxiv.org/abs/1503.03585?context=stat)\n[stat.ML](https://arxiv.org/abs/1503.03585?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1503.03585)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1503.03585)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1503.03585)\n### [2 blog links](https://arxiv.org/tb/1503.03585)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1503.html#Sohl-DicksteinW15)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/Sohl-DicksteinW15)\n[Jascha Sohl-Dickstein]()\n[Eric A. Weiss]()\n[Niru Maheswaranathan]()\n[Surya Ganguli]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArx",
          "original_query": "Deep unsupervised learning using nonequilibrium thermodynamics",
          "cleaned_query": "Deep unsupervised learning using nonequilibrium thermodynamics"
        },
        {
          "success": true,
          "title": "Estimation of Non-Normalized Statistical Models by Score ...",
          "url": "https://jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf",
          "content": "Journal of Machine Learning Research 6 (2005) 695\u2013709 Submitted 11/04; Revised 3/05; Published 4/05\nEstimation of Non-Normalized Statistical Models\nby Score Matching\nAapo Hyv\u00a8arinen aapo.hyvarinen@helsinki.fi\nHelsinki Institute for Information Technology (BRU)\nDepartment of Computer Science\nFIN-00014 University of Helsinki, Finland\nEditor: Peter Dayan\nAbstract\nOne often wants to estimate statistical models where the probability density function is\nknown only up to a multiplicative normalization constant. Typically, one then has to resort\nto Markov Chain Monte Carlo methods, or approximations of the normalization constant.\nHere, we propose that such models can be estimated by minimizing the expected squared\ndistance between the gradient of the log-density given by the model and the gradient of\nthe log-density of the observed data. While the estimation of the gradient of log-density\nfunction is, in principle, a very difficult non-parametric problem, we prove a surprising\nresult that gives a simple formula for this objective function. The density function of the\nobserved data does not appear in this formula, which simplifies to a sample average of a\nsum of some derivatives of the log-density given by the model. The validity of the method\nis demonstrated on multivariate Gaussian and independent component analysis models,\nand by estimating an overcomplete filter set for natural image data.\nKeywords: statistical estimation, non-normalized densities, pseudo-likelihood, Markov\nchain Monte Carlo, contrastive divergence\n1. Introduction\nIn many cases, probabilistic models in machine learning, statistics, or signal processing are\ngiven in the form of non-normalized probability densities. That is, the model contains an\nunknown normalization constant whose computation is too difficult for practical purposes.\nAssume we observe a random vector x \u2208 R\nn which has a probability density function\n(pdf) denoted by px(.). We have a parametrized density model p(.; \u03b8), where \u03b8 is an m\u0002dimensional vector of parameters. We want to estimate the parameter \u03b8 from x, i.e. we\nwant to approximate px(.) by p(.; \u03b8\u02c6) for the estimated parameter value \u03b8\u02c6. (We shall here\nconsider the case of continuous-valued variables only.)\nThe problem we consider here is that we only are able to compute the pdf given by the\nmodel up to a multiplicative constant Z(\u03b8):\np(\u03be; \u03b8) =\n1\nZ(\u03b8)\nq(\u03be; \u03b8).\nThat is, we do know the functional form of q as an analytical expression (or any form that\ncan be easily computed), but we do not know how to easily compute Z which is given by\n c 2005 Aapo Hyv\u00a8arinen.\nHyvarinen \u00a8\nan integral that is often analytically intractable:\nZ(\u03b8) =\nZ\n\u03be\u2208Rn\nq(\u03be; \u03b8) d\u03be.\nIn higher dimensions (in fact, for almost any n > 2), the numerical computation of this\nintegral is practically impossible as well.\nUsually, estimation of non-normalized models is approached by Markov Chain Monte\nCarlo (MCMC) methods, which are very slow, or by making some approximations, which\nmay be quite poor (Mackay, 2003).\nNon-normalized models are often encountered in continous-valued Markov random fields,\nwhich are widely used in image modelling, see e.g. (Bouman and Sauer, 1993; Li, 2001).\nIn general, undirected graphical models cannot be normalized except in the Gaussian case.\nOther recent work in image modelling also includes non-normalized models (Hyv\u00a8arinen and\nHoyer, 2001; Teh et al., 2003). Presumably, the number of useful applications for non\u0002normalized models is much larger than the present literature suggests. Non-normalized\nmodels have been avoided because their estimation has been considered too difficult; the\nadvent of efficient estimation methods may significantly increase their utility.\nIn this paper, we propose a simple method for estimating such non-normalized models.\nThis is based on minimizing the expected squared distance of the score function of x and\nthe score function given by the model. (By score function, we mean here the gradient\nof log-density.) We show that this distance can be estimated by a very simple formula\ninvolving only sample averages of some derivatives of the logarithm of the pdf given by the\nmodel. Thus, the computations involved are essentially not more complicated than in the\ncase where we know an analytical expression for the normalization constant. The proposed\nformula is exact and does not involve any approximations, which is why we are able to\nprove the local consistency of the resulting method. Minimization of the proposed objective\nfunction thus provides an estimation method that is computationally simple yet statistically\nlocally consistent.\n2. Estimation by Score Matching\nIn the following, we use extensively the gradient of the log-density with respect to the data\nvector. For simplicity, we call this the score function, although according the conventional\ndefinition, it is actually the score function with respect to a hypothetical location parameter\n(Schervish, 1995). For the model density, we denote the score function by \u03c8(\u03be; \u03b8):\n\u03c8(\u03be; \u03b8) =\n\uf8eb\n\uf8ec\uf8ec\uf8ed\n\u2202 log p(\u03be;\u03b8)\n\u2202\u03be1\n.\n.\n.\n\u2202 log p(\u03be;\u03b8)\n\u2202\u03ben\n\uf8f6\n\uf8f7\uf8f7\uf8f8 =\n\uf8eb\n\uf8ec\uf8ed\n\u03c81(\u03be; \u03b8)\n.\n.\n.\n\u03c8n(\u03be; \u03b8)\n\uf8f6\n\uf8f7\uf8f8 = \u2207\u03be log p(\u03be; \u03b8).\nThe point in using the score function is that it does not depend on Z(\u03b8). In fact we\nobviously have\n\u03c8(\u03be; \u03b8) = \u2207\u03be log q(\u03be; \u03b8). (1)\nLikewise, we denote by \u03c8x(.) = \u2207\u03be log px(.) the score function of the distribution of observed\ndata x. This could in principle be estimated by computing the gradient of the logarithm of\n696\nEstimation by Score Matching\na non-parametric estimate of the pdf\u2014but we will see below that no such computation is\nnecessary. Note that score functions are mappings from R\nn\nto R\nn\n.\nWe now propose that the model is estimated by minimizing the expected squared dis\u0002tance between the model score function \u03c8(.; \u03b8) and the data score function \u03c8x\n(.). We define\nthis squared distance as\nJ(\u03b8) =\n1\n2\nZ\n\u03be\u2208Rn\npx(\u03be)k\u03c8(\u03be; \u03b8) \u2212 \u03c8x(\u03be)k\n2\nd\u03be. (2)\nThus, our score matching estimator of \u03b8 is given by\n\u03b8\u02c6 = arg min\n\u03b8\nJ(\u03b8).\nThe motivation for this estimator is that the score function can be directly computed\nfrom q as in (1), and we do not need to compute Z. However, this may still seem to be a\nvery difficult way of estimating \u03b8, since we might have to compute an estimator of the data\nscore function \u03c8xfrom the observed sample, which is basically a non-parametric estimation\nproblem. However, no such non-parametric estimation is needed. This is because we can\nuse a simple trick of partial integration to compute the objective function very easily, as\nshown by the following theorem:\nTheorem 1 Assume that the model score function \u03c8(\u03be; \u03b8) is differentiable, as well as some\nweak regularity conditions.1\nThen, the objective function J in (2) can be expressed as\nJ(\u03b8) =\nZ\n\u03be\u2208Rn\npx(\u03be)\nXn\ni=1\n\u0014\n\u2202i\u03c8i(\u03be; \u03b8) +\n1\n2\n\u03c8i(\u03be; \u03b8)\n2\n\u0015\nd\u03be + const. (3)\nwhere the constant does not depend on \u03b8,\n\u03c8i(\u03be; \u03b8) =\n\u2202 log q(\u03be; \u03b8)\n\u2202\u03bei\nis the i-th element of the model score function, and\n\u2202i\u03c8i(\u03be; \u03b8) =\n\u2202\u03c8i(\u03be; \u03b8)\n\u2202\u03bei\n=\n\u2202\n2\nlog q(\u03be; \u03b8)\n\u2202\u03be\n2\ni\nis the partial derivative of the i-th element of the model score function with respect to the\ni-th variable.\nThe proof, given in the Appendix, is based a simple trick of partial integration that has\npreviously been used in the theory of independent component analysis for modelling the\ndensities of the independent components (Pham and Garrat, 1997).\nWe have thus proven the remarkable fact that the squared distance of the model score\nfunction from the data score function can be computed as a simple expectation of certain\n1. Namely: the data pdf px(\u03be) is differentiable, the expectations Ex{k\u03c8(x; \u03b8)k\n2\n} and Ex{k\u03c8x(x)k\n2\n} are\nfinite for any \u03b8, and px(\u03be)\u03c8(\u03be; \u03b8) goes to zero for any \u03b8 when k\u03bek \u2192 \u221e.\n697\nHyvarinen \u00a8\nfunctions of the non-normalized model pdf. If we have an analytical expression for the\nnon-normalized density function q, these functions are readily obtained by derivation using\n(1) and taking further derivatives.\nIn practice, we have T observations of the random vector x, denoted by x(1), . . . , x(T).\nThe sample version of J is obviousl",
          "original_query": "Estimation of non-normalized statistical models by score matching",
          "cleaned_query": "Estimation of non-normalized statistical models by score matching",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "Random Features for Large-Scale Kernel Machines - NIPS papers",
          "url": "https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines",
          "content": "#### Random Features for Large-Scale Kernel Machines\n\nPart of\n[Advances in Neural Information Processing Systems 20 (NIPS 2007)](https://papers.nips.cc/paper_files/paper/2007)\n\n[Bibtex](https://papers.nips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Bibtex.bib) [Metadata](https://papers.nips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Metadata.json) [Paper](https://papers.nips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf)\n\n#### Authors\n\n_Ali Rahimi, Benjamin Recht_\n\n#### Abstract\n\nTo accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user speci\ufb01ed shift- invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classi\ufb01cation and regression tasks linear machine learning al- gorithms applied to these features outperform state-of-the-art large-scale kernel machines.\n\nDo not remove: This comment is monitored to verify that the site is working properly",
          "original_query": "Random features for large-scale kernel machines",
          "cleaned_query": "Random features for large-scale kernel machines"
        },
        {
          "success": true,
          "title": "Dynamical regimes of diffusion models | Nature Communications",
          "url": "https://www.nature.com/articles/s41467-024-54281-3",
          "content": "Dynamical regimes of diffusion models | Nature Communications\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature Communications](https://media.springernature.com/full/nature-cms/uploads/product/ncomms/header-7001f06bc3fe2437048388e9f2f44215.svg)](https://www.nature.com/ncomms)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41467-024-54281-3?error=cookies_not_supported&code=f8f26ad1-5ae3-45c0-a378-b48be6ece2c9)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41467)\n* [RSS feed](https://www.nature.com/ncomms.rss)\nDynamical regimes of diffusion models\n[Download PDF](https://www.nature.com/articles/s41467-024-54281-3.pdf)\n[Download PDF](https://www.nature.com/articles/s41467-024-54281-3.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:17 November 2024# Dynamical regimes of diffusion models\n* [Giulio Biroli](#auth-Giulio-Biroli-Aff1)[1](#Aff1),\n* [Tony Bonnaire](#auth-Tony-Bonnaire-Aff1)[ORCID:orcid.org/0000-0003-2149-8795](https://orcid.org/0000-0003-2149-8795)[1](#Aff1),\n* [Valentin de Bortoli](#auth-Valentin-Bortoli-Aff2)[2](#Aff2)&amp;\n* \u2026* [Marc M\u00e9zard](#auth-Marc-M_zard-Aff3)[3](#Aff3)Show authors\n[*Nature Communications*](https://www.nature.com/ncomms)**volume15**, Article\u00a0number:9957(2024)[Cite this article](#citeas)\n* 19kAccesses\n* 44Citations\n* 104Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41467-024-54281-3/metrics)\n### Subjects\n* [Computer science](https://www.nature.com/subjects/computer-science)\n* [Phase transitions and critical phenomena](https://www.nature.com/subjects/phase-transitions-and-critical-phenomena)\n* [Statistical physics](https://www.nature.com/subjects/statistical-physics)\n## Abstract\nWe study generative diffusion models in the regime where both the data dimension and the sample size are large, and the score function is trained optimally. Using statistical physics methods, we identify three distinct dynamical regimes during the generative diffusion process. The generative dynamics, starting from pure noise, first encounters a speciation transition, where the broad structure of the data emerges, akin to symmetry breaking in phase transitions. This is followed by a collapse phase, where the dynamics is attracted to a specific training point through a mechanism similar to condensation in a glass phase. The speciation time can be obtained from a spectral analysis of the data\u2019s correlation matrix, while the collapse time relates to an excess entropy measure, and reveals the existence of a curse of dimensionality for diffusion models. These theoretical findings are supported by analytical solutions for Gaussian mixtures and confirmed by numerical experiments on real datasets.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-024-53165-w/MediaObjects/41467_2024_53165_Fig1_HTML.png)\n### [Generative learning for forecasting the dynamics of high-dimensional complex systems](https://www.nature.com/articles/s41467-024-53165-w?fromPaywallRec=false)\nArticleOpen access16 October 2024\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41592-024-02377-5/MediaObjects/41592_2024_2377_Fig1_HTML.png)\n### [DynaMight: estimating molecular motions with improved reconstruction from cryo-EM images](https://www.nature.com/articles/s41592-024-02377-5?fromPaywallRec=false)\nArticleOpen access09 August 2024\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42005-023-01516-2/MediaObjects/42005_2023_1516_Fig1_HTML.png)\n### [Automatically discovering ordinary differential equations from data with sparse regression](https://www.nature.com/articles/s42005-023-01516-2?fromPaywallRec=false)\nArticleOpen access09 January 2024\n## Introduction\nMachine learning has recently witnessed thrilling advancements, especially in the realm of generative models. At the forefront of this progress are diffusion models (DMs), which have emerged as powerful tools for modeling complex data distributions and generating new realistic samples. They have become the state of the art in generating images, videos, audio or 3D scenes[1](#ref-CR1),[2](#ref-CR2),[3](#ref-CR3),[4](#ref-CR4),[5](#ref-CR5),[6](#ref-CR6),[7](#ref-CR7),[8](https://www.nature.com/articles/s41467-024-54281-3#ref-CR8). Although the practical success of generative DMs is widely recognized, their full theoretical understanding remains an open challenge. Rigorous results assessing their convergence on finite-dimensional data have been obtained in refs.[9](#ref-CR9),[10](#ref-CR10),[11](#ref-CR11),[12](#ref-CR12),[13](#ref-CR13),[14](https://www.nature.com/articles/s41467-024-54281-3#ref-CR14). However, realistic data live in high-dimensional spaces, where interpolation between data points should face the curse of dimensionality[15](https://www.nature.com/articles/s41467-024-54281-3#ref-CR15). A thorough understanding of how generative models escape this curse is still lacking. This requires approaches able to take into account that the number and the dimension of the data are simultaneously very large. In this work, we face this challenge using statistical physics methods which have been developed to study probability distributions, disordered systems and stochastic processes in high dimensions[16](#ref-CR16),[17](#ref-CR17),[18](https://www.nature.com/articles/s41467-024-54281-3#ref-CR18).\nDMs work in two stages. In the forward diffusion, one starts from a data point (e.g., an image) and gradually adds noise to it, until the image has become pure noise. In the backward process, one gradually denoises the image using a diffusion in an effective force field (the score) which is learnt by leveraging techniques from score matching[19](https://www.nature.com/articles/s41467-024-54281-3#ref-CR19),[20](https://www.nature.com/articles/s41467-024-54281-3#ref-CR20)and deep neural networks. In this study, we focus on DMs which are efficient enough to learn the exact empirical score, i.e., the one obtained by noising the empirical distribution of data. In practical implementations, this should happen when one performs a long training of a strongly over-parameterized deep network to learn the score, in the situation when the number of data is not too large.\nWithin this framework, we develop a theoretical approach able to characterize the dynamics of DMs in the simultaneous limit of large dimensions and large dataset. We show that the backward generative diffusion process consists of three subsequent dynamical regimes. The first one is basically pure Brownian motion. In the second one, the backward trajectory finds one of the main classes of the data (for instance if the data consist of images of horses and cars, a given trajectory will specialize towards one of these two categories). In the third regime, the diffusion \u201ccollapses\u201d onto one of the examples of the dataset: a given trajectory commits to the attraction basin of one of the data points, and the backward evolution brings it back to that exact data point. Since DMs are defined as the time reversal of a forward noising process, the generative process has to collapse on the training dataset under the exact empirical score assumption[21](https://www.nature.com/articles/s41467-024-542",
          "original_query": "Dynamical regimes of diffusion models",
          "cleaned_query": "Dynamical regimes of diffusion models"
        },
        {
          "success": true,
          "title": "Leveraging Model Guidance to Extract Training Data from Personalized Diffusion Models",
          "url": "https://arxiv.org/abs/2410.03039",
          "content": "[2410.03039] Leveraging Model Guidance to Extract Training Data from Personalized Diffusion Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2410.03039\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2410.03039**(cs)\n[Submitted on 3 Oct 2024 ([v1](https://arxiv.org/abs/2410.03039v1)), last revised 26 Sep 2025 (this version, v3)]\n# Title:Leveraging Model Guidance to Extract Training Data from Personalized Diffusion Models\nAuthors:[Xiaoyu Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+X),[Jiaru Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+J),[Zhiwei Steven Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+Z+S)\nView a PDF of the paper titled Leveraging Model Guidance to Extract Training Data from Personalized Diffusion Models, by Xiaoyu Wu and 2 other authors\n[View PDF](https://arxiv.org/pdf/2410.03039)[HTML (experimental)](https://arxiv.org/html/2410.03039v3)> > Abstract:\n> Diffusion Models (DMs) have become powerful image generation tools, especially for few-shot fine-tuning where a pretrained DM is fine-tuned on a small image set to capture specific styles or objects. Many people upload these personalized checkpoints online, fostering communities such as Civitai and HuggingFace. However, model owners may overlook the data leakage risks when releasing fine-tuned checkpoints. Moreover, concerns regarding copyright violations arise when unauthorized data is used during fine-tuning. In this paper, we ask: &#34;Can training data be extracted from these fine-tuned DMs shared online?&#34; A successful extraction would present not only data leakage threats but also offer tangible evidence of copyright infringement. To answer this, we propose FineXtract, a framework for extracting fine-tuning data. Our method approximates fine-tuning as a gradual shift in the model&#39;s learned distribution -- from the original pretrained DM toward the fine-tuning data. By extrapolating the models before and after fine-tuning, we guide the generation toward high-probability regions within the fine-tuned data distribution. We then apply a clustering algorithm to extract the most probable images from those generated using this extrapolated guidance. Experiments on DMs fine-tuned with datasets including WikiArt, DreamBooth, and real-world checkpoints posted online validate the effectiveness of our method, extracting about 20% of fine-tuning data in most cases. The code is available [> this https URL\n](https://github.com/Nicholas0228/FineXtract)> . Comments:|Accepted at the International Conference on Machine Learning (ICML) 2025|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)|\nCite as:|[arXiv:2410.03039](https://arxiv.org/abs/2410.03039)[cs.CV]|\n|(or[arXiv:2410.03039v3](https://arxiv.org/abs/2410.03039v3)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2410.03039](https://doi.org/10.48550/arXiv.2410.03039)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Xiaoyu Wu [[view email](https://arxiv.org/show-email/fba301da/2410.03039)]\n**[[v1]](https://arxiv.org/abs/2410.03039v1)**Thu, 3 Oct 2024 23:06:11 UTC (42,629 KB)\n**[[v2]](https://arxiv.org/abs/2410.03039v2)**Sun, 22 Jun 2025 08:37:39 UTC (9,378 KB)\n**[v3]**Fri, 26 Sep 2025 02:37:14 UTC (23,572 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Leveraging Model Guidance to Extract Training Data from Personalized Diffusion Models, by Xiaoyu Wu and 2 other authors\n* [View PDF](https://arxiv.org/pdf/2410.03039)\n* [HTML (experimental)](https://arxiv.org/html/2410.03039v3)\n* [TeX Source](https://arxiv.org/src/2410.03039)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2410.03039&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2410.03039&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2024-10](https://arxiv.org/list/cs.CV/2024-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/2410.03039?context=cs)\n[cs.AI](https://arxiv.org/abs/2410.03039?context=cs.AI)\n[cs.LG](https://arxiv.org/abs/2410.03039?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2410.03039)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2410.03039)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2410.03039)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community,",
          "original_query": "Extracting training data from diffusion models",
          "cleaned_query": "Extracting training data from diffusion models"
        },
        {
          "success": true,
          "title": "The Spectral Bias of Shallow Neural Network Learning is Shaped by the Choice of Non-linearity",
          "url": "https://arxiv.org/abs/2503.10587",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2503.10587** (cs)\n\n\\[Submitted on 13 Mar 2025\\]\n\n# Title:The Spectral Bias of Shallow Neural Network Learning is Shaped by the Choice of Non-linearity\n\nAuthors: [Justin Sahs](https://arxiv.org/search/cs?searchtype=author&query=Sahs,+J), [Ryan Pyle](https://arxiv.org/search/cs?searchtype=author&query=Pyle,+R), [Fabio Anselmi](https://arxiv.org/search/cs?searchtype=author&query=Anselmi,+F), [Ankit Patel](https://arxiv.org/search/cs?searchtype=author&query=Patel,+A)\n\nView a PDF of the paper titled The Spectral Bias of Shallow Neural Network Learning is Shaped by the Choice of Non-linearity, by Justin Sahs and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.10587) [HTML (experimental)](https://arxiv.org/html/2503.10587v1)\n\n> Abstract:Despite classical statistical theory predicting severe overfitting, modern massively overparameterized neural networks still generalize well. This unexpected property is attributed to the network's so-called implicit bias, which describes its propensity to converge to solutions that generalize effectively, among the many possible that correctly label the training data. The aim of our research is to explore this bias from a new perspective, focusing on how non-linear activation functions contribute to shaping it. First, we introduce a reparameterization which removes a continuous weight rescaling symmetry. Second, in the kernel regime, we leverage this reparameterization to generalize recent findings that relate shallow Neural Networks to the Radon transform, deriving an explicit formula for the implicit bias induced by a broad class of activation functions. Specifically, by utilizing the connection between the Radon transform and the Fourier transform, we interpret the kernel regime's inductive bias as minimizing a spectral seminorm that penalizes high-frequency components, in a manner dependent on the activation function. Finally, in the adaptive regime, we demonstrate the existence of local dynamical attractors that facilitate the formation of clusters of hyperplanes where the input to a neuron's activation function is zero, yielding alignment between many neurons' response functions. We confirm these theoretical results with simulations. All together, our work provides a deeper understanding of the mechanisms underlying the generalization capabilities of overparameterized neural networks and its relation with the implicit bias, offering potential pathways for designing more efficient and robust models.\n\n| | |\n| --- | --- |\n| Comments: | 18 pages, 10 figures in main text |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2503.10587](https://arxiv.org/abs/2503.10587) \\[cs.LG\\] |\n| | (or [arXiv:2503.10587v1](https://arxiv.org/abs/2503.10587v1) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2503.10587](https://doi.org/10.48550/arXiv.2503.10587) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Justin Sahs \\[ [view email](https://arxiv.org/show-email/2546b6dc/2503.10587)\\]\n\n**\\[v1\\]**\nThu, 13 Mar 2025 17:36:46 UTC (9,455 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled The Spectral Bias of Shallow Neural Network Learning is Shaped by the Choice of Non-linearity, by Justin Sahs and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.10587)\n- [HTML (experimental)](https://arxiv.org/html/2503.10587v1)\n- [TeX Source](https://arxiv.org/src/2503.10587)\n- [Other Formats](https://arxiv.org/format/2503.10587)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2503.10587&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2503.10587&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-03](https://arxiv.org/list/cs.LG/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.10587?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2503.10587?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.10587)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.10587)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.10587)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2503.10587&description=The Spectral Bias of Shallow Neural Network Learning is Shaped by the Choice of Non-linearity) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2503.10587&title=The Spectral Bias of Shallow Neural Network Learning is Shaped by the Choice of Non-linearity)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.10587) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "On the spectral bias of neural networks",
          "cleaned_query": "On the spectral bias of neural networks"
        }
      ],
      "generated_ideas": [
        "Spectrally-Tuned Noise Schedules to Delay Speciation/Collapse in Diffusion Models\nDesign diffusion noise schedules (and/or time reparameterizations) that explicitly control the speciation and collapse times identified by the dynamical-regimes theory. Operationalize this by estimating the data correlation spectrum and excess-entropy proxy, then optimizing a schedule to keep dynamics in the \u201cstructure-emergence\u201d regime longer while postponing collapse, with measurable gains in diversity and reduced training-point attraction.",
        "Score-Matching Diagnostics for Memorization Risk in Personalized Diffusion Fine-Tunes\nDevelop a practical estimator of \u201ccollapse propensity\u201d for a fine-tuned diffusion checkpoint using score-matching-style quantities (derivatives of the learned log-density/score) computed on candidate inputs and synthetic probes. Validate that this diagnostic predicts susceptibility to FineXtract-like extraction, enabling a pre-release audit tool that flags checkpoints likely to leak training images.",
        "Random-Feature Score Networks for Fast High-Dimensional Diffusion Training\nReplace part of the score network with a random Fourier feature (RFF) backbone to approximate shift-invariant kernels over latent/image patches, training only a lightweight linear head via score matching. Evaluate whether this reduces compute while preserving score quality, and analyze approximation-error vs. sample-size tradeoffs in the large-dimension regime where diffusion exhibits phase-transition-like behavior.",
        "Activation-Function Design to Control Diffusion Score Spectral Content\nUse the activation-dependent spectral seminorm characterization of shallow nets to design score-network nonlinearities that preferentially learn/represent targeted frequency bands of the score field. Implement a \u201cspectral-bias toolkit\u201d for diffusion U-Nets (e.g., swapping or mixing activations in specific blocks) and measure effects on speciation timing, sample fidelity, and high-frequency artifact formation.",
        "Thermodynamic Regularizers for Reverse Diffusion Using Entropy Production\nIntroduce an explicit regularizer inspired by nonequilibrium thermodynamics that penalizes excessive entropy production along the learned reverse diffusion trajectory. Implement this as an auxiliary loss computed from score norms and Jacobian traces across timesteps, with the goal of stabilizing training and mitigating collapse toward individual training points.",
        "Fine-Tuning as a Trajectory: Extrapolation-Guidance with Intermediate Checkpoints\nExtend FineXtract\u2019s \u201cdistribution shift\u201d approximation by collecting multiple intermediate fine-tuning checkpoints and fitting a low-dimensional trajectory (e.g., geodesic or polynomial) in parameter space. Use this fitted path to create more accurate extrapolated guidance directions for extraction\u2014and conversely to design countermeasures that deliberately \u201ccurve\u201d the trajectory to reduce extrapolatability without harming personalization.",
        "Spectrum-Aware Data Curation to Reduce Collapse in High Dimensions\nLeverage the link between speciation time and the data correlation matrix spectrum to propose dataset preprocessing/augmentation that reshapes the spectrum (e.g., whitening, low-rank deflation, or targeted augmentations). Test whether controlled spectral flattening reduces the collapse phase (training-point attraction) while maintaining semantic structure emergence, especially for small fine-tuning sets.",
        "Hybrid Energy-Based Diffusion via Score Matching Without Normalization Constants\nConstruct a diffusion model whose reverse process score is partially parameterized as the gradient of an unnormalized energy model (EBM), trained with score matching to avoid partition-function estimation. Evaluate whether this hybrid improves likelihood estimation and controllable sampling (e.g., conditional/posterior computations emphasized in nonequilibrium diffusion framing) while providing a clearer theoretical bridge between EBMs and diffusion.",
        "Random-Feature Kernels for Detecting and Clustering Extracted Memorized Samples\nDevelop a scalable clustering-and-near-duplicate detection pipeline for generated candidates (as in FineXtract) using random-feature embeddings that approximate perceptual kernels over images. This makes extraction post-processing faster and more reliable at scale, and enables systematic measurement of memorization rates across fine-tuning methods, schedules, and activation choices.",
        "Empirical Mapping Between Neuron Hyperplane Clustering and Diffusion Collapse\nInvestigate whether the \u201chyperplane clustering\u201d attractors observed in adaptive-regime shallow networks have an analogue in diffusion score networks during late timesteps associated with collapse. Create instrumentation to track alignment/degeneracy in intermediate activations across timesteps and correlate it with training-sample attraction metrics; then propose architectural or activation modifications that disrupt these attractors to improve generalization and privacy."
      ],
      "generated_ideas_raw": "1. **Spectrally-Tuned Noise Schedules to Delay Speciation/Collapse in Diffusion Models**  \nDesign diffusion noise schedules (and/or time reparameterizations) that explicitly control the speciation and collapse times identified by the dynamical-regimes theory. Operationalize this by estimating the data correlation spectrum and excess-entropy proxy, then optimizing a schedule to keep dynamics in the \u201cstructure-emergence\u201d regime longer while postponing collapse, with measurable gains in diversity and reduced training-point attraction.\n\n2. **Score-Matching Diagnostics for Memorization Risk in Personalized Diffusion Fine-Tunes**  \nDevelop a practical estimator of \u201ccollapse propensity\u201d for a fine-tuned diffusion checkpoint using score-matching-style quantities (derivatives of the learned log-density/score) computed on candidate inputs and synthetic probes. Validate that this diagnostic predicts susceptibility to FineXtract-like extraction, enabling a pre-release audit tool that flags checkpoints likely to leak training images.\n\n3. **Random-Feature Score Networks for Fast High-Dimensional Diffusion Training**  \nReplace part of the score network with a random Fourier feature (RFF) backbone to approximate shift-invariant kernels over latent/image patches, training only a lightweight linear head via score matching. Evaluate whether this reduces compute while preserving score quality, and analyze approximation-error vs. sample-size tradeoffs in the large-dimension regime where diffusion exhibits phase-transition-like behavior.\n\n4. **Activation-Function Design to Control Diffusion Score Spectral Content**  \nUse the activation-dependent spectral seminorm characterization of shallow nets to design score-network nonlinearities that preferentially learn/represent targeted frequency bands of the score field. Implement a \u201cspectral-bias toolkit\u201d for diffusion U-Nets (e.g., swapping or mixing activations in specific blocks) and measure effects on speciation timing, sample fidelity, and high-frequency artifact formation.\n\n5. **Thermodynamic Regularizers for Reverse Diffusion Using Entropy Production**  \nIntroduce an explicit regularizer inspired by nonequilibrium thermodynamics that penalizes excessive entropy production along the learned reverse diffusion trajectory. Implement this as an auxiliary loss computed from score norms and Jacobian traces across timesteps, with the goal of stabilizing training and mitigating collapse toward individual training points.\n\n6. **Fine-Tuning as a Trajectory: Extrapolation-Guidance with Intermediate Checkpoints**  \nExtend FineXtract\u2019s \u201cdistribution shift\u201d approximation by collecting multiple intermediate fine-tuning checkpoints and fitting a low-dimensional trajectory (e.g., geodesic or polynomial) in parameter space. Use this fitted path to create more accurate extrapolated guidance directions for extraction\u2014and conversely to design countermeasures that deliberately \u201ccurve\u201d the trajectory to reduce extrapolatability without harming personalization.\n\n7. **Spectrum-Aware Data Curation to Reduce Collapse in High Dimensions**  \nLeverage the link between speciation time and the data correlation matrix spectrum to propose dataset preprocessing/augmentation that reshapes the spectrum (e.g., whitening, low-rank deflation, or targeted augmentations). Test whether controlled spectral flattening reduces the collapse phase (training-point attraction) while maintaining semantic structure emergence, especially for small fine-tuning sets.\n\n8. **Hybrid Energy-Based Diffusion via Score Matching Without Normalization Constants**  \nConstruct a diffusion model whose reverse process score is partially parameterized as the gradient of an unnormalized energy model (EBM), trained with score matching to avoid partition-function estimation. Evaluate whether this hybrid improves likelihood estimation and controllable sampling (e.g., conditional/posterior computations emphasized in nonequilibrium diffusion framing) while providing a clearer theoretical bridge between EBMs and diffusion.\n\n9. **Random-Feature Kernels for Detecting and Clustering Extracted Memorized Samples**  \nDevelop a scalable clustering-and-near-duplicate detection pipeline for generated candidates (as in FineXtract) using random-feature embeddings that approximate perceptual kernels over images. This makes extraction post-processing faster and more reliable at scale, and enables systematic measurement of memorization rates across fine-tuning methods, schedules, and activation choices.\n\n10. **Empirical Mapping Between Neuron Hyperplane Clustering and Diffusion Collapse**  \nInvestigate whether the \u201chyperplane clustering\u201d attractors observed in adaptive-regime shallow networks have an analogue in diffusion score networks during late timesteps associated with collapse. Create instrumentation to track alignment/degeneracy in intermediate activations across timesteps and correlate it with training-sample attraction metrics; then propose architectural or activation modifications that disrupt these attractors to improve generalization and privacy.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Spectrally-Tuned Noise Schedules to Delay Speciation/Collapse in Diffusion Models\nDesign diffusion noise schedules (and/or time reparameterizations) that explicitly control the speciation and collapse",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Score-Matching Diagnostics for Memorization Risk in Personalized Diffusion Fine-Tunes\nDevelop a practical estimator of \u201ccollapse propensity\u201d for a fine-tuned diffusion checkpoint using score-matching-",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Random-Feature Score Networks for Fast High-Dimensional Diffusion Training\nReplace part of the score network with a random Fourier feature (RFF) backbone to approximate shift-invariant kernels over la",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Activation-Function Design to Control Diffusion Score Spectral Content\nUse the activation-dependent spectral seminorm characterization of shallow nets to design score-network nonlinearities that prefe",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Thermodynamic Regularizers for Reverse Diffusion Using Entropy Production\nIntroduce an explicit regularizer inspired by nonequilibrium thermodynamics that penalizes excessive entropy production along ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Fine-Tuning as a Trajectory: Extrapolation-Guidance with Intermediate Checkpoints\nExtend FineXtract\u2019s \u201cdistribution shift\u201d approximation by collecting multiple intermediate fine-tuning checkpoints and",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Spectrum-Aware Data Curation to Reduce Collapse in High Dimensions\nLeverage the link between speciation time and the data correlation matrix spectrum to propose dataset preprocessing/augmentation that",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Hybrid Energy-Based Diffusion via Score Matching Without Normalization Constants\nConstruct a diffusion model whose reverse process score is partially parameterized as the gradient of an unnormalized e",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Random-Feature Kernels for Detecting and Clustering Extracted Memorized Samples\nDevelop a scalable clustering-and-near-duplicate detection pipeline for generated candidates (as in FineXtract) using ra",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Empirical Mapping Between Neuron Hyperplane Clustering and Diffusion Collapse\nInvestigate whether the \u201chyperplane clustering\u201d attractors observed in adaptive-regime shallow networks have an analogue i",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 10,
      "paper_title": "Adjoint Schr\u00f6dinger Bridge Sampler",
      "contribution": "Combines Schr\u00f6dinger-bridge stochastic optimal control with adjoint matching to learn scalable, importance-weight-free diffusion samplers that transport arbitrary source distributions to unnormalized energy-defined targets.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12399,
      "output_tokens": 1058,
      "predecessor_details": [
        {
          "success": true,
          "title": "[2006.11239] Denoising Diffusion Probabilistic Models - arXiv",
          "url": "https://arxiv.org/abs/2006.11239",
          "content": "[2006.11239] Denoising Diffusion Probabilistic Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2006.11239\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2006.11239**(cs)\n[Submitted on 19 Jun 2020 ([v1](https://arxiv.org/abs/2006.11239v1)), last revised 16 Dec 2020 (this version, v2)]\n# Title:Denoising Diffusion Probabilistic Models\nAuthors:[Jonathan Ho](https://arxiv.org/search/cs?searchtype=author&amp;query=Ho,+J),[Ajay Jain](https://arxiv.org/search/cs?searchtype=author&amp;query=Jain,+A),[Pieter Abbeel](https://arxiv.org/search/cs?searchtype=author&amp;query=Abbeel,+P)\nView a PDF of the paper titled Denoising Diffusion Probabilistic Models, by Jonathan Ho and 2 other authors\n[View PDF](https://arxiv.org/pdf/2006.11239)> > Abstract:\n> We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at [> this https URL\n](https://github.com/hojonathanho/diffusion)> Subjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2006.11239](https://arxiv.org/abs/2006.11239)[cs.LG]|\n|(or[arXiv:2006.11239v2](https://arxiv.org/abs/2006.11239v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2006.11239](https://doi.org/10.48550/arXiv.2006.11239)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jonathan Ho [[view email](https://arxiv.org/show-email/c50e7925/2006.11239)]\n**[[v1]](https://arxiv.org/abs/2006.11239v1)**Fri, 19 Jun 2020 17:24:44 UTC (9,134 KB)\n**[v2]**Wed, 16 Dec 2020 21:15:05 UTC (9,137 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Denoising Diffusion Probabilistic Models, by Jonathan Ho and 2 other authors\n* [View PDF](https://arxiv.org/pdf/2006.11239)\n* [TeX Source](https://arxiv.org/src/2006.11239)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2006.11239&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2006.11239&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2020-06](https://arxiv.org/list/cs.LG/2020-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/2006.11239?context=cs)\n[stat](https://arxiv.org/abs/2006.11239?context=stat)\n[stat.ML](https://arxiv.org/abs/2006.11239?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2006.11239)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2006.11239)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2006.11239)\n### [10 blog links](https://arxiv.org/tb/2006.11239)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2006.html#abs-2006-11239)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2006-11239)\n[Jonathan Ho]()\n[Ajay Jain]()\n[Pieter Abbeel]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2006.11239)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Denoising Diffusion Probabilistic Models",
          "cleaned_query": "Denoising Diffusion Probabilistic Models"
        },
        {
          "success": true,
          "title": "Score-Based Generative Modeling through Stochastic Differential ...",
          "url": "https://arxiv.org/abs/2011.13456",
          "content": "[2011.13456] Score-Based Generative Modeling through Stochastic Differential Equations[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2011.13456\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2011.13456**(cs)\n[Submitted on 26 Nov 2020 ([v1](https://arxiv.org/abs/2011.13456v1)), last revised 10 Feb 2021 (this version, v2)]\n# Title:Score-Based Generative Modeling through Stochastic Differential Equations\nAuthors:[Yang Song](https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+Y),[Jascha Sohl-Dickstein](https://arxiv.org/search/cs?searchtype=author&amp;query=Sohl-Dickstein,+J),[Diederik P. Kingma](https://arxiv.org/search/cs?searchtype=author&amp;query=Kingma,+D+P),[Abhishek Kumar](https://arxiv.org/search/cs?searchtype=author&amp;query=Kumar,+A),[Stefano Ermon](https://arxiv.org/search/cs?searchtype=author&amp;query=Ermon,+S),[Ben Poole](https://arxiv.org/search/cs?searchtype=author&amp;query=Poole,+B)\nView a PDF of the paper titled Score-Based Generative Modeling through Stochastic Differential Equations, by Yang Song and 4 other authors\n[View PDF](https://arxiv.org/pdf/2011.13456)> > Abstract:\n> Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model. Comments:|ICLR 2021 (Oral)|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2011.13456](https://arxiv.org/abs/2011.13456)[cs.LG]|\n|(or[arXiv:2011.13456v2](https://arxiv.org/abs/2011.13456v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2011.13456](https://doi.org/10.48550/arXiv.2011.13456)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Yang Song [[view email](https://arxiv.org/show-email/18c3b4ae/2011.13456)]\n**[[v1]](https://arxiv.org/abs/2011.13456v1)**Thu, 26 Nov 2020 19:39:10 UTC (32,781 KB)\n**[v2]**Wed, 10 Feb 2021 18:17:04 UTC (56,849 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Score-Based Generative Modeling through Stochastic Differential Equations, by Yang Song and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2011.13456)\n* [TeX Source](https://arxiv.org/src/2011.13456)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2011.13456&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2011.13456&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2020-11](https://arxiv.org/list/cs.LG/2020-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/2011.13456?context=cs)\n[stat](https://arxiv.org/abs/2011.13456?context=stat)\n[stat.ML](https://arxiv.org/abs/2011.13456?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2011.13456)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2011.13456)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2011.13456)\n### [3 blog links](https://arxiv.org/tb/2011.13456)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2011.html#abs-2011-13456)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2011-13456)\n[Yang Song]()\n[Jascha Sohl-Dickstein]()\n[Diederik P. Kingma]()\n[Abhishek Kumar]()\n[Stefano Ermon]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate To",
          "original_query": "Score-Based Generative Modeling through Stochastic Differential Equations",
          "cleaned_query": "Score-Based Generative Modeling through Stochastic Differential Equations"
        },
        {
          "success": true,
          "title": "[1308.0215] A survey of the Schr\u00f6dinger problem and some of its ...",
          "url": "https://arxiv.org/abs/1308.0215",
          "content": "# Mathematics > Probability\n\n**arXiv:1308.0215** (math)\n\n\\[Submitted on 1 Aug 2013\\]\n\n# Title:A survey of the Schr\u00f6dinger problem and some of its connections with optimal transport\n\nAuthors: [Christian L\u00e9onard](https://arxiv.org/search/math?searchtype=author&query=L%C3%A9onard,+C) (MODAL'X)\n\nView a PDF of the paper titled A survey of the Schr\\\\\"odinger problem and some of its connections with optimal transport, by Christian L\\\\'eonard (MODAL'X)\n\n[View PDF](https://arxiv.org/pdf/1308.0215)\n\n> Abstract:This article is aimed at presenting the Schr\u00f6dinger problem and some of its connections with optimal transport. We hope that it can be used as a basic user's guide to Schr\u00f6dinger problem. We also give a survey of the related literature. In addition, some new results are proved.\n\n| | |\n| --- | --- |\n| Comments: | To appear in Discrete \\\\& Continuous Dynamical Systems - Series A. Special issue on optimal transport |\n| Subjects: | Probability (math.PR); Functional Analysis (math.FA); Optimization and Control (math.OC) |\n| Cite as: | [arXiv:1308.0215](https://arxiv.org/abs/1308.0215) \\[math.PR\\] |\n| (or [arXiv:1308.0215v1](https://arxiv.org/abs/1308.0215v1) \\[math.PR\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1308.0215](https://doi.org/10.48550/arXiv.1308.0215) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | Discrete Contin. Dyn. Syst. A, 2014, 34(4): 1533-1574 |\n\n## Submission history\n\nFrom: Christian Leonard \\[ [view email](https://arxiv.org/show-email/6240ebf0/1308.0215)\\]\u00a0\\[via CCSD proxy\\] **\\[v1\\]**\nThu, 1 Aug 2013 14:16:38 UTC (45 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled A survey of the Schr\\\\\"odinger problem and some of its connections with optimal transport, by Christian L\\\\'eonard (MODAL'X)\n\n- [View PDF](https://arxiv.org/pdf/1308.0215)\n- [TeX Source](https://arxiv.org/src/1308.0215)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nmath.PR\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1308.0215&function=prev&context=math.PR)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1308.0215&function=next&context=math.PR)\n\n[new](https://arxiv.org/list/math.PR/new) \\| [recent](https://arxiv.org/list/math.PR/recent) \\| [2013-08](https://arxiv.org/list/math.PR/2013-08)\n\nChange to browse by:\n\n[math](https://arxiv.org/abs/1308.0215?context=math) [math.FA](https://arxiv.org/abs/1308.0215?context=math.FA) [math.OC](https://arxiv.org/abs/1308.0215?context=math.OC)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1308.0215)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1308.0215)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1308.0215)\n\n### [1 blog link](https://arxiv.org/tb/1308.0215)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1308.0215) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "A survey of the Schr\u00f6dinger problem and its links with optimal transport",
          "cleaned_query": "A survey of the Schr\u00f6dinger problem and its links with optimal transport"
        },
        {
          "success": true,
          "title": "Schr\u00f6dinger Bridge Samplers",
          "url": "https://arxiv.org/pdf/1912.13170",
          "content": "Schr\u00a8odinger Bridge Samplers\nEspen Bernton\u2217, Jeremy Heng\u2020, Arnaud Doucet\u2021 and Pierre E. Jacob\u00a7\nAbstract\nConsider a reference Markov process with initial distribution \u03c00 and transition kernels\n{Mt}t\u2208[1:T], for some T \u2208 N. Assume that you are given distribution \u03c0T , which is not equal\nto the marginal distribution of the reference process at time T. In this scenario, Schr\u00a8odinger\naddressed the problem of identifying the Markov process with initial distribution \u03c00 and\nterminal distribution equal to \u03c0T which is the closest to the reference process in terms of\nKullback\u2013Leibler divergence. This special case of the so-called Schr\u00a8odinger bridge problem\ncan be solved using iterative proportional fitting, also known as the Sinkhorn algorithm. We\nleverage these ideas to develop novel Monte Carlo schemes, termed Schr\u00a8odinger bridge sam\u0002plers, to approximate a target distribution \u03c0 on R\nd and to estimate its normalizing constant.\nThis is achieved by iteratively modifying the transition kernels of the reference Markov chain\nto obtain a process whose marginal distribution at time T becomes closer to \u03c0T = \u03c0, via\nregression-based approximations of the corresponding iterative proportional fitting recursion.\nWe report preliminary experiments and make connections with other problems arising in the\noptimal transport, optimal control and physics literatures.\nKeywords: Annealed importance sampling; iterative proportional fitting; normalizing constant;\nsequential Monte Carlo samplers; Schr\u00a8odinger bridge; Sinkhorn\u2019s algorithm; optimal transport.\n1 Introduction\n1.1 Outline and literature review\nLet \u03c0 be a distribution which admits a density, with respect to some dominating measure on\na measurable space (E, E), that can only be evaluated pointwise up to a normalizing constant\nZ. We are interested in approximating expectations with respect to \u03c0 as well as the value of\nZ. State-of-the-art Monte Carlo methods to address this problem include Annealed Importance\nSampling (AIS; Crooks, 1998; Neal, 2001) and Sequential Monte Carlo (SMC; Del Moral et al.,\n2006). The basis of these methods is to simulate N non-homogeneous Markov chains with initial\n\u2217Department of Statistics, Columbia University, USA.\n\u2020ESSEC Business School, Singapore.\n\u2021Department of Statistics, University of Oxford, UK.\n\u00a7Department of Statistics, Harvard University, USA.\n1\narXiv:1912.13170v1 [stat.CO] 31 Dec 2019\ndistribution \u03c00 and transition kernels {Mt}t\u2208[1:T]\n, designed such that the marginal distribution of\neach Markov chain at time T is approximately equal to \u03c0\n1\n. However, this marginal distribution\nis typically not analytically available, prohibiting its direct application as a proposal distribution\nwithin importance sampling. In AIS and SMC, this intractability is circumvented by introducing\nan appropriate auxiliary target distribution on the path space E\nT +1 whose marginal at time T\ncoincides with \u03c0 and with respect to which importance weights can be calculated. This allows us\nto obtain consistent estimates of expectations with respect to \u03c0 and of its normalizing constant\nZ.\nThese methods have found many applications in physics and statistics, but can perform\npoorly when the marginal distribution of the samples at time T differs significantly from \u03c0,\nresulting in importance weights with high variance. Building upon previous contributions for\ninference in partially observed diffusions and state-space models (Richard and Zhang, 2007; Kap\u0002pen and Ruiz, 2016; Guarniero et al., 2017), the controlled SMC sampler methodology of Heng\net al. (2017) uses ideas from optimal control to iteratively modify the initial distribution and\ntransition kernels of the reference Markov process to reduce the Kullback\u2013Leibler divergence\nbetween the induced path distribution and the auxiliary target distribution on E\nT +1. When\napplicable, controlled SMC samplers demonstrate clear improvements over AIS and SMC. How\u0002ever, a limitation of this approach is that one must be able to sample from a modified initial\ndistribution. Practically, this means that \u03c00 must be conjugate with respect to the policy of\nthe underlying optimal control problem. Additionally, the transition kernels of the reference\nMarkov process must also be conjugate with respect to the chosen policy.\nWe propose here an alternative approach which is more widely applicable. First, we only\nmodify the transition kernels and not the initial distribution, and relax the requirement that\nthese kernels have to be conjugate with respect to the policy. Second, instead of minimizing\nthe KL divergence on path space E\nT +1 with respect to a fixed auxiliary target distribution, the\nauxiliary target is itself being optimized across iterations. We describe our algorithm as an\napproximation of iterative proportional fitting (IPF), an algorithm introduced in various forms\nby Deming and Stephan (1940); Sinkhorn (1967); Ireland and Kullback (1968); Kullback (1968).\nIn finite state-spaces, this algorithm is also known as Sinkhorn\u2019s algorithm and has recently\ngained much attention in machine learning (Cuturi, 2013; Peyr\u00b4e and Cuturi, 2019). Under\nweak regularity conditions, IPF is known to converge to the solution of the Schr\u00a8odinger bridge\nproblem in both finite and continuous state-spaces; see, e.g., (Sinkhorn, 1967; R\u00a8uschendorf,\n1995). However, whereas in finite state-spaces, the steps of IPF can be computed exactly, these\nsteps are intractable in all but trivial scenarios in continuous state-spaces. Recent computational\napproaches proposed to approximate the IPF recursion in continuous state-spaces either rely on\ndeterministic (Chen et al., 2016) or stochastic (Reich, 2019) discretization of the space using\nN atoms, and then fall back on the finite state-space IPF algorithm. We propose here an\n1We can also use an interacting particle system instead of independent Markov chains (Del Moral et al., 2006).\n2\napproximate IPF scheme which instead relies on regression-based approximations in the spirit\nof Heng et al. (2017). We demonstrate experimentally its performance on various problems.\nThe rest of this paper is organized as follows. In the remaining part of Section 1, we define\nour notation, formalize the problem statement, review SMC samplers and their limitations, and\ngive a brief overview of the proposed method. In Section 2, we discuss Schr\u00a8odinger bridges and\ntheir various formulations, and introduce numerical algorithms to approximate them. In Section\n3, we discuss our main computational contribution, which we term the sequential Schr\u00a8odinger\nbridge sampler. In Section 4, we discuss connections between the Schr\u00a8odinger bridge problem\nand various other topics. Section 5 contains numerical experiments, and Section 6 concludes.\n1.2 Notation\nGiven integers n \u2264 m and a sequence {xt}t\u2208N, we define the set [n : m] = {n, . . . , m} and\nwrite the subsequence xn:m = (xn, xn+1, . . . , xm). Let (E, E) be an arbitrary measurable space,\nP(E) and M(E) denote the set of all probability measures and Markov transition kernels on E,\nrespectively. Given \u00b5, \u03bd \u2208 P(E), we write \u00b5 \u001c \u03bd if \u00b5 is absolutely continuous with respect to\n\u03bd, and denote the corresponding Radon\u2013Nikodym derivative as d\u00b5/d\u03bd. The Kullback\u2013Leibler\n(KL) divergence from \u03bd \u2208 P(E) to \u00b5 \u2208 P(E) is defined as\nKL(\u00b5|\u03bd) = Z\nE\nlog d\u00b5\nd\u03bd\n(x)\u00b5(dx)\nif the integral is finite and \u00b5 \u001c \u03bd, and KL(\u00b5|\u03bd) = \u221e otherwise. The set of all real-valued, E\u0002measurable and bounded functions on E is denoted by B(E). Given \u00b5 \u2208 P(E), M \u2208 M(E)\nand \u03d5 \u2208 B(E), we define the integral \u00b5(\u03d5) = R\nE\n\u03d5(x)\u00b5(dx) and the function M(\u03d5)(\u00b7) =\nR\nE\n\u03d5(y)M(\u00b7, dy) \u2208 B(E). For ease of presentation, we will often assume that measures and transi\u0002tion kernels admit densities with respect to a \u03c3-finite dominating measure dx, in which case we\nwrite the densities of \u00b5 \u2208 P(E) and M \u2208 M(E) as \u00b5(dx) = \u00b5(x)dx and M(x, dy) = M(x, y)dy,\nrespectively.\n1.3 Problem formulation and SMC samplers\nIn this article, we will restrict ourselves to E := R\nd\n, with E being the corresponding Borel \u03c3\u0002algebra. We are i",
          "original_query": "Sequential Monte Carlo for Schr\u00f6dinger Bridges / Sequential Schr\u00f6dinger Bridge methods",
          "cleaned_query": "Sequential Monte Carlo for Schr\u00f6dinger Bridges"
        },
        {
          "success": true,
          "title": "Feynman path integration in quantum dynamics",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/001046559190265M",
          "content": "[Skip to main content](https://www.sciencedirect.com/science/article/abs/pii/001046559190265M#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/science/article/abs/pii/001046559190265M#screen-reader-main-title)\n\n- [Access through\u00a0**your institution**](https://www.sciencedirect.com/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2F001046559190265M)\n- [Purchase PDF](https://www.sciencedirect.com/getaccess/pii/001046559190265M/purchase)\n\nSearch ScienceDirect\n\n## Article preview\n\n- [Abstract](https://www.sciencedirect.com/science/article/abs/pii/001046559190265M#preview-section-abstract)\n- [References (68)](https://www.sciencedirect.com/science/article/abs/pii/001046559190265M#preview-section-references)\n- [Cited by (142)](https://www.sciencedirect.com/science/article/abs/pii/001046559190265M#preview-section-cited-by)\n\n[![Elsevier](https://sdfestaticassets-us-east-1.sciencedirectassets.com/prod/0252b05f1c89f902c3409111ceb5a6d9843d1f5b/image/elsevier-non-solus.png)](https://www.sciencedirect.com/journal/computer-physics-communications)\n\n## [Computer Physics Communications](https://www.sciencedirect.com/journal/computer-physics-communications)\n\n[Volume 63, Issues 1\u20133](https://www.sciencedirect.com/journal/computer-physics-communications/vol/63/issue/1), February 1991, Pages 389-414\n\n[![Computer Physics Communications](https://ars.els-cdn.com/content/image/1-s2.0-S0010465524X00065-cov150h.gif)](https://www.sciencedirect.com/journal/computer-physics-communications/vol/63/issue/1)\n\n# Feynman path integration in quantum dynamics\n\nAuthor links open overlay panelNancyMakri1\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/0010-4655(91)90265-M](https://doi.org/10.1016/0010-4655(91)90265-M) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=001046559190265M&orderBeanReset=true)\n\n## Abstract\n\nThis article discusses the use of path integral techniques for the study of dynamical processes in many-body quantum systems. Recently developed approaches are described, which enable the evaluation of the real-time path integral with Monte Carlo methodology. Various methods for accelerating the convergence of the discretized path integral are also discussed. Several numerical examples are presented, which illustrate the basic ideas described in the paper.\n\nRecommended articles\n\n- J. Bartholomew _et al._\n\n\n### Phys. Rev. B\n\n\n\n(1985)\nA. Wallquist _et al._\n\n\n### Chem. Phys. Lett.\n\n\n\n(1985)\nA. Wallquist _et al._\n\n\n### J. Chem. Phys.\n\n\n\n(1986)\n\n- D.M. Ceperley _et al._\n\n\n### Phys. Rev. Lett.\n\n\n\n(1986)\n\n- N. Makri _et al._\n\n\n### Chem. Phys. Lett.\n\n\n\n(1988)\n\n- N. Makri _et al._\n\n\n### J. Chem. Phys.\n\n\n\n(1988)\n\n- R.A. Friesner _et al._\n\n\n### J. Chem. Phys.\n\n\n\n(1984)\n\n- P. Zhang _et al._\n\n\n### Chem. Phys. Lett.\n\n\n\n(1988)\n\n- M. Takahashi _et al._\n\n\n### J. Phys. Soc. Jpn.\n\n\n\n(1984)\nM. Takahashi _et al._\n\n\n### J. Phys. Soc. Jpn.\n\n\n\n(1984)\n\n- R.P. Feynman\n\n\n### Rev. Mod. Phys.\n\n\n\n(1948)\nR.P. Feynman _et al._\n\n### Quantum Mechanics and Path Integrals\n\n\n(1965)\n\n- N. Metropolis _et al._\n\n\n### J. Chem. Phys.\n\n\n\n(1953)\nJ.P. Valleau _et al._\n- M. Parrinello _et al._\n\n\n### J. Chem. Phys.\n\n\n\n(1984)\nC.D. Jonah _et al._\n\n\n### Chem. Phys. Lett.\n\n\n\n(1986)\n\n\nR.A. Kuharski _et al._\n\n### Chem. Phys. Lett.\n\n(1984)\nR.A. Kuharski _et al._\n\n### J. Chem. Phys.\n\n(1985)\nJ. Schnitker _et al._\n\n### J. Chem. Phys.\n\n(1986)\nJ. Schnitker _et al._\n\n### J. Chem. Phys.\n\n(1987)\n\nA. Nichols _et al._\n\n### J. Chem. Phys.\n\n(1984)\nM. Sprik _et al._\n\n### J. Chem. Phys.\n\n(1985)\n\n### J. Stat. Phys.\n\n(1986)\n\nP. Pechukas\n\n### Phys. Rev.\n\n(1969)\nP. Pechukas\n\n### Phys. Rev.\n\n(1969)\n\nW.H. Miller\n\n### J. Chem. Phys.\n\n(1970)\nW.H. Miller\n\n### Adv. Chem. Phys.\n\n(1974)\n\nW.H. Miller _et al._\n\n### J. Chem. Phys.\n\n(1983)\nR. Jaquet _et al._\n\n### J. Phys. Chem.\n\n(1984)\nK. Yamashita _et al._\n\n### J. Chem. Phys.\n\n(1985)\n\nD. Thirumalai _et al._\n\n### J. Chem. Phys.\n\n(1983)\nD. Thirumalai _et al._\n\n### J. Chem. Phys.\n\n(1983)\nD. Thirumalai _et al._\n\n### J. Chem. Phys.\n\n(1984)\nD. Thirumalai _et al._\n\n### Chem. Phys. Lett.\n\n(1985)\nD. Thirumalai _et al._\n\n### Ann. Rev. Phys. Chem.\n\n(1986)\n\nE.C. Behrman _et al._\n\n### J. Chem. Phys.\n\n(1983)\nE.C. Behrman _et al._\n\n### J. Chem. Phys.\n\n(1985)\n\nJ.D. Doll\n\n### J. Chem. Phys.\n\n(1984)\nJ.D. Doll _et al._\n\n### Science\n\n(1986)\n\nJ. Chang _et al._\n\n### J. Chem. Phys.\n\n(1987)\n\nJ.D. Doll _et al._\n\n### J. Chem. Phys.\n\n(1987)\n\nL.S. Schulman\n\n### Techniques and Applications of Path Integration\n\n(1981)\n\nH. Goldstein\n\n### Classical Mechanics\n\n(1980)\n\nB.J. Berne _et al._\n\n### Ann. Rev. Phys. Chem.\n\n(1986)\n\nM. Herman _et al._\n\n### J. Chem. Phys.\n\n(1982)\nD. Thirumalai _et al._\n\n### J. Chem. Phys.\n\n(1983)\n\nW.H. Miller\n\n### J. Chem. Phys.\n\n(1975)\n\nJ. Doll _et al._\n\n### J. Chem. Phys.\n\n(1979)\nD. Freeman _et al._\n\n### J. Chem. Phys.\n\n(1984)\n\nJ.D. Doll _et al._\n\n### Adv. Chem. Phys.\n\n(1990)\n\nR.D. Coalson\n\n### J. Chem. Phys.\n\n(1986)\n\nH. Kono _et al._\n\n### J. Chem. Phys.\n\n(1988)\n\nK.S. Schweizer _et al._\n\n### J. Chem. Phys.\n\n(1981)\n\nY. Fujiwara _et al._\n\n### Phys. Rev. A\n\n(1982)\n\nN. Makri _et al._\n\n### J. Chem. Phys.\n\n(1989)\n\nJ.D. Doll _et al._\n\n### Phys. Rev. Lett.\n\n(1985)\nR.D. Coalson _et al._\n\n### J. Chem. Phys.\n\n(1986)\nR.D. Coalson _et al._\n\n### J. Chem. Phys.\n\n(1989)\n\nView more references\n\n- ### [Numerical path integral solution to strong Coulomb correlation in one dimensional Hooke's atom](https://www.sciencedirect.com/science/article/pii/S0010465516302831)\n\n\n\n2017, Computer Physics Communications\n\n\n\n\n\n\n\nCitation Excerpt :\n\n\n\nFeynman path integral (PI) approach offers an intuitive description of quantum mechanics \\[1,2\\], where classical mechanics emerges transparently from disappearing wave nature of particles along with vanishing Planck constant. Therefore, it is robust in numerical calculations in cases close to classical ones, like molecular quantum dynamics in real time \\[3\\], but becomes more challenging and laborious for states of electrons, where the wave nature plays larger role. Furthermore, the PI presentation of stationary states also involves full time-dependent quantum dynamics, in contrast with the conventional solution of the time-dependent Schr\u00f6dinger equation, where time evolution appears as simple change of the wave function phase, only.\n\n\n\n\n\n\n\nShow abstract\n\n\n\n\n\n\n\nWe present a new approach based on real time domain Feynman path integrals (RTPI) for electronic structure calculations and quantum dynamics, which includes correlations between particles exactly but within the numerical accuracy. We demonstrate that incoherent propagation by keeping the wave function real is a novel method for finding and simulation of the ground state, similar to Diffusion Monte Carlo (DMC) method, but introducing new useful tools lacking in DMC. We use 1D Hooke\u2019s atom, a two-electron system with very strong correlation, as our test case, which we solve with incoherent RTPI (iRTPI) and compare against DMC. This system provides an excellent test case due to exact solutions for some confinements and because in 1D the Coulomb singularity is stronger than in two or three dimensional space. The use of Monte Carlo grid is shown to be efficient for which we determine useful numerical parameters. Furthermore, we discuss another novel approach achieved by combining the strengths of iRTPI and DMC. We also show usefulness of the perturbation theory for analytical approximates in case of strong confinements.\n\n- ### [Quantum dynamics at finite temperature: Time-dependent quantum Monte Carlo study](https://www.sciencedirect.com/science/article/pii/S0003491616300690)\n\n\n\n2016, Annals of Physics\n\n\n\n\n\n\n\nCitation Excerpt :\n\n\n\nRare examples of real-time simulation techniques at finite temperature include multi configuration time-dependent self-consistent field \\[6\\] and real-time path integral \\[7\\]. Although time-polynomial the efficiency of the former method is hampered by the calculation of large number of Coulomb and exchange integrals while ",
          "original_query": "Path-integral / Importance-weighted diffusion samplers for sampling from energy functions",
          "cleaned_query": "Path-integral"
        },
        {
          "success": true,
          "title": "Highly Scalable Diffusion Samplers via Adjoint Matching",
          "url": "https://arxiv.org/abs/2504.11713",
          "content": "[2504.11713] Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2504.11713\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2504.11713**(cs)\n[Submitted on 16 Apr 2025 ([v1](https://arxiv.org/abs/2504.11713v1)), last revised 28 May 2025 (this version, v3)]\n# Title:Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching\nAuthors:[Aaron Havens](https://arxiv.org/search/cs?searchtype=author&amp;query=Havens,+A),[Benjamin Kurt Miller](https://arxiv.org/search/cs?searchtype=author&amp;query=Miller,+B+K),[Bing Yan](https://arxiv.org/search/cs?searchtype=author&amp;query=Yan,+B),[Carles Domingo-Enrich](https://arxiv.org/search/cs?searchtype=author&amp;query=Domingo-Enrich,+C),[Anuroop Sriram](https://arxiv.org/search/cs?searchtype=author&amp;query=Anuroop),[Brandon Wood](https://arxiv.org/search/cs?searchtype=author&amp;query=Wood,+B),[Daniel Levine](https://arxiv.org/search/cs?searchtype=author&amp;query=Levine,+D),[Bin Hu](https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+B),[Brandon Amos](https://arxiv.org/search/cs?searchtype=author&amp;query=Amos,+B),[Brian Karrer](https://arxiv.org/search/cs?searchtype=author&amp;query=Karrer,+B),[Xiang Fu](https://arxiv.org/search/cs?searchtype=author&amp;query=Fu,+X),[Guan-Horng Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+G),[Ricky T. Q. Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+R+T+Q)\nView a PDF of the paper titled Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching, by Aaron Havens and 12 other authors\n[View PDF](https://arxiv.org/pdf/2504.11713)[HTML (experimental)](https://arxiv.org/html/2504.11713v3)> > Abstract:\n> We introduce Adjoint Sampling, a highly scalable and efficient algorithm for learning diffusion processes that sample from unnormalized densities, or energy functions. It is the first on-policy approach that allows significantly more gradient updates than the number of energy evaluations and model samples, allowing us to scale to much larger problem settings than previously explored by similar methods. Our framework is theoretically grounded in stochastic optimal control and shares the same theoretical guarantees as Adjoint Matching, being able to train without the need for corrective measures that push samples towards the target distribution. We show how to incorporate key symmetries, as well as periodic boundary conditions, for modeling molecules in both cartesian and torsional coordinates. We demonstrate the effectiveness of our approach through extensive experiments on classical energy functions, and further scale up to neural network-based energy models where we perform amortized conformer generation across many molecular systems. To encourage further research in developing highly scalable sampling methods, we plan to open source these challenging benchmarks, where successful methods can directly impact progress in computational chemistry. Subjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2504.11713](https://arxiv.org/abs/2504.11713)[cs.LG]|\n|(or[arXiv:2504.11713v3](https://arxiv.org/abs/2504.11713v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2504.11713](https://doi.org/10.48550/arXiv.2504.11713)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Aaron Havens [[view email](https://arxiv.org/show-email/62651a52/2504.11713)]\n**[[v1]](https://arxiv.org/abs/2504.11713v1)**Wed, 16 Apr 2025 02:20:06 UTC (11,967 KB)\n**[[v2]](https://arxiv.org/abs/2504.11713v2)**Fri, 18 Apr 2025 15:57:13 UTC (11,970 KB)\n**[v3]**Wed, 28 May 2025 17:16:01 UTC (13,496 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching, by Aaron Havens and 12 other authors\n* [View PDF](https://arxiv.org/pdf/2504.11713)\n* [HTML (experimental)](https://arxiv.org/html/2504.11713v3)\n* [TeX Source](https://arxiv.org/src/2504.11713)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2504.11713&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2504.11713&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-04](https://arxiv.org/list/cs.LG/2025-04)\nChange to browse by:\n[cs](https://arxiv.org/abs/2504.11713?context=cs)\n[cs.AI](https://arxiv.org/abs/2504.11713?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2504.11713)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2504.11713)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2504.11713)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/ab",
          "original_query": "Adjoint Sampling",
          "cleaned_query": "Adjoint Sampling"
        }
      ],
      "generated_ideas": [
        "Schr\u00f6dinger-Bridge-Consistent Noise Schedules for Diffusion Models\nLearn diffusion noise schedules by explicitly minimizing a Schr\u00f6dinger bridge (SB) objective: choose the forward noising SDE/Markov kernel so the entropic OT cost between data and prior is minimized under a parameterized schedule. Implement by alternating (i) score network training and (ii) Sinkhorn/IPF-style updates on time-discretized kernels, yielding schedules optimized for sample quality vs. step count.",
        "Path-Integral Correctors for Predictor\u2013Corrector Diffusion Sampling\nIntroduce a \u201cFeynman corrector\u201d that uses short-horizon path-integral reweighting to correct discretization error in reverse-time SDE samplers. Concretely, augment predictor\u2013corrector with a Monte Carlo estimate of a local action functional (from discretized path integrals) to adaptively accept/reject or reweight micro-steps, targeting better fidelity at very low step counts.",
        "Regression-Based Continuous-Space IPF for Score Models (Neural Sinkhorn on Paths)\nBuild a scalable continuous-state iterative proportional fitting algorithm where the IPF potentials are represented by neural regressors trained on simulated trajectories, following the \u201cSchr\u00f6dinger bridge samplers\u201d idea. Use this to compute SB solutions between a Gaussian prior and complex data distributions, and then derive a sampler that interpolates between SB dynamics and standard diffusion reverse SDE for controllable diversity\u2013quality tradeoffs.",
        "Exact-Likelihood Schr\u00f6dinger Bridge Neural ODEs\nCombine the SB viewpoint (entropic OT with boundary constraints) with the probability-flow (neural ODE) formulation to produce a deterministic bridge with exact likelihoods and boundary matching guarantees. Implement by parameterizing the drift as a time-dependent control and training it with a boundary-penalized KL-to-reference objective, enabling likelihood evaluation and controlled generation in one model.",
        "Diffusion as Entropic Optimal Control with Learned Terminal Constraints (Multi-Marginal SB)\nExtend the two-marginal SB to multi-marginal constraints that enforce intermediate-time distributions (e.g., coarse-to-fine decompression targets or curriculum marginals). Train a diffusion model where intermediate marginals are learned/optimized to reduce weight degeneracy and improve controllability, operationalized via multi-step IPF updates and score matching at multiple time slices.",
        "Variance-Reduced Normalizing Constant Estimation Using Diffusion Bridges\nUse SB samplers to build AIS/SMC-style estimators of partition functions for high-dimensional unnormalized models, but replace heuristic annealing paths with SB-optimized transition kernels. Concretely, estimate \\(Z\\) by constructing a bridge whose final marginal matches the target and whose path-space KL to a reference diffusion is minimized, reducing weight variance compared to standard AIS schedules.",
        "Inverse Problems with Schr\u00f6dinger-Bridge Priors and On-Policy Refinement\nRecast the inverse-problem methodology (e.g., inpainting/colorization) as an SB with one marginal constrained by measurements (a \u201cdata-consistent\u201d terminal distribution) and the other by a diffusion prior. Implement an alternating scheme: solve an approximate SB using neural IPF potentials, then refine on-policy using Adjoint Sampling gradients to better satisfy measurement constraints while maintaining realistic samples.",
        "Benchmark Suite: Diffusion vs. Schr\u00f6dinger Bridge vs. Adjoint Sampling on Shared Energy Landscapes\nCreate a standardized set of challenging energy functions (multi-modal, stiff, periodic, symmetry-constrained) and evaluate diffusion reverse-SDE samplers, SB samplers (IPF-based), and Adjoint Sampling under identical compute/energy-evaluation budgets. The contribution is an empirical and theoretical analysis identifying which regimes favor entropic transport (SB), score-based diffusion, or on-policy control, guiding future sampler design and hybrid methods."
      ],
      "generated_ideas_raw": "1. **Schr\u00f6dinger-Bridge-Consistent Noise Schedules for Diffusion Models**  \n   Learn diffusion noise schedules by explicitly minimizing a Schr\u00f6dinger bridge (SB) objective: choose the forward noising SDE/Markov kernel so the entropic OT cost between data and prior is minimized under a parameterized schedule. Implement by alternating (i) score network training and (ii) Sinkhorn/IPF-style updates on time-discretized kernels, yielding schedules optimized for sample quality vs. step count.\n\n2. **Adjoint-Sampling-Driven Training for Diffusion Models with Unnormalized Targets**  \n   Extend Adjoint Sampling to train diffusion/score models directly on unnormalized densities in image/3D domains (beyond molecular energies), treating the diffusion as an on-policy sampler for an energy-based target. The contribution is an end-to-end method where the score network is updated using adjoint-matching gradients while drastically reducing expensive energy evaluations via reuse of trajectories and control-variates.\n\n3. **Path-Integral Correctors for Predictor\u2013Corrector Diffusion Sampling**  \n   Introduce a \u201cFeynman corrector\u201d that uses short-horizon path-integral reweighting to correct discretization error in reverse-time SDE samplers. Concretely, augment predictor\u2013corrector with a Monte Carlo estimate of a local action functional (from discretized path integrals) to adaptively accept/reject or reweight micro-steps, targeting better fidelity at very low step counts.\n\n4. **Regression-Based Continuous-Space IPF for Score Models (Neural Sinkhorn on Paths)**  \n   Build a scalable continuous-state iterative proportional fitting algorithm where the IPF potentials are represented by neural regressors trained on simulated trajectories, following the \u201cSchr\u00f6dinger bridge samplers\u201d idea. Use this to compute SB solutions between a Gaussian prior and complex data distributions, and then derive a sampler that interpolates between SB dynamics and standard diffusion reverse SDE for controllable diversity\u2013quality tradeoffs.\n\n5. **Exact-Likelihood Schr\u00f6dinger Bridge Neural ODEs**  \n   Combine the SB viewpoint (entropic OT with boundary constraints) with the probability-flow (neural ODE) formulation to produce a deterministic bridge with exact likelihoods and boundary matching guarantees. Implement by parameterizing the drift as a time-dependent control and training it with a boundary-penalized KL-to-reference objective, enabling likelihood evaluation and controlled generation in one model.\n\n6. **Symmetry- and Periodicity-Preserving Diffusion on Manifolds via Controlled SDEs**  \n   Generalize the periodic-boundary and symmetry handling from Adjoint Sampling to score-based SDEs defined on manifolds (e.g., torus for angles, SO(3) for rotations, quotient spaces for permutation symmetries). The key contribution is a principled construction of forward/reverse SDEs and score parameterizations that are intrinsically equivariant and respect periodicity, improving molecular conformer and rigid-body pose generation without ad-hoc wrapping.\n\n7. **Diffusion as Entropic Optimal Control with Learned Terminal Constraints (Multi-Marginal SB)**  \n   Extend the two-marginal SB to multi-marginal constraints that enforce intermediate-time distributions (e.g., coarse-to-fine decompression targets or curriculum marginals). Train a diffusion model where intermediate marginals are learned/optimized to reduce weight degeneracy and improve controllability, operationalized via multi-step IPF updates and score matching at multiple time slices.\n\n8. **Variance-Reduced Normalizing Constant Estimation Using Diffusion Bridges**  \n   Use SB samplers to build AIS/SMC-style estimators of partition functions for high-dimensional unnormalized models, but replace heuristic annealing paths with SB-optimized transition kernels. Concretely, estimate \\(Z\\) by constructing a bridge whose final marginal matches the target and whose path-space KL to a reference diffusion is minimized, reducing weight variance compared to standard AIS schedules.\n\n9. **Inverse Problems with Schr\u00f6dinger-Bridge Priors and On-Policy Refinement**  \n   Recast the inverse-problem methodology (e.g., inpainting/colorization) as an SB with one marginal constrained by measurements (a \u201cdata-consistent\u201d terminal distribution) and the other by a diffusion prior. Implement an alternating scheme: solve an approximate SB using neural IPF potentials, then refine on-policy using Adjoint Sampling gradients to better satisfy measurement constraints while maintaining realistic samples.\n\n10. **Benchmark Suite: Diffusion vs. Schr\u00f6dinger Bridge vs. Adjoint Sampling on Shared Energy Landscapes**  \n   Create a standardized set of challenging energy functions (multi-modal, stiff, periodic, symmetry-constrained) and evaluate diffusion reverse-SDE samplers, SB samplers (IPF-based), and Adjoint Sampling under identical compute/energy-evaluation budgets. The contribution is an empirical and theoretical analysis identifying which regimes favor entropic transport (SB), score-based diffusion, or on-policy control, guiding future sampler design and hybrid methods.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Schr\u00f6dinger-Bridge-Consistent Noise Schedules for Diffusion Models\nLearn diffusion noise schedules by explicitly minimizing a Schr\u00f6dinger bridge (SB) objective: choose the forward noising SDE/Markov k",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Path-Integral Correctors for Predictor\u2013Corrector Diffusion Sampling\nIntroduce a \u201cFeynman corrector\u201d that uses short-horizon path-integral reweighting to correct discretization error in reverse-time SD",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Regression-Based Continuous-Space IPF for Score Models (Neural Sinkhorn on Paths)\nBuild a scalable continuous-state iterative proportional fitting algorithm where the IPF potentials are represented by",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Exact-Likelihood Schr\u00f6dinger Bridge Neural ODEs\nCombine the SB viewpoint (entropic OT with boundary constraints) with the probability-flow (neural ODE) formulation to produce a deterministic bridge wi",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Diffusion as Entropic Optimal Control with Learned Terminal Constraints (Multi-Marginal SB)\nExtend the two-marginal SB to multi-marginal constraints that enforce intermediate-time distributions (e.g.,",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Variance-Reduced Normalizing Constant Estimation Using Diffusion Bridges\nUse SB samplers to build AIS/SMC-style estimators of partition functions for high-dimensional unnormalized models, but replace ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Inverse Problems with Schr\u00f6dinger-Bridge Priors and On-Policy Refinement\nRecast the inverse-problem methodology (e.g., inpainting/colorization) as an SB with one marginal constrained by measurements (",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Benchmark Suite: Diffusion vs. Schr\u00f6dinger Bridge vs. Adjoint Sampling on Shared Energy Landscapes\nCreate a standardized set of challenging energy functions (multi-modal, stiff, periodic, symmetry-con",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 11,
      "paper_title": "Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies",
      "contribution": "Demonstrates that adding an explicit, compute-aware inference phase (using search/optimization strategies such as tree search, sampling and adaptation) on top of trained RL policies substantially breaks zero-shot performance ceilings in complex multi-agent and combinatorial tasks, yielding large empirical gains with modest extra wall-clock time.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 9715,
      "output_tokens": 945,
      "predecessor_details": [
        {
          "success": true,
          "title": "Mastering the game of Go with deep neural networks and tree search",
          "url": "https://www.semanticscholar.org/paper/Mastering-the-game-of-Go-with-deep-neural-networks-Silver-Huang/846aedd869a00c09b40f1f1f35673cb22bc87490",
          "content": "[Skip to search form](https://www.semanticscholar.org/paper/Mastering-the-game-of-Go-with-deep-neural-networks-Silver-Huang/846aedd869a00c09b40f1f1f35673cb22bc87490#search-form) [Skip to main content](https://www.semanticscholar.org/paper/Mastering-the-game-of-Go-with-deep-neural-networks-Silver-Huang/846aedd869a00c09b40f1f1f35673cb22bc87490#main-content) [Skip to account menu](https://www.semanticscholar.org/paper/Mastering-the-game-of-Go-with-deep-neural-networks-Silver-Huang/846aedd869a00c09b40f1f1f35673cb22bc87490#account-menu)\n\n- DOI: [10.1038/nature16961](https://doi.org/10.1038/nature16961)\n- Corpus ID: 515925\n\n# Mastering the game of Go with deep neural networks and tree search\n\n```\n@article{Silver2016MasteringTG,\n title={Mastering the game of Go with deep neural networks and tree search},\n author={David Silver and Aja Huang and Chris J. Maddison and Arthur Guez and L. Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Vedavyas Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy P. Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},\n journal={Nature},\n year={2016},\n volume={529},\n pages={484-489},\n url={https://api.semanticscholar.org/CorpusID:515925}\n}\n```\n\n- [David Silver](https://www.semanticscholar.org/author/David-Silver/145824029), [Aja Huang](https://www.semanticscholar.org/author/Aja-Huang/1885349), +17 authors[D. Hassabis](https://www.semanticscholar.org/author/D.-Hassabis/48987704)\n- Published in [Nature](https://www.semanticscholar.org/venue?name=Nature)27 January 2016\n- Computer Science\n\nTLDR\n\nUsing this search algorithm, the program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0.5, the first time that a computer program has defeated a human professional player in the full-sized game of Go.Expand\n\n[View on Springer](https://doi.org/10.1038/nature16961)\n\n[nature.com](https://www.nature.com/articles/nature16961.pdf)\n\nSave to LibrarySave\n\nCreate AlertAlert\n\nCite\n\nShare\n\n15,333 Citations\n\n[Highly Influential Citations](https://www.semanticscholar.org/paper/Mastering-the-game-of-Go-with-deep-neural-networks-Silver-Huang/846aedd869a00c09b40f1f1f35673cb22bc87490#citing-papers)\n\n539\n\n[Background Citations](https://www.semanticscholar.org/paper/Mastering-the-game-of-Go-with-deep-neural-networks-Silver-Huang/846aedd869a00c09b40f1f1f35673cb22bc87490#citing-papers)\n\n6,851\n\n[Methods Citations](https://www.semanticscholar.org/paper/Mastering-the-game-of-Go-with-deep-neural-networks-Silver-Huang/846aedd869a00c09b40f1f1f35673cb22bc87490#citing-papers)\n\n2,325\n\n[Results Citations](https://www.semanticscholar.org/paper/Mastering-the-game-of-Go-with-deep-neural-networks-Silver-Huang/846aedd869a00c09b40f1f1f35673cb22bc87490#citing-papers)\n\n82\n\n[View All](https://www.semanticscholar.org/paper/Mastering-the-game-of-Go-with-deep-neural-networks-Silver-Huang/846aedd869a00c09b40f1f1f35673cb22bc87490#citing-papers)\n\n## Topics\n\nAI-Generated\n\n[Games Of Self-play (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/65818268062?corpusId=515925) [Human Expert Games (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/48331845832?corpusId=515925) [AlphaGo (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/14177424738?corpusId=515925) [Game Of Go (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/55415771968?corpusId=515925) [European Go Champion (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/59898586099?corpusId=515925) [Backgammon (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/43462980948?corpusId=515925) [Monte Carlo Tree Search (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/66732875821?corpusId=515925) [Computer Go Program (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/58907410061?corpusId=515925) [Superhuman Performance (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/41072379478?corpusId=515925) [Expert Moves (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/67387824320?corpusId=515925)\n\n## 15,333 Citations\n\nCitation Type\n\nHas PDF\n\nAuthor\n\nMore Filters\n\nMore Filters\n\nFilters\n\nSort by RelevanceSort by Most Influenced PapersSort by Citation CountSort by Recency\n\n[**Playing Go without Game Tree Search Using Convolutional Neural Networks**](https://www.semanticscholar.org/paper/Playing-Go-without-Game-Tree-Search-Using-Neural-Barratt-Pan/eb438fbd38b1a2f06425532c394bac94709602c3)\n\n[J. Barratt](https://www.semanticscholar.org/author/J.-Barratt/120124220)[Chuanbo Pan](https://www.semanticscholar.org/author/Chuanbo-Pan/4048830)\n\nComputer Science\n\n[ArXiv](https://www.semanticscholar.org/venue?name=ArXiv)\n\n- 2019\n\nTLDR\n\nThis work attempts to mimic human intuition in the game of Go by creating a convolutional neural policy network which, without any sort of tree search, should play the game at or above the level of most humans.Expand\n\n- [3](https://www.semanticscholar.org/paper/eb438fbd38b1a2f06425532c394bac94709602c3#citing-papers)\n- [Highly Influenced](https://www.semanticscholar.org/paper/eb438fbd38b1a2f06425532c394bac94709602c3?sort=is-influential#citing-papers)\n\\[PDF\\]\n\n- 9 Excerpts\n\nSave\n\n[**GoGoGo : Improving Deep Neural Network Based Go Playing AI with Residual Networks**](https://www.semanticscholar.org/paper/GoGoGo-%3A-Improving-Deep-Neural-Network-Based-Go-AI-Liu/12c5e7a0bd799a12aa199219234dddefc5d5163a)\n\n[Xingyu Liu](https://www.semanticscholar.org/author/Xingyu-Liu/49543200)\n\nComputer Science\n\n- 2016\n\nTLDR\n\nAlphaGo, a Go-playing AI built by Google DeepMind, used a new approach of combining deep neural networks with tree search to solve the Go playing problem and defeated the 18-time Go world champion Lee Sedol.Expand\n\n- [PDF](https://www.semanticscholar.org/paper/12c5e7a0bd799a12aa199219234dddefc5d5163a)\n\n\nSave\n\n[**Beyond Games: A Systematic Review of Neural Monte Carlo Tree Search Applications**](https://www.semanticscholar.org/paper/Beyond-Games%3A-A-Systematic-Review-of-Neural-Monte-Kemmerling-L%C3%BCtticke/916437f03195d740ba3ebc20d283869ce11f17a4)\n\n[Marco Kemmerling](https://www.semanticscholar.org/author/Marco-Kemmerling/1644556014)[Daniel L\u00fctticke](https://www.semanticscholar.org/author/Daniel-L%C3%BCtticke/1740649500)[R. Schmitt](https://www.semanticscholar.org/author/R.-Schmitt/46253301)\n\nComputer Science\n\nAppl. Intell.\n\n- 2024\n\nTLDR\n\nA systematic literature review of peer-reviewed articles detailing the application of neural Monte Carlo tree search methods in domains other than games to map the current landscape of algorithms in the family of neural monte carlo tree search as they are applied to practical problems.Expand\n\n- [5](https://www.semanticscholar.org/paper/916437f03195d740ba3ebc20d283869ce11f17a4#citing-papers)\n- [Highly Influenced](https://www.semanticscholar.org/paper/916437f03195d740ba3ebc20d283869ce11f17a4?sort=is-influential#citing-papers)\n\\[PDF\\]\n\n- 6 Excerpts\n\nSave\n\n[**Mastering the game of Go without human knowledge**](https://www.semanticscholar.org/paper/Mastering-the-game-of-Go-without-human-knowledge-Silver-Schrittwieser/c27db32efa8137cbf654902f8f728f338e55cd1c)\n\n[David Silver](https://www.semanticscholar.org/author/David-Silver/145824029)[Julian Schrittwieser](https://www.semanticscholar.org/author/Julian-Schrittwieser/4337102)+14 authors[D. Hassabis](https://www.semanticscholar.org/author/D.-Hassabis/48987704)\n\nComputer Science\n\n[Nature](https://www.semanticscholar.org/venue?name=Nature)\n\n- 2017\n\nTLDR\n\nAn algorithm based solely on reinforcement learning is introduced, without human data, guidance or domain knowledge beyond game rules, that achieves superhuman performance, winning 100\u20130 against the previously published, champion-defeating AlphaGo.Expand\n\n- [8,3",
          "original_query": "Mastering the game of Go with deep neural networks and tree search",
          "cleaned_query": "Mastering the game of Go with deep neural networks and tree search"
        },
        {
          "success": true,
          "title": "Bandit Based Monte-Carlo Planning - Springer Link",
          "url": "https://link.springer.com/chapter/10.1007/11871842_29",
          "content": "Bandit Based Monte-Carlo Planning | Springer Nature Link (formerly SpringerLink)\n[Skip to main content](#main-content)\nAdvertisement\n[![Springer Nature Link](https://link.springer.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/chapter/10.1007/11871842_29?)\n# Bandit Based Monte-Carlo Planning\n* Conference paper\n* pp 282\u2013293\n* [Cite this conference paper](#citeas)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/book/978-3-540-46056-5?as=webp)Machine Learning: ECML 2006](https://link.springer.com/book/10.1007/11871842)(ECML 2006)\nBandit Based Monte-Carlo Planning\n* [Levente Kocsis](#auth-Levente-Kocsis)[21](#Aff21)&amp;\n* [Csaba Szepesv\u00e1ri](#auth-Csaba-Szepesv_ri)[21](#Aff21)\nPart of the book series:[Lecture Notes in Computer Science](https://link.springer.com/series/558)((LNAI,volume 4212))\nIncluded in the following conference series:\n* [European Conference on Machine Learning](https://link.springer.com/conference/ecml)\n* 22kAccesses\n* 2098Citations\n* 23[Altmetric](https://link.altmetric.com/details/21037691)\n## Abstract\nFor large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.\n[Download to read the full chapter text](https://link.springer.com/content/pdf/10.1007/11871842_29.pdf)\n## Chapter PDF\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs00186-024-00868-x/MediaObjects/186_2024_868_Fig1_HTML.png)\n### [Low-complexity algorithm for restless bandits with imperfect observations](https://link.springer.com/10.1007/s00186-024-00868-x?fromPaywallRec=false)\nArticle05 September 2024\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-319-46227-1?as&#x3D;webp)\n### [Linear Bandits in Unknown Environments](https://link.springer.com/10.1007/978-3-319-46227-1_18?fromPaywallRec=false)\nChapter\u00a9 2016\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-030-51186-9?as&#x3D;webp)\n### [Markov Decision Processes with Discounted Costs over a Finite Horizon: Action Elimination](https://link.springer.com/10.1007/978-3-030-51186-9_14?fromPaywallRec=false)\nChapter\u00a9 2021\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Applied Probability](https://link.springer.com/subjects/applied-probability)\n* [Discrete Optimization](https://link.springer.com/subjects/discrete-optimization)\n* [Operations Research and Decision Theory](https://link.springer.com/subjects/operations-research-and-decision-theory)\n* [Probability and Statistics in Computer Science](https://link.springer.com/subjects/probability-and-statistics-in-computer-science)\n* [Stochastic Systems and Control](https://link.springer.com/subjects/stochastic-systems-and-control)\n* [Stochastic Learning and Adaptive Control](https://link.springer.com/subjects/stochastic-learning-and-adaptive-control)\n## References\n1. Auer, P., Cesa-Bianchi, N., Fischer, P.: Finite time analysis of the multiarmed bandit problem. Machine Learning\u00a047(2-3), 235\u2013256 (2002)\n[Article](https://doi.org/10.1023/A:1013689704352)[MATH](http://www.emis.de/MATH-item?1012.68093)[Google Scholar]()\n2. Auer, P., Cesa-Bianchi, N., Freund, Y., Schapire, R.E.: The nonstochastic multiarmed bandit problem. SIAM Journal on Computing\u00a032, 48\u201377 (2002)\n[Article](https://doi.org/10.1137/S0097539701398375)[MATH](http://www.emis.de/MATH-item?1029.68087)[MathSciNet](http://www.ams.org/mathscinet-getitem?mr=1954855)[Google Scholar]()\n3. Barto, A.G., Bradtke, S.J., Singh, S.P.: Real-time learning and control using asynchronous dynamic programming. Technical report 91-57, Computer Science Department, University of Massachusetts (1991)\n[Google Scholar]()\n4. Billings, D., Davidson, A., Schaeffer, J., Szafron, D.: The challenge of poker. Artificial Intelligence\u00a0134, 201\u2013240 (2002)\n[Article](https://doi.org/10.1016/S0004-3702(01)00130-8)[MATH](http://www.emis.de/MATH-item?0982.68125)[Google Scholar]()\n5. Bouzy, B., Helmstetter, B.: Monte Carlo Go developments. In: van den Herik, H.J., Iida, H., Heinz, E.A. (eds.) Advances in Computer Games 10, pp. 159\u2013174 (2004)\n[Google Scholar]()\n6. Chang, H.S., Fu, M., Hu, J., Marcus, S.I.: An adaptive sampling algorithm for solving Markov decision processes. Operations Research\u00a053(1), 126\u2013139 (2005)\n[Article](https://doi.org/10.1287/opre.1040.0145)[MATH](http://www.emis.de/MATH-item?1165.90672)[MathSciNet](http://www.ams.org/mathscinet-getitem?mr=2131102)[Google Scholar]()\n7. Chung, M., Buro, M., Schaeffer, J.: Monte Carlo planning in RTS games. In: CIG 2005, Colchester, UK (2005)\n[Google Scholar]()\n8. Kearns, M., Mansour, Y., Ng, A.Y.: A sparse sampling algorithm for near-optimal planning in large Markovian decisi on processes. In: Proceedings of IJCAI 1999, pp. 1324\u20131331 (1999)\n[Google Scholar]()\n9. Lai, T.L., Robbins, H.: Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics\u00a06, 4\u201322 (1985)\n[Article](https://doi.org/10.1016/0196-8858(85)90002-8)[MATH](http://www.emis.de/MATH-item?0568.62074)[MathSciNet](http://www.ams.org/mathscinet-getitem?mr=776826)[Google Scholar]()\n10. P\u00e9ret, L., Garcia, F.: On-line search for solving Markov decision processes via heuristic sampling. In: de M\u00e1ntaras, R.L., Saitta, L. (eds.) ECAI, pp. 530\u2013534 (2004)\n[Google Scholar]()\n11. Sheppard, B.: World-championship-caliber Scrabble. Artificial Intelligence\u00a0134(1\u20132), 241\u2013275 (2002)\n[Article](https://doi.org/10.1016/S0004-3702(01)00166-7)[MATH](http://www.emis.de/MATH-item?0982.68126)[Google Scholar]()\n12. Smith, S.J.J., Nau, D.S.: An analysis of forward pruning. In: AAAI, pp. 1386\u20131391 (1994)\n[Google Scholar]()\n13. Tesauro, G., Galperin, G.R.: On-line policy improvement using Monte-Carlo search. In: Mozer, M.C., Jordan, M.I., Petsche, T. (eds.) NIPS 9, pp. 1068\u20131074 (1997)\n[Google Scholar]()\n14. Vanderbei, R.: Optimal sailing strategies, statistics and operations research program. University of Princeton (1996),[http://www.sor.princeton.edu/\\~rvdb/sail/sail.html](http://www.sor.princeton.edu/~rvdb/sail/sail.html)\n[Download references](https://citation-needed.springer.com/v2/references/10.1007/11871842_29?format=refman&amp;flavour=references)\n## Author information\n### Authors and Affiliations\n1. Computer and Automation Research Institute of the Hungarian Academy of Sciences, Kende u. 13-17, 1111, Budapest, Hungary\nLevente Kocsis\u00a0&amp;\u00a0Csaba Szepesv\u00e1ri\nAuthors\n1. Levente Kocsis\n[View author publications](https://link.springer.com/search?sortBy=newestFirst&amp;dc.creator=Levente%20Kocsis)\nSearch author on:[PubMed]()[Google Scholar]()\n2. Csaba Szepesv\u00e1ri\n[View author publications](https://link.springer.com/search?sortBy=newestFirst&amp;dc.creator=Csaba%20Szepesv%C3%A1ri)\nSearch author on:[PubMed]()[Google Scholar]()\n## Editor information\n### Editors and Affiliations\n1. Knowledge Engineering Group, Technische Universit\u00e4t Darmstadt,\nJohannes F\u00fcrnkranz\n2. Max Planck Institute for Computer Science, Saarbr\u00fccken, Germany\nTobias Scheffer\n3. Faculty of Computer Science, Otto-von-Guericke-University Magdeburg, Germany\nMyra Spiliopoulou\n## Rights and permissions\n[Reprints and permissions]()\n## Copyright information\n\u00a92006 Springer-Verlag Berlin Heidelberg\n## About this paper\n### Cite this paper\nKocsis, L., Szepesv\u00e1ri, C. (2006). Bandit Based Monte-Carlo Planning.\nIn: F\u00fcrnkranz, J., Scheffer, T., Spiliopoul",
          "original_query": "Bandit based Monte\u2011Carlo planning (UCT)",
          "cleaned_query": "Bandit based Monte\u2011Carlo planning (UCT)"
        },
        {
          "success": true,
          "title": "Neural combinatorial optimization with reinforcement learning ...",
          "url": "https://link.springer.com/article/10.1007/s10462-024-11045-1",
          "content": "Neural combinatorial optimization with reinforcement learning in industrial engineering: a survey | Artificial Intelligence Review\n[Skip to main content](#main)\nAdvertisement\n[![Springer Nature Link](https://link.springer.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1007/s10462-024-11045-1?)\n# Neural combinatorial optimization with reinforcement learning in industrial engineering: a survey\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:14 February 2025\n* Volume\u00a058, article\u00a0number130, (2025)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://link.springer.com/content/pdf/10.1007/s10462-024-11045-1.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/10462?as=webp)Artificial Intelligence Review](https://link.springer.com/journal/10462)[Aims and scope](https://link.springer.com/journal/10462/aims-and-scope)[Submit manuscript](https://submission.nature.com/new-submission/10462/3)\nNeural combinatorial optimization with reinforcement learning in industrial engineering: a survey\n[Download PDF](https://link.springer.com/content/pdf/10.1007/s10462-024-11045-1.pdf)\n* [K. T. Chung](#auth-K__T_-Chung-Aff1)[ORCID:orcid.org/0000-0001-7427-0917](https://orcid.org/0000-0001-7427-0917)[1](#Aff1),\n* [C. K. M. Lee](#auth-C__K__M_-Lee-Aff1)[ORCID:orcid.org/0000-0001-8577-4547](https://orcid.org/0000-0001-8577-4547)[1](#Aff1)&amp;\n* [Y. P. Tsang](#auth-Y__P_-Tsang-Aff1)[ORCID:orcid.org/0000-0002-6128-345X](https://orcid.org/0000-0002-6128-345X)[1](#Aff1)\n* 6553Accesses\n* 8Citations\n* 1Altmetric\n* [Explore all metrics](https://link.springer.com/article/10.1007/s10462-024-11045-1/metrics)\n## Abstract\nIn recent trends, machine learning is widely used to support decision-making in various domains and industrial operations. Because of the increasing complexity of modern industries, industrial engineering aims not only to increase cost-effectiveness and productivity but also to consider sustainability, resilience, and human centricity, resulting in many-objective, constrained, and stochastic operations research. Based on the above stringent requirements, combinatorial optimization (CO) problems are thus developed to support the complicated decision-making process in operations research. Due to the computational complexity of exact algorithms and the uncertain solution quality of heuristic methods, there is a growing trend to leverage the power of machine learning in solving CO problems, known as neural combinatorial optimization (NCO), where reinforcement learning (RL) is the core to achieve the sequential decision support. This survey study provides a comprehensive investigation of the theories and recent advancements in applying RL to solve hard CO problems, such as vehicle routing, bin packing, assignment, scheduling, and planning problems, and, in addition, summarizes the applications of neural combinatorial optimization with reinforcement learning (NCO-RL). The detailed review found that although the research domain of NCO-RL is still under-explored, its research potential has been proven to address environmental sustainability, adaptability, and human factors. Last but not least, the technical challenges and opportunities of the NCO-RL to embrace the industry 5.0 paradigm are discussed.\n### Similar content being viewed by others\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-030-86286-2?as&#x3D;webp)\n### [Reinforcement Learning for the Knapsack Problem](https://link.springer.com/10.1007/978-3-030-86286-2_1?fromPaywallRec=false)\nChapter\u00a9 2021\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs43153-023-00422-y/MediaObjects/43153_2023_422_Fig1_HTML.png)\n### [Comparison of reinforcement learning techniques for controlling a CSTR process](https://link.springer.com/10.1007/s43153-023-00422-y?fromPaywallRec=false)\nArticle11 December 2023\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-25312-6?as&#x3D;webp)\n### [An Architecture for\u00a0Deploying Reinforcement Learning in\u00a0Industrial Environments](https://link.springer.com/10.1007/978-3-031-25312-6_67?fromPaywallRec=false)\nChapter\u00a9 2022\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Combinatorial Chemistry](https://link.springer.com/subjects/combinatorial-chemistry)\n* [Discrete Optimization](https://link.springer.com/subjects/discrete-optimization)\n* [Dynamic Combinatorial Chemistry](https://link.springer.com/subjects/dynamic-combinatorial-chemistry)\n* [Industrial and Production Engineering](https://link.springer.com/subjects/industrial-and-production-engineering)\n* [Operations Research and Decision Theory](https://link.springer.com/subjects/operations-research-and-decision-theory)\n* [Operations Research, Management Science](https://link.springer.com/subjects/operations-research-management-science-)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=10462)\nAvoid common mistakes on your manuscript.\n## 1Introduction\nIndustrial engineering is concerned with the integrated system of people, materials, information, equipment, and energy to predict, evaluate, and improve the performance of the processes and different stages of the systems (Salvendy[2001](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR62)). Decision-making is a fundamental part of industrial engineering applications, ranging from investment decisions, layout design, production scheduling, inventory control, and routing problems in various domains, including manufacturing, logistics, and supply chain management (Sgarbossa et al.[2020](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR64); Triantaphyllou and Mann[1995](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR79)). Because of the increasing complexity of modern industries, industrial engineering aims not only to enhance cost-effectiveness and productivity but also to consider sustainability, adaptability, and human factors to achieve a broad range of objectives and operational requirements (Colabianchi et al.[2021](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR15); Kadir et al.[2019](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR28); Manavalan and Jayakrishna[2019](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR48); Sgarbossa et al.[2020](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR64)). In addition, facing the challenges in today\u2019s competitive markets, modern manufacturing has evolved to adopt the demand dynamics and unexpected disruptions for timely response. Since conventional static models are inadequate in these areas, dynamic models have been studied to capture the relationships among variables better (Mittal et al.[2008](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR52)).\nWhen there is a need to model and optimize complex decision-making problems, operations research provides a wide range of methods and techniques for decision-makers to reliably determine optimal solutions for specific operational problems (Shannon et al.[1980](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR65)). Typically, operations research focuses on the cost minimization of existing processes and formulating decision intelligence (Bengio et al.[2021](https://link.springer.com/article/10.1007/s10462-024-11045-1#ref-CR5); Dekker et al.[2012](https://link.springer.com/article/10.1007/s104",
          "original_query": "Neural Combinatorial Optimization with Reinforcement Learning",
          "cleaned_query": "Neural Combinatorial Optimization with Reinforcement Learning"
        },
        {
          "success": true,
          "title": "[1803.08475] Attention, Learn to Solve Routing Problems! - arXiv",
          "url": "https://arxiv.org/abs/1803.08475",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "Attention, Learn to Solve Routing Problems!",
          "cleaned_query": "Attention, Learn to Solve Routing Problems!"
        },
        {
          "success": true,
          "title": "The Cross-Entropy Method - Springer Link",
          "url": "https://link.springer.com/book/10.1007/978-1-4757-4321-0",
          "content": "The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning | Springer Nature Link (formerly SpringerLink)\n[Skip to main content](#main-content)\nAdvertisement\n[![Springer Nature Link](https://link.springer.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/book/10.1007/978-1-4757-4321-0?)\n[![](https://media.springernature.com/w90/springer-static/cover-hires/book/978-1-4757-4321-0?as=webp)](https://link.springer.com/book/10.1007/978-1-4757-4321-0/cover)\n# The Cross-Entropy Method\nA Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation and Machine Learning\n* Book\n* &copy;2004\n* 1st edition\n* [View latest edition](https://link.springer.com/book/9780387212401)\n[Accessibility Information](#accessibility-information)\n## Overview\nAuthors:\n* [Reuven Y. Rubinstein](#author-0-0)[0](#Aff-0-0),\n* [Dirk P. Kroese](#author-0-1)[1](#Aff-0-1)\n1. Reuven Y. Rubinstein\n1. Department of Industrial Engineering and Management, Technion, Technion City, Haifa, Israel\n[View author publications](https://link.springer.com/search?dc.creator=Reuven+Y.+Rubinstein&sortBy=newestFirst)\nSearch author on:[PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Reuven+Y.+Rubinstein)[Google Scholar](http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=\"Reuven+Y.+Rubinstein\"&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en)\n2. Dirk P. Kroese\n1. Department of Mathematics, University of Queensland, Brisbane, Australia\n[View author publications](https://link.springer.com/search?dc.creator=Dirk+P.+Kroese&sortBy=newestFirst)\nSearch author on:[PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Dirk+P.+Kroese)[Google Scholar](http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=\"Dirk+P.+Kroese\"&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en)\n* A comprehensive and accessible introduction to the cross-entropy (CE) method\n* Based on an advanced undergraduate course on the CE method, given at the Israel Institute of Technology (Technion) for the last three years\n* Includes supplementary material:[sn.pub/extras](https://extras.springer.com/?query=978-0-387-21240-1)\nPart of the book series:[Information Science and Statistics](https://link.springer.com/series/3816)(ISS)\n* 13kAccesses\n* 936Citations\n* 8[Altmetric](https://link.altmetric.com/details/32599055)\nThis is a preview of subscription content,[log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/book/10.1007/978-1-4757-4321-0?error=cookies_not_supported&code=5facf87d-0eba-451f-a77f-ad63f58c53a5)to check access.\n## Access this book\n[Log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/book/10.1007/978-1-4757-4321-0?error=cookies_not_supported&code=5facf87d-0eba-451f-a77f-ad63f58c53a5)\nSoftcover BookUSD129.99\nPrice excludes VAT (USA)\n* Compact, lightweight edition\n* Dispatched in 3 to 5 business days\n* Free shipping worldwide -[see info](https://support.springernature.com/en/support/solutions/articles/6000233448-coronavirus-disease-covid-19-delivery-information)Buy Softcover Book\nHardcover BookUSD179.99\nPrice excludes VAT (USA)\n* Durable hardcover edition\n* Dispatched in 3 to 5 business days\n* Free shipping worldwide -[see info](https://support.springernature.com/en/support/solutions/articles/6000233448-coronavirus-disease-covid-19-delivery-information)Buy Hardcover Book\nTax calculation will be finalised at checkout\n[Licence this eBook for your library](https://single-ebooks.springernature.com/search?query=10.1007/978-1-4757-4321-0)\n[Learn about institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/ebooks)\n## Other ways to access\n[Licence this eBook for your library](https://single-ebooks.springernature.com/search?query=10.1007/978-1-4757-4321-0)\n[Institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/ebooks)\n## About this book\nThis book is a comprehensive and accessible introduction to the cross-entropy (CE) method. The CE method started life around 1997 when the first author proposed an adaptive algorithm for rare-event simulation using a cross-entropy minimization technique. It was soon realized that the underlying ideas had a much wider range of application than just in rare-event simulation; they could be readily adapted to tackle quite general combinatorial and multi-extremal optimization problems, including many problems associated with the field of learning algorithms and neural computation. The book is based on an advanced undergraduate course on the CE method, given at the Israel Institute of Technology (Technion) for the last three years. It is aimed at a broad audience of engineers, computer scientists, mathematicians, statisticians and in general anyone, theorist or practitioner, who is interested in smart simulation, fast optimization, learning algorithms, image processing, etc. Our aim was to write a book on the CE method which was accessible to advanced undergraduate students and engineers who simply want to apply the CE method in their work, while at the same time accentu\u00ad ating the unifying and novel mathematical ideas behind the CE method, so as to stimulate further research at a postgraduate level.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs40430-025-05764-x/MediaObjects/40430_2025_5764_Fig1_HTML.png)\n### [A single-loop Kriging model coupled with cross-entropy importance sampling for time-variant reliability analysis of rare events](https://link.springer.com/10.1007/s40430-025-05764-x?fromPaywallRec=true)\nArticle10 July 2025\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-031-35897-5?as&#x3D;webp)\n### [Introducing Computer Science and Arts for All (CSA4ALL): Developing an Inclusive Curriculum and Portal for K5 Children](https://link.springer.com/10.1007/978-3-031-35897-5_24?fromPaywallRec=true)\nChapter\u00a9 2023\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10639-017-9677-z/MediaObjects/10639_2017_9677_Fig1_HTML.gif)\n### [Lines, roamers, and squares: Oh my! using floor robots to enhance Hispanic students\u2019 understanding of programming](https://link.springer.com/10.1007/s10639-017-9677-z?fromPaywallRec=true)\nArticle21 December 2017\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects.\n* [Computer Modelling](https://link.springer.com/subjects/computer-modelling)\n* [Computational Intelligence](https://link.springer.com/subjects/computational-intelligence)\n* [Operations Research and Decision Theory](https://link.springer.com/subjects/operations-research-and-decision-theory)\n* [Probability and Statistics in Computer Science](https://link.springer.com/subjects/probability-and-statistics-in-computer-science)\n* [Statistics in Engineering, Physics, Computer Science, Chemistry and Earth Sciences](https://link.springer.com/subjects/statistics-in-engineering-physics-computer-science-chemistry-and-earth-sciences)\n* [Operations Research, Management Science](https://link.springer.com/subjects/operations-research-management-science-)\nSearch within this book\nSearch\n## Table of contents (8 chapters)\n1. ### Front Matter\nPages i-xx\n[Download chapterPDF](https://link.springer.com/content/pdf/bfm:978-1-4757-4321-0/1)\n2. ### [Preliminaries](https://link.springer.com/chapter/10.1007/978-1-4757-4321-0_1)\n* Reuven Y. Rubinstein, Dirk P. Kroese\nPages 1-28\n* ### [A Tutorial Introduction to the ",
          "original_query": "The Cross\u2011Entropy Method: A Unified Approach to Combinatorial Optimization, Monte\u2011Carlo Simulation and Machine Learning",
          "cleaned_query": "The Cross\u2011Entropy Method: A Unified Approach to Combinatorial Optimization, Monte\u2011Carlo Simulation and Machine Learning"
        }
      ],
      "generated_ideas": [
        "UCT with Learned Value-of-Information for Industrial Planning\nBuild a variant of UCT where exploration bonuses are predicted by a neural \u201cvalue-of-information\u201d head trained from rollout outcomes, rather than fixed UCB terms. Evaluate on stochastic scheduling and inventory MDPs where simulation is expensive, measuring sample-efficiency gains and robustness under model misspecification.",
        "AlphaGo-Style Policy\u2013Value Guided Search for Vehicle Routing with Time Windows\nCombine an attention-based routing policy (as in \u201cAttention, Learn to Solve Routing Problems!\u201d) with a learned value network that estimates remaining route cost, and use them to guide a tree search (PUCT/UCT) over partial tours. The contribution is a concrete hybrid solver that can trade compute for solution quality at inference time, benchmarked on VRPTW and dynamic VRP variants.",
        "Cross-Entropy\u2013Distilled Neural Policies from Tree Search for Combinatorial Optimization\nUse MCTS/UCT (guided by a weak attention policy) to generate improved solution distributions, then apply cross-entropy method updates to fit a parametric policy that mimics the elite set of search trajectories. This creates a repeatable \u201csearch \u2192 CE distill \u2192 search\u201d loop and tests whether CE-based distillation reduces the need for long self-play/search in large CO instances.",
        "Many-Objective NCO-RL with Pareto-Front Value Networks\nExtend the policy\u2013value paradigm to multi-objective industrial CO (cost, emissions, resilience, human-centric constraints) by learning a vector value function and training a policy conditioned on preference weights. Provide an actionable algorithm that approximates Pareto fronts via self-play style generation of diverse trade-off solutions and validates on multi-depot VRP with emissions and fairness constraints.",
        "Uncertainty-Calibrated Policy\u2013Value Networks for Risk-Sensitive Routing\nIncorporate predictive uncertainty (e.g., deep ensembles) into policy/value guidance so UCT can prioritize branches that reduce tail-risk under stochastic travel times and demand. The key contribution is a risk-aware search objective (CVaR/quantile) integrated with neural guidance, with empirical evaluation on stochastic VRP and production planning under disruptions.",
        "Hierarchical MCTS with Attention-Based Macro-Actions for Large-Scale CO\nLearn a library of macro-actions (e.g., cluster-level route decisions, block insertions for scheduling) using attention models, then run UCT over macro-action sequences before refining locally. This directly targets scaling issues by reducing branching factor and provides ablations quantifying when macro-level search beats token-level (node-by-node) construction.",
        "Offline-to-Online Adaptation: Self-Play Search for Nonstationary Industrial Environments\nPretrain routing/scheduling policies on historical data, then adapt online using short-horizon UCT rollouts with policy/value priors to handle distribution shift (new depots, seasonal demand, machine downtime). The contribution is a deployable adaptation protocol with bounded compute budgets and metrics on regret and constraint violations over time.",
        "Bandit-Based Node Selection with Learned Action Elimination in Tree Search\nExtend UCT with a learned pruning/elimination module that predicts dominated actions at a state (e.g., insertions that will violate time windows or exceed capacity later), reducing wasted rollouts. This idea contributes a principled blend of bandit confidence bounds and neural feasibility/dominance prediction, tested for speedups without degrading optimality gaps.",
        "Search-Improved Training Data Generation for Constraint Satisfaction in NCO\nUse tree search guided by a constraint-aware policy to intentionally target \u201chard\u201d boundary cases (near-feasible/near-infeasible solutions), then train the policy to better satisfy constraints under tight margins. The key contribution is a curriculum generator driven by MCTS statistics (visit counts, backup values) that improves constraint satisfaction rates in bin packing, assignment, and scheduling.",
        "Compute-Adaptive Inference: Dynamic Budget Allocation Between Policy Sampling, CE Optimization, and UCT\nCreate an inference-time controller that chooses\u2014per instance\u2014how much compute to spend on (i) direct neural decoding, (ii) cross-entropy refinement, or (iii) UCT search, based on predicted instance hardness and uncertainty. This yields a practical \u201canytime\u201d NCO system with measurable Pareto improvements in solution quality vs. latency on heterogeneous industrial workloads."
      ],
      "generated_ideas_raw": "1. **UCT with Learned Value-of-Information for Industrial Planning**\n   Build a variant of UCT where exploration bonuses are predicted by a neural \u201cvalue-of-information\u201d head trained from rollout outcomes, rather than fixed UCB terms. Evaluate on stochastic scheduling and inventory MDPs where simulation is expensive, measuring sample-efficiency gains and robustness under model misspecification.\n\n2. **AlphaGo-Style Policy\u2013Value Guided Search for Vehicle Routing with Time Windows**\n   Combine an attention-based routing policy (as in \u201cAttention, Learn to Solve Routing Problems!\u201d) with a learned value network that estimates remaining route cost, and use them to guide a tree search (PUCT/UCT) over partial tours. The contribution is a concrete hybrid solver that can trade compute for solution quality at inference time, benchmarked on VRPTW and dynamic VRP variants.\n\n3. **Cross-Entropy\u2013Distilled Neural Policies from Tree Search for Combinatorial Optimization**\n   Use MCTS/UCT (guided by a weak attention policy) to generate improved solution distributions, then apply cross-entropy method updates to fit a parametric policy that mimics the elite set of search trajectories. This creates a repeatable \u201csearch \u2192 CE distill \u2192 search\u201d loop and tests whether CE-based distillation reduces the need for long self-play/search in large CO instances.\n\n4. **Many-Objective NCO-RL with Pareto-Front Value Networks**\n   Extend the policy\u2013value paradigm to multi-objective industrial CO (cost, emissions, resilience, human-centric constraints) by learning a vector value function and training a policy conditioned on preference weights. Provide an actionable algorithm that approximates Pareto fronts via self-play style generation of diverse trade-off solutions and validates on multi-depot VRP with emissions and fairness constraints.\n\n5. **Uncertainty-Calibrated Policy\u2013Value Networks for Risk-Sensitive Routing**\n   Incorporate predictive uncertainty (e.g., deep ensembles) into policy/value guidance so UCT can prioritize branches that reduce tail-risk under stochastic travel times and demand. The key contribution is a risk-aware search objective (CVaR/quantile) integrated with neural guidance, with empirical evaluation on stochastic VRP and production planning under disruptions.\n\n6. **Hierarchical MCTS with Attention-Based Macro-Actions for Large-Scale CO**\n   Learn a library of macro-actions (e.g., cluster-level route decisions, block insertions for scheduling) using attention models, then run UCT over macro-action sequences before refining locally. This directly targets scaling issues by reducing branching factor and provides ablations quantifying when macro-level search beats token-level (node-by-node) construction.\n\n7. **Offline-to-Online Adaptation: Self-Play Search for Nonstationary Industrial Environments**\n   Pretrain routing/scheduling policies on historical data, then adapt online using short-horizon UCT rollouts with policy/value priors to handle distribution shift (new depots, seasonal demand, machine downtime). The contribution is a deployable adaptation protocol with bounded compute budgets and metrics on regret and constraint violations over time.\n\n8. **Bandit-Based Node Selection with Learned Action Elimination in Tree Search**\n   Extend UCT with a learned pruning/elimination module that predicts dominated actions at a state (e.g., insertions that will violate time windows or exceed capacity later), reducing wasted rollouts. This idea contributes a principled blend of bandit confidence bounds and neural feasibility/dominance prediction, tested for speedups without degrading optimality gaps.\n\n9. **Search-Improved Training Data Generation for Constraint Satisfaction in NCO**\n   Use tree search guided by a constraint-aware policy to intentionally target \u201chard\u201d boundary cases (near-feasible/near-infeasible solutions), then train the policy to better satisfy constraints under tight margins. The key contribution is a curriculum generator driven by MCTS statistics (visit counts, backup values) that improves constraint satisfaction rates in bin packing, assignment, and scheduling.\n\n10. **Compute-Adaptive Inference: Dynamic Budget Allocation Between Policy Sampling, CE Optimization, and UCT**\n   Create an inference-time controller that chooses\u2014per instance\u2014how much compute to spend on (i) direct neural decoding, (ii) cross-entropy refinement, or (iii) UCT search, based on predicted instance hardness and uncertainty. This yields a practical \u201canytime\u201d NCO system with measurable Pareto improvements in solution quality vs. latency on heterogeneous industrial workloads.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "UCT with Learned Value-of-Information for Industrial Planning\nBuild a variant of UCT where exploration bonuses are predicted by a neural \u201cvalue-of-information\u201d head trained from rollout outcomes, rath",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "AlphaGo-Style Policy\u2013Value Guided Search for Vehicle Routing with Time Windows\nCombine an attention-based routing policy (as in \u201cAttention, Learn to Solve Routing Problems!\u201d) with a learned value netw",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Cross-Entropy\u2013Distilled Neural Policies from Tree Search for Combinatorial Optimization\nUse MCTS/UCT (guided by a weak attention policy) to generate improved solution distributions, then apply cross-e",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Many-Objective NCO-RL with Pareto-Front Value Networks\nExtend the policy\u2013value paradigm to multi-objective industrial CO (cost, emissions, resilience, human-centric constraints) by learning a vector v",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Uncertainty-Calibrated Policy\u2013Value Networks for Risk-Sensitive Routing\nIncorporate predictive uncertainty (e.g., deep ensembles) into policy/value guidance so UCT can prioritize branches that reduce ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Hierarchical MCTS with Attention-Based Macro-Actions for Large-Scale CO\nLearn a library of macro-actions (e.g., cluster-level route decisions, block insertions for scheduling) using attention models, ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Offline-to-Online Adaptation: Self-Play Search for Nonstationary Industrial Environments\nPretrain routing/scheduling policies on historical data, then adapt online using short-horizon UCT rollouts wit",
          "is_match": true
        },
        {
          "idea_idx": 7,
          "idea_text": "Bandit-Based Node Selection with Learned Action Elimination in Tree Search\nExtend UCT with a learned pruning/elimination module that predicts dominated actions at a state (e.g., insertions that will v",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Search-Improved Training Data Generation for Constraint Satisfaction in NCO\nUse tree search guided by a constraint-aware policy to intentionally target \u201chard\u201d boundary cases (near-feasible/near-infeas",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Compute-Adaptive Inference: Dynamic Budget Allocation Between Policy Sampling, CE Optimization, and UCT\nCreate an inference-time controller that chooses\u2014per instance\u2014how much compute to spend on (i) d",
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 12,
      "paper_title": "High-Dimensional Calibration from Swap Regret",
      "contribution": "Shows that multi-dimensional online calibration over any convex P and norm ||\u00b7|| reduces to a swap-regret control implied by optimal regularizers for online linear optimization, and uses TreeSwap+FTL to obtain efficient high-dimensional calibration rates (T = exp(O(\u03c1/\u03b5^2))) recovering and generalizing prior polynomial-in-d bounds without requiring OLO subroutines or knowledge of \u03c1.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11830,
      "output_tokens": 951,
      "predecessor_details": [
        {
          "success": true,
          "title": "(PDF) Asymptotic Calibration - ResearchGate",
          "url": "https://www.researchgate.net/publication/2526224_Asymptotic_Calibration",
          "content": "(PDF) Asymptotic Calibration\n* [Home](directory/publications)\n* [Instrumentation Engineering](topic/Instrumentation-Engineering/publications)\n* [Calibration](topic/Calibration/publications)\nArticlePDF Available\n# Asymptotic Calibration\n* August 2002\n* [Biometrika](journal/Biometrika-1464-3510)85(2)\nDOI:[10.1093/biomet/85.2.379](https://doi.org/10.1093/biomet/85.2.379)\nAuthors:\n[![Dean P Foster](https://c5.rgstatic.net/m/448675030402/images/icons/icons/author-avatar.svg)](scientific-contributions/Dean-P-Foster-7366453)\n[Dean P Foster](scientific-contributions/Dean-P-Foster-7366453)\n[Dean P Foster](scientific-contributions/Dean-P-Foster-7366453)\n* This person is not on ResearchGate, or hasn't claimed this research yet.\n[![Rakesh V. Vohra at University of Pennsylvania](https://c5.rgstatic.net/m/4671872220764/images/template/default/profile/profile_default_m.jpg)](profile/Rakesh-Vohra)\n[Rakesh V. Vohra](profile/Rakesh-Vohra)\n* [University of Pennsylvania](https://www.researchgate.net/institution/University_of_Pennsylvania)\n![](https://i1.rgstatic.net/publication/2526224_Asymptotic_Calibration/links/0c960536d2f909a8c6000000/smallpreview.png)\n[Download full-text PDF](profile/Rakesh-Vohra/publication/2526224_Asymptotic_Calibration/links/0c960536d2f909a8c6000000/Asymptotic-Calibration.pdf)[Read full-text](publication/2526224_Asymptotic_Calibration#read)\n[Download full-text PDF](https://www.researchgate.net/profile/Rakesh-Vohra/publication/2526224_Asymptotic_Calibration/links/0c960536d2f909a8c6000000/Asymptotic-Calibration.pdf)\n[Read full-text](publication/2526224_Asymptotic_Calibration#read)\n[Download citation](https://www.researchgate.net/publication/2526224_Asymptotic_Calibration/citation/download)\nCopy linkLink copied\n[\nRead full-text\n](publication/2526224_Asymptotic_Calibration#read)[\nDownload citation\n](https://www.researchgate.net/publication/2526224_Asymptotic_Calibration/citation/download)\nCopy linkLink copied\n## Abstract\nCan we forecast the probability of an arbitrary sequence of events happening so that the stated probability of an event happening is close to its empirical probability? In other words, on the subset of days where we forecast 2/3 chance of a particular event occurring, about 2/3 of the time that event should occur. We can view this prediction problem as a game played against nature, where at the beginning of the game Nature picks a data sequence and the forecaster picks a forecasting algorithm. If the forecaster isn't allowed to randomize, then Nature wins---there will always be data for which the forecaster does poorly. This paper shows that if the forecaster can randomize, the forecaster wins in the sense that the forecasted probabilities and the empirical probabilities can be made arbitrarily close to each other.\n![ResearchGate Logo](images/icons/svgicons/researchgate-logo-white.svg)\n**Discover the world's research**\n* 25+ million members\n* 160+ million publication pages\n* 2.3+ billion citations[Join for free](signup.SignUp.html)\n[](publication/2526224_Asymptotic_Calibration#read-preview)\nContent uploaded by[Rakesh V. Vohra](profile/Rakesh-Vohra)\nAuthor content\nAll content in this area was uploaded by Rakesh V. Vohra on May 09, 2014\nContent may be subject to copyright.\nBiometrika(1998),85,2,pp.379-390\nPrintedinGreatBritain\nAsymptoticcalibration\nBY\nDEANP.FOSTER\nDepartmentofStatistics,TheWhartonSchool,UniversityofPennsylvania,Philadelphia,\nPennsylvania19104,U.S.A.\nfoster@hellspark.wharton.upenn.edu\nAND\nRAKESHV.VOHRA\nDepartmentofManagementScience,FisherCollegeofBusiness,OhioStateUniversity,\nColumbus,Ohio43210,U.S.A.\nvohra.l@osu.edu\nSUMMARY\nCanweforecasttheprobabilityofanarbitrarysequenceofeventshappeningsothat\nthestatedprobabilityofaneventhappeningisclosetoitsempiricalprobability?Wecan\nviewthispredictionproblemasagameplayedagainstNature,whereatthebeginningof\nthegameNaturepicksadatasequenceandtheforecasterpicksaforecastingalgorithm.\nIftheforecasterisnotallowedtorandomise,thenNaturewins;therewillalwaysbedata\nforwhichtheforecasterdoespoorly.Thispapershowsthat,iftheforecastercanrandomise,\ntheforecasterwinsinthesensethattheforecastedprobabilitiesandtheempiricalprob-\nabilitiescanbemadearbitrarilyclosetoeachother.\nSomekeywords:Brierscore;Calibration;Competitiveratio;Regret;Universalpredictionofsequences;\nWorstcase.\n1.INTRODUCTION\nProbabilityforecastingistheactofassigningprobabilitiestoanuncertainevent.Itis\nanactivitywidelypractisedinmeteorologicalcircles.Forexample,since1965,theU.S.\nNationalWeatherServicehasbeeninthehabitofmakingandannouncing'probability\nofprecipitation'forecasts.Suchaforecastisinterpretedtobetheprobabilitythatprecipi-\ntation,definedtobeatleast001inches,willoccurinaspecifiedtimeperiodandarea.\nTheseforecastsarenowpopularlyacceptedbytheAmericanpublicasmeaningfuland\ninformative.\nTherearemanycriteriaforjudgingtheeffectivenessoftheprobabilityforecast(Murphy\n&amp;Epstein,1967).Inthispaperwelimitourselvestotheconsiderationofcalibration,\nsometimestermedreliability.Dawid(1982)offersthefollowingintuitivedefinitionof\ncalibration:\n'Supposethat,inalong(conceptuallyinfinite)sequenceofweatherforecasts,welookatall\nthosedaysforwhichtheforecastprobabilityofprecipitationwas,say,closetosomegiven\nvalue\nco\nand(assumingtheseformaninfinitesequence)determinethelongrunproportionp\nofsuchdaysonwhichtheforecastevent(rain)infactoccurred.Theplotofpagainst\nco\nis\ntermedtheforecaster'sempiricalcalibrationcurve.Ifthecurveisthediagonalp\n=\nco,\nthe\nforecastermaybetermed(empirically)wellcalibrated'.\n380DEANP.FOSTERANDRAKESHV.VOHRA\nWegivearigorousdefinitionlater.\nCalibrationbyitselfisnotasufficientconditionforaforecasttobedeemedgood.To\nseethis,supposetherearetwoweatherforecastersfacingthefollowingweathersequence:\ndry,wet,dry,wet,....Onealwaysforecastsaprobabilityof{ofraineachdayandthe\notheralternates0,1,\n0,1,...\n.Bothforecastersarewellcalibrated,buttheforecastsofthe\nfirstareclearlylessusefulthanthoseofthesecond.Nowconsidertwouncalibrated\nforecasts,thefirstofwhichalwaysforecastsaprobabilityof^andthesecondofwhich\nalternates\n1,\n0,1,\n0,...,alwaysgeneratinganincorrectforecast.Whichofthesetwois\nbetterisamatterofdebate;thefirsthasalowerquadraticerrorbutthesecondgetsthe\n'pattern'ofraincorrect.Bothseemdominatedbythetwoforecastsdiscussedpreviously.\nThus,\ncalibrationdoesseemtobeanappealingminimalpropertythatanyprobability\nforecastshouldsatisfy.\nThenotionofcalibrationonlymakessenseifonecanconstructforecaststhatare\ncalibrated.Regrettably,Oakes(1985)hasprovedthatnodeterministicforecasting\nsequencecanbecalibratedforallpossiblesequences;seeDawid(1985)foradifferent\nproof.\nSpecifically,Oakesshowsthatitisimpossibletoconstructajointdistributionfor\naninfinitesequenceofeventswhoseposteriormeanisguaranteedtobecalibratedfor\neverypossiblesequenceofoutcomes.\nAwayaroundthisimpossibilityresultistorelaxtherequirementthataforecastbe\ncalibratedagainstallpossiblesequences.Perhapsitissufficientthattheforecasterbe\ncalibratedforsomerestrictedfamilyofdistributions.Dawid(1985)arguesthatthiscan\nresultinforecastingschemesthatarecomputationallyburdensomeandinsomecasesnot\ncomputableatall.Alternatively,onecanrejectthenotionthatcalibrationisatalla\ndesirableorusefulnotion.Schervish(1985),forexample,offerstwoargumentsforthis\nview.Thefirstisthatcalibrationisalongruncriterion:intheshortrun,whenweare\nalive,aforecastermaydoquitewell.Thesecondisthat,whileamalevolentNaturemay\nbeabletomakeoneforecasterlookbadaccordingtothecalibrationcriterion,itisharder\nforhertomakemanyforecasterslookbadatthesametime.\nOurgoalinthispaperistorescuethenotionofcalibration.Wegetaroundtheimpossi-\nbilityresultofOakesbybroadeningthedefinitionofcalibrationtoincluderandomised\nforecasts.Bycarefullychoosingourdefinitionofcalibrationforrandomisedforecasts,we\nshowhowtoconstructaforecastwhichisinfactapproximatelycalibrated.Finally,we\ngeneraliseourresultstothecasewhenwhatisbeingforecastisadistribution,notjust\napoint.\n2.NOTATIONANDDEFINITIONS\nForeaseofexpositionassumeourforecastingmethod,F,isassignedthetaskoffore-\ncastingtheprobabilitiesoftwosta",
          "original_query": "Foster, D. P. and Vohra, R. (1998). Calibration of Forecasters",
          "cleaned_query": "Foster, D. P. and Vohra, R.. Calibration of Forecasters"
        },
        {
          "success": true,
          "title": "[PDF] An analog of the minimax theorem for vector payoffs.",
          "url": "https://www.semanticscholar.org/paper/An-analog-of-the-minimax-theorem-for-vector-Blackwell/f3867cb34340d049c10605dee5e7e587db51ceaa",
          "content": "[PDF] An analog of the minimax theorem for vector payoffs. | Semantic Scholar\n[Skip to search form](#search-form)[Skip to main content](#main-content)[Skip to account menu](#account-menu)\n[Semantic ScholarSemantic Scholar's Logo](https://www.semanticscholar.org/)\nSearch 231,142,857 papers from all fields of science\nSearch\n* DOI:[10.2140/PJM.1956.6.1](https://doi.org/10.2140/PJM.1956.6.1)\n* Corpus ID: 15946738# An analog of the minimax theorem for vector payoffs.\n```\n@article{Blackwell1956AnAO,\ntitle={An analog of the minimax theorem for vector payoffs.},\nauthor={David Blackwell},\njournal={Pacific Journal of Mathematics},\nyear={1956},\nvolume={6},\npages={1-8},\nurl={https://api.semanticscholar.org/CorpusID:15946738}\n}\n```\n* [D. Blackwell](https://www.semanticscholar.org/author/D.-Blackwell/146468468)\n* Published1 March 1956\n* Mathematics\n* Pacific Journal of Mathematics\nfor all i, j . Thus in the (two-person, zero-sum) game with matrix \u039bf, player I has a strategy insuring an expected gain of at least v, and player II has a strategy insuring an expected loss of at most v. An alternative statement, which follows from the von Neumann theorem and an appropriate law of large numbers is that, for any e&gt;&gt;0, I can, in a long series of plays of the game with matrix M, guarantee, with probability approaching 1 as the number of plays becomes infinite, that his average\u2026Expand\n[View via Publisher](https://doi.org/10.2140/PJM.1956.6.1)\n[msp.org](http://msp.org/pjm/1956/6-1/pjm-v6-n1-p01-s.pdf)\nSave to LibrarySave\nCreate AlertAlert\nCite\nShare\n870 Citations\n[\nHighly Influential Citations\n](#citing-papers)[](https://www.semanticscholar.org/faq#influential-citations)\n134\n[\nBackground Citations\n](#citing-papers)\n363\n[\nMethods Citations\n](#citing-papers)\n199\n[\nResults Citations\n](#citing-papers)\n25\n[View All](#citing-papers)\n## 870 Citations\nCitation Type\nHas PDF\nAuthor\nMore Filters\nMore Filters\nFilters\nSort by RelevanceSort by Most Influenced PapersSort by Citation CountSort by Recency\n[### On the distribution of pure strategy equilibria in finite games with vector payoffs\n](https://www.semanticscholar.org/paper/On-the-distribution-of-pure-strategy-equilibria-in-Stanford/370be948e67e585c5dd4249338e5ac2107becb6b)[W. Stanford](https://www.semanticscholar.org/author/W.-Stanford/48323847)\nMathematics, Economics\n* 1997\n* [\n15\n](https://www.semanticscholar.org/paper/370be948e67e585c5dd4249338e5ac2107becb6b#citing-papers)\nSave\n[### General procedures leading to correlated equilibria\n](https://www.semanticscholar.org/paper/General-procedures-leading-to-correlated-equilibria-Cahn/86c9793d9d02298fe5fcf3abdb16efffc31656d1)[Amotz Cahn](https://www.semanticscholar.org/author/Amotz-Cahn/153466928)\nEconomics\n[Int. J. Game Theory](https://www.semanticscholar.org/venue?name=Int.%20J.%20Game%20Theory)\n* 2004\nTLDR\nIt is shown that if only one player, say player i, plays with probabilities proportional to the regrets, while the other players are \u201cnot too sophisticated,\u201d then the result that player i\u2019s regrets converge to zero continues to hold.Expand\n* [\n34\n](https://www.semanticscholar.org/paper/86c9793d9d02298fe5fcf3abdb16efffc31656d1#citing-papers)\n* [\nPDF\n](https://www.semanticscholar.org/paper/86c9793d9d02298fe5fcf3abdb16efffc31656d1)\nSave\n[### A partial folk theorem for games with private learning\n](https://www.semanticscholar.org/paper/A-partial-folk-theorem-for-games-with-private-Wiseman/2adf23e76eff04384821dffcfd5b55f6eee81f2c)[Thomas Wiseman](https://www.semanticscholar.org/author/Thomas-Wiseman/145411560)\nEconomics\n* 2011\nThe payoff matrix of a finite stage game is realized randomly, and then the stage game is repeated infinitely. The distribution over states of the world (a state corresponds to a payoff matrix) is\u2026Expand\n* [\n31\n](https://www.semanticscholar.org/paper/2adf23e76eff04384821dffcfd5b55f6eee81f2c#citing-papers)\n* [\nPDF\n](https://www.semanticscholar.org/paper/2adf23e76eff04384821dffcfd5b55f6eee81f2c)\n* 1 Excerpt\nSave\n[### ON BLACKWELL'S MINIMAX THEOREM AND THE COMPOUND DECISION METHOD\n](https://www.semanticscholar.org/paper/ON-BLACKWELL'S-MINIMAX-THEOREM-AND-THE-COMPOUND-Hannan/7cf521839a090fdbf54f6d5c460b10be66c733f9)[J. Hannan](https://www.semanticscholar.org/author/J.-Hannan/82548657)\nMathematics\n* 1997\nBlackwell (1956a) proved a minimax theorem for games with a vector loss and characterized sets such that a player has a strategy under which, whatever strategy the other player uses, the average\u2026Expand\n* [\n2\n](https://www.semanticscholar.org/paper/7cf521839a090fdbf54f6d5c460b10be66c733f9#citing-papers)\n* [Highly Influenced](https://www.semanticscholar.org/paper/7cf521839a090fdbf54f6d5c460b10be66c733f9?sort=is-influential#citing-papers)\n* [\nPDF\n](https://www.semanticscholar.org/paper/7cf521839a090fdbf54f6d5c460b10be66c733f9)\n* 23 Excerpts\nSave\n[### Delft University of Technology A robust saturated strategy for n-player prisoner's dilemma\n](https://www.semanticscholar.org/paper/Delft-University-of-Technology-A-robust-saturated-Giordano-Blanchini/8594676052491868c2e4ab417b455f795ad79df1)[Giulia Bauso Giordano](https://www.semanticscholar.org/author/Giulia-Bauso-Giordano/2242060002)[Franco Dario Blanchini](https://www.semanticscholar.org/author/Franco-Dario-Blanchini/2242059726)[Doi](https://www.semanticscholar.org/author/Doi/2242018905)\nMathematics\n* 2018\nWe study diffusion of cooperation in an n-population game in continuous time. At each instant, the game involves n random individuals, one from each population. The game has the structure of a\u2026Expand\n* 1 Excerpt\nSave\n[### A General Class of Adaptive Strategies\n](https://www.semanticscholar.org/paper/A-General-Class-of-Adaptive-Strategies-Hart-Mas-Colell/2d4092f712a94d294914d29cc2516ac4db64b6da)[S. Hart](https://www.semanticscholar.org/author/S.-Hart/144237049)[A. Mas-Colell](https://www.semanticscholar.org/author/A.-Mas-Colell/1399551631)\nEconomics, Mathematics\n[J. Econ. Theory](https://www.semanticscholar.org/venue?name=J.%20Econ.%20Theory)\n* 2001\nWe exhibit and characterize an entire class of simple adaptive strategies, in the repeated play of a game, having the Hannan-consistency property: In the long-run, the player is guaranteed an average\u2026Expand\n* [\n258\n](https://www.semanticscholar.org/paper/2d4092f712a94d294914d29cc2516ac4db64b6da#citing-papers)\n* [Highly Influenced](https://www.semanticscholar.org/paper/2d4092f712a94d294914d29cc2516ac4db64b6da?sort=is-influential#citing-papers)\n* [\nPDF\n](https://www.semanticscholar.org/paper/2d4092f712a94d294914d29cc2516ac4db64b6da)\n* 3 Excerpts\nSave\n[### Approachability of convex sets in generalized quitting games\n](https://www.semanticscholar.org/paper/Approachability-of-convex-sets-in-generalized-games-Flesch-Laraki/1331952c094d503c4660e0acd1a7f370dcec702a)[J. Flesch](https://www.semanticscholar.org/author/J.-Flesch/1755828)[R. Laraki](https://www.semanticscholar.org/author/R.-Laraki/2024372)[Vianney Perchet](https://www.semanticscholar.org/author/Vianney-Perchet/3087994)\nMathematics\nGames Econ. Behav.\n* 2018\n* [\n4\n](https://www.semanticscholar.org/paper/1331952c094d503c4660e0acd1a7f370dcec702a#citing-papers)\n* [Highly Influenced](https://www.semanticscholar.org/paper/1331952c094d503c4660e0acd1a7f370dcec702a?sort=is-influential#citing-papers)\n[[PDF]](https://www.semanticscholar.org/reader/1331952c094d503c4660e0acd1a7f370dcec702a)\n* 4 Excerpts\nSave\n[### Games with vector payoffs : a dynamic programming approach\n](https://www.semanticscholar.org/paper/Games-with-vector-payoffs-:-a-dynamic-programming-Kamble/7d95d976b01b7fcdaf0b0f475b2fd2df10e25f58)[Vijay Kamble](https://www.semanticscholar.org/author/Vijay-Kamble/144193607)\nComputer Science, Mathematics\n* 2015\nTLDR\nIt is shown that for large classes of sequential games, these optimal guarantees can be characterized as the fixed point of a dynamic programming operator defined on the space of extremal elements of subsets of some partially ordered topological space and opens up the possibility of using many well-known dyn",
          "original_query": "Blackwell, D. (1956). An analog of the minimax theorem for vector payoffs (Approachability)",
          "cleaned_query": "Blackwell, D.. An analog of the minimax theorem for vector payoffs (Approachability)"
        },
        {
          "success": true,
          "title": "Online convex programming and generalized infinitesimal gradient ...",
          "url": "https://dl.acm.org/doi/10.5555/3041838.3041955",
          "content": "ABSTRACT Convex programming involves a convex set F \u2286 R n and a convex cost function c : F \u2192 R. The goal of convex programming is to find a point in F which minimizes c. In online convex programming, the convex set is known in advance, but in each step of some repeated optimization problem, one must select a point in F before seeing the cost function for that step. This can be used to model factory production, farm production, and many other industrial optimization problems where one is unaware of the value of the items produced until they have already been constructed. We introduce an algorithm for this domain. We also apply this algorithm to repeated games, and show that it is really a generalization of infinitesimal gradient ascent, and the results here imply that generalized infinitesimal gradient ascent (GIGA) is universally consistent. ||||I|||| * Advanced Search\n * Browse\n * About\n * \n + Sign in\n + Register\n * Search ACM Digital Library\n Search Search\n * Advanced Search\n * Journals\n * Magazines\n * Proceedings\n * \n + Browse\n + About\n * Books\n * SIGs\n * More\n + Conferences\n + People\n * \n Search ACM Digital Library\n Search Search\n Advanced Search\n Browse\n * Browse Digital Library\n * Collections\n * More\n * Home\n * Browse by Title\n * Proceedings\n * ICML'03\n * Online convex programming and generalized infinitesimal gradient ascent\n Article\n Free Access\n Share on\n\n Online convex programming and generalized infinitesimal gradient ascent\n\n * Author:\n * Martin Zinkevich\n\n Carnegie Mellon University, Pittsburgh, PA\n\n Carnegie Mellon University, Pittsburgh, PA\n\n View Profile\n Authors Info & Claims\n ICML'03: Proceedings of the Twentieth International Conference on International Conference on Machine Learning August 2003 Pages 928\u2013935\n Published: 21 August 2003 Publication History\n * 225 citation\n * 0\n * Downloads\n Metrics\n Total Citations 225\n Total Downloads 0\n Last 12 Months 0\n Last 6 weeks 0\n * Get Citation Alerts\n * Save to Binder\n Close modal\n\n Save to Binder\n\n Create a New Binder\n Name\n 256\n + Cancel\n + Create\n * Export Citation\n Publisher Site\n * \n\n ICML'03: Proceedings of the Twentieth International Conference on International Conference on Machine Learning\n\n Online convex programming and generalized infinitesimal gradient ascent\n Pages 928\u2013935\n Previous Chapter Next Chapter\n * \n + \n * \n + ABSTRACT\n + References\n + Comments\n\n ICML'03: Proceedings of the Twentieth International Conference on International Conference on Machine Learning\n\n Online convex programming and generalized infinitesimal gradient ascent\n Pages 928\u2013935\n Previous Chapter Next Chapter\n * \n + \n * \n + ABSTRACT\n + References\n + Comments\n * \n * \n * \n * 27 References\n * \n * \n * \n\n ABSTRACT\n\n Convex programming involves a convex set F \u2286 Rn and a convex cost function c : F \u2192 R. The goal of convex programming is to find a point in F which minimizes c. In online convex programming, the convex set is known in advance, but in each step of some repeated optimization problem, one must select a point in F before seeing the cost function for that step. This can be used to model factory production, farm production, and many other industrial optimization problems where one is unaware of the value of the items produced until they have already been constructed. We introduce an algorithm for this domain. We also apply this algorithm to repeated games, and show that it is really a generalization of infinitesimal gradient ascent, and the results here imply that generalized infinitesimal gradient ascent (GIGA) is universally consistent.\n\n References\n\n 1. Amari, S. (1998). Natural gradient works efficiently in learning. Neural Computation, 10 , 251-276. Google Scholar Digital Library\n 2. Bansal, N., Blum, A., Chawla, S., & Meyerson, A. (2003). Online oblivious routing. Fifteenth ACM Symposium on Parallelism in Algorithms and Architecture . Google Scholar Digital Library\n 3. Blackwell, D. (1956). An analog of the minimax theorem for vector payoffs. South Pacific J. of Mathematics , 1-8. Google Scholar\n 4. Boot, J. (2003). Quadratic programming: Algorithms, anomolies, applications . Rand McNally & Co. Google Scholar\n 5. Boser, B., Guyon, I., & Vapnik, V. (1992). A training algorithm for optimal margin classifiers. Proceedings of the Fifth Annual Conference on Computational Learning Theory . Google Scholar Digital Library\n 6. Boyd, S., & Vandenberghe, L. (2003). Convex optimization. In press, available at http://www.stanford.edu/~boyd/cvxbook.html. Google Scholar Digital Library\n 7. Cameron, N. (1985). Introduction to linear and convex programming . Cambridge University Press. Google Scholar\n 8. Cesa-Bianchi, N., Long, P., & Warmuth, M. K. (1994). Worst-case quadratic bounds for on-line prediction of linear functions by gradient descent. IEEE Transactions on Neural Networks, 7 , 604-619. Google Scholar Digital Library\n 9. Della Pietra, S., Della Pietra, V., & Lafferty, J. (1999). Duality and auxilary functions for Bregman distances (Technical Report CMU-CS-01-109). Carnegie Mellon University. Google Scholar\n 10. Foster, D. (1999). A proof of calibration via Blackwell's approachability theorem. Games and Economic Behavior (pp. 73-79). Google Scholar\n 11. Foster, D., & Vohra, R. (1999). Regret in the on-line decision problem. Games and Economic Behavior , 29 , 7-35. Google Scholar\n 12. Freund, Y., & Schapire, R. (1999). Adaptive game playing using multiplicative weights. Games and Economic Behavior (pp. 79-103). Google Scholar\n 13. Fudenberg, D., & Levine, D. (1995). Universal consistency and cautious fictitious play. Journal of Economic Dynamics and Control, 19 , 1065-1089. Google Scholar Cross Ref\n 14. Fudenberg, D., & Levine, D. (1997). Conditional universal consistency. Available at http://ideas.repec.org/s/cla/levarc.html. Google Scholar\n 15. Gordon, G. (1999). Approximate solutions to markov decision processes . Doctoral dissertation, Carnegie Mellon University. Google Scholar Digital Library\n 16. Hannan, J. (1957). Approximation to bayes risk in repeated play. Annals of Mathematics Studies, 39 , 97-139. Google Scholar\n 17. Hart, S., & Mas-Colell, A. (2000). A simple adaptive procedure leading to correlated equilibrium. Econometrica , 68 , 1127-1150. Google Scholar Cross Ref\n 18. Hart, S., & Mas-Colell, A. (2001). A general class of adaptive strategies. Journal of Economic Theory , 98 , 26-54. Google Scholar Cross Ref\n 19. Hastie, T., Tibishirani, R., & Friedman, J. (2001). The elements of statistical learning . Springer. Google Scholar\n 20. Herbster, M., & Warmuth, M. K. (2001). Tracking the best linear predictor. Journal of Machine Learning Research, 1 , 281-309. Google Scholar Digital Library\n 21. Kalai, A., & Vempala, S. (2002). Geometric algorithms for online optimization (Technical Report). MIT. Google Scholar\n 22. Kivinen, J., & Warmuth, M. (1997). Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132 , 1-64. Google Scholar Digital Library\n 23. Kivinen, J., & Warmuth, M. (2001). Relative loss bounds for multidimensional regression problems. Machine Learning Journal, 45 , 301-329. Google Scholar Digital Library\n 24. Littlestone, N., & Warmuth, M. K. (1989). The weighted majority algorithm. Proceedings of the Second Annual Conference on Computational Learning Theory . Google Scholar Digital Library\n 25. Mahony, R., & Williamson, R. (2001). Prior knowledge and preferential structures in gradient descent algorithms. Journal of Machine Learning Research , 1 , 311-355. Google Scholar Digital Library\n 26. Singh, S., Kearns, M., & Mansour, Y. (2000). Nash convergence of gradient dynamics in general-sum games. Proceedings of the Sixteenth Conference in Uncertainty in Artificial Intelligence (pp. 541-548). Google Scholar Digital Library\n 27. Zinkevich, M. (2003). Online convex programming and generalized infinitesimal gradient ascent (Technical Report CMU-CS-03-110). CMU. Google Scholar\n Show All References\n\n Comments\n\n Please enable JavaScript to view the",
          "original_query": "Zinkevich, M. (2003). Online Convex Programming and Generalized Infinitesimal Gradient Ascent",
          "cleaned_query": "Zinkevich, M.. Online Convex Programming and Generalized Infinitesimal Gradient Ascent"
        },
        {
          "success": true,
          "title": "From External to Internal Regret - ACM Digital Library",
          "url": "https://dl.acm.org/doi/10.5555/1314498.1314543",
          "content": "From External to Internal Regret | The Journal of Machine Learning Research[skip to main content](#skip-to-main-content)\n[](#global-menu)\nSearch ACM Digital Library\nSearchSearch\n[Advanced Search](https://dl.acm.org/search/advanced)\n[The Journal of Machine Learning Research](#)\n**## Export Citations\nSelect Citation formatBibTeXEndNoteACM Ref**\n* Please download or close your previous search result export first before starting a new bulk export.\nPreview is not available.\nBy clicking download,**a status dialog**will open to start the export process. The process may take**a few minutes**but once it finishes a file will be downloadable from your browser. You may continue to browse the DL while the export process is in progress.\n* ```\n```\n* [Download citation**](javascript:void(0))\n* [Copy citation**](javascript:void(0))\narticle\n**Free access\nShare on\n* **\n* **\n* **\n* **\n* **\n# From External to Internal Regret\nAuthors:[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)AvrimBlum](#artseq-00001),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)YishayMansour](#artseq-00002)[Authors Info &amp; Claims](#tab-contributors)\n[The Journal of Machine Learning Research,Volume8](https://dl.acm.org/toc/10.5555/1314498)\nPages1307-1324\nPublished:01 December 2007[Publication History](#core-history)\n**59citation**173Downloads\nMetrics\n[\nTotal Citations59\n](#tab-citations)[\nTotal Downloads173\n](#tab-metrics-inner)\nLast 12 Months63\nLast 6 weeks4\n**Get Citation Alerts\n**## New Citation Alert added!\nThis alert has been successfully added and will be sent to:\nYou will be notified whenever a record that you have chosen has been cited.\nTo manage your alert preferences, click on the button below.\n[Manage my Alerts](https://dl.acm.org/action/showPreferences?menuTab=Alerts)\n**## New Citation Alert!\nPlease[log in to your account](https://dl.acm.org/action/showLogin?redirectUri=/doi/10.5555/1314498.1314543)\n**\n**\n[**PDF](https://dl.acm.org/doi/pdf/10.5555/1314498.1314543)[**eReader](https://dl.acm.org/doi/epdf/10.5555/1314498.1314543)\n**Contents\n## Abstract\nExternal regret compares the performance of an online algorithm, selecting among*N*actions, to the performance of the best of those actions in hindsight. Internal regret compares the loss of an online algorithm to the loss of a modified online algorithm, which consistently replaces one action by another.\nIn this paper we give a simple generic reduction that, given an algorithm for the external regret problem, converts it to an efficient online algorithm for the internal regret problem. We provide methods that work both in the*full information*model, in which the loss of every action is observed at each time step, and the*partial information*(bandit) model, where at each time step only the loss of the selected action is observed. The importance of internal regret in game theory is due to the fact that in a general game, if each player has sublinear internal regret, then the empirical frequencies converge to a correlated equilibrium.\nFor external regret we also derive a quantitative regret bound for a very general setting of regret, which includes an arbitrary set of modification rules (that possibly modify the online algorithm) and an arbitrary set of time selection functions (each giving different weight to each time step). The regret for a given time selection and modification rule is the difference between the cost of the online algorithm and the cost of the modified online algorithm, where the costs are weighted by the time selection function. This can be viewed as a generalization of the previously-studied*sleeping experts*setting.\n## Formats available\nYou can view the full content in the following formats:\n[**PDF](https://dl.acm.org/doi/pdf/10.5555/1314498.1314543)\n## Cited By\n[View all**](https://dl.acm.org/action/ajaxShowCitedBy?doi=10.5555/1314498.1314543)\n* Moor DBerglund PKarlbom HDai ZKretschman KLalmas MAntonie LPei JYu XChierichetti FLauw HSun YParthasarathy S(2025)Optimising Budget Management via Primal-Dual Approximation with Constrained Polynomial Weights UpdateProceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.210.1145/3711896.3737071(2138-2149)Online publication date: 3-Aug-2025\n[https://dl.acm.org/doi/10.1145/3711896.3737071](https://dl.acm.org/doi/10.1145/3711896.3737071)\n* Anagnostides IKalavasis ASandholm TKouck\u00fd MBansal N(2025)Computational Lower Bounds for No-Regret Learning in Normal-Form GamesProceedings of the 57th Annual ACM Symposium on Theory of Computing10.1145/3717823.3718250(530-541)Online publication date: 15-Jun-2025\n[https://dl.acm.org/doi/10.1145/3717823.3718250](https://dl.acm.org/doi/10.1145/3717823.3718250)\n* Galgana RGolrezaei N(2025)Learning in Repeated Multiunit Pay-as-Bid AuctionsManufacturing &amp; Service Operations Management10.1287/msom.2023.0403**27**:1(200-229)Online publication date: 1-Jan-2025\n[https://dl.acm.org/doi/10.1287/msom.2023.0403](https://dl.acm.org/doi/10.1287/msom.2023.0403)\n* [Show More Cited By](javascript:void(0))\n## Index Terms\n1. From External to Internal Regret\n1. [Computing methodologies](https://dl.acm.org/topic/ccs2012/10010147?SeriesKey=jmlr&amp;expand=all)\n1. [Machine learning](https://dl.acm.org/topic/ccs2012/10010147.10010257?SeriesKey=jmlr&amp;expand=all)\n1. [Modeling and simulation](https://dl.acm.org/topic/ccs2012/10010147.10010341?SeriesKey=jmlr&amp;expand=all)\n1. [Model development and analysis](https://dl.acm.org/topic/ccs2012/10010147.10010341.10010342?SeriesKey=jmlr&amp;expand=all)\n1. [Model verification and validation](https://dl.acm.org/topic/ccs2012/10010147.10010341.10010342.10010344?SeriesKey=jmlr&amp;expand=all)\n1. [Mathematics of computing](https://dl.acm.org/topic/ccs2012/10002950?SeriesKey=jmlr&amp;expand=all)\n1. [Information theory](https://dl.acm.org/topic/ccs2012/10002950.10003712?SeriesKey=jmlr&amp;expand=all)\n1. [Theory of computation](https://dl.acm.org/topic/ccs2012/10003752?SeriesKey=jmlr&amp;expand=all)\n1. [Design and analysis of algorithms](https://dl.acm.org/topic/ccs2012/10003752.10003809?SeriesKey=jmlr&amp;expand=all)\n1. [Online algorithms](https://dl.acm.org/topic/ccs2012/10003752.10003809.10010047?SeriesKey=jmlr&amp;expand=all)\n1. [Online learning algorithms](https://dl.acm.org/topic/ccs2012/10003752.10003809.10010047.10010048?SeriesKey=jmlr&amp;expand=all)\n1. [Models of computation](https://dl.acm.org/topic/ccs2012/10003752.10003753?SeriesKey=jmlr&amp;expand=all)\n1. [Interactive computation](https://dl.acm.org/topic/ccs2012/10003752.10003753.10003759?SeriesKey=jmlr&amp;expand=all)\n1. [Theory and algorithms for application domains](https://dl.acm.org/topic/ccs2012/10003752.10010070?SeriesKey=jmlr&amp;expand=all)\n1. [Machine learning theory](https://dl.acm.org/topic/ccs2012/10003752.10010070.10010071?SeriesKey=jmlr&amp;expand=all)\n1. [Online learning theory](https://dl.acm.org/topic/ccs2012/10003752.10010070.10010071.10010079?SeriesKey=jmlr&amp;expand=all)\n## Recommendations\n* [### From external to internal regret\n](https://dl.acm.org/doi/10.1007/11503415_42)\nCOLT'05: Proceedings of the 18th annual conference on Learning Theory\nExternal regret compares the performance of an online algorithm, selecting among*N*actions, to the performance of the best of those actions in hindsight. Internal regret\ncompares the loss of an online algorithm to the loss of a modified online algorithm,...\n****\n**\n[Read More](https://dl.acm.org/doi/10.1007/11503415_42)\n* [### No internal regret with non-convex loss functions\n](https://dl.acm.org/doi/10.1609/aaai.v38i13.29412)\nAAAI'24/IAAI'24/EAAI'24: Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence\nInternal regret is a measure of performance of an online learning algorithm, which\nmeasures the change in performance by substituting",
          "original_query": "Blum, A. and Mansour, Y. (2007). From External to Internal Regret",
          "cleaned_query": "Blum, A. and Mansour, Y.. From External to Internal Regret"
        },
        {
          "success": true,
          "title": "[PDF] High-Dimensional Calibration from Swap Regret - OpenReview",
          "url": "https://openreview.net/pdf?id=UVDihUz0iT",
          "content": "High-Dimensional Calibration from Swap Regret\nMaxwell Fishelson\u2217\nmaxfish@mit.edu\nNoah Golowich\u2020\nnzg@mit.edu\nMehryar Mohri\u2021\nmohri@google.com\nJon Schneider\u00a7\njschnei@google.com\nAbstract\nWe study the online calibration of multi-dimensional forecasts over an arbitrary\nconvex set P \u2282 R\nd\nrelative to an arbitrary norm k\u00b7k. We connect this with the\nproblem of external regret minimization for online linear optimization, showing that\nif it is possible to guarantee O(\n\u221a\n\u03c1T) worst-case regret after T rounds when actions\nare drawn from P and losses are drawn from the dual k\u00b7k\u2217unit norm ball, then it\nis also possible to obtain \u000f-calibrated forecasts after T = exp(O(\u03c1/\u000f2)) rounds.\nWhen P is the d-dimensional simplex and k\u00b7k is the `1-norm, the existence of\nO(\n\u221a\nT log d)-regret algorithms for learning with experts implies that it is possible\nto obtain \u000f-calibrated forecasts after T = exp(O(log d/\u000f2)) = d\nO(1/\u000f2)\nrounds,\nrecovering a recent result of [Pen25].\nInterestingly, our algorithm obtains this guarantee without requiring access to any\nonline linear optimization subroutine or knowledge of the optimal rate \u03c1 \u2013 in fact,\nour algorithm is identical for every setting of P and k\u00b7k. Instead, we show that\nthe optimal regularizer for the above OLO problem can be used to upper bound\nthe above calibration error by a swap regret, which we then minimize by running\nthe recent TreeSwap algorithm ([DDFG24, PR24]) with Follow-The-Leader as a\nsubroutine. The resulting algorithm is highly efficient and plays a distribution over\nsimple averages of past observations in each round.\nFinally, we prove that any online calibration algorithm that guarantees \u000fT `1-\ncalibration error over the d-dimensional simplex requires T \u2265 exp(poly(1/\u000f))\n(assuming d \u2265 poly(1/\u000f)). This strengthens the corresponding d\n\u2126(log 1/\u000f)\nlower\nbound of [Pen25], and shows that an exponential dependence on 1/\u000f is necessary.\n1 Introduction\nConsider the problem faced by a forecaster who must report probabilistic predictions for a sequence\nof events (e.g. whether it will rain or not tomorrow). One of the most common methods to evaluate\nthe quality of such a forecaster is to verify whether they are calibrated: for example, does it indeed\nrain with probability 40% on days where the forecaster makes this prediction? In addition to\ncalibration being a natural property to expect from predictions, several applications across machine\nlearning, fairness, and game theory require the ability to produce online calibrated predictions\n[ZME20, GPSW17, HJKRR18, FV97].\nWhen events have binary outcomes, calibration can be quantified by the notion of expected calibration\nerror, which measures the expected distance between a prediction made by a forecaster and the actual\nempirical probability of the outcome on the days where they made that prediction. In a seminal result\nby Foster and Vohra [FV98], it was proved that it is possible for an online forecaster to efficiently\n\u2217MIT.\n\u2020MIT. Supported by a NSF Graduate Research Fellowship and a Fannie & Hertz Foundation Graduate\nFellowship.\n\u2021Google Research and Courant Institute of Mathematical Sciences, New York.\n\u00a7Google Research.\n39th Conference on Neural Information Processing Systems (NeurIPS 2025).\nguarantee a sublinear calibration error of O(T\n2/3\n) against any adversarial sequence of T binary\nevents. Equivalently, this can be interpreted as requiring at most O(\u000f\n\u22123\n) rounds of forecasting to\nguarantee an \u000f per-round calibration error on average.\nHowever, many applications require forecasting sequences of multi-dimensional outcomes. The\nprevious definition of calibration error easily extends to the multi-dimensional setting where pre\u0002dictions and outcomes belong to a d-dimensional convex set P \u2282 R\nd\n. Specifically, if a forecaster\nmakes a sequence of predictions p1, p2, . . . , pT \u2208 P for the outcomes y1, y2, . . . , yT \u2208 P, their\nk\u00b7k-calibration error (for any norm k\u00b7k over R\nd\n) is given by\nCalk\u00b7k\nT =\nX\nT\nt=1\nkpt \u2212 \u03bdpt k\nwhere \u03bdpt\nis the average of the outcomes yt on rounds where the learner predicted pt.\nThe algorithm of Foster and Vohra extends to the multidimensional calibration setting, but at the cost\nof producing bounds that decay exponentially in the dimension d. In particular, their algorithm only\nguarantees that the forecaster achieves an average calibration error of \u000f after (1/\u000f)\n\u2126(d)\nrounds. Until\nrecently, no known algorithm achieved a sub-exponential dependence on d in any non-trivial instance\nof multi-dimensional calibration.\nIn 2025, [Pen25] presented a new algorithm for high-dimensional calibration, demonstrating that it is\npossible to obtain `1-calibration rates of \u000fT in d\nO(1/\u000f2)\nrounds for predictions over the d-dimensional\nsimplex (i.e., multi-class calibration). In particular, this is the first known algorithm achieving\npolynomial calibration rates in d for fixed constant \u000f. [Pen25] complements this with a lower bound,\nshowing that in the worst case d\n\u2126(log 1/\u000f)\nrounds are necessary to obtain this rate (implying that a\nfully polynomial bound poly(d, 1/\u000f) is impossible).\n1.1 Our results\nAlthough the algorithm of [Pen25] is simple to describe, its analysis is fairly nuanced and tailored\nto `1-calibration over the simplex (e.g., by analyzing the KL divergence between predictions and\ndistributions of historical outcomes). We present a very similar algorithm (TreeCal) for multi\u0002dimensional calibration over an arbitrary convex set P \u2282 R\nd\n, but with a simple, unified analysis\nthat provides simultaneous guarantees for calibration with respect to any norm k\u00b7k. In particular, we\nprove the following theorem.\nTheorem 1.1 (Informal restatement of Corollary C.5). Fix a convex set P and a norm k \u00b7 k. Assume\nthere exists a function R : P \u2192 R that is 1-strongly-convex with respect to k \u00b7 k and has range\n(maxx\u2208P R(x) \u2212 minp\u2208P R(x)) at most \u03c1. Then TreeCal guarantees that the calibration error of\nits predictions is bounded by Calk\u00b7k\nT \u2264 \u000fT for T \u2265 (diamk\u00b7k(P)/\u000f)\nO(\u03c1/\u000f2)\n.\nInterestingly, the function R(p) and parameter \u03c1 appearing in the statement of Theorem 1.1 have an\nindependent learning-theoretic interpretation: if we consider the online linear optimization problem\nwhere a learner plays actions in P and the adversary plays linear losses that are unit bounded in the\ndual norm k\u00b7k\u2217, then it is possible for the learner to guarantee a regret bound of at most O(\n\u221a\n\u03c1T) by\nplaying Follow-The-Regularized-Leader (FTRL) with R(p) as a regularizer. In fact, since universality\nresults for mirror descent guarantee that some instantiation of FTRL achieves near-optimal rates for\nonline linear optimization (as long as the action and loss sets are centrally convex) [SST11, GSJ24],\nthis allows us to relate the performance of Theorem 3.1 directly to what rates are possible in online\nlinear optimization.\nCorollary 1.2 (Informal restatement of Corollary C.6). Let P \u2286 R\nd be a centrally symmetric convex\nset, and let L = {y \u2208 R\nd\n| kyk\u2217 \u2264 1} for some norm k\u00b7k. Then if there exists an algorithm for\nonline linear optimization with action set P and loss set L that incurs regret at most O(\n\u221a\n\u03c1T),\nTreeCal guarantees that the calibration error of its predictions is bounded by Calk\u00b7k\nT \u2264 \u000fT for\nT \u2265 (diamk\u00b7k(P)/\u000f)\nO(\u03c1/\u000f2)\n.\nTheorem 1.1 and its corollary allow us to immediately recover several existing and novel bounds on\ncalibration error in a variety of settings:\n2\n\u2022 When P is the d-simplex \u2206d and k\u00b7k is the `1-norm, the existence of the negative entropy\nregularizer R(x) = Pd\ni=1 xi\nlog xi (which is 1-strongly convex w.r.t. the `1 norm with range\n\u03c1 = log d) implies that the `1 calibration error of TreeCal is at most (1/\u000f)\nO(log d/\u000f2) =\nd\nO\u02dc(1/\u000f2)\n. This recovers the result of [Pen25].\n\u2022 When P is the `2 ball and k\u00b7k is the `2 norm, the Euclidean regularizer (R(x) = kxk\n2\n)\nimplies a calibration bound of (1/\u000f)\nO(1/\u000f2)\n(notably, this bound is independent of d).\nIt should be emphasized here that running TreeCal does not require any online linear optimization\nsubroutine, nor any knowledge of these regularizers R(",
          "original_query": "[Pen25] (2025). High\u2011dimensional calibration over the simplex",
          "cleaned_query": "[Pen25]. High\u2011dimensional calibration over the simplex"
        },
        {
          "success": true,
          "title": "Directive-type Memorandum 24-009, \"Public Complaints or Requests Regarding Public Displays or Public Expressions of Religion on DoD Property,\" December 16, 2024",
          "url": "https://www.esd.whs.mil/Portals/54/Documents/DD/issuances/dtm/DTM-24-009.PDF?ver=TZcYWPIK7aeN7QXdat7Xxw==",
          "content": "OFFICE OF THE UNDER SECRETARY OF DEFENSE\n4000 DEFENSE PENTAGON\nWASHINGTON, D.C. 20301-4000\n PERSONNEL AND\n READINESS\nDecember 16, 2024 \nMEMORANDUM FOR SENIOR PENTAGON LEADERSHIP\nDEFENSE AGENCY AND DOD FIELD ACTIVITY DIRECTORS\nSUBJECT: Directive-type Memorandum 24-009 \u2013 \u201cPublic Complaints or Requests Regarding \nPublic Displays or Public Expressions of Religion on DoD Property\u201d \nReferences: See Attachment 1.\nPurpose. In accordance with the authority of DoD Directive 5124.02 and Section 1049 of \nPublic Law 118-31 (also known and referred to in this issuance as the \u201cNational Defense \nAuthorization Act for Fiscal Year 2024\u201d), this directive-type memorandum (DTM): \n\u2022 Establishes policy, assigns responsibilities, and provides procedures for the \ntimely determination of a covered complaint or request as defined in the Glossary \nregarding public displays or expressions of religion on DoD property. \n\u2022 Is effective December 16, 2024; it must be incorporated into DoD Instruction \n(DoDI) 1300.17. This DTM will expire effective December 16, 2025. \nApplicability. This DTM: \n\u2022 Applies to: \no OSD, the Military Departments, the Office of the Chairman of the \nJoint Chiefs of Staff and the Joint Staff, the Combatant Commands, the \nOffice of Inspector General of the Department of Defense, the Defense \nAgencies, the DoD Field Activities, and all other organizational entities \nwithin the DoD, referred to in this DTM as \u201cthe DoD Components.\u201d \no Covered complaints or requests regarding public display or expression \nof religion that have occurred or taken place on DoD property. \n\u2022 Does not apply to requests for access to DoD installations for the purpose of \nconducting or performing a public display or expression of religion. Such \nrequests will be in accordance with Volume 3 of DoD Manual 5200.08. \n\u2022 Requests by Service members for the accommodation of religious practices \nare governed by DoDI 1300.17 and applicable Military Department and Service \npolicies; such requests are not subject to this DTM. \nDTM-24-009, December 16, 2024 \n2\nDefinitions. See Glossary.\nPolicy. \nIn accordance with Section 1049 of the National Defense Authorization Act for Fiscal \nYear 2024, the DoD will: \n\u2022 Provide a timely determination for a covered complaint or request regarding a \npublic display or public expression of religion on DoD property made by an \nindividual or entity other than a Service member of the DoD or a civilian \nemployee or contractor of the DoD. \n\u2022 Ensure compliance with the requirements in Section 1049 of the National \nDefense Authorization Act for Fiscal Year 2024. \n\u2022 Allow for the continued public display or expression of religion that is the \nsubject of a covered complaint or request until a determination is made in \naccordance with this DTM. However, military installation commanders may \norder the immediate removal of a display or cessation of expression upon a \ndetermination in writing to the official authorized to act on covered complaints \nand requests, by an installation commander that continuation of the display or \nexpression will have an adverse impact on military readiness, unit cohesion, good \norder and discipline, or health and safety. \nResponsibilities. \n\u2022 Under Secretary of Defense for Personnel and Readiness. The Under \nSecretary of Defense for Personnel and Readiness oversees implementation of and \ncompliance with this DTM.\n\u2022 Assistant Secretary of Defense for Manpower and Reserve Affairs \n(ASD(M&RA)). Under the authority, direction, and control of the Under \nSecretary of Defense for Personnel and Readiness, the ASD(M&RA) monitors \ndeterminations to ensure religious liberty policy compliance and consistency of \ndeterminations throughout the DoD. \n\u2022 DoD Component Heads. The DoD Component heads: \no Ensure complaints or requests received by their personnel regarding \npublic displays or public expressions of religion on DoD property are \nprocessed in accordance with Attachment 2 and Section 1049 of the \nNational Defense Authorization Act for Fiscal Year 2024. \no May delegate the authority to act on covered complaints or requests \nsubject to this DTM. Such delegation must be in writing and may be no \nlower than a Presidentially Appointed, Senate-confirmed official. Further \nre-delegation is not authorized. \nDTM-24-009, December 16, 2024 \n3\no Ensure any official to whom authority is delegated under this DTM, \nand subordinate officers and officials, including commanders, judge \nadvocate generals, and chaplains who may review and endorse covered \ncomplaints or requests, are trained on religious liberty policy, timelines, \nauthority restrictions, and the policies and procedures in this DTM. \no Ensure records and information established and created in accordance \nwith this DTM are retained in accordance with DoDI 5015.02 and DoD \nComponent records management disposition schedules. \no Ensure any action in accordance with this DTM will follow protocols \noutlined in DoDI 5400.11. \nProcedures. See Attachment 2. \nReleasability. Cleared for public release. Available on the Directives Division Website \nat https://www.esd.whs.mil/DD/.\nAshish S. Vazirani \nPerforming the Duties of the Under Secretary of \nDefense for Personnel and Readiness\nAttachment:\nAs stated\nDTM-24-009, December 16, 2024 \n4 Attachment 1 \nATTACHMENT 1\nREFERENCES\nDoD Directive 5124.02, \u201cUnder Secretary of Defense for Personnel and Readiness \n(USD(P&R)),\u201d June 23, 2008 \nDoD Instruction 1300.17, \u201cReligious Liberty in the Military Services,\u201d September 1, 2020 \nDoD Instruction 4165.14, \u201cReal Property Inventory and Reporting,\u201d September 8, 2023 \nDoD Instruction 5015.02, \u201cDoD Records Management Program,\u201d February 24, 2015, as \namended\nDoD Instruction 5120.08, \u201cArmed Forces Chaplains Board,\u201d April 24, 2024 \nDoD Instruction 5400.11, \u201cDoD Privacy and Civil Liberties Program,\u201d January 29, 2019, as \namended\nDoD Manual 5200.08, Volume 3, \u201cPhysical Security Program: Access to DoD Installations,\u201d \nJanuary 2, 2019, as amended \nPublic Law 118-31, Section 1049, \u201cNational Defense Authorization Act for Fiscal Year 2024,\u201d \nDecember 22, 2023 \nDTM-24-009, December 16, 2024 \n5 Attachment 2\nATTACHMENT 2\nPROCEDURES\n1. GENERAL. The DoD Component heads will follow the procedures in this attachment to \nreview and determine resolution of covered complaints or requests.\n2. INITIATION OF THE COMPLAINT OR REQUEST. All covered complaints received \nwithin DoD will be forwarded to the official authorized to act on them for the DoD Component \nconcerned not later than 10 working days after such receipt. The command or activity \nforwarding a covered complaint will provide, to the extent practicable, sufficient contextual \ninformation to allow for consideration of the complaint. \n3. PROCESSING OF NON-COVERED COMPLAINTS OR REQUESTS. Non-covered \ncomplaints or requests will be reviewed to determine an appropriate resolution pursuant to \napplicable laws and DoD, Military Department, and Service-level policies and regulations. The \nDoD Components may, as appropriate, use the determination of covered complaints or requests \nto inform the decision-making process for a non-covered complaint. \n4. PROCESSING OF COVERED COMPLAINTS OR REQUESTS. Not later than 30 working \ndays after receiving the covered complaint or request, and after consultation as described in \nParagraph 4, the official authorized to act on a covered complaint or request will make a \ndetermination regarding the complaint or request and: \na. Provides timely notification of such determination to: \n(1) The individual or entity who made the complaint or request. \n(2) The officer or official who forwarded the complaint or request. \nb. Provides a copy of the determination of the complaint or request, and documentation \nof any action taken, to: \n(1) All impacted commanders or officials.\n(2) The Office of the ASD(M&RA).\nDTM-24-009, December 16, 2024 \n6 Attachment 2\n4. CONSULTATION. \na. Officials authorized to act on covered complaints or requests, other than those for a \nMilitary Department, will c",
          "original_query": "[DDFG24 / PR24] (2024). TreeSwap: Efficient swap-regret minimization",
          "cleaned_query": "[DDFG24",
          "note": "Non-academic source"
        }
      ],
      "generated_ideas": [
        "Approachability-Based Calibration Under Administrative Time Constraints\nDesign an online forecasting algorithm that stays calibrated while meeting hard deadlines (e.g., \u201cdetermination within X days\u201d) by modeling deadlines as additional vector-payoff constraints and using Blackwell approachability. Evaluate on synthetic \u201ccomplaint arrival\u201d streams where missing a deadline incurs convex penalties, and prove sublinear calibration error subject to bounded deadline-violation regret.",
        "Multi-Objective Calibration: Joint Reliability\u2013Readiness Tradeoffs via Vector Payoffs\nFormulate calibration as one component in a vector payoff (calibration error, operational disruption, and decision timeliness), then apply Blackwell\u2019s vector minimax/approachability to guarantee convergence to a feasible tradeoff set. Produce an algorithm that outputs forecasts plus an operational \u201crisk score,\u201d with guarantees that no objective is systematically sacrificed.",
        "TreeCal with Bandit Feedback and Internal-Regret Guarantees\nExtend the TreeCal/TreeSwap-style high-dimensional calibration framework to the setting where only the realized outcome component for a chosen \u201cquery\u201d is observed (bandit-like partial monitoring). Use a Blum\u2013Mansour external-to-internal regret reduction adapted to TreeSwap to obtain provable calibration in high dimension with minimal observation cost, and characterize the sample complexity gap versus full information.",
        "Swap-Regret-Calibrated Forecasting for Correlated Equilibrium in Large Games\nBuild a practical pipeline from swap-regret minimization (TreeSwap + FTL) to computing correlated equilibria in repeated games with large action spaces, leveraging the known link between internal regret and correlated equilibrium. Provide finite-time bounds that explicitly trade off calibration error (in an appropriate norm) and convergence rates to approximate correlated equilibria.",
        "Calibrated Online Convex Programming with Forecast-In-The-Loop Decisions\nIntegrate online convex programming (Zinkevich-style OCP) with calibrated probabilistic forecasting so that decisions are optimized against costs that depend on predicted distributions, while forecasts remain calibrated against realized outcomes. Provide a joint algorithm that alternates OCP updates with a swap-regret-based calibration module, and prove combined regret + calibration guarantees.",
        "Learning to Calibrate Across Multiple \u201cComplaint Types\u201d via Time-Selection Functions\nModel heterogeneous event streams (e.g., different categories of public complaints) using Blum\u2013Mansour\u2019s generalized regret with time-selection functions, so calibration is guaranteed per subgroup without requiring explicit i.i.d. assumptions. Construct an algorithm that enforces subgroup calibration online (each subgroup as a time selector) and analyze the tradeoff between number of selectors and convergence rates.",
        "Operationally Safe Calibration: Immediate-Removal Overrides as Adversarial Interventions\nStudy calibration when an external authority can intermittently override the process (analogous to \u201cimmediate removal/cessation\u201d triggers) causing censored or modified observations. Propose a robust calibration notion and an algorithm that remains calibrated with respect to the uncensored latent process while controlling regret against the intervention policy, using approachability to handle the induced vector constraints."
      ],
      "generated_ideas_raw": "1. **Approachability-Based Calibration Under Administrative Time Constraints**  \nDesign an online forecasting algorithm that stays calibrated while meeting hard deadlines (e.g., \u201cdetermination within X days\u201d) by modeling deadlines as additional vector-payoff constraints and using Blackwell approachability. Evaluate on synthetic \u201ccomplaint arrival\u201d streams where missing a deadline incurs convex penalties, and prove sublinear calibration error subject to bounded deadline-violation regret.\n\n2. **Multi-Objective Calibration: Joint Reliability\u2013Readiness Tradeoffs via Vector Payoffs**  \nFormulate calibration as one component in a vector payoff (calibration error, operational disruption, and decision timeliness), then apply Blackwell\u2019s vector minimax/approachability to guarantee convergence to a feasible tradeoff set. Produce an algorithm that outputs forecasts plus an operational \u201crisk score,\u201d with guarantees that no objective is systematically sacrificed.\n\n3. **TreeCal with Bandit Feedback and Internal-Regret Guarantees**  \nExtend the TreeCal/TreeSwap-style high-dimensional calibration framework to the setting where only the realized outcome component for a chosen \u201cquery\u201d is observed (bandit-like partial monitoring). Use a Blum\u2013Mansour external-to-internal regret reduction adapted to TreeSwap to obtain provable calibration in high dimension with minimal observation cost, and characterize the sample complexity gap versus full information.\n\n4. **Swap-Regret-Calibrated Forecasting for Correlated Equilibrium in Large Games**  \nBuild a practical pipeline from swap-regret minimization (TreeSwap + FTL) to computing correlated equilibria in repeated games with large action spaces, leveraging the known link between internal regret and correlated equilibrium. Provide finite-time bounds that explicitly trade off calibration error (in an appropriate norm) and convergence rates to approximate correlated equilibria.\n\n5. **Adaptive Discretization TreeCal: Dimension-Free Calibration in \u201cEasy\u201d Sequences**  \nCreate an adaptive partitioning version of TreeCal that refines the prediction tree only where empirical evidence indicates miscalibration, aiming to replace worst-case exponential-in-\\(1/\\varepsilon\\) behavior with data-dependent rates. Prove guarantees in terms of a sequence complexity measure (e.g., variation or covering numbers of outcome averages) and demonstrate gains on structured, non-adversarial streams.\n\n6. **Norm-Conditional Calibration for Policy-Relevant Error Metrics**  \nDevelop a calibration algorithm that simultaneously controls calibration error under multiple norms (e.g., \\(\\ell_1\\), \\(\\ell_2\\), \\(\\ell_\\infty\\)) by treating each norm constraint as a coordinate in a vector-payoff approachability target set. This yields actionable guidance for applications where different stakeholders care about different deviation measures, with explicit bounds on the achievable Pareto frontier.\n\n7. **Calibrated Online Convex Programming with Forecast-In-The-Loop Decisions**  \nIntegrate online convex programming (Zinkevich-style OCP) with calibrated probabilistic forecasting so that decisions are optimized against costs that depend on predicted distributions, while forecasts remain calibrated against realized outcomes. Provide a joint algorithm that alternates OCP updates with a swap-regret-based calibration module, and prove combined regret + calibration guarantees.\n\n8. **Lower Bounds for High-Dimensional Calibration Under Additional Feasibility Constraints**  \nExtend the exponential-in-\\(1/\\varepsilon\\) lower bound results for online calibration by adding realistic constraints (limited randomization, bounded memory, or restricted forecast sets \\(P\\)) and proving sharper impossibility frontiers. Identify which constraints fundamentally break Foster\u2013Vohra-style \u201cforecaster wins with randomization\u201d results, and quantify the minimal randomness/memory needed.\n\n9. **Learning to Calibrate Across Multiple \u201cComplaint Types\u201d via Time-Selection Functions**  \nModel heterogeneous event streams (e.g., different categories of public complaints) using Blum\u2013Mansour\u2019s generalized regret with time-selection functions, so calibration is guaranteed per subgroup without requiring explicit i.i.d. assumptions. Construct an algorithm that enforces subgroup calibration online (each subgroup as a time selector) and analyze the tradeoff between number of selectors and convergence rates.\n\n10. **Operationally Safe Calibration: Immediate-Removal Overrides as Adversarial Interventions**  \nStudy calibration when an external authority can intermittently override the process (analogous to \u201cimmediate removal/cessation\u201d triggers) causing censored or modified observations. Propose a robust calibration notion and an algorithm that remains calibrated with respect to the uncensored latent process while controlling regret against the intervention policy, using approachability to handle the induced vector constraints.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Approachability-Based Calibration Under Administrative Time Constraints\nDesign an online forecasting algorithm that stays calibrated while meeting hard deadlines (e.g., \u201cdetermination within X days\u201d) ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Multi-Objective Calibration: Joint Reliability\u2013Readiness Tradeoffs via Vector Payoffs\nFormulate calibration as one component in a vector payoff (calibration error, operational disruption, and decision",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "TreeCal with Bandit Feedback and Internal-Regret Guarantees\nExtend the TreeCal/TreeSwap-style high-dimensional calibration framework to the setting where only the realized outcome component for a chos",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Swap-Regret-Calibrated Forecasting for Correlated Equilibrium in Large Games\nBuild a practical pipeline from swap-regret minimization (TreeSwap + FTL) to computing correlated equilibria in repeated ga",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Calibrated Online Convex Programming with Forecast-In-The-Loop Decisions\nIntegrate online convex programming (Zinkevich-style OCP) with calibrated probabilistic forecasting so that decisions are optim",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Learning to Calibrate Across Multiple \u201cComplaint Types\u201d via Time-Selection Functions\nModel heterogeneous event streams (e.g., different categories of public complaints) using Blum\u2013Mansour\u2019s generalize",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Operationally Safe Calibration: Immediate-Removal Overrides as Adversarial Interventions\nStudy calibration when an external authority can intermittently override the process (analogous to \u201cimmediate r",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 13,
      "paper_title": "In Search of Adam\u2019s Secret Sauce",
      "contribution": "Through a large empirical study and a focused theoretical simplification (\u03b21 = \u03b22), the paper shows that Adam\u2019s empirical advantage over signed/momentum methods largely stems from its coupled mean/variance estimation \u2014 giving a near\u2011optimal, interpretable optimizer that can be seen as an online mean/variance estimator arising from a mean\u2011field Gaussian variational inference view.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11375,
      "output_tokens": 1056,
      "predecessor_details": [
        {
          "success": true,
          "title": "[1412.6980] Adam: A Method for Stochastic Optimization - arXiv",
          "url": "https://arxiv.org/abs/1412.6980",
          "content": "[1412.6980] Adam: A Method for Stochastic Optimization\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1412.6980\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1412.6980**(cs)\n[Submitted on 22 Dec 2014 ([v1](https://arxiv.org/abs/1412.6980v1)), last revised 30 Jan 2017 (this version, v9)]\n# Title:Adam: A Method for Stochastic Optimization\nAuthors:[Diederik P. Kingma](https://arxiv.org/search/cs?searchtype=author&amp;query=Kingma,+D+P),[Jimmy Ba](https://arxiv.org/search/cs?searchtype=author&amp;query=Ba,+J)\nView a PDF of the paper titled Adam: A Method for Stochastic Optimization, by Diederik P. Kingma and Jimmy Ba\n[View PDF](https://arxiv.org/pdf/1412.6980)> > Abstract:\n> We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm. Comments:|Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015|\nSubjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:1412.6980](https://arxiv.org/abs/1412.6980)[cs.LG]|\n|(or[arXiv:1412.6980v9](https://arxiv.org/abs/1412.6980v9)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1412.6980](https://doi.org/10.48550/arXiv.1412.6980)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Diederik P Kingma M.Sc. [[view email](https://arxiv.org/show-email/02d1da11/1412.6980)]\n**[[v1]](https://arxiv.org/abs/1412.6980v1)**Mon, 22 Dec 2014 13:54:29 UTC (280 KB)\n**[[v2]](https://arxiv.org/abs/1412.6980v2)**Sat, 17 Jan 2015 20:26:06 UTC (283 KB)\n**[[v3]](https://arxiv.org/abs/1412.6980v3)**Fri, 27 Feb 2015 21:04:48 UTC (289 KB)\n**[[v4]](https://arxiv.org/abs/1412.6980v4)**Tue, 3 Mar 2015 17:51:27 UTC (289 KB)\n**[[v5]](https://arxiv.org/abs/1412.6980v5)**Thu, 23 Apr 2015 16:46:07 UTC (289 KB)\n**[[v6]](https://arxiv.org/abs/1412.6980v6)**Tue, 23 Jun 2015 19:57:17 UTC (958 KB)\n**[[v7]](https://arxiv.org/abs/1412.6980v7)**Mon, 20 Jul 2015 09:43:23 UTC (519 KB)\n**[[v8]](https://arxiv.org/abs/1412.6980v8)**Thu, 23 Jul 2015 20:27:47 UTC (526 KB)\n**[v9]**Mon, 30 Jan 2017 01:27:54 UTC (490 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Adam: A Method for Stochastic Optimization, by Diederik P. Kingma and Jimmy Ba\n* [View PDF](https://arxiv.org/pdf/1412.6980)\n* [TeX Source](https://arxiv.org/src/1412.6980)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1412.6980&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1412.6980&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2014-12](https://arxiv.org/list/cs.LG/2014-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/1412.6980?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1412.6980)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1412.6980)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1412.6980)\n### [42 blog links](https://arxiv.org/tb/1412.6980)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/conf/iclr/iclr2015.html#KingmaB14)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/conf/iclr/KingmaB14)\n[Diederik P. Kingma]()\n[Jimmy Ba]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an ide",
          "original_query": "Adam: A Method for Stochastic Optimization (Kingma & Ba, 2014)",
          "cleaned_query": "Adam: A Method for Stochastic Optimization"
        },
        {
          "success": true,
          "title": "Adaptive Subgradient Methods for Online Learning and Stochastic ...",
          "url": "https://dl.acm.org/doi/10.5555/1953048.2021068",
          "content": "[skip to main content](https://dl.acm.org/dl.acm.org#skip-to-main-content)\n\nContents\n\n## Abstract\n\nWe present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.\n\n## Formats available\n\nYou can view the full content in the following formats:\n\n[PDF](https://dl.acm.org/doi/pdf/10.5555/1953048.2021068)\n\n## References\n\n\\[1\\]\n\nJ. Abernethy, P. L. Bartlett, A. Rakhlin, and A. Tewari. Optimal strategies and minimax lower bounds for online convex games. In _Proceedings of the Twenty First Annual Conference on Computational Learning Theory_, 2008.\n\n[Google Scholar](https://scholar.google.com/scholar?q=J.+Abernethy%2C+P.+L.+Bartlett%2C+A.+Rakhlin%2C+and+A.+Tewari.+Optimal+strategies+and+minimax+lower+bounds+for+online+convex+games.+In+Proceedings+of+the+Twenty+First+Annual+Conference+on+Computational+Learning+Theory%2C+2008.)\n\n\\[2\\]\n\nT. Ando. Concavity of certain maps on positive definite matrices and applications to Hadamard products. _Linear Algebra and its Applications_, 26:203-241, 1979.\n\n[Google Scholar](https://scholar.google.com/scholar?q=T.+Ando.+Concavity+of+certain+maps+on+positive+definite+matrices+and+applications+to+Hadamard+products.+Linear+Algebra+and+its+Applications%2C+26%3A203-241%2C+1979.)\n\n\\[3\\]\n\nA. Asuncion and D. J. Newman. UCI machine learning repository, 2007. URL http://www.ics. uci.edu/~mlearn/MLRepository.html.\n\n[Google Scholar](https://scholar.google.com/scholar?q=A.+Asuncion+and+D.+J.+Newman.+UCI+machine+learning+repository%2C+2007.+URL+http%3A%2F%2Fwww.ics.+uci.edu%2F~mlearn%2FMLRepository.html.)\n\n\\[4\\]\n\nP. Auer and C. Gentile. Adaptive and self-confident online learning algorithms. In _Proceedings of the Thirteenth Annual Conference on Computational Learning Theory_, 2000.\n\n[Crossref](https://doi.org/10.5555/648299.755168)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.5555%2F648299.755168)\n\n\\[5\\]\n\nP. L. Bartlett, E. Hazan, and A. Rakhlin. Adaptive online gradient descent. In _Advances in Neural Information Processing Systems 20_, 2007.\n\n[Google Scholar](https://scholar.google.com/scholar?q=P.+L.+Bartlett%2C+E.+Hazan%2C+and+A.+Rakhlin.+Adaptive+online+gradient+descent.+In+Advances+in+Neural+Information+Processing+Systems+20%2C+2007.)\n\n\\[6\\]\n\nA. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. _Operations Research Letters_, 31:167-175, 2003.\n\n[Crossref](https://doi.org/10.1016/S0167-6377(02)00231-6)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1016%2FS0167-6377%2802%2900231-6)\n\n\\[7\\]\n\nJ. V. Bondar. Comments on and complements to _Inequalities: Theory of Majorization and Its Applications. Linear Algebra and its Applications_, 199:115-129, 1994.\n\n[Google Scholar](https://scholar.google.com/scholar?q=J.+V.+Bondar.+Comments+on+and+complements+to+Inequalities%3A+Theory+of+Majorization+and+Its+Applications.+Linear+Algebra+and+its+Applications%2C+199%3A115-129%2C+1994.)\n\n\\[8\\]\n\nA. Bordes, L. Bottou, and P. Gallinari. Sgd-qn: Careful quasi-newton stochastic gradient descent. _Journal of Machine Learning Research_, 10:1737-1754, 2009.\n\n[Crossref](https://doi.org/10.5555/1577069.1755842)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.5555%2F1577069.1755842)\n\n\\[9\\]\n\nS. Boyd and L. Vandenberghe. _Convex Optimization_. Cambridge University Press, 2004.\n\n[Crossref](https://doi.org/10.5555/993483)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.5555%2F993483)\n\n\\[10\\]\n\nP. Brucker. An _O(n)_ algorithm for quadratic knapsack problems. _Operations Research Letters_, 3 (3):163-166, 1984.\n\n[Google Scholar](https://scholar.google.com/scholar?q=P.+Brucker.+An+O%28n%29+algorithm+for+quadratic+knapsack+problems.+Operations+Research+Letters%2C+3+%283%29%3A163-166%2C+1984.)\n\n\\[11\\]\n\nN. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms. _IEEE Transactions on Information Theory_, 50(9):2050-2057, September 2004.\n\n[Crossref](https://doi.org/10.1109/TIT.2004.833339)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1109%2FTIT.2004.833339)\n\n\\[12\\]\n\nN. Cesa-Bianchi, A. Conconi, and C. Gentile. A second-order perceptron algorithm. _SIAM Journal on Computing_, 34(3):640-668, 2005.\n\n[Crossref](https://doi.org/10.1137/S0097539703432542)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1137%2FS0097539703432542)\n\n\\[13\\]\n\nN. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with expert advice. _Machine Learning_, 66:321-352, 2007.\n\n[Crossref](https://doi.org/10.1007/s10994-006-5001-7)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1007%2Fs10994-006-5001-7)\n\n\\[14\\]\n\nK. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. Online passive aggressive algorithms. _Journal of Machine Learning Research_, 7:551-585, 2006.\n\n[Crossref](https://doi.org/10.5555/1248547.1248566)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.5555%2F1248547.1248566)\n\n\\[15\\]\n\nK. Crammer, M. Dredze, and F. Pereira. Exact convex confidence-weighted learning. In _Advances in Neural Information Processing Systems 22_, 2008.\n\n[Google Scholar](https://scholar.google.com/scholar?q=K.+Crammer%2C+M.+Dredze%2C+and+F.+Pereira.+Exact+convex+confidence-weighted+learning.+In+Advances+in+Neural+Information+Processing+Systems+22%2C+2008.)\n\n\\[16\\]\n\nK. Crammer, M. Dredze, and A. Kulesza. Adaptive regularization of weight vectors. In _Advances in Neural Information Processing Systems 23_, 2009.\n\n[Google Scholar](https://scholar.google.com/scholar?q=K.+Crammer%2C+M.+Dredze%2C+and+A.+Kulesza.+Adaptive+regularization+of+weight+vectors.+In+Advances+in+Neural+Information+Processing+Systems+23%2C+2009.)\n\n\\[17\\]\n\nC. Davis. Notions generalizing convexity for functions defined on spaces of matrices. In _Proceedings of the Symposia in Pure Mathematics_, volume 7, pages 187-201. American Mathematical Society, 1963.\n\n[Google Scholar](https://scholar.google.com/scholar?q=C.+Davis.+Notions+generalizing+convexity+for+functions+defined+on+spaces+of+matrices.+In+Proceedings+of+the+Symposia+in+Pure+Mathematics%2C+volume+7%2C+pages+187-201.+American+Mathematical+Society%2C+1963.)\n\n\\[18\\]\n\nJ. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei. ImageNet: a large-scale hierarchical image database. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2009.\n\n[Google Scholar](https://scholar.google.com/scholar?q=J.+Deng%2C+W.+Dong%2C+R.+Socher%2C+L.+Li%2C+K.+Li%2C+and+L.+Fei-Fei.+ImageNet%3A+a+large-scale+hierarchical+image+database.+In+Proceedings+of+the+IEEE+Conference+on+Computer+Vision+and+Pattern+Recognition%2C+2009.)\n\n\\[19\\]\n\nJ. Duchi and Y. Singer. Efficient online and batch learning using forward backward splitting. _Journal of Machine Learning Research_, 10:2873-2908, 2009.\n\n[Crossref](https://doi.org/10.5555/1577069.1755882)\n\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.5555%2F1577069.1",
          "original_query": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization (AdaGrad) (Duchi, Hazan & Singer, 2011)",
          "cleaned_query": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization (AdaGrad)"
        },
        {
          "success": true,
          "title": "signSGD: Compressed Optimisation for Non-Convex Problems - arXiv",
          "url": "https://arxiv.org/abs/1802.04434",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1802.04434** (cs)\n\n\\[Submitted on 13 Feb 2018 ( [v1](https://arxiv.org/abs/1802.04434v1)), last revised 7 Aug 2018 (this version, v3)\\]\n\n# Title:signSGD: Compressed Optimisation for Non-Convex Problems\n\nAuthors: [Jeremy Bernstein](https://arxiv.org/search/cs?searchtype=author&query=Bernstein,+J), [Yu-Xiang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+Y), [Kamyar Azizzadenesheli](https://arxiv.org/search/cs?searchtype=author&query=Azizzadenesheli,+K), [Anima Anandkumar](https://arxiv.org/search/cs?searchtype=author&query=Anandkumar,+A)\n\nView a PDF of the paper titled signSGD: Compressed Optimisation for Non-Convex Problems, by Jeremy Bernstein and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/1802.04434)\n\n> Abstract:Training large neural networks requires distributing learning across multiple workers, where the cost of communicating gradients can be a significant bottleneck. signSGD alleviates this problem by transmitting just the sign of each minibatch stochastic gradient. We prove that it can get the best of both worlds: compressed gradients and SGD-level convergence rate. The relative $\\\\ell\\_1/\\\\ell\\_2$ geometry of gradients, noise and curvature informs whether signSGD or SGD is theoretically better suited to a particular problem. On the practical side we find that the momentum counterpart of signSGD is able to match the accuracy and convergence speed of Adam on deep Imagenet models. We extend our theory to the distributed setting, where the parameter server uses majority vote to aggregate gradient signs from each worker enabling 1-bit compression of worker-server communication in both directions. Using a theorem by Gauss we prove that majority vote can achieve the same reduction in variance as full precision distributed SGD. Thus, there is great promise for sign-based optimisation schemes to achieve fast communication and fast convergence. Code to reproduce experiments is to be found at [this https URL](https://github.com/jxbz/signSGD) .\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC); Optimization and Control (math.OC) |\n| Cite as: | [arXiv:1802.04434](https://arxiv.org/abs/1802.04434) \\[cs.LG\\] |\n| | (or [arXiv:1802.04434v3](https://arxiv.org/abs/1802.04434v3) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1802.04434](https://doi.org/10.48550/arXiv.1802.04434) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jeremy Bernstein \\[ [view email](https://arxiv.org/show-email/c30de5d0/1802.04434)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1802.04434v1)**\nTue, 13 Feb 2018 02:14:35 UTC (201 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1802.04434v2)**\nSat, 23 Jun 2018 18:01:27 UTC (478 KB)\n\n**\\[v3\\]**\nTue, 7 Aug 2018 18:55:19 UTC (477 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled signSGD: Compressed Optimisation for Non-Convex Problems, by Jeremy Bernstein and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/1802.04434)\n- [TeX Source](https://arxiv.org/src/1802.04434)\n- [Other Formats](https://arxiv.org/format/1802.04434)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1802.04434&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1802.04434&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2018-02](https://arxiv.org/list/cs.LG/2018-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1802.04434?context=cs)\n\n[cs.DC](https://arxiv.org/abs/1802.04434?context=cs.DC)\n\n[math](https://arxiv.org/abs/1802.04434?context=math)\n\n[math.OC](https://arxiv.org/abs/1802.04434?context=math.OC)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1802.04434)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1802.04434)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1802.04434)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1802.html#abs-1802-04434) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1802-04434)\n\n[Jeremy Bernstein](https://dblp.uni-trier.de/search/author?author=Jeremy%20Bernstein)\n\n[Yu-Xiang Wang](https://dblp.uni-trier.de/search/author?author=Yu-Xiang%20Wang)\n\n[Kamyar Azizzadenesheli](https://dblp.uni-trier.de/search/author?author=Kamyar%20Azizzadenesheli)\n\n[Anima Anandkumar](https://dblp.uni-trier.de/search/author?author=Anima%20Anandkumar)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1802.04434&description=signSGD: Compressed Optimisation for Non-Convex Problems) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1802.04434&title=signSGD: Compressed Optimisation for Non-Convex Problems)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1802.04434) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "signSGD: Compressed Optimization for Non\u2011Convex Problems (Bernstein et al., 2018)",
          "cleaned_query": "signSGD: Compressed Optimization for Non\u2011Convex Problems"
        },
        {
          "success": true,
          "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
          "url": "https://arxiv.org/abs/1704.04289",
          "content": "# Statistics > Machine Learning\n\n**arXiv:1704.04289** (stat)\n\n\\[Submitted on 13 Apr 2017 ( [v1](https://arxiv.org/abs/1704.04289v1)), last revised 19 Jan 2018 (this version, v2)\\]\n\n# Title:Stochastic Gradient Descent as Approximate Bayesian Inference\n\nAuthors: [Stephan Mandt](https://arxiv.org/search/stat?searchtype=author&query=Mandt,+S), [Matthew D. Hoffman](https://arxiv.org/search/stat?searchtype=author&query=Hoffman,+M+D), [David M. Blei](https://arxiv.org/search/stat?searchtype=author&query=Blei,+D+M)\n\nView a PDF of the paper titled Stochastic Gradient Descent as Approximate Bayesian Inference, by Stephan Mandt and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/1704.04289)\n\n> Abstract:Stochastic Gradient Descent with a constant learning rate (constant SGD) simulates a Markov chain with a stationary distribution. With this perspective, we derive several new results. (1) We show that constant SGD can be used as an approximate Bayesian posterior inference algorithm. Specifically, we show how to adjust the tuning parameters of constant SGD to best match the stationary distribution to a posterior, minimizing the Kullback-Leibler divergence between these two distributions. (2) We demonstrate that constant SGD gives rise to a new variational EM algorithm that optimizes hyperparameters in complex probabilistic models. (3) We also propose SGD with momentum for sampling and show how to adjust the damping coefficient accordingly. (4) We analyze MCMC algorithms. For Langevin Dynamics and Stochastic Gradient Fisher Scoring, we quantify the approximation errors due to finite learning rates. Finally (5), we use the stochastic process perspective to give a short proof of why Polyak averaging is optimal. Based on this idea, we propose a scalable approximate MCMC algorithm, the Averaged Stochastic Gradient Sampler.\n\n| | |\n| --- | --- |\n| Comments: | 35 pages, published version (JMLR 2017) |\n| Subjects: | Machine Learning (stat.ML); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:1704.04289](https://arxiv.org/abs/1704.04289) \\[stat.ML\\] |\n| (or [arXiv:1704.04289v2](https://arxiv.org/abs/1704.04289v2) \\[stat.ML\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1704.04289](https://doi.org/10.48550/arXiv.1704.04289) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | Journal of Machine Learning Research 18 (2017) 1-35 |\n\n## Submission history\n\nFrom: Stephan Mandt \\[ [view email](https://arxiv.org/show-email/046b0812/1704.04289)\\] **[\\[v1\\]](https://arxiv.org/abs/1704.04289v1)**\nThu, 13 Apr 2017 22:17:30 UTC (3,148 KB)\n**\\[v2\\]**\nFri, 19 Jan 2018 21:07:09 UTC (3,018 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Stochastic Gradient Descent as Approximate Bayesian Inference, by Stephan Mandt and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/1704.04289)\n- [TeX Source](https://arxiv.org/src/1704.04289)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1704.04289&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1704.04289&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2017-04](https://arxiv.org/list/stat.ML/2017-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1704.04289?context=cs) [cs.LG](https://arxiv.org/abs/1704.04289?context=cs.LG) [stat](https://arxiv.org/abs/1704.04289?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1704.04289)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1704.04289)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1704.04289)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1704.04289) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Stochastic Gradient Descent as Approximate Bayesian Inference (Mandt, Hoffman & Blei, 2017)",
          "cleaned_query": "Stochastic Gradient Descent as Approximate Bayesian Inference"
        },
        {
          "success": true,
          "title": "[1711.05101] Decoupled Weight Decay Regularization - arXiv",
          "url": "https://arxiv.org/abs/1711.05101",
          "content": "[1711.05101] Decoupled Weight Decay Regularization\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1711.05101\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1711.05101**(cs)\n[Submitted on 14 Nov 2017 ([v1](https://arxiv.org/abs/1711.05101v1)), last revised 4 Jan 2019 (this version, v3)]\n# Title:Decoupled Weight Decay Regularization\nAuthors:[Ilya Loshchilov](https://arxiv.org/search/cs?searchtype=author&amp;query=Loshchilov,+I),[Frank Hutter](https://arxiv.org/search/cs?searchtype=author&amp;query=Hutter,+F)\nView a PDF of the paper titled Decoupled Weight Decay Regularization, by Ilya Loshchilov and 1 other authors\n[View PDF](https://arxiv.org/pdf/1711.05101)> > Abstract:\n> L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \\emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it &#34;weight decay&#34; in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \\emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam&#39;s generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at [> this https URL\n](https://github.com/loshchil/AdamW-and-SGDW)> Comments:|Published as a conference paper at ICLR 2019|\nSubjects:|Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Optimization and Control (math.OC)|\nCite as:|[arXiv:1711.05101](https://arxiv.org/abs/1711.05101)[cs.LG]|\n|(or[arXiv:1711.05101v3](https://arxiv.org/abs/1711.05101v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1711.05101](https://doi.org/10.48550/arXiv.1711.05101)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Ilya Loshchilov [[view email](https://arxiv.org/show-email/b65b465b/1711.05101)]\n**[[v1]](https://arxiv.org/abs/1711.05101v1)**Tue, 14 Nov 2017 14:24:06 UTC (5,111 KB)\n**[[v2]](https://arxiv.org/abs/1711.05101v2)**Wed, 14 Feb 2018 14:03:35 UTC (7,746 KB)\n**[v3]**Fri, 4 Jan 2019 21:01:49 UTC (8,347 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Decoupled Weight Decay Regularization, by Ilya Loshchilov and 1 other authors\n* [View PDF](https://arxiv.org/pdf/1711.05101)\n* [TeX Source](https://arxiv.org/src/1711.05101)\n* [Other Formats](https://arxiv.org/format/1711.05101)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1711.05101&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1711.05101&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2017-11](https://arxiv.org/list/cs.LG/2017-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/1711.05101?context=cs)\n[cs.NE](https://arxiv.org/abs/1711.05101?context=cs.NE)\n[math](https://arxiv.org/abs/1711.05101?context=math)\n[math.OC](https://arxiv.org/abs/1711.05101?context=math.OC)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1711.05101)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1711.05101)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1711.05101)\n### [5 blog links](https://arxiv.org/tb/1711.05101)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1711.html#abs-1711-05101)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1711-05101)\n[Ilya Loshchilov]()\n[Frank Hutter]()\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css)export BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHav",
          "original_query": "Decoupled Weight Decay Regularization (AdamW) (Loshchilov & Hutter, 2019)",
          "cleaned_query": "Decoupled Weight Decay Regularization (AdamW)"
        },
        {
          "success": true,
          "title": "Training Compute-Optimal Large Language Models - arXiv",
          "url": "https://arxiv.org/abs/2203.15556",
          "content": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
          "original_query": "Training Compute\u2011Optimal Large Language Models (Chinchilla) (Hoffmann et al., 2022)",
          "cleaned_query": "Training Compute\u2011Optimal Large Language Models (Chinchilla)"
        }
      ],
      "generated_ideas": [
        "Bayes-Calibrated AdamW via Stationary-Distribution Matching\nExtend \u201cSGD as approximate Bayesian inference\u201d to AdamW by explicitly modeling the optimizer-induced stationary distribution under decoupled weight decay. Derive a practical rule to set (learning rate, \u03b2\u2019s, weight decay) to minimize KL between the induced stationary distribution and a Laplace/diagonal-Gaussian posterior approximation, then validate on uncertainty-sensitive tasks (calibration, selective prediction) without changing the model architecture.",
        "Adaptive Proximal Geometry for Modern Deep Nets (Mirror-AdamW)\nCombine adaptive subgradient methods\u2019 data-dependent proximal functions with Adam-style moment estimation to create a mirror-descent variant that learns a proximal metric online (diagonal or low-rank+diagonal). Implement an efficient update that uses running gradient statistics to adapt the proximal function and provide regret-style guarantees in the convex case, then test whether it yields better rare-feature learning and stability in sparse/mixture-of-experts layers.",
        "When Does SignSGD Beat Adam? A Geometry-Aware Switching Criterion\nOperationalize signSGD\u2019s theory about \u2113\u2081/\u2113\u2082 geometry by building an online diagnostic that estimates gradient sparsity, noise anisotropy, and curvature proxies from Adam moments. Use this diagnostic to switch per-layer (or per-block) between sign updates and full-precision AdamW updates, aiming to reduce communication and improve stability on \u201cspiky-gradient\u201d components (embeddings, attention projections).",
        "Token-Budget-Aware Weight Decay: Decoupling Regularization from LR at Scale\nCreate a principled schedule for decoupled weight decay that depends on tokens seen (not steps) and is invariant across batch size and learning-rate changes, leveraging AdamW\u2019s decoupling insight. Empirically test whether \u201cconstant effective prior\u201d weight decay improves transfer and prevents late-stage overfitting in compute-optimal LLM regimes compared to standard cosine-decay+fixed wd.",
        "Averaged Stochastic Gradient Sampler with Adam-Style Preconditioning\nExtend the Averaged Stochastic Gradient Sampler concept to an Adam-preconditioned sampler that uses Polyak averaging plus adaptive second moments, aiming for scalable approximate posterior sampling in large models. Provide a clear recipe for choosing constant step size and damping to target a desired temperature, then evaluate on Bayesian model selection signals (marginal-likelihood proxies) and uncertainty benchmarks.",
        "Hyperparameter-Free AdamW via Online Regret-Minimizing Tuning\nUse the adaptive online learning framework (best proximal function in hindsight) to create an \u201cauto-tuned AdamW\u201d where learning rate and \u03b2\u2082 are updated online to minimize a surrogate regret bound derived from observed gradients. Implement a lightweight controller that updates hyperparameters every N steps using only moment statistics, and test whether it matches tuned baselines across datasets and model sizes without sweeps.",
        "Layerwise Priors for LLMs: Decoupled Weight Decay as Structured Bayesian Regularization\nInterpret decoupled weight decay as an explicit Gaussian prior and generalize it to layerwise (or parameter-group) priors whose strengths are learned during training via a variational-EM style procedure (building on SGD-as-inference). Concretely, learn separate decay coefficients for embeddings, attention, and MLP blocks by optimizing a tractable evidence lower bound approximation estimated from training dynamics."
      ],
      "generated_ideas_raw": "1. **Compute-Optimal Optimizer Scheduling for LLM Pretraining**\n   Design a training policy that co-optimizes learning-rate schedule, Adam/AdamW hyperparameters (\u03b2\u2081, \u03b2\u2082, \u03b5), and weight decay as a function of token budget to minimize loss at fixed compute (in the spirit of compute-optimal scaling laws). Concretely, fit a small set of schedule parameters on proxy runs (e.g., 1\u20135B tokens) and extrapolate to full runs, testing whether phase-wise transitions (e.g., AdamW \u2192 momentum SGD) improve final perplexity per FLOP.\n\n2. **Bayes-Calibrated AdamW via Stationary-Distribution Matching**\n   Extend \u201cSGD as approximate Bayesian inference\u201d to AdamW by explicitly modeling the optimizer-induced stationary distribution under decoupled weight decay. Derive a practical rule to set (learning rate, \u03b2\u2019s, weight decay) to minimize KL between the induced stationary distribution and a Laplace/diagonal-Gaussian posterior approximation, then validate on uncertainty-sensitive tasks (calibration, selective prediction) without changing the model architecture.\n\n3. **Adaptive Proximal Geometry for Modern Deep Nets (Mirror-AdamW)**\n   Combine adaptive subgradient methods\u2019 data-dependent proximal functions with Adam-style moment estimation to create a mirror-descent variant that learns a proximal metric online (diagonal or low-rank+diagonal). Implement an efficient update that uses running gradient statistics to adapt the proximal function and provide regret-style guarantees in the convex case, then test whether it yields better rare-feature learning and stability in sparse/mixture-of-experts layers.\n\n4. **Sign-Based AdamW for 1-Bit Distributed Pretraining**\n   Develop a distributed optimizer that keeps AdamW\u2019s preconditioning locally but communicates only 1-bit signs (\u00e0 la signSGD) plus a small set of scalar moments per layer (e.g., norm or quantized scale). Use majority vote aggregation for signs and decouple weight decay at the server, evaluating wall-clock throughput and final perplexity on multi-node LLM training under realistic bandwidth constraints.\n\n5. **When Does SignSGD Beat Adam? A Geometry-Aware Switching Criterion**\n   Operationalize signSGD\u2019s theory about \u2113\u2081/\u2113\u2082 geometry by building an online diagnostic that estimates gradient sparsity, noise anisotropy, and curvature proxies from Adam moments. Use this diagnostic to switch per-layer (or per-block) between sign updates and full-precision AdamW updates, aiming to reduce communication and improve stability on \u201cspiky-gradient\u201d components (embeddings, attention projections).\n\n6. **Token-Budget-Aware Weight Decay: Decoupling Regularization from LR at Scale**\n   Create a principled schedule for decoupled weight decay that depends on tokens seen (not steps) and is invariant across batch size and learning-rate changes, leveraging AdamW\u2019s decoupling insight. Empirically test whether \u201cconstant effective prior\u201d weight decay improves transfer and prevents late-stage overfitting in compute-optimal LLM regimes compared to standard cosine-decay+fixed wd.\n\n7. **Averaged Stochastic Gradient Sampler with Adam-Style Preconditioning**\n   Extend the Averaged Stochastic Gradient Sampler concept to an Adam-preconditioned sampler that uses Polyak averaging plus adaptive second moments, aiming for scalable approximate posterior sampling in large models. Provide a clear recipe for choosing constant step size and damping to target a desired temperature, then evaluate on Bayesian model selection signals (marginal-likelihood proxies) and uncertainty benchmarks.\n\n8. **Hyperparameter-Free AdamW via Online Regret-Minimizing Tuning**\n   Use the adaptive online learning framework (best proximal function in hindsight) to create an \u201cauto-tuned AdamW\u201d where learning rate and \u03b2\u2082 are updated online to minimize a surrogate regret bound derived from observed gradients. Implement a lightweight controller that updates hyperparameters every N steps using only moment statistics, and test whether it matches tuned baselines across datasets and model sizes without sweeps.\n\n9. **Layerwise Priors for LLMs: Decoupled Weight Decay as Structured Bayesian Regularization**\n   Interpret decoupled weight decay as an explicit Gaussian prior and generalize it to layerwise (or parameter-group) priors whose strengths are learned during training via a variational-EM style procedure (building on SGD-as-inference). Concretely, learn separate decay coefficients for embeddings, attention, and MLP blocks by optimizing a tractable evidence lower bound approximation estimated from training dynamics.\n\n10. **Bandwidth-Compute Co-Optimal Training Laws: Joint Scaling of Tokens, Batch, and Gradient Compression**\n    Extend compute-optimal training analysis to include communication cost by adding a bandwidth term and allowing sign-based compression as a decision variable. Produce actionable scaling recommendations (tokens vs. model size vs. batch size vs. compression level) and validate by training matched-FLOP, matched-walltime runs that trade compute for communication using signSGD-style 1-bit updates.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Bayes-Calibrated AdamW via Stationary-Distribution Matching\nExtend \u201cSGD as approximate Bayesian inference\u201d to AdamW by explicitly modeling the optimizer-induced stationary distribution under decoupled",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Adaptive Proximal Geometry for Modern Deep Nets (Mirror-AdamW)\nCombine adaptive subgradient methods\u2019 data-dependent proximal functions with Adam-style moment estimation to create a mirror-descent vari",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "When Does SignSGD Beat Adam? A Geometry-Aware Switching Criterion\nOperationalize signSGD\u2019s theory about \u2113\u2081/\u2113\u2082 geometry by building an online diagnostic that estimates gradient sparsity, noise anisotro",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Token-Budget-Aware Weight Decay: Decoupling Regularization from LR at Scale\nCreate a principled schedule for decoupled weight decay that depends on tokens seen (not steps) and is invariant across batc",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Averaged Stochastic Gradient Sampler with Adam-Style Preconditioning\nExtend the Averaged Stochastic Gradient Sampler concept to an Adam-preconditioned sampler that uses Polyak averaging plus adaptive ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Hyperparameter-Free AdamW via Online Regret-Minimizing Tuning\nUse the adaptive online learning framework (best proximal function in hindsight) to create an \u201cauto-tuned AdamW\u201d where learning rate and \u03b2",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Layerwise Priors for LLMs: Decoupled Weight Decay as Structured Bayesian Regularization\nInterpret decoupled weight decay as an explicit Gaussian prior and generalize it to layerwise (or parameter-grou",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 14,
      "paper_title": "An Optimized Franz-Parisi Criterion and its Equivalence with SQ Lower Bounds",
      "contribution": "They refine the Franz\u2013Parisi (FP) geometric criterion to better capture overlap structure and prove that this optimized FP is equivalent to Statistical Query (SQ) lower bounds under a mild, verifiable assumption, thereby unifying physics-inspired geometry with SQ complexity for a broad class of statistical models.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 6,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 8331,
      "output_tokens": 1059,
      "predecessor_details": [
        {
          "success": true,
          "title": "The Franz-Parisi Criterion and Computational Trade-offs in ... - arXiv",
          "url": "https://arxiv.org/abs/2205.09727",
          "content": "# Mathematics > Statistics Theory\n\n**arXiv:2205.09727** (math)\n\n\\[Submitted on 19 May 2022 ( [v1](https://arxiv.org/abs/2205.09727v1)), last revised 13 Oct 2022 (this version, v2)\\]\n\n# Title:The Franz-Parisi Criterion and Computational Trade-offs in High Dimensional Statistics\n\nAuthors: [Afonso S. Bandeira](https://arxiv.org/search/math?searchtype=author&query=Bandeira,+A+S), [Ahmed El Alaoui](https://arxiv.org/search/math?searchtype=author&query=Alaoui,+A+E), [Samuel B. Hopkins](https://arxiv.org/search/math?searchtype=author&query=Hopkins,+S+B), [Tselil Schramm](https://arxiv.org/search/math?searchtype=author&query=Schramm,+T), [Alexander S. Wein](https://arxiv.org/search/math?searchtype=author&query=Wein,+A+S), [Ilias Zadik](https://arxiv.org/search/math?searchtype=author&query=Zadik,+I)\n\nView a PDF of the paper titled The Franz-Parisi Criterion and Computational Trade-offs in High Dimensional Statistics, by Afonso S. Bandeira and 5 other authors\n\n[View PDF](https://arxiv.org/pdf/2205.09727)\n\n> Abstract:Many high-dimensional statistical inference problems are believed to possess inherent computational hardness. Various frameworks have been proposed to give rigorous evidence for such hardness, including lower bounds against restricted models of computation (such as low-degree functions), as well as methods rooted in statistical physics that are based on free energy landscapes. This paper aims to make a rigorous connection between the seemingly different low-degree and free-energy based approaches. We define a free-energy based criterion for hardness and formally connect it to the well-established notion of low-degree hardness for a broad class of statistical problems, namely all Gaussian additive models and certain models with a sparse planted signal. By leveraging these rigorous connections we are able to: establish that for Gaussian additive models the \"algebraic\" notion of low-degree hardness implies failure of \"geometric\" local MCMC algorithms, and provide new low-degree lower bounds for sparse linear regression which seem difficult to prove directly. These results provide both conceptual insights into the connections between different notions of hardness, as well as concrete technical tools such as new methods for proving low-degree lower bounds.\n\n| | |\n| --- | --- |\n| Comments: | 52 pages, 1 figure |\n| Subjects: | Statistics Theory (math.ST); Statistical Mechanics (cond-mat.stat-mech); Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2205.09727](https://arxiv.org/abs/2205.09727) \\[math.ST\\] |\n| | (or [arXiv:2205.09727v2](https://arxiv.org/abs/2205.09727v2) \\[math.ST\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2205.09727](https://doi.org/10.48550/arXiv.2205.09727) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Alexander Wein \\[ [view email](https://arxiv.org/show-email/7ea7824d/2205.09727)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2205.09727v1)**\nThu, 19 May 2022 17:39:29 UTC (80 KB)\n\n**\\[v2\\]**\nThu, 13 Oct 2022 05:17:58 UTC (81 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled The Franz-Parisi Criterion and Computational Trade-offs in High Dimensional Statistics, by Afonso S. Bandeira and 5 other authors\n\n- [View PDF](https://arxiv.org/pdf/2205.09727)\n- [TeX Source](https://arxiv.org/src/2205.09727)\n- [Other Formats](https://arxiv.org/format/2205.09727)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nmath.ST\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2205.09727&function=prev&context=math.ST)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2205.09727&function=next&context=math.ST)\n\n[new](https://arxiv.org/list/math.ST/new) \\| [recent](https://arxiv.org/list/math.ST/recent) \\| [2022-05](https://arxiv.org/list/math.ST/2022-05)\n\nChange to browse by:\n\n[cond-mat](https://arxiv.org/abs/2205.09727?context=cond-mat)\n\n[cond-mat.stat-mech](https://arxiv.org/abs/2205.09727?context=cond-mat.stat-mech)\n\n[cs](https://arxiv.org/abs/2205.09727?context=cs)\n\n[cs.CC](https://arxiv.org/abs/2205.09727?context=cs.CC)\n\n[cs.DS](https://arxiv.org/abs/2205.09727?context=cs.DS)\n\n[math](https://arxiv.org/abs/2205.09727?context=math)\n\n[stat](https://arxiv.org/abs/2205.09727?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2205.09727?context=stat.ML)\n\n[stat.TH](https://arxiv.org/abs/2205.09727?context=stat.TH)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2205.09727)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2205.09727)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2205.09727)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2205.09727&description=The Franz-Parisi Criterion and Computational Trade-offs in High Dimensional Statistics) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2205.09727&title=The Franz-Parisi Criterion and Computational Trade-offs in High Dimensional Statistics)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2205.09727) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "The franz-parisi criterion and computational trade-offs in high dimensional statistics",
          "cleaned_query": "The franz-parisi criterion and computational trade-offs in high dimensional statistics"
        },
        {
          "success": true,
          "title": "Recipes for metastable states in Spin Glasses - cond-mat - arXiv",
          "url": "https://arxiv.org/abs/cond-mat/9503167",
          "content": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
          "original_query": "Recipes for metastable states in spin glasses",
          "cleaned_query": "Recipes for metastable states in spin glasses"
        },
        {
          "success": true,
          "title": "Predictions using the Low-Degree Likelihood Ratio - math",
          "url": "https://arxiv.org/abs/1907.11636",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: These notes survey and explore an emerging method, which we call the\nlow-degree method, for predicting and understanding\nstatistical-versus-computational tradeoffs in high-dimensional inference\nproblems. In short, the method posits that a certain quantity -- the second\nmoment of the low-degree likelihood ratio -- gives insight into how much\ncomputational time is required to solve a given hypothesis testing problem,\nwhich can in turn be used to predict the computational hardness of a variety of\nstatistical inference tasks. While this method originated in the study of the\nsum-of-squares (SoS) hierarchy of convex programs, we present a self-contained\nintroduction that does not require knowledge of SoS. In addition to showing how\nto carry out predictions using the method, we include a discussion\ninvestigating both rigorous and conjectural consequences of these predictions.\n These notes include some new results, simplified proofs, and refined\nconjectures. For instance, we point out a formal connection between spectral\nmethods and the low-degree likelihood ratio, and we give a sharp low-degree\nlower bound against subexponential-time algorithms for tensor PCA.\n \n \n \n \n Submission history From: Alexander Wein [ view email]\n [v1] \nFri, 26 Jul 2019 15:46:05 UTC (45 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > math > arXiv:1907.11636\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Mathematics > Statistics Theory\n\n arXiv:1907.11636 (math)\n [Submitted on 26 Jul 2019]\n\n Title: Notes on Computational Hardness of Hypothesis Testing: Predictions using the Low-Degree Likelihood Ratio\n\n Authors: Dmitriy Kunisky, Alexander S. Wein, Afonso S. Bandeira\n Download PDF\n Abstract: These notes survey and explore an emerging method, which we call the low-degree method, for predicting and understanding statistical-versus-computational tradeoffs in high-dimensional inference problems. In short, the method posits that a certain quantity -- the second moment of the low-degree likelihood ratio -- gives insight into how much computational time is required to solve a given hypothesis testing problem, which can in turn be used to predict the computational hardness of a variety of statistical inference tasks. While this method originated in the study of the sum-of-squares (SoS) hierarchy of convex programs, we present a self-contained introduction that does not require knowledge of SoS. In addition to showing how to carry out predictions using the method, we include a discussion investigating both rigorous and conjectural consequences of these predictions.\n These notes include some new results, simplified proofs, and refined conjectures. For instance, we point out a formal connection between spectral methods and the low-degree likelihood ratio, and we give a sharp low-degree lower bound against subexponential-time algorithms for tensor PCA.\n Comments: 44 pages \n Subjects: Statistics Theory (math.ST) ; Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)\n Cite as: arXiv:1907.11636 [math.ST] \n (or arXiv:1907.11636v1 [math.ST] for this version) \n https://doi.org/10.48550/arXiv.1907.11636 \n Focus to learn more \n arXiv-issued DOI via DataCite \n \n\n Submission history\n\n From: Alexander Wein [view email]\n [v1] Fri, 26 Jul 2019 15:46:05 UTC (45 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * PostScript\n * Other formats\n (license)\n Current browse context:\n math.ST\n < prev | next >\n new | recent | 1907\n Change to browse by:\n cs\n cs.CC\n cs.DS\n math\n stat\n stat.ML\n stat.TH\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs and how to get involved.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Notes on computational hardness of hypothesis testing: Predictions using the low-degree likelihood ratio",
          "cleaned_query": "Notes on computational hardness of hypothesis testing: Predictions using the low-degree likelihood ratio"
        },
        {
          "success": true,
          "title": "Statistical Algorithms and a Lower Bound for Detecting Planted Clique",
          "url": "https://arxiv.org/abs/1201.1214",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: We introduce a framework for proving lower bounds on computational problems\nover distributions against algorithms that can be implemented using access to a\nstatistical query oracle. For such algorithms, access to the input distribution\nis limited to obtaining an estimate of the expectation of any given function on\na sample drawn randomly from the input distribution, rather than directly\naccessing samples. Most natural algorithms of interest in theory and in\npractice, e.g., moments-based methods, local search, standard iterative methods\nfor convex optimization, MCMC and simulated annealing can be implemented in\nthis framework. Our framework is based on, and generalizes, the statistical\nquery model in learning theory (Kearns, 1998).\n Our main application is a nearly optimal lower bound on the complexity of any\nstatistical query algorithm for detecting planted bipartite clique\ndistributions (or planted dense subgraph distributions) when the planted clique\nhas size $O(n^{1/2-\\delta})$ for any constant $\\delta &gt; 0$. The assumed\nhardness of variants of these problems has been used to prove hardness of\nseveral other problems and as a guarantee for security in cryptographic\napplications. Our lower bounds provide concrete evidence of hardness, thus\nsupporting these assumptions.\n \n \n \n \n Submission history From: Vitaly Feldman [ view email]\n \n [v1] \n Thu, 5 Jan 2012 16:39:21 UTC (23 KB) \n [v2] \n Wed, 9 May 2012 19:34:30 UTC (35 KB) \n [v3] \n Fri, 22 Mar 2013 03:54:58 UTC (53 KB) \n [v4] \n Wed, 3 Apr 2013 15:08:58 UTC (53 KB) \n [v5] \n Mon, 8 Jun 2015 17:38:56 UTC (47 KB) [v6] \nMon, 15 Aug 2016 01:17:38 UTC (48 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > cs > arXiv:1201.1214\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Computational Complexity\n\n arXiv:1201.1214 (cs)\n [Submitted on 5 Jan 2012 (v1), last revised 15 Aug 2016 (this version, v6)]\n\n Title: Statistical Algorithms and a Lower Bound for Detecting Planted Clique\n\n Authors: Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh Vempala, Ying Xiao\n Download PDF\n Abstract: We introduce a framework for proving lower bounds on computational problems over distributions against algorithms that can be implemented using access to a statistical query oracle. For such algorithms, access to the input distribution is limited to obtaining an estimate of the expectation of any given function on a sample drawn randomly from the input distribution, rather than directly accessing samples. Most natural algorithms of interest in theory and in practice, e.g., moments-based methods, local search, standard iterative methods for convex optimization, MCMC and simulated annealing can be implemented in this framework. Our framework is based on, and generalizes, the statistical query model in learning theory (Kearns, 1998).\n Our main application is a nearly optimal lower bound on the complexity of any statistical query algorithm for detecting planted bipartite clique distributions (or planted dense subgraph distributions) when the planted clique has size $O(n^{1/2-\\delta})$ for any constant $\\delta > 0$. The assumed hardness of variants of these problems has been used to prove hardness of several other problems and as a guarantee for security in cryptographic applications. Our lower bounds provide concrete evidence of hardness, thus supporting these assumptions.\n Subjects: Computational Complexity (cs.CC) ; Data Structures and Algorithms (cs.DS)\n ACM classes: F.2; G.1.6; G.3 \n Cite as: arXiv:1201.1214 [cs.CC] \n (or arXiv:1201.1214v6 [cs.CC] for this version) \n https://doi.org/10.48550/arXiv.1201.1214 \n Focus to learn more \n arXiv-issued DOI via DataCite \n \n\n Submission history\n\n From: Vitaly Feldman [view email]\n [v1] Thu, 5 Jan 2012 16:39:21 UTC (23 KB)\n [v2] Wed, 9 May 2012 19:34:30 UTC (35 KB)\n [v3] Fri, 22 Mar 2013 03:54:58 UTC (53 KB)\n [v4] Wed, 3 Apr 2013 15:08:58 UTC (53 KB)\n [v5] Mon, 8 Jun 2015 17:38:56 UTC (47 KB)\n [v6] Mon, 15 Aug 2016 01:17:38 UTC (48 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * PostScript\n * Other formats\n (license)\n Current browse context:\n cs.CC\n < prev | next >\n new | recent | 1201\n Change to browse by:\n cs\n cs.DS\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n\n DBLP - CS Bibliography\n\n listing | bibtex\n Vitaly Feldman\n Elena Grigorescu\n Lev Reyzin\n Santosh Vempala\n Santosh S. Vempala\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Statistical algorithms and a lower bound for detecting planted cliques",
          "cleaned_query": "Statistical algorithms and a lower bound for detecting planted cliques"
        },
        {
          "success": true,
          "title": "Statistical Query Algorithms and Low-Degree Tests Are ... - arXiv",
          "url": "https://arxiv.org/abs/2009.06107",
          "content": "# Computer Science > Computational Complexity\n\n**arXiv:2009.06107** (cs)\n\n\\[Submitted on 13 Sep 2020 ( [v1](https://arxiv.org/abs/2009.06107v1)), last revised 26 Jun 2021 (this version, v3)\\]\n\n# Title:Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent\n\nAuthors: [Matthew Brennan](https://arxiv.org/search/cs?searchtype=author&query=Brennan,+M), [Guy Bresler](https://arxiv.org/search/cs?searchtype=author&query=Bresler,+G), [Samuel B. Hopkins](https://arxiv.org/search/cs?searchtype=author&query=Hopkins,+S+B), [Jerry Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+J), [Tselil Schramm](https://arxiv.org/search/cs?searchtype=author&query=Schramm,+T)\n\nView a PDF of the paper titled Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent, by Matthew Brennan and Guy Bresler and Samuel B. Hopkins and Jerry Li and Tselil Schramm\n\n[View PDF](https://arxiv.org/pdf/2009.06107)\n\n> Abstract:Researchers currently use a number of approaches to predict and substantiate information-computation gaps in high-dimensional statistical estimation problems. A prominent approach is to characterize the limits of restricted models of computation, which on the one hand yields strong computational lower bounds for powerful classes of algorithms and on the other hand helps guide the development of efficient algorithms. In this paper, we study two of the most popular restricted computational models, the statistical query framework and low-degree polynomials, in the context of high-dimensional hypothesis testing. Our main result is that under mild conditions on the testing problem, the two classes of algorithms are essentially equivalent in power. As corollaries, we obtain new statistical query lower bounds for sparse PCA, tensor PCA and several variants of the planted clique problem.\n\n| | |\n| --- | --- |\n| Comments: | Version 3 fixes typos and adds note on presentation at COLT 2021 |\n| Subjects: | Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2009.06107](https://arxiv.org/abs/2009.06107) \\[cs.CC\\] |\n| | (or [arXiv:2009.06107v3](https://arxiv.org/abs/2009.06107v3) \\[cs.CC\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2009.06107](https://doi.org/10.48550/arXiv.2009.06107) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Samuel Hopkins \\[ [view email](https://arxiv.org/show-email/d6d24264/2009.06107)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2009.06107v1)**\nSun, 13 Sep 2020 22:55:18 UTC (130 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2009.06107v2)**\nWed, 11 Nov 2020 05:28:15 UTC (130 KB)\n\n**\\[v3\\]**\nSat, 26 Jun 2021 17:06:23 UTC (125 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent, by Matthew Brennan and Guy Bresler and Samuel B. Hopkins and Jerry Li and Tselil Schramm\n\n- [View PDF](https://arxiv.org/pdf/2009.06107)\n- [TeX Source](https://arxiv.org/src/2009.06107)\n- [Other Formats](https://arxiv.org/format/2009.06107)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CC\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2009.06107&function=prev&context=cs.CC)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2009.06107&function=next&context=cs.CC)\n\n[new](https://arxiv.org/list/cs.CC/new) \\| [recent](https://arxiv.org/list/cs.CC/recent) \\| [2020-09](https://arxiv.org/list/cs.CC/2020-09)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2009.06107?context=cs)\n\n[cs.DS](https://arxiv.org/abs/2009.06107?context=cs.DS)\n\n[cs.LG](https://arxiv.org/abs/2009.06107?context=cs.LG)\n\n[math](https://arxiv.org/abs/2009.06107?context=math)\n\n[math.ST](https://arxiv.org/abs/2009.06107?context=math.ST)\n\n[stat](https://arxiv.org/abs/2009.06107?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2009.06107?context=stat.ML)\n\n[stat.TH](https://arxiv.org/abs/2009.06107?context=stat.TH)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2009.06107)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2009.06107)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2009.06107)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2009.html#abs-2009-06107) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2009-06107)\n\n[Matthew Brennan](https://dblp.uni-trier.de/search/author?author=Matthew%20Brennan)\n\n[Guy Bresler](https://dblp.uni-trier.de/search/author?author=Guy%20Bresler)\n\n[Samuel B. Hopkins](https://dblp.uni-trier.de/search/author?author=Samuel%20B.%20Hopkins)\n\n[Jerry Li](https://dblp.uni-trier.de/search/author?author=Jerry%20Li)\n\n[Tselil Schramm](https://dblp.uni-trier.de/search/author?author=Tselil%20Schramm)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2009.06107&description=Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2009.06107&title=Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endo",
          "original_query": "Statistical query algorithms and low degree tests are almost equivalent",
          "cleaned_query": "Statistical query algorithms and low degree tests are almost equivalent"
        },
        {
          "success": true,
          "title": "A simple proof of the Gaussian correlation conjecture extended to ...",
          "url": "https://arxiv.org/abs/1408.1028",
          "content": "\n \n \n \n \n We gratefully acknowledge support from \nthe Simons Foundation and member institutions. \n \n \n \n \n \n \n",
          "original_query": "A simple proof of the gaussian correlation conjecture extended to multivariate gamma distributions",
          "cleaned_query": "A simple proof of the gaussian correlation conjecture extended to multivariate gamma distributions"
        }
      ],
      "generated_ideas": [
        "Franz\u2013Parisi Criterion for Non-Gaussian Additive Models (Sub-Gaussian & Heavy-Tailed)\nExtend the rigorous Franz\u2013Parisi \u2194 low-degree connection beyond Gaussian additive noise by developing an invariance/robustification theory that controls the free-energy landscape under sub-Gaussian or polynomial-tail noise. Prove when low-degree hardness still implies failure of local MCMC (e.g., Langevin/Metropolis) and identify tail regimes where the implication breaks.",
        "An \u201cFP-Second-Moment\u201d Bridge: Matching the Franz\u2013Parisi Potential to Low-Degree Likelihood Ratio Moments\nDefine an explicit mapping between the Franz\u2013Parisi potential and the truncated (degree\u2011d) likelihood ratio second moment used in the low-degree method. Show equivalence (or tight inequalities) under verifiable conditions, yielding a practical recipe: compute/approximate one object to certify the other, and apply it to models where FP calculations are simpler than low-degree expansions (or vice versa).",
        "Metastable-State Geometry as a Predictor of SQ/Low-Degree Barriers\nUse \u201crecipes for metastable states\u201d from spin glass theory to construct candidate metastable basins for planted inference models and compute their overlap structure/complexity. Translate these geometric quantities into lower bounds in the statistical query (SQ) framework using the Brennan\u2013Bresler\u2013Hopkins\u2013Li\u2013Schramm near-equivalence, producing a new, geometry-driven pipeline for SQ hardness.",
        "Degree\u2013Temperature Trade-offs: Simulated Annealing Schedules Guided by FP Barriers\nUse FP barrier heights/shapes to design annealing schedules (temperature as a function of time) that are provably optimal within a class of local MCMC/SA algorithms for Gaussian additive inference problems. Establish lower bounds showing any schedule must take exponential (or subexponential) time in regimes predicted hard by low-degree, and upper bounds demonstrating matching schedules in easier regimes.",
        "Adaptive SQ Lower Bounds from Metastable Overlap Concentration\nCreate new SQ lower bound techniques that leverage metastability: show that if the posterior mass concentrates in exponentially many low-overlap basins (a spin-glass \u201ccomplexity\u201d condition), then any adaptive SQ algorithm requires super-polynomial queries to distinguish null vs planted. This would turn qualitative metastable-state predictions into quantitative, oracle-model complexity bounds for broad planted problems.",
        "A Unified \u201cLandscape-to-Algorithm\u201d Benchmark Suite with Certified Hard/Easy Regimes\nBuild an open benchmark suite of planted inference instances (Gaussian additive + sparse planted + tensor/spiked variants) where each instance comes with (i) computed low-degree LR second moments, (ii) computed/approximated FP potentials, and (iii) predicted SQ complexity. Validate the predictions by systematically testing spectral methods, SQ-implementable algorithms, and local MCMC, creating a reproducible platform to discover new algorithmic transitions and to stress-test conjectured hardness."
      ],
      "generated_ideas_raw": "1. **Franz\u2013Parisi Criterion for Non-Gaussian Additive Models (Sub-Gaussian & Heavy-Tailed)**\n   Extend the rigorous Franz\u2013Parisi \u2194 low-degree connection beyond Gaussian additive noise by developing an invariance/robustification theory that controls the free-energy landscape under sub-Gaussian or polynomial-tail noise. Prove when low-degree hardness still implies failure of local MCMC (e.g., Langevin/Metropolis) and identify tail regimes where the implication breaks.\n\n2. **An \u201cFP-Second-Moment\u201d Bridge: Matching the Franz\u2013Parisi Potential to Low-Degree Likelihood Ratio Moments**\n   Define an explicit mapping between the Franz\u2013Parisi potential and the truncated (degree\u2011d) likelihood ratio second moment used in the low-degree method. Show equivalence (or tight inequalities) under verifiable conditions, yielding a practical recipe: compute/approximate one object to certify the other, and apply it to models where FP calculations are simpler than low-degree expansions (or vice versa).\n\n3. **Metastable-State Geometry as a Predictor of SQ/Low-Degree Barriers**\n   Use \u201crecipes for metastable states\u201d from spin glass theory to construct candidate metastable basins for planted inference models and compute their overlap structure/complexity. Translate these geometric quantities into lower bounds in the statistical query (SQ) framework using the Brennan\u2013Bresler\u2013Hopkins\u2013Li\u2013Schramm near-equivalence, producing a new, geometry-driven pipeline for SQ hardness.\n\n4. **Local MCMC Lower Bounds for Planted Dense Subgraph via Low-Degree \u2194 FP**\n   Starting from planted dense subgraph/planted clique distributions studied in SQ lower bounds, build an FP-style landscape analysis tailored to sparse graph models (e.g., via Gaussian proxies or interpolation). Prove that in the conjectured hard regime (e.g., clique size \\(n^{1/2-\\delta}\\)), the resulting FP barrier implies provable slowdown/failure for local MCMC or simulated annealing on natural energy functions.\n\n5. **Hardness of Sparse Linear Regression Under Correlated Designs Using FP and Low-Degree**\n   Generalize the sparse regression low-degree lower bounds (Paper 1) from i.i.d. Gaussian designs to correlated Gaussian designs (e.g., Toeplitz, spiked covariance, or general covariance with bounded condition number). Develop FP criteria and low-degree calculations that capture how correlation reshapes computational thresholds, yielding explicit \u201chardness phase diagrams\u201d as a function of sparsity, SNR, and covariance spectrum.\n\n6. **Spectral\u2013Low-Degree Correspondence for Designing Optimal Polytime Tests at the Edge**\n   Build on the formal connection between spectral methods and low-degree likelihood ratios (Paper 3) to systematically derive *which* spectral operator (matrix/tensor unfolding, non-backtracking, Bethe Hessian-like) matches the optimal degree\u2011d statistic for a given model. Deliver an actionable algorithm-design procedure that outputs a concrete spectral test and prove it achieves the best possible performance among all degree\u2011d/SQ algorithms.\n\n7. **Quantifying the Gap Between SQ and Low-Degree Beyond \u201cMild Conditions\u201d**\n   Identify explicit testing problems that violate the conditions of \u201cSQ and low-degree are almost equivalent\u201d (Paper 5), then characterize precisely the separation (or prove none exists) via tailored constructions. The contribution would be a taxonomy: which structural properties (e.g., discontinuous likelihoods, rare-event signals, non-product priors) cause SQ to be strictly weaker/stronger than low-degree.\n\n8. **Degree\u2013Temperature Trade-offs: Simulated Annealing Schedules Guided by FP Barriers**\n   Use FP barrier heights/shapes to design annealing schedules (temperature as a function of time) that are provably optimal within a class of local MCMC/SA algorithms for Gaussian additive inference problems. Establish lower bounds showing any schedule must take exponential (or subexponential) time in regimes predicted hard by low-degree, and upper bounds demonstrating matching schedules in easier regimes.\n\n9. **Adaptive SQ Lower Bounds from Metastable Overlap Concentration**\n   Create new SQ lower bound techniques that leverage metastability: show that if the posterior mass concentrates in exponentially many low-overlap basins (a spin-glass \u201ccomplexity\u201d condition), then any adaptive SQ algorithm requires super-polynomial queries to distinguish null vs planted. This would turn qualitative metastable-state predictions into quantitative, oracle-model complexity bounds for broad planted problems.\n\n10. **A Unified \u201cLandscape-to-Algorithm\u201d Benchmark Suite with Certified Hard/Easy Regimes**\n   Build an open benchmark suite of planted inference instances (Gaussian additive + sparse planted + tensor/spiked variants) where each instance comes with (i) computed low-degree LR second moments, (ii) computed/approximated FP potentials, and (iii) predicted SQ complexity. Validate the predictions by systematically testing spectral methods, SQ-implementable algorithms, and local MCMC, creating a reproducible platform to discover new algorithmic transitions and to stress-test conjectured hardness.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Franz\u2013Parisi Criterion for Non-Gaussian Additive Models (Sub-Gaussian & Heavy-Tailed)\nExtend the rigorous Franz\u2013Parisi \u2194 low-degree connection beyond Gaussian additive noise by developing an invarianc",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "An \u201cFP-Second-Moment\u201d Bridge: Matching the Franz\u2013Parisi Potential to Low-Degree Likelihood Ratio Moments\nDefine an explicit mapping between the Franz\u2013Parisi potential and the truncated (degree\u2011d) like",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Metastable-State Geometry as a Predictor of SQ/Low-Degree Barriers\nUse \u201crecipes for metastable states\u201d from spin glass theory to construct candidate metastable basins for planted inference models and ",
          "is_match": true
        },
        {
          "idea_idx": 3,
          "idea_text": "Degree\u2013Temperature Trade-offs: Simulated Annealing Schedules Guided by FP Barriers\nUse FP barrier heights/shapes to design annealing schedules (temperature as a function of time) that are provably opt",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Adaptive SQ Lower Bounds from Metastable Overlap Concentration\nCreate new SQ lower bound techniques that leverage metastability: show that if the posterior mass concentrates in exponentially many low-",
          "is_match": true
        },
        {
          "idea_idx": 5,
          "idea_text": "A Unified \u201cLandscape-to-Algorithm\u201d Benchmark Suite with Certified Hard/Easy Regimes\nBuild an open benchmark suite of planted inference instances (Gaussian additive + sparse planted + tensor/spiked var",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 15,
      "paper_title": "MaxSup: Overcoming Representation Collapse in Label Smoothing",
      "contribution": "A theoretical decomposition of label smoothing that exposes an error-amplification term, and a simple logit-level regularizer (Max Suppression) that penalizes the top-1 logit to retain LS\u2019s benefits while avoiding overconfident misclassifications and representation collapse.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 4,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 9601,
      "output_tokens": 979,
      "predecessor_details": [
        {
          "success": true,
          "title": "Rethinking the Inception Architecture for Computer Vision - arXiv",
          "url": "https://arxiv.org/abs/1512.00567",
          "content": "[1512.00567] Rethinking the Inception Architecture for Computer Vision\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1512.00567\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:1512.00567**(cs)\n[Submitted on 2 Dec 2015 ([v1](https://arxiv.org/abs/1512.00567v1)), last revised 11 Dec 2015 (this version, v3)]\n# Title:Rethinking the Inception Architecture for Computer Vision\nAuthors:[Christian Szegedy](https://arxiv.org/search/cs?searchtype=author&amp;query=Szegedy,+C),[Vincent Vanhoucke](https://arxiv.org/search/cs?searchtype=author&amp;query=Vanhoucke,+V),[Sergey Ioffe](https://arxiv.org/search/cs?searchtype=author&amp;query=Ioffe,+S),[Jonathon Shlens](https://arxiv.org/search/cs?searchtype=author&amp;query=Shlens,+J),[Zbigniew Wojna](https://arxiv.org/search/cs?searchtype=author&amp;query=Wojna,+Z)\nView a PDF of the paper titled Rethinking the Inception Architecture for Computer Vision, by Christian Szegedy and 4 other authors\n[View PDF](https://arxiv.org/pdf/1512.00567)> > Abstract:\n> Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set. Subjects:|Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:1512.00567](https://arxiv.org/abs/1512.00567)[cs.CV]|\n|(or[arXiv:1512.00567v3](https://arxiv.org/abs/1512.00567v3)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.1512.00567](https://doi.org/10.48550/arXiv.1512.00567)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Christian Szegedy [[view email](https://arxiv.org/show-email/6da3ef78/1512.00567)]\n**[[v1]](https://arxiv.org/abs/1512.00567v1)**Wed, 2 Dec 2015 03:44:38 UTC (219 KB)\n**[[v2]](https://arxiv.org/abs/1512.00567v2)**Thu, 10 Dec 2015 19:34:38 UTC (220 KB)\n**[v3]**Fri, 11 Dec 2015 20:27:50 UTC (228 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Rethinking the Inception Architecture for Computer Vision, by Christian Szegedy and 4 other authors\n* [View PDF](https://arxiv.org/pdf/1512.00567)\n* [TeX Source](https://arxiv.org/src/1512.00567)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1512.00567&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1512.00567&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2015-12](https://arxiv.org/list/cs.CV/2015-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/1512.00567?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1512.00567)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1512.00567)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1512.00567)\n### [26 blog links](https://arxiv.org/tb/1512.00567)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1512.html#SzegedyVISW15)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/SzegedyVISW15)\n[Christian Szegedy]()\n[Vincent Vanhoucke]()\n[Sergey Ioffe]()\n[Jonathon Shlens]()\n[Zbigniew Wojna]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](http",
          "original_query": "Rethinking the Inception Architecture for Computer Vision",
          "cleaned_query": "Rethinking the Inception Architecture for Computer Vision"
        },
        {
          "success": true,
          "title": "[1906.02629] When Does Label Smoothing Help? - arXiv",
          "url": "https://arxiv.org/abs/1906.02629",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "When does label smoothing help?",
          "cleaned_query": "When does label smoothing help?"
        },
        {
          "success": true,
          "title": "Why Do Better Loss Functions Lead to Less Transferable Features?",
          "url": "https://arxiv.org/abs/2010.16402",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2010.16402** (cs)\n\n\\[Submitted on 30 Oct 2020 ( [v1](https://arxiv.org/abs/2010.16402v1)), last revised 3 Nov 2021 (this version, v2)\\]\n\n# Title:Why Do Better Loss Functions Lead to Less Transferable Features?\n\nAuthors: [Simon Kornblith](https://arxiv.org/search/cs?searchtype=author&query=Kornblith,+S), [Ting Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+T), [Honglak Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee,+H), [Mohammad Norouzi](https://arxiv.org/search/cs?searchtype=author&query=Norouzi,+M)\n\nView a PDF of the paper titled Why Do Better Loss Functions Lead to Less Transferable Features?, by Simon Kornblith and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2010.16402)\n\n> Abstract:Previous work has proposed many new loss functions and regularizers that improve test accuracy on image classification tasks. However, it is not clear whether these loss functions learn better representations for downstream tasks. This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. We show that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross-entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks, and the choice of loss has little effect when networks are fully fine-tuned on the new tasks. Using centered kernel alignment to measure similarity between hidden representations of networks, we find that differences among loss functions are apparent only in the last few layers of the network. We delve deeper into representations of the penultimate layer, finding that different objectives and hyperparameter combinations lead to dramatically different levels of class separation. Representations with higher class separation obtain higher accuracy on the original task, but their features are less useful for downstream tasks. Our results suggest there exists a trade-off between learning invariant features for the original task and features relevant for transfer tasks.\n\n| | |\n| --- | --- |\n| Comments: | NeurIPS 2021 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2010.16402](https://arxiv.org/abs/2010.16402) \\[cs.CV\\] |\n| | (or [arXiv:2010.16402v2](https://arxiv.org/abs/2010.16402v2) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2010.16402](https://doi.org/10.48550/arXiv.2010.16402) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Simon Kornblith \\[ [view email](https://arxiv.org/show-email/743c4656/2010.16402)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2010.16402v1)**\nFri, 30 Oct 2020 17:50:31 UTC (304 KB)\n\n**\\[v2\\]**\nWed, 3 Nov 2021 18:32:53 UTC (652 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Why Do Better Loss Functions Lead to Less Transferable Features?, by Simon Kornblith and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2010.16402)\n- [TeX Source](https://arxiv.org/src/2010.16402)\n- [Other Formats](https://arxiv.org/format/2010.16402)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2010.16402&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2010.16402&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2020-10](https://arxiv.org/list/cs.CV/2020-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2010.16402?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2010.16402?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2010.16402)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2010.16402)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2010.16402)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2010.html#abs-2010-16402) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2010-16402)\n\n[Simon Kornblith](https://dblp.uni-trier.de/search/author?author=Simon%20Kornblith)\n\n[Honglak Lee](https://dblp.uni-trier.de/search/author?author=Honglak%20Lee)\n\n[Ting Chen](https://dblp.uni-trier.de/search/author?author=Ting%20Chen)\n\n[Mohammad Norouzi](https://dblp.uni-trier.de/search/author?author=Mohammad%20Norouzi)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2010.16402&description=Why Do Better Loss Functions Lead to Less Transferable Features?) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2010.16402&title=Why Do Better Loss Functions Lead to Less Transferable Features?)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2010.16402) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Why do better loss functions lead to less transferable features?",
          "cleaned_query": "Why do better loss functions lead to less transferable features?"
        },
        {
          "success": true,
          "title": "[2303.02970] Rethinking Confidence Calibration for Failure Prediction",
          "url": "https://arxiv.org/abs/2303.02970",
          "content": "[2303.02970] Rethinking Confidence Calibration for Failure Prediction\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2303.02970\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2303.02970**(cs)\n[Submitted on 6 Mar 2023]\n# Title:Rethinking Confidence Calibration for Failure Prediction\nAuthors:[Fei Zhu](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+F),[Zhen Cheng](https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng,+Z),[Xu-Yao Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X),[Cheng-Lin Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+C)\nView a PDF of the paper titled Rethinking Confidence Calibration for Failure Prediction, by Fei Zhu and 3 other authors\n[View PDF](https://arxiv.org/pdf/2303.02970)> > Abstract:\n> Reliable confidence estimation for the predictions is important in many safety-critical applications. However, modern deep neural networks are often overconfident for their incorrect predictions. Recently, many calibration methods have been proposed to alleviate the overconfidence problem. With calibrated confidence, a primary and practical purpose is to detect misclassification errors by filtering out low-confidence predictions (known as failure prediction). In this paper, we find a general, widely-existed but actually-neglected phenomenon that most confidence calibration methods are useless or harmful for failure prediction. We investigate this problem and reveal that popular confidence calibration methods often lead to worse confidence separation between correct and incorrect samples, making it more difficult to decide whether to trust a prediction or not. Finally, inspired by the natural connection between flat minima and confidence separation, we propose a simple hypothesis: flat minima is beneficial for failure prediction. We verify this hypothesis via extensive experiments and further boost the performance by combining two different flat minima techniques. Our code is available at [> this https URL\n](https://github.com/Impression2805/FMFP)> Comments:|Accepted to ECCV 2022. Code is available at[this https URL](https://github.com/Impression2805/FMFP)|\nSubjects:|Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:2303.02970](https://arxiv.org/abs/2303.02970)[cs.LG]|\n|(or[arXiv:2303.02970v1](https://arxiv.org/abs/2303.02970v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2303.02970](https://doi.org/10.48550/arXiv.2303.02970)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Fei Zhu [[view email](https://arxiv.org/show-email/e470dfd2/2303.02970)]\n**[v1]**Mon, 6 Mar 2023 08:54:18 UTC (9,920 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Rethinking Confidence Calibration for Failure Prediction, by Fei Zhu and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2303.02970)\n* [TeX Source](https://arxiv.org/src/2303.02970)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2303.02970&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2303.02970&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2023-03](https://arxiv.org/list/cs.LG/2023-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2303.02970?context=cs)\n[cs.CV](https://arxiv.org/abs/2303.02970?context=cs.CV)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2303.02970)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2303.02970)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2303.02970)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2303.02970)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Rethinking confidence calibration for failure prediction",
          "cleaned_query": "Rethinking confidence calibration for failure prediction"
        },
        {
          "success": true,
          "title": "Regularizing Neural Networks by Penalizing Confident Output...",
          "url": "https://openreview.net/forum?id=HyhbYrGYe",
          "content": "[![back arrow](https://openreview.net/images/arrow_left.svg)Go to **ICLR 2017 conference** homepage](https://openreview.net/group?id=ICLR.cc/2017/conference)\n\n\u00d7\n\n## Regularizing Neural Networks by Penalizing Confident Output Distributions [![](https://openreview.net/images/pdf_icon_blue.svg)](https://openreview.net/pdf?id=HyhbYrGYe)\n\n## submission by Gabriel Pereyra \u2022 Regularizing Neural Networks by Penalizing Confident Output Distributions\n\n[Gabriel Pereyra](https://openreview.net/profile?email=pereyra%40google.com), [George Tucker](https://openreview.net/profile?email=gjt%40google.com), [Jan Chorowski](https://openreview.net/profile?email=chorowski%40google.com), [Lukasz Kaiser](https://openreview.net/profile?email=lukaszkaiser%40google.com), [Geoffrey Hinton](https://openreview.net/profile?email=geoffhinton%40google.com)\n\n15 Feb 2017 (modified: 21 Oct 2023)Submitted to ICLR 2017Readers: Everyone[Show Bibtex](https://openreview.net/forum?id=HyhbYrGYe) [Show Revisions](https://openreview.net/revisions?id=HyhbYrGYe)\n\nTL;DR: We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.\n\nAbstract: We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n\nConflicts: google.com\n\nCommunity Implementations: [![CatalyzeX](https://openreview.net/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/arxiv:1701.06548/code)\n\nRevealed to Everyone\n\n* * *\n\n04 Nov 2016 (modified: 21 Oct 2023)Submitted to ICLR 2017\n\nAbstract: We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n\nKeywords: Deep learning, Supervised Learning, Speech, Structured prediction\n\n* * *\n\nReply Type:\n\nall\n\n- Select All\n- official review\n- public comment\n- acceptance\n\nAuthor:\n\neverybody\n\n- Select All\n- AnonReviewer1\n- AnonReviewer2\n- pcs\n- George Tucker\n\nVisible To:\n\nall readers\n\n- Select All\n- everyone\n\nHidden From:\n\nnobody\n\n- Select All\n- everyone\n\n4 Replies\n\n[\\[\u2013\\]](https://openreview.net/forum?id=HyhbYrGYe) [\\[+\\]](https://openreview.net/forum?id=HyhbYrGYe)\n\n## ICLR committee final decision\n\n## acceptance by pcs \u2022 ICLR committee final decision\n\nICLR 2017 pcs\n\n20 Mar 2017, 09:49ICLR 2017 workshop acceptanceReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=B1lYzOFpse)\n\nDecision: Accept\n\n[\\[\u2013\\]](https://openreview.net/forum?id=HyhbYrGYe) [\\[+\\]](https://openreview.net/forum?id=HyhbYrGYe)\n\n## Thorough evaluation of smoothing techniques, interesting introduction of confidence penalization\n\n## official review by AnonReviewer2 \u2022 Thorough evaluation of smoothing techniques, interesting introduction of confidence penalization\n\nICLR 2017 workshop AnonReviewer2\n\n10 Mar 2017, 15:29ICLR 2017 workshop official reviewReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=Hykbw3gjx)\n\nReview: The paper proposes using confidence as a term for regularization in neural networks, helping to prevent overfitting by penalizing overly confident predictions. The experiments range across a number of fields and architectures, helping to show both the generality of the technique and where it appears to be most helpful.\nThe work and experiments are rather detailed and exhaustive, especially when delving in to the Appendix for specific details of the various experiments. The confidence penalty regularization is compared to and combined with dropout and label smoothing. I do agree with another reviewer that some of the baseline systems are weaker than others. Seeing an LSTM used without recurrent dropout (variational dropout, zoneout, ...) as a baseline for language modeling is unfortunate for example. Even with that acknowledged, the results and analysis over a variety of datasets is enough to convince me of the capability of confidence penalization as a regularization technique.\nOverall, I think the paper makes a good contribution to the knowledge and application of various smoothing techniques and introduces the benefits of confidence penalization as a competing and/or complementary regularization technique. The paper is clearly written and detailed in the number and variety of experiments performed.\n\nRating: 7: Good paper, accept\n\nConfidence: 4: The reviewer is confident but not absolutely certain that the evaluation is correct\n\n[\\[\u2013\\]](https://openreview.net/forum?id=HyhbYrGYe) [\\[+\\]](https://openreview.net/forum?id=HyhbYrGYe)\n\n## Another softmax smoothing technique\n\n## official review by AnonReviewer1 \u2022 Another softmax smoothing technique\n\nICLR 2017 workshop AnonReviewer1\n\n08 Mar 2017, 14:36ICLR 2017 workshop official reviewReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=rkTFDWA5e)\n\nReview: The paper proposes to add a regularizing term to the objective function which penalizes the estimation of distributions with small entropy. It is one of these small tricks that were tried out by various groups even though only few people mention it in publications because the changes in performance are small and the additional hyperparameter makes it unattractive. This is also reflected in the paper here, as the improvements are very small compared to the baselines and usually vanish if more care is taking w.r.t. traditional regularization approaches.\nFurther remarks:\n\\- The evaluation is done on a broad spectrum of tasks, but the selections of the baseline systems is questionable. Especially on WSJ, there is no good reason to take an attention based seq2seq model but not also a network trained in a hybrid fashion or with CTC. Especially the CTC experiment would have been of great interest since the criterion tends to favor sharp probabilities.\n\\- A theoretical perspective on the convergence is not well established and a proper justification why this is should be able to improve neural network training is missing (except for the norms of the gradients on MNIST). If the argument is that gradients saturate too quickly if probabilities go too high then I would like to see an experiment with the squared error criterion as comparison, where this effect is not that large.\nIn total I appreciate the work and broad evaluation but would suggest to include this method in a larger comparison paper that describes several of these tricks. The paper is well written and certainly correct, and the required scope is clearly l",
          "original_query": "Regularizing neural networks by penalizing confident output distributions",
          "cleaned_query": "Regularizing neural networks by penalizing confident output distributions"
        },
        {
          "success": true,
          "title": "Are All Losses Created Equal: A Neural Collapse Perspective - arXiv",
          "url": "https://arxiv.org/abs/2210.02192",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2210.02192** (cs)\n\n\\[Submitted on 4 Oct 2022 ( [v1](https://arxiv.org/abs/2210.02192v1)), last revised 8 Oct 2022 (this version, v2)\\]\n\n# Title:Are All Losses Created Equal: A Neural Collapse Perspective\n\nAuthors: [Jinxin Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou,+J), [Chong You](https://arxiv.org/search/cs?searchtype=author&query=You,+C), [Xiao Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+X), [Kangning Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+K), [Sheng Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+S), [Qing Qu](https://arxiv.org/search/cs?searchtype=author&query=Qu,+Q), [Zhihui Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu,+Z)\n\nView a PDF of the paper titled Are All Losses Created Equal: A Neural Collapse Perspective, by Jinxin Zhou and 6 other authors\n\n[View PDF](https://arxiv.org/pdf/2210.02192)\n\n> Abstract:While cross entropy (CE) is the most commonly used loss to train deep neural networks for classification tasks, many alternative losses have been developed to obtain better empirical performance. Among them, which one is the best to use is still a mystery, because there seem to be multiple factors affecting the answer, such as properties of the dataset, the choice of network architecture, and so on. This paper studies the choice of loss function by examining the last-layer features of deep networks, drawing inspiration from a recent line work showing that the global optimal solution of CE and mean-square-error (MSE) losses exhibits a Neural Collapse phenomenon. That is, for sufficiently large networks trained until convergence, (i) all features of the same class collapse to the corresponding class mean and (ii) the means associated with different classes are in a configuration where their pairwise distances are all equal and maximized. We extend such results and show through global solution and landscape analyses that a broad family of loss functions including commonly used label smoothing (LS) and focal loss (FL) exhibits Neural Collapse. Hence, all relevant losses(i.e., CE, LS, FL, MSE) produce equivalent features on training data. Based on the unconstrained feature model assumption, we provide either the global landscape analysis for LS loss or the local landscape analysis for FL loss and show that the (only!) global minimizers are neural collapse solutions, while all other critical points are strict saddles whose Hessian exhibit negative curvature directions either in the global scope for LS loss or in the local scope for FL loss near the optimal solution. The experiments further show that Neural Collapse features obtained from all relevant losses lead to largely identical performance on test data as well, provided that the network is sufficiently large and trained until convergence.\n\n| | |\n| --- | --- |\n| Comments: | 32 page, 10 figures, NeurIPS 2022 |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Information Theory (cs.IT); Optimization and Control (math.OC); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2210.02192](https://arxiv.org/abs/2210.02192) \\[cs.LG\\] |\n| | (or [arXiv:2210.02192v2](https://arxiv.org/abs/2210.02192v2) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2210.02192](https://doi.org/10.48550/arXiv.2210.02192) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jinxin Zhou \\[ [view email](https://arxiv.org/show-email/c5eac529/2210.02192)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2210.02192v1)**\nTue, 4 Oct 2022 00:36:45 UTC (8,578 KB)\n\n**\\[v2\\]**\nSat, 8 Oct 2022 22:59:07 UTC (8,576 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Are All Losses Created Equal: A Neural Collapse Perspective, by Jinxin Zhou and 6 other authors\n\n- [View PDF](https://arxiv.org/pdf/2210.02192)\n- [TeX Source](https://arxiv.org/src/2210.02192)\n- [Other Formats](https://arxiv.org/format/2210.02192)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2210.02192&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2210.02192&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2022-10](https://arxiv.org/list/cs.LG/2022-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2210.02192?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2210.02192?context=cs.AI)\n\n[cs.IT](https://arxiv.org/abs/2210.02192?context=cs.IT)\n\n[math](https://arxiv.org/abs/2210.02192?context=math)\n\n[math.IT](https://arxiv.org/abs/2210.02192?context=math.IT)\n\n[math.OC](https://arxiv.org/abs/2210.02192?context=math.OC)\n\n[stat](https://arxiv.org/abs/2210.02192?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2210.02192?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2210.02192)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2210.02192)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2210.02192)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2210.02192&description=Are All Losses Created Equal: A Neural Collapse Perspective) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2210.02192&title=Are All Losses Created Equal: A Neural Collapse Perspective)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv i",
          "original_query": "Are all losses created equal: A neural collapse perspective",
          "cleaned_query": "Are all losses created equal: A neural collapse perspective"
        }
      ],
      "generated_ideas": [
        "Layerwise Label Smoothing to Preserve Transferable Features\n- Build on findings that loss choice mainly affects the last few layers and that higher class separation can harm transfer. Train Inception-style models with label smoothing applied only to the classifier head (or last block) while keeping earlier layers trained with vanilla CE, then quantify transfer (linear probe and partial fine-tune) and representation similarity (CKA) to identify optimal \u201csmoothing depth.\u201d",
        "Neural-Collapse-Aware Regularization for Better Transfer\n- Use the neural collapse perspective showing many losses converge to similar collapsed solutions, yet transfer degrades when class separation is too strong. Introduce a penalty that constrains the last-layer class means to deviate from the simplex ETF geometry (e.g., encourage controlled anisotropy or subspace overlap), and test whether \u201cless-than-complete collapse\u201d improves downstream transfer without sacrificing in-domain accuracy.",
        "Representation Surgery: Replacing Only the Last Layers to Recover Transfer\n- Since loss differences manifest mainly in the last layers, test a \u201chead transplant\u201d procedure: pretrain multiple ImageNet models with different losses (CE, LS, confidence penalty, focal), then swap/replace the last block + classifier with a standardized CE-trained head while freezing earlier layers. Measure whether transfer deficits disappear, isolating whether harm is architectural-local (last-layer geometry) or distributed.",
        "Per-Class/Imbalance-Aware Smoothing for Failure Prediction in Long-Tailed Data\n- Extend label smoothing analyses to settings where failure prediction is critical and classes are imbalanced. Develop a smoothing rule that depends on class frequency and confusion structure (e.g., smoothing mass allocated to \u201cnearby\u201d classes estimated from running confusion), and evaluate impacts on neural collapse measures, calibration, and error detection on long-tailed benchmarks."
      ],
      "generated_ideas_raw": "1. **Layerwise Label Smoothing to Preserve Transferable Features**\n   - Build on findings that loss choice mainly affects the last few layers and that higher class separation can harm transfer. Train Inception-style models with label smoothing applied only to the classifier head (or last block) while keeping earlier layers trained with vanilla CE, then quantify transfer (linear probe and partial fine-tune) and representation similarity (CKA) to identify optimal \u201csmoothing depth.\u201d\n\n2. **Failure-Prediction-First Training via Flat-Minima + Entropy Scheduling**\n   - Extend the observation that common calibration can hurt failure prediction and that flat minima improves confidence separation. Combine flat-minima techniques (e.g., SAM-like perturbations) with a scheduled confidence penalty/label smoothing (high early, low late) to explicitly maximize correct-vs-incorrect confidence separation (AUROC for error detection) while maintaining top-1 accuracy.\n\n3. **Neural-Collapse-Aware Regularization for Better Transfer**\n   - Use the neural collapse perspective showing many losses converge to similar collapsed solutions, yet transfer degrades when class separation is too strong. Introduce a penalty that constrains the last-layer class means to deviate from the simplex ETF geometry (e.g., encourage controlled anisotropy or subspace overlap), and test whether \u201cless-than-complete collapse\u201d improves downstream transfer without sacrificing in-domain accuracy.\n\n4. **Factorized-Convolution Architecture Search Under Calibration and Failure Metrics**\n   - Build on Inception\u2019s factorized convolutions for compute efficiency, but optimize not only accuracy/compute\u2014also failure prediction quality. Run a constrained NAS or module-swap study over factorization patterns (e.g., 1\u00d7n + n\u00d71 vs depthwise variants) with objectives including ECE, error-detection AUROC, and compute (MACs), producing architectures explicitly tuned for reliable confidence under mobile budgets.\n\n5. **Confidence Separation as a Training Objective (Beyond Calibration)**\n   - Paper 4 suggests the key for failure prediction is confidence separation rather than calibration error. Define and optimize a differentiable surrogate of confidence separation (e.g., margin between confidence distributions of correct vs incorrect samples using minibatch correctness indicators or a teacher), and compare to post-hoc calibration, label smoothing, and confidence penalty on misclassification detection.\n\n6. **Representation Surgery: Replacing Only the Last Layers to Recover Transfer**\n   - Since loss differences manifest mainly in the last layers, test a \u201chead transplant\u201d procedure: pretrain multiple ImageNet models with different losses (CE, LS, confidence penalty, focal), then swap/replace the last block + classifier with a standardized CE-trained head while freezing earlier layers. Measure whether transfer deficits disappear, isolating whether harm is architectural-local (last-layer geometry) or distributed.\n\n7. **Per-Class/Imbalance-Aware Smoothing for Failure Prediction in Long-Tailed Data**\n   - Extend label smoothing analyses to settings where failure prediction is critical and classes are imbalanced. Develop a smoothing rule that depends on class frequency and confusion structure (e.g., smoothing mass allocated to \u201cnearby\u201d classes estimated from running confusion), and evaluate impacts on neural collapse measures, calibration, and error detection on long-tailed benchmarks.\n\n8. **Flat-Minima Diagnostics Using CKA Drift and Collapse Metrics**\n   - Paper 4 links flat minima to confidence separation, while Papers 3/6 provide tools (CKA, neural collapse statistics) to characterize representations. Create a diagnostic suite that tracks (i) sharpness/flatness proxies, (ii) CKA layer drift across training, and (iii) collapse indicators (within-class variance, ETF alignment) to predict\u2014early in training\u2014final failure prediction AUROC and transfer performance.\n\n9. **Compute-Efficient Ensembles: Diversity via Loss-Induced Last-Layer Geometry**\n   - Inception demonstrates strong ensemble gains; Papers 3/6 imply different losses reshape late-layer geometry. Build ensembles where all members share the same backbone architecture and training recipe except for the final-stage objective (CE vs LS vs confidence penalty vs focal), then quantify whether increased late-layer diversity improves robustness, calibration, and especially failure prediction compared to same-loss ensembles at equal compute.\n\n10. **Task-Adaptive Loss Switching for Fine-Tuning Under Limited Labels**\n   - Paper 3 notes loss choice matters for fixed-feature transfer but less for full fine-tuning; Paper 2 analyzes when label smoothing helps. Propose a fine-tuning protocol that switches objectives by regime: start with transfer-friendly objective (low class separation) for feature adaptation, then switch to accuracy-optimizing objective late, with switching decided by validation-set collapse/separation metrics\u2014targeting improved sample efficiency and more reliable confidence on small downstream datasets.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Layerwise Label Smoothing to Preserve Transferable Features\n- Build on findings that loss choice mainly affects the last few layers and that higher class separation can harm transfer. Train Inception-",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Neural-Collapse-Aware Regularization for Better Transfer\n- Use the neural collapse perspective showing many losses converge to similar collapsed solutions, yet transfer degrades when class separation ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Representation Surgery: Replacing Only the Last Layers to Recover Transfer\n- Since loss differences manifest mainly in the last layers, test a \u201chead transplant\u201d procedure: pretrain multiple ImageNet m",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Per-Class/Imbalance-Aware Smoothing for Failure Prediction in Long-Tailed Data\n- Extend label smoothing analyses to settings where failure prediction is critical and classes are imbalanced. Develop a ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 16,
      "paper_title": "Memory Mosaics at scale",
      "contribution": "Scaled and redesigned networks of associative key\u2013value memories (Memory Mosaics v2) that match transformers on training\u2011knowledge storage while substantially improving new\u2011task and in\u2011context learning at large scale.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 5,
      "input_tokens": 11690,
      "output_tokens": 858,
      "predecessor_details": [
        {
          "success": true,
          "title": "[2405.06394] Memory Mosaics - arXiv",
          "url": "https://arxiv.org/abs/2405.06394",
          "content": "[2405.06394] Memory Mosaics\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nIn just 5 minutes help us improve arXiv:\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2405.06394\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2405.06394**(cs)\n[Submitted on 10 May 2024 ([v1](https://arxiv.org/abs/2405.06394v1)), last revised 27 Feb 2025 (this version, v3)]\n# Title:Memory Mosaics\nAuthors:[Jianyu Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+J),[Niklas Nolte](https://arxiv.org/search/cs?searchtype=author&amp;query=Nolte,+N),[Ranajoy Sadhukhan](https://arxiv.org/search/cs?searchtype=author&amp;query=Sadhukhan,+R),[Beidi Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+B),[L\u00e9on Bottou](https://arxiv.org/search/cs?searchtype=author&amp;query=Bottou,+L)\nView a PDF of the paper titled Memory Mosaics, by Jianyu Zhang and 4 other authors\n[View PDF](https://arxiv.org/pdf/2405.06394)[HTML (experimental)](https://arxiv.org/html/2405.06394v3)> > Abstract:\n> Memory Mosaics are networks of associative memories working in concert to achieve a prediction task of interest. Like transformers, memory mosaics possess compositional capabilities and in-context learning capabilities. Unlike transformers, memory mosaics achieve these capabilities in comparatively transparent way (&#34;predictive disentanglement&#34;). We illustrate these capabilities on a toy example and also show that memory mosaics perform as well or better than transformers on medium-scale language modeling tasks. Subjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)|\nCite as:|[arXiv:2405.06394](https://arxiv.org/abs/2405.06394)[cs.LG]|\n|(or[arXiv:2405.06394v3](https://arxiv.org/abs/2405.06394v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2405.06394](https://doi.org/10.48550/arXiv.2405.06394)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jianyu Zhang [[view email](https://arxiv.org/show-email/38c6a8cc/2405.06394)]\n**[[v1]](https://arxiv.org/abs/2405.06394v1)**Fri, 10 May 2024 11:08:20 UTC (1,868 KB)\n**[[v2]](https://arxiv.org/abs/2405.06394v2)**Mon, 13 May 2024 20:27:34 UTC (1,868 KB)\n**[v3]**Thu, 27 Feb 2025 21:46:01 UTC (3,567 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Memory Mosaics, by Jianyu Zhang and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2405.06394)\n* [HTML (experimental)](https://arxiv.org/html/2405.06394v3)\n* [TeX Source](https://arxiv.org/src/2405.06394)\n[![license icon](https://arxiv.org/icons/licenses/by-nc-nd-4.0.png)view license](http://creativecommons.org/licenses/by-nc-nd/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2405.06394&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2405.06394&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2024-05](https://arxiv.org/list/cs.LG/2024-05)\nChange to browse by:\n[cs](https://arxiv.org/abs/2405.06394?context=cs)\n[cs.AI](https://arxiv.org/abs/2405.06394?context=cs.AI)\n[cs.NE](https://arxiv.org/abs/2405.06394?context=cs.NE)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2405.06394)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2405.06394)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2405.06394)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2405.06394)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Memory Mosaics (Zhang et al., 2025)",
          "cleaned_query": "Memory Mosaics"
        },
        {
          "success": true,
          "title": "[1706.03762] Attention Is All You Need",
          "url": "https://arxiv.org/abs/1706.03762",
          "content": "[1706.03762] Attention Is All You Need[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1706.03762\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:1706.03762**(cs)\n[Submitted on 12 Jun 2017 ([v1](https://arxiv.org/abs/1706.03762v1)), last revised 2 Aug 2023 (this version, v7)]\n# Title:Attention Is All You Need\nAuthors:[Ashish Vaswani](https://arxiv.org/search/cs?searchtype=author&amp;query=Vaswani,+A),[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&amp;query=Shazeer,+N),[Niki Parmar](https://arxiv.org/search/cs?searchtype=author&amp;query=Parmar,+N),[Jakob Uszkoreit](https://arxiv.org/search/cs?searchtype=author&amp;query=Uszkoreit,+J),[Llion Jones](https://arxiv.org/search/cs?searchtype=author&amp;query=Jones,+L),[Aidan N. Gomez](https://arxiv.org/search/cs?searchtype=author&amp;query=Gomez,+A+N),[Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaiser,+L),[Illia Polosukhin](https://arxiv.org/search/cs?searchtype=author&amp;query=Polosukhin,+I)\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n[View PDF](https://arxiv.org/pdf/1706.03762)[HTML (experimental)](https://arxiv.org/html/1706.03762v7)> > Abstract:\n> The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Comments:|15 pages, 5 figures|\nSubjects:|Computation and Language (cs.CL); Machine Learning (cs.LG)|\nCite as:|[arXiv:1706.03762](https://arxiv.org/abs/1706.03762)[cs.CL]|\n|(or[arXiv:1706.03762v7](https://arxiv.org/abs/1706.03762v7)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Llion Jones [[view email](https://arxiv.org/show-email/f53b7360/1706.03762)]\n**[[v1]](https://arxiv.org/abs/1706.03762v1)**Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB)\n**[[v2]](https://arxiv.org/abs/1706.03762v2)**Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB)\n**[[v3]](https://arxiv.org/abs/1706.03762v3)**Tue, 20 Jun 2017 05:20:02 UTC (1,125 KB)\n**[[v4]](https://arxiv.org/abs/1706.03762v4)**Fri, 30 Jun 2017 17:29:30 UTC (1,124 KB)\n**[[v5]](https://arxiv.org/abs/1706.03762v5)**Wed, 6 Dec 2017 03:30:32 UTC (1,124 KB)\n**[[v6]](https://arxiv.org/abs/1706.03762v6)**Mon, 24 Jul 2023 00:48:54 UTC (1,124 KB)\n**[v7]**Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n* [View PDF](https://arxiv.org/pdf/1706.03762)\n* [HTML (experimental)](https://arxiv.org/html/1706.03762v7)\n* [TeX Source](https://arxiv.org/src/1706.03762)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1706.03762&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1706.03762&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2017-06](https://arxiv.org/list/cs.CL/2017-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1706.03762?context=cs)\n[cs.LG](https://arxiv.org/abs/1706.03762?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1706.03762)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1706.03762)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1706.03762)\n### [123 blog links](https://arxiv.org/tb/1706.03762)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1706.html#VaswaniSPUJGKP17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/VaswaniSPUJGKP17)\n[Ashish Vaswani]()\n[Noam Shazeer]()\n[Niki Parmar]()\n[Jakob Uszkoreit]()\n[Llion Jones]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces",
          "original_query": "Attention Is All You Need (Vaswani et al., 2017)",
          "cleaned_query": "Attention Is All You Need"
        },
        {
          "success": true,
          "title": "[2005.14165] Language Models are Few-Shot Learners - arXiv",
          "url": "https://arxiv.org/abs/2005.14165",
          "content": "[2005.14165] Language Models are Few-Shot Learners[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2005.14165\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2005.14165**(cs)\n[Submitted on 28 May 2020 ([v1](https://arxiv.org/abs/2005.14165v1)), last revised 22 Jul 2020 (this version, v4)]\n# Title:Language Models are Few-Shot Learners\nAuthors:[Tom B. Brown](https://arxiv.org/search/cs?searchtype=author&amp;query=Brown,+T+B),[Benjamin Mann](https://arxiv.org/search/cs?searchtype=author&amp;query=Mann,+B),[Nick Ryder](https://arxiv.org/search/cs?searchtype=author&amp;query=Ryder,+N),[Melanie Subbiah](https://arxiv.org/search/cs?searchtype=author&amp;query=Subbiah,+M),[Jared Kaplan](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaplan,+J),[Prafulla Dhariwal](https://arxiv.org/search/cs?searchtype=author&amp;query=Dhariwal,+P),[Arvind Neelakantan](https://arxiv.org/search/cs?searchtype=author&amp;query=Neelakantan,+A),[Pranav Shyam](https://arxiv.org/search/cs?searchtype=author&amp;query=Shyam,+P),[Girish Sastry](https://arxiv.org/search/cs?searchtype=author&amp;query=Sastry,+G),[Amanda Askell](https://arxiv.org/search/cs?searchtype=author&amp;query=Askell,+A),[Sandhini Agarwal](https://arxiv.org/search/cs?searchtype=author&amp;query=Agarwal,+S),[Ariel Herbert-Voss](https://arxiv.org/search/cs?searchtype=author&amp;query=Herbert-Voss,+A),[Gretchen Krueger](https://arxiv.org/search/cs?searchtype=author&amp;query=Krueger,+G),[Tom Henighan](https://arxiv.org/search/cs?searchtype=author&amp;query=Henighan,+T),[Rewon Child](https://arxiv.org/search/cs?searchtype=author&amp;query=Child,+R),[Aditya Ramesh](https://arxiv.org/search/cs?searchtype=author&amp;query=Ramesh,+A),[Daniel M. Ziegler](https://arxiv.org/search/cs?searchtype=author&amp;query=Ziegler,+D+M),[Jeffrey Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+J),[Clemens Winter](https://arxiv.org/search/cs?searchtype=author&amp;query=Winter,+C),[Christopher Hesse](https://arxiv.org/search/cs?searchtype=author&amp;query=Hesse,+C),[Mark Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+M),[Eric Sigler](https://arxiv.org/search/cs?searchtype=author&amp;query=Sigler,+E),[Mateusz Litwin](https://arxiv.org/search/cs?searchtype=author&amp;query=Litwin,+M),[Scott Gray](https://arxiv.org/search/cs?searchtype=author&amp;query=Gray,+S),[Benjamin Chess](https://arxiv.org/search/cs?searchtype=author&amp;query=Chess,+B),[Jack Clark](https://arxiv.org/search/cs?searchtype=author&amp;query=Clark,+J),[Christopher Berner](https://arxiv.org/search/cs?searchtype=author&amp;query=Berner,+C),[Sam McCandlish](https://arxiv.org/search/cs?searchtype=author&amp;query=McCandlish,+S),[Alec Radford](https://arxiv.org/search/cs?searchtype=author&amp;query=Radford,+A),[Ilya Sutskever](https://arxiv.org/search/cs?searchtype=author&amp;query=Sutskever,+I),[Dario Amodei](https://arxiv.org/search/cs?searchtype=author&amp;query=Amodei,+D)\nView a PDF of the paper titled Language Models are Few-Shot Learners, by Tom B. Brown and 30 other authors\n[View PDF](https://arxiv.org/pdf/2005.14165)> > Abstract:\n> Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3&#39;s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. Comments:|40+32 pages|\nSubjects:|Computation and Language (cs.CL)|\nCite as:|[arXiv:2005.14165](https://arxiv.org/abs/2005.14165)[cs.CL]|\n|(or[arXiv:2005.14165v4](https://arxiv.org/abs/2005.14165v4)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2005.14165](https://doi.org/10.48550/arXiv.2005.14165)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Tom B Brown [[view email](https://arxiv.org/show-email/b5cb66e9/2005.14165)]\n**[[v1]](https://arxiv.org/abs/2005.14165v1)**Thu, 28 May 2020 17:29:03 UTC (6,995 KB)\n**[[v2]](https://arxiv.org/abs/2005.14165v2)**Mon, 1 Jun 2020 17:08:53 UTC (6,997 KB)\n**[[v3]](https://arxiv.org/abs/2005.14165v3)**Fri, 5 Jun 2020 02:52:35 UTC (6,998 KB)\n**[v4]**Wed, 22 Jul 2020 19:47:17 UTC (6,998 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Language Models are Few-Shot Learners, by Tom B. Brown and 30 other authors\n* [View PDF](https://arxiv.org/pdf/2005.14165)\n* [TeX Source](https://arxiv.org/src/2005.14165)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2005.14165&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2005.14165&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2020-05](https://arxiv.org/list/cs.CL/2020-05)\nChange to browse by:\n[cs](https://arxiv.org/abs/2005.14165?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2005.14165)\n* [",
          "original_query": "Language Models are Few\u2011Shot Learners (Brown et al., 2020)",
          "cleaned_query": "Language Models are Few\u2011Shot Learners"
        },
        {
          "success": true,
          "title": "[2008.02217] Hopfield Networks is All You Need - arXiv",
          "url": "https://arxiv.org/abs/2008.02217",
          "content": "# Computer Science > Neural and Evolutionary Computing\n\n**arXiv:2008.02217** (cs)\n\n\\[Submitted on 16 Jul 2020 ( [v1](https://arxiv.org/abs/2008.02217v1)), last revised 28 Apr 2021 (this version, v3)\\]\n\n# Title:Hopfield Networks is All You Need\n\nAuthors: [Hubert Ramsauer](https://arxiv.org/search/cs?searchtype=author&query=Ramsauer,+H), [Bernhard Sch\u00e4fl](https://arxiv.org/search/cs?searchtype=author&query=Sch%C3%A4fl,+B), [Johannes Lehner](https://arxiv.org/search/cs?searchtype=author&query=Lehner,+J), [Philipp Seidl](https://arxiv.org/search/cs?searchtype=author&query=Seidl,+P), [Michael Widrich](https://arxiv.org/search/cs?searchtype=author&query=Widrich,+M), [Thomas Adler](https://arxiv.org/search/cs?searchtype=author&query=Adler,+T), [Lukas Gruber](https://arxiv.org/search/cs?searchtype=author&query=Gruber,+L), [Markus Holzleitner](https://arxiv.org/search/cs?searchtype=author&query=Holzleitner,+M), [Milena Pavlovi\u0107](https://arxiv.org/search/cs?searchtype=author&query=Pavlovi%C4%87,+M), [Geir Kjetil Sandve](https://arxiv.org/search/cs?searchtype=author&query=Sandve,+G+K), [Victor Greiff](https://arxiv.org/search/cs?searchtype=author&query=Greiff,+V), [David Kreil](https://arxiv.org/search/cs?searchtype=author&query=Kreil,+D), [Michael Kopp](https://arxiv.org/search/cs?searchtype=author&query=Kopp,+M), [G\u00fcnter Klambauer](https://arxiv.org/search/cs?searchtype=author&query=Klambauer,+G), [Johannes Brandstetter](https://arxiv.org/search/cs?searchtype=author&query=Brandstetter,+J), [Sepp Hochreiter](https://arxiv.org/search/cs?searchtype=author&query=Hochreiter,+S)\n\nView a PDF of the paper titled Hopfield Networks is All You Need, by Hubert Ramsauer and 15 other authors\n\n[View PDF](https://arxiv.org/pdf/2008.02217)\n\n> Abstract:We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new Hopfield network can store exponentially (with the dimension of the associative space) many patterns, retrieves the pattern with one update, and has exponentially small retrieval errors. It has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. The new update rule is equivalent to the attention mechanism used in transformers. This equivalence enables a characterization of the heads of transformer models. These heads perform in the first layers preferably global averaging and in higher layers partial averaging via metastable states. The new modern Hopfield network can be integrated into deep learning architectures as layers to allow the storage of and access to raw input data, intermediate results, or learned prototypes. These Hopfield layers enable new ways of deep learning, beyond fully-connected, convolutional, or recurrent networks, and provide pooling, memory, association, and attention mechanisms. We demonstrate the broad applicability of the Hopfield layers across various domains. Hopfield layers improved state-of-the-art on three out of four considered multiple instance learning problems as well as on immune repertoire classification with several hundreds of thousands of instances. On the UCI benchmark collections of small classification tasks, where deep learning methods typically struggle, Hopfield layers yielded a new state-of-the-art when compared to different machine learning methods. Finally, Hopfield layers achieved state-of-the-art on two drug design datasets. The implementation is available at: [this https URL](https://github.com/ml-jku/hopfield-layers)\n\n| | |\n| --- | --- |\n| Comments: | 10 pages (+ appendix); 12 figures; Blog: [this https URL](https://ml-jku.github.io/hopfield-layers/;) GitHub: [this https URL](https://github.com/ml-jku/hopfield-layers) |\n| Subjects: | Neural and Evolutionary Computing (cs.NE); Computation and Language (cs.CL); Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2008.02217](https://arxiv.org/abs/2008.02217) \\[cs.NE\\] |\n| (or [arXiv:2008.02217v3](https://arxiv.org/abs/2008.02217v3) \\[cs.NE\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2008.02217](https://doi.org/10.48550/arXiv.2008.02217) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Hubert Ramsauer \\[ [view email](https://arxiv.org/show-email/9c145fe9/2008.02217)\\] **[\\[v1\\]](https://arxiv.org/abs/2008.02217v1)**\nThu, 16 Jul 2020 17:52:37 UTC (3,823 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2008.02217v2)**\nTue, 22 Dec 2020 14:16:15 UTC (15,003 KB)\n**\\[v3\\]**\nWed, 28 Apr 2021 07:24:49 UTC (15,038 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Hopfield Networks is All You Need, by Hubert Ramsauer and 15 other authors\n\n- [View PDF](https://arxiv.org/pdf/2008.02217)\n- [TeX Source](https://arxiv.org/src/2008.02217)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.NE\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2008.02217&function=prev&context=cs.NE)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2008.02217&function=next&context=cs.NE)\n\n[new](https://arxiv.org/list/cs.NE/new) \\| [recent](https://arxiv.org/list/cs.NE/recent) \\| [2020-08](https://arxiv.org/list/cs.NE/2020-08)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2008.02217?context=cs) [cs.CL](https://arxiv.org/abs/2008.02217?context=cs.CL) [cs.LG](https://arxiv.org/abs/2008.02217?context=cs.LG) [stat](https://arxiv.org/abs/2008.02217?context=stat) [stat.ML](https://arxiv.org/abs/2008.02217?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2008.02217)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2008.02217)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2008.02217)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2008.html#abs-2008-02217) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2008-02217)\n\n[Hubert Ramsauer](https://dblp.uni-trier.de/search/author?author=Hubert%20Ramsauer) [Michael Widrich](https://dblp.uni-trier.de/search/author?author=Michael%20Widrich) [Lukas Gruber](https://dblp.uni-trier.de/search/author?author=Lukas%20Gruber) [Geir Kjetil Sandve](https://dblp.uni-trier.de/search/author?author=Geir%20Kjetil%20Sandve) [Michael Kopp](https://dblp.uni-trier.de/search/author?author=Michael%20Kopp)\n\n\u2026\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _",
          "original_query": "Hopfield Networks is All You Need (Ramsauer et al., 2020)",
          "cleaned_query": "Hopfield Networks is All You Need"
        },
        {
          "success": true,
          "title": "Key-Value Memory Networks for Directly Reading Documents - arXiv",
          "url": "https://arxiv.org/abs/1606.03126",
          "content": "# Computer Science > Computation and Language\n\n**arXiv:1606.03126** (cs)\n\n\\[Submitted on 9 Jun 2016 ( [v1](https://arxiv.org/abs/1606.03126v1)), last revised 10 Oct 2016 (this version, v2)\\]\n\n# Title:Key-Value Memory Networks for Directly Reading Documents\n\nAuthors: [Alexander Miller](https://arxiv.org/search/cs?searchtype=author&query=Miller,+A), [Adam Fisch](https://arxiv.org/search/cs?searchtype=author&query=Fisch,+A), [Jesse Dodge](https://arxiv.org/search/cs?searchtype=author&query=Dodge,+J), [Amir-Hossein Karimi](https://arxiv.org/search/cs?searchtype=author&query=Karimi,+A), [Antoine Bordes](https://arxiv.org/search/cs?searchtype=author&query=Bordes,+A), [Jason Weston](https://arxiv.org/search/cs?searchtype=author&query=Weston,+J)\n\nView a PDF of the paper titled Key-Value Memory Networks for Directly Reading Documents, by Alexander Miller and 5 other authors\n\n[View PDF](https://arxiv.org/pdf/1606.03126)\n\n> Abstract:Directly reading documents and being able to answer questions from them is an unsolved challenge. To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs often suffer from being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, WikiMovies, a QA dataset that contains raw text alongside a preprocessed KB, in the domain of movies. Our method reduces the gap between all three settings. It also achieves state-of-the-art results on the existing WikiQA benchmark.\n\n| | |\n| --- | --- |\n| Subjects: | Computation and Language (cs.CL) |\n| Cite as: | [arXiv:1606.03126](https://arxiv.org/abs/1606.03126) \\[cs.CL\\] |\n| | (or [arXiv:1606.03126v2](https://arxiv.org/abs/1606.03126v2) \\[cs.CL\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1606.03126](https://doi.org/10.48550/arXiv.1606.03126) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jason Weston \\[ [view email](https://arxiv.org/show-email/a67ba61d/1606.03126)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1606.03126v1)**\nThu, 9 Jun 2016 21:33:55 UTC (110 KB)\n\n**\\[v2\\]**\nMon, 10 Oct 2016 20:14:10 UTC (122 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Key-Value Memory Networks for Directly Reading Documents, by Alexander Miller and 5 other authors\n\n- [View PDF](https://arxiv.org/pdf/1606.03126)\n- [TeX Source](https://arxiv.org/src/1606.03126)\n- [Other Formats](https://arxiv.org/format/1606.03126)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1606.03126&function=prev&context=cs.CL)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1606.03126&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2016-06](https://arxiv.org/list/cs.CL/2016-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1606.03126?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1606.03126)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1606.03126)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1606.03126)\n\n### [1 blog link](https://arxiv.org/tb/1606.03126)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1606.html#MillerFDKBW16) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/MillerFDKBW16)\n\n[Alexander Miller](https://dblp.uni-trier.de/search/author?author=Alexander%20Miller)\n\n[Alexander H. Miller](https://dblp.uni-trier.de/search/author?author=Alexander%20H.%20Miller)\n\n[Adam Fisch](https://dblp.uni-trier.de/search/author?author=Adam%20Fisch)\n\n[Jesse Dodge](https://dblp.uni-trier.de/search/author?author=Jesse%20Dodge)\n\n[Amir-Hossein Karimi](https://dblp.uni-trier.de/search/author?author=Amir-Hossein%20Karimi)\n\n\u2026\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1606.03126&description=Key-Value Memory Networks for Directly Reading Documents) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1606.03126&title=Key-Value Memory Networks for Directly Reading Documents)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1606.03126) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Key\u2013Value Memory Networks (Miller et al., 2016 and related works)",
          "cleaned_query": "Key\u2013Value Memory Networks"
        }
      ],
      "generated_ideas": [
        "Energy-Based Diagnostics for Memory Mosaic Heads\nDevelop an analysis toolkit that maps each Memory Mosaic associative memory (and transformer attention head baselines) to modern Hopfield energy minima types (global, metastable subset, single-pattern). Quantify how these regimes evolve across layers and during training, and correlate regimes with in-context learning accuracy and calibration on medium-scale language modeling.",
        "Adaptive Pattern Budgeting via Metastable-State Routing\nAdd a router that dynamically selects how many patterns a Mosaic memory should retrieve (single vs subset vs global averaging) conditioned on the query, using the Hopfield interpretation as a control signal. Evaluate whether adaptive retrieval improves long-context generalization and reduces hallucinations compared to fixed top\u2011k retrieval.",
        "Key\u2013Value Memory Mosaics for Document QA Without Fine-Tuning\nCombine Key-Value Memory Networks\u2019 separate addressing/output encodings with Memory Mosaics\u2019 compositional memories to create a modular document-QA system that performs multi-hop retrieval and answer synthesis purely in-context. Benchmark on WikiMovies/WikiQA-style settings with explicit ablations: (i) KV-only, (ii) Mosaic-only, (iii) KV+Mosaic with disentangled prediction traces.",
        "Predictive-Disentanglement Metrics and Benchmarks\nFormalize \u201cpredictive disentanglement\u201d as measurable properties: e.g., sparsity/orthogonality of memory contributions, stability of retrieved patterns under paraphrased prompts, and causal influence scores per memory. Release a benchmark suite where tasks are constructed to require identifiable sub-predictions (entity tracking, rule application, schema grounding) and compare Memory Mosaics vs transformers.",
        "Compositional In-Context Learning with Modular Memory Editing\nDesign a method to edit or add a single associative memory module inside a trained Mosaic (e.g., inject a new rulebook or domain lexicon) without retraining the whole model. Test whether targeted memory edits produce localized behavior changes (minimal side effects) on few-shot tasks, compared with prompt-only steering and transformer adapters.",
        "Long-Context Scaling Laws: Associative Memories vs Attention\nRun controlled scaling experiments varying context length, memory size, and number of Mosaic modules while keeping total compute comparable to transformers. Derive empirical scaling laws for perplexity and few-shot accuracy, and identify regimes where associative retrieval yields better compute/accuracy tradeoffs than quadratic attention.",
        "Robust Retrieval Under Adversarial Prompting via Energy Regularization\nIntroduce training-time regularizers that penalize unstable retrieval dynamics (e.g., large energy gaps that flip minima under small query perturbations) using modern Hopfield energy formulations. Evaluate robustness against adversarial demonstrations, instruction injection, and distractor passages in retrieval-augmented prompting.",
        "Hierarchical Memory Mosaics for Multi-Hop Reasoning\nBuild a two-level Mosaic: lower-level associative memories retrieve candidate facts/snippets (KV-style), while a higher-level memory retrieves \u201creasoning templates\u201d (e.g., chains, arithmetic procedures) conditioned on the retrieved facts. Test on synthetic compositional tasks and multi-hop QA, measuring both accuracy and interpretability of the retrieved templates.",
        "Interpretable Distillation: From Transformers to Memory Mosaics\nCreate a distillation pipeline where a transformer teacher\u2019s attention patterns (or induced Hopfield updates) supervise a student Memory Mosaic to match intermediate retrieval distributions. Evaluate whether the student maintains teacher performance on language modeling and few-shot tasks while offering clearer module-level explanations and potentially reduced inference cost.",
        "Continual Learning with Mosaic-Based Episodic Memory and Forgetting Control\nExtend Memory Mosaics with an explicit episodic memory that stores recent task episodes (keys/values) and uses controlled consolidation into longer-term associative modules. Study catastrophic forgetting across sequential domains, comparing consolidation strategies (e.g., reservoir sampling vs energy-based prototype merging) and measuring retention, plasticity, and retrieval transparency."
      ],
      "generated_ideas_raw": "1. **Energy-Based Diagnostics for Memory Mosaic Heads**\n   Develop an analysis toolkit that maps each Memory Mosaic associative memory (and transformer attention head baselines) to modern Hopfield energy minima types (global, metastable subset, single-pattern). Quantify how these regimes evolve across layers and during training, and correlate regimes with in-context learning accuracy and calibration on medium-scale language modeling.\n\n2. **Adaptive Pattern Budgeting via Metastable-State Routing**\n   Add a router that dynamically selects how many patterns a Mosaic memory should retrieve (single vs subset vs global averaging) conditioned on the query, using the Hopfield interpretation as a control signal. Evaluate whether adaptive retrieval improves long-context generalization and reduces hallucinations compared to fixed top\u2011k retrieval.\n\n3. **Key\u2013Value Memory Mosaics for Document QA Without Fine-Tuning**\n   Combine Key-Value Memory Networks\u2019 separate addressing/output encodings with Memory Mosaics\u2019 compositional memories to create a modular document-QA system that performs multi-hop retrieval and answer synthesis purely in-context. Benchmark on WikiMovies/WikiQA-style settings with explicit ablations: (i) KV-only, (ii) Mosaic-only, (iii) KV+Mosaic with disentangled prediction traces.\n\n4. **Predictive-Disentanglement Metrics and Benchmarks**\n   Formalize \u201cpredictive disentanglement\u201d as measurable properties: e.g., sparsity/orthogonality of memory contributions, stability of retrieved patterns under paraphrased prompts, and causal influence scores per memory. Release a benchmark suite where tasks are constructed to require identifiable sub-predictions (entity tracking, rule application, schema grounding) and compare Memory Mosaics vs transformers.\n\n5. **Compositional In-Context Learning with Modular Memory Editing**\n   Design a method to edit or add a single associative memory module inside a trained Mosaic (e.g., inject a new rulebook or domain lexicon) without retraining the whole model. Test whether targeted memory edits produce localized behavior changes (minimal side effects) on few-shot tasks, compared with prompt-only steering and transformer adapters.\n\n6. **Long-Context Scaling Laws: Associative Memories vs Attention**\n   Run controlled scaling experiments varying context length, memory size, and number of Mosaic modules while keeping total compute comparable to transformers. Derive empirical scaling laws for perplexity and few-shot accuracy, and identify regimes where associative retrieval yields better compute/accuracy tradeoffs than quadratic attention.\n\n7. **Robust Retrieval Under Adversarial Prompting via Energy Regularization**\n   Introduce training-time regularizers that penalize unstable retrieval dynamics (e.g., large energy gaps that flip minima under small query perturbations) using modern Hopfield energy formulations. Evaluate robustness against adversarial demonstrations, instruction injection, and distractor passages in retrieval-augmented prompting.\n\n8. **Hierarchical Memory Mosaics for Multi-Hop Reasoning**\n   Build a two-level Mosaic: lower-level associative memories retrieve candidate facts/snippets (KV-style), while a higher-level memory retrieves \u201creasoning templates\u201d (e.g., chains, arithmetic procedures) conditioned on the retrieved facts. Test on synthetic compositional tasks and multi-hop QA, measuring both accuracy and interpretability of the retrieved templates.\n\n9. **Interpretable Distillation: From Transformers to Memory Mosaics**\n   Create a distillation pipeline where a transformer teacher\u2019s attention patterns (or induced Hopfield updates) supervise a student Memory Mosaic to match intermediate retrieval distributions. Evaluate whether the student maintains teacher performance on language modeling and few-shot tasks while offering clearer module-level explanations and potentially reduced inference cost.\n\n10. **Continual Learning with Mosaic-Based Episodic Memory and Forgetting Control**\n   Extend Memory Mosaics with an explicit episodic memory that stores recent task episodes (keys/values) and uses controlled consolidation into longer-term associative modules. Study catastrophic forgetting across sequential domains, comparing consolidation strategies (e.g., reservoir sampling vs energy-based prototype merging) and measuring retention, plasticity, and retrieval transparency.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Energy-Based Diagnostics for Memory Mosaic Heads\nDevelop an analysis toolkit that maps each Memory Mosaic associative memory (and transformer attention head baselines) to modern Hopfield energy minima",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Adaptive Pattern Budgeting via Metastable-State Routing\nAdd a router that dynamically selects how many patterns a Mosaic memory should retrieve (single vs subset vs global averaging) conditioned on th",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Key\u2013Value Memory Mosaics for Document QA Without Fine-Tuning\nCombine Key-Value Memory Networks\u2019 separate addressing/output encodings with Memory Mosaics\u2019 compositional memories to create a modular doc",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Predictive-Disentanglement Metrics and Benchmarks\nFormalize \u201cpredictive disentanglement\u201d as measurable properties: e.g., sparsity/orthogonality of memory contributions, stability of retrieved patterns",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Compositional In-Context Learning with Modular Memory Editing\nDesign a method to edit or add a single associative memory module inside a trained Mosaic (e.g., inject a new rulebook or domain lexicon) ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Long-Context Scaling Laws: Associative Memories vs Attention\nRun controlled scaling experiments varying context length, memory size, and number of Mosaic modules while keeping total compute comparable",
          "is_match": true
        },
        {
          "idea_idx": 6,
          "idea_text": "Robust Retrieval Under Adversarial Prompting via Energy Regularization\nIntroduce training-time regularizers that penalize unstable retrieval dynamics (e.g., large energy gaps that flip minima under sm",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Hierarchical Memory Mosaics for Multi-Hop Reasoning\nBuild a two-level Mosaic: lower-level associative memories retrieve candidate facts/snippets (KV-style), while a higher-level memory retrieves \u201creas",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Interpretable Distillation: From Transformers to Memory Mosaics\nCreate a distillation pipeline where a transformer teacher\u2019s attention patterns (or induced Hopfield updates) supervise a student Memory",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Continual Learning with Mosaic-Based Episodic Memory and Forgetting Control\nExtend Memory Mosaics with an explicit episodic memory that stores recent task episodes (keys/values) and uses controlled co",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 17,
      "paper_title": "The emergence of sparse attention: impact of data distribution and benefits of repetition",
      "contribution": "Shows that sparse-attention circuits emerge as predictable phase-transitions in training dynamics driven by task structure, optimizer/architecture choices, and data distribution\u2014and that repeating examples can dramatically accelerate this emergence.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 13490,
      "output_tokens": 1028,
      "predecessor_details": [
        {
          "success": true,
          "title": "[1706.03762] Attention Is All You Need - arXiv",
          "url": "https://arxiv.org/abs/1706.03762",
          "content": "[1706.03762] Attention Is All You Need[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1706.03762\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:1706.03762**(cs)\n[Submitted on 12 Jun 2017 ([v1](https://arxiv.org/abs/1706.03762v1)), last revised 2 Aug 2023 (this version, v7)]\n# Title:Attention Is All You Need\nAuthors:[Ashish Vaswani](https://arxiv.org/search/cs?searchtype=author&amp;query=Vaswani,+A),[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&amp;query=Shazeer,+N),[Niki Parmar](https://arxiv.org/search/cs?searchtype=author&amp;query=Parmar,+N),[Jakob Uszkoreit](https://arxiv.org/search/cs?searchtype=author&amp;query=Uszkoreit,+J),[Llion Jones](https://arxiv.org/search/cs?searchtype=author&amp;query=Jones,+L),[Aidan N. Gomez](https://arxiv.org/search/cs?searchtype=author&amp;query=Gomez,+A+N),[Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaiser,+L),[Illia Polosukhin](https://arxiv.org/search/cs?searchtype=author&amp;query=Polosukhin,+I)\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n[View PDF](https://arxiv.org/pdf/1706.03762)[HTML (experimental)](https://arxiv.org/html/1706.03762v7)> > Abstract:\n> The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Comments:|15 pages, 5 figures|\nSubjects:|Computation and Language (cs.CL); Machine Learning (cs.LG)|\nCite as:|[arXiv:1706.03762](https://arxiv.org/abs/1706.03762)[cs.CL]|\n|(or[arXiv:1706.03762v7](https://arxiv.org/abs/1706.03762v7)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Llion Jones [[view email](https://arxiv.org/show-email/f53b7360/1706.03762)]\n**[[v1]](https://arxiv.org/abs/1706.03762v1)**Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB)\n**[[v2]](https://arxiv.org/abs/1706.03762v2)**Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB)\n**[[v3]](https://arxiv.org/abs/1706.03762v3)**Tue, 20 Jun 2017 05:20:02 UTC (1,125 KB)\n**[[v4]](https://arxiv.org/abs/1706.03762v4)**Fri, 30 Jun 2017 17:29:30 UTC (1,124 KB)\n**[[v5]](https://arxiv.org/abs/1706.03762v5)**Wed, 6 Dec 2017 03:30:32 UTC (1,124 KB)\n**[[v6]](https://arxiv.org/abs/1706.03762v6)**Mon, 24 Jul 2023 00:48:54 UTC (1,124 KB)\n**[v7]**Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n* [View PDF](https://arxiv.org/pdf/1706.03762)\n* [HTML (experimental)](https://arxiv.org/html/1706.03762v7)\n* [TeX Source](https://arxiv.org/src/1706.03762)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1706.03762&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1706.03762&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2017-06](https://arxiv.org/list/cs.CL/2017-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1706.03762?context=cs)\n[cs.LG](https://arxiv.org/abs/1706.03762?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1706.03762)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1706.03762)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1706.03762)\n### [123 blog links](https://arxiv.org/tb/1706.03762)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1706.html#VaswaniSPUJGKP17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/VaswaniSPUJGKP17)\n[Ashish Vaswani]()\n[Noam Shazeer]()\n[Niki Parmar]()\n[Jakob Uszkoreit]()\n[Llion Jones]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces",
          "original_query": "Attention is all you need",
          "cleaned_query": "Attention is all you need"
        },
        {
          "success": true,
          "title": "[2209.11895] In-context Learning and Induction Heads - arXiv",
          "url": "https://arxiv.org/abs/2209.11895",
          "content": "[2209.11895] In-context Learning and Induction Heads\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2209.11895\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2209.11895**(cs)\n[Submitted on 24 Sep 2022]\n# Title:In-context Learning and Induction Heads\nAuthors:[Catherine Olsson](https://arxiv.org/search/cs?searchtype=author&amp;query=Olsson,+C),[Nelson Elhage](https://arxiv.org/search/cs?searchtype=author&amp;query=Elhage,+N),[Neel Nanda](https://arxiv.org/search/cs?searchtype=author&amp;query=Nanda,+N),[Nicholas Joseph](https://arxiv.org/search/cs?searchtype=author&amp;query=Joseph,+N),[Nova DasSarma](https://arxiv.org/search/cs?searchtype=author&amp;query=DasSarma,+N),[Tom Henighan](https://arxiv.org/search/cs?searchtype=author&amp;query=Henighan,+T),[Ben Mann](https://arxiv.org/search/cs?searchtype=author&amp;query=Mann,+B),[Amanda Askell](https://arxiv.org/search/cs?searchtype=author&amp;query=Askell,+A),[Yuntao Bai](https://arxiv.org/search/cs?searchtype=author&amp;query=Bai,+Y),[Anna Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+A),[Tom Conerly](https://arxiv.org/search/cs?searchtype=author&amp;query=Conerly,+T),[Dawn Drain](https://arxiv.org/search/cs?searchtype=author&amp;query=Drain,+D),[Deep Ganguli](https://arxiv.org/search/cs?searchtype=author&amp;query=Ganguli,+D),[Zac Hatfield-Dodds](https://arxiv.org/search/cs?searchtype=author&amp;query=Hatfield-Dodds,+Z),[Danny Hernandez](https://arxiv.org/search/cs?searchtype=author&amp;query=Hernandez,+D),[Scott Johnston](https://arxiv.org/search/cs?searchtype=author&amp;query=Johnston,+S),[Andy Jones](https://arxiv.org/search/cs?searchtype=author&amp;query=Jones,+A),[Jackson Kernion](https://arxiv.org/search/cs?searchtype=author&amp;query=Kernion,+J),[Liane Lovitt](https://arxiv.org/search/cs?searchtype=author&amp;query=Lovitt,+L),[Kamal Ndousse](https://arxiv.org/search/cs?searchtype=author&amp;query=Ndousse,+K),[Dario Amodei](https://arxiv.org/search/cs?searchtype=author&amp;query=Amodei,+D),[Tom Brown](https://arxiv.org/search/cs?searchtype=author&amp;query=Brown,+T),[Jack Clark](https://arxiv.org/search/cs?searchtype=author&amp;query=Clark,+J),[Jared Kaplan](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaplan,+J),[Sam McCandlish](https://arxiv.org/search/cs?searchtype=author&amp;query=McCandlish,+S),[Chris Olah](https://arxiv.org/search/cs?searchtype=author&amp;query=Olah,+C)\nView a PDF of the paper titled In-context Learning and Induction Heads, by Catherine Olsson and 25 other authors\n[View PDF](https://arxiv.org/pdf/2209.11895)> > Abstract:\n> &#34;Induction heads&#34; are attention heads that implement a simple algorithm to complete token sequences like [A][B] ... [A] -&gt; [B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all &#34;in-context learning&#34; in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in in-context learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence. Subjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:2209.11895](https://arxiv.org/abs/2209.11895)[cs.LG]|\n|(or[arXiv:2209.11895v1](https://arxiv.org/abs/2209.11895v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2209.11895](https://doi.org/10.48550/arXiv.2209.11895)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Catherine Olsson [[view email](https://arxiv.org/show-email/31fdc312/2209.11895)]\n**[v1]**Sat, 24 Sep 2022 00:43:19 UTC (9,724 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled In-context Learning and Induction Heads, by Catherine Olsson and 25 other authors\n* [View PDF](https://arxiv.org/pdf/2209.11895)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2209.11895&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2209.11895&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2022-09](https://arxiv.org/list/cs.LG/2022-09)\nChange to browse by:\n[cs](https://arxiv.org/abs/2209.11895?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2209.11895)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2209.11895)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2209.11895)\n### [1 blog link](https://arxiv.org/tb/2209.11895)\n([what is this?](https://info.arxiv.org/help/trackback.html))\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recomme",
          "original_query": "In\u2011context learning and induction heads. Transformer circuits thread, 2022",
          "cleaned_query": "In\u2011context learning and induction heads. Transformer circuits thread, 2022"
        },
        {
          "success": true,
          "title": "Exact solutions to the nonlinear dynamics of learning in deep linear ...",
          "url": "https://arxiv.org/abs/1312.6120",
          "content": "# Computer Science > Neural and Evolutionary Computing\n\n**arXiv:1312.6120** (cs)\n\n\\[Submitted on 20 Dec 2013 ( [v1](https://arxiv.org/abs/1312.6120v1)), last revised 19 Feb 2014 (this version, v3)\\]\n\n# Title:Exact solutions to the nonlinear dynamics of learning in deep linear neural networks\n\nAuthors: [Andrew M. Saxe](https://arxiv.org/search/cs?searchtype=author&query=Saxe,+A+M), [James L. McClelland](https://arxiv.org/search/cs?searchtype=author&query=McClelland,+J+L), [Surya Ganguli](https://arxiv.org/search/cs?searchtype=author&query=Ganguli,+S)\n\nView a PDF of the paper titled Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, by Andrew M. Saxe and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/1312.6120)\n\n> Abstract:Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.\n\n| | |\n| --- | --- |\n| Comments: | Submission to ICLR2014. Revised based on reviewer feedback |\n| Subjects: | Neural and Evolutionary Computing (cs.NE); Disordered Systems and Neural Networks (cond-mat.dis-nn); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1312.6120](https://arxiv.org/abs/1312.6120) \\[cs.NE\\] |\n| (or [arXiv:1312.6120v3](https://arxiv.org/abs/1312.6120v3) \\[cs.NE\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1312.6120](https://doi.org/10.48550/arXiv.1312.6120) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Andrew Saxe \\[ [view email](https://arxiv.org/show-email/b8d5672b/1312.6120)\\] **[\\[v1\\]](https://arxiv.org/abs/1312.6120v1)**\nFri, 20 Dec 2013 20:24:00 UTC (249 KB)\n**[\\[v2\\]](https://arxiv.org/abs/1312.6120v2)**\nFri, 24 Jan 2014 20:39:04 UTC (422 KB)\n**\\[v3\\]**\nWed, 19 Feb 2014 17:26:57 UTC (424 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Exact solutions to the nonlinear dynamics of learning in deep linear neural networks, by Andrew M. Saxe and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/1312.6120)\n- [TeX Source](https://arxiv.org/src/1312.6120)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.NE\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1312.6120&function=prev&context=cs.NE)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1312.6120&function=next&context=cs.NE)\n\n[new](https://arxiv.org/list/cs.NE/new) \\| [recent](https://arxiv.org/list/cs.NE/recent) \\| [2013-12](https://arxiv.org/list/cs.NE/2013-12)\n\nChange to browse by:\n\n[cond-mat](https://arxiv.org/abs/1312.6120?context=cond-mat) [cond-mat.dis-nn](https://arxiv.org/abs/1312.6120?context=cond-mat.dis-nn) [cs](https://arxiv.org/abs/1312.6120?context=cs) [cs.CV](https://arxiv.org/abs/1312.6120?context=cs.CV) [cs.LG](https://arxiv.org/abs/1312.6120?context=cs.LG) [q-bio](https://arxiv.org/abs/1312.6120?context=q-bio) [q-bio.NC](https://arxiv.org/abs/1312.6120?context=q-bio.NC) [stat](https://arxiv.org/abs/1312.6120?context=stat) [stat.ML](https://arxiv.org/abs/1312.6120?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1312.6120)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1312.6120)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1312.6120)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/conf/iclr/iclr2014.html#SaxeMG13) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/conf/iclr/SaxeMG13)\n\n[Andrew M. Saxe](https://dblp.uni-trier.de/search/author?author=Andrew%20M.%20Saxe) [James L. McClelland](https://dblp.uni-trier.de/search/author?author=James%20L.%20McClelland) [Surya Ganguli](https://dblp.uni-trier.de/search/author?author=Surya%20Ganguli)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.or",
          "original_query": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
          "cleaned_query": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks"
        },
        {
          "success": true,
          "title": "Data Distributional Properties Drive Emergent In-Context Learning ...",
          "url": "https://arxiv.org/abs/2205.05055",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2205.05055** (cs)\n\n\\[Submitted on 22 Apr 2022 ( [v1](https://arxiv.org/abs/2205.05055v1)), last revised 17 Nov 2022 (this version, v6)\\]\n\n# Title:Data Distributional Properties Drive Emergent In-Context Learning in Transformers\n\nAuthors: [Stephanie C.Y. Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan,+S+C), [Adam Santoro](https://arxiv.org/search/cs?searchtype=author&query=Santoro,+A), [Andrew K. Lampinen](https://arxiv.org/search/cs?searchtype=author&query=Lampinen,+A+K), [Jane X. Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+J+X), [Aaditya Singh](https://arxiv.org/search/cs?searchtype=author&query=Singh,+A), [Pierre H. Richemond](https://arxiv.org/search/cs?searchtype=author&query=Richemond,+P+H), [Jay McClelland](https://arxiv.org/search/cs?searchtype=author&query=McClelland,+J), [Felix Hill](https://arxiv.org/search/cs?searchtype=author&query=Hill,+F)\n\nView a PDF of the paper titled Data Distributional Properties Drive Emergent In-Context Learning in Transformers, by Stephanie C.Y. Chan and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2205.05055)\n\n> Abstract:Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. This observation raises the question: what aspects of the training regime lead to this emergent behavior? Here, we show that this behavior is driven by the distributions of the training data itself. In-context learning emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having large numbers of rarely occurring classes. In-context learning also emerges more strongly when item meanings or interpretations are dynamic rather than fixed. These properties are exemplified by natural language, but are also inherent to naturalistic data in a wide range of other domains. They also depart significantly from the uniform, i.i.d. training distributions typically used for standard supervised learning. In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously. However, our later experiments uncovered that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution -- another common property of naturalistic data, including language. In further experiments, we found that naturalistic data distributions were only able to elicit in-context learning in transformers, and not in recurrent models. In sum, our findings indicate how the transformer architecture works together with particular properties of the training data to drive the intriguing emergent in-context learning behaviour of large language models, and how future work might encourage both in-context and in-weights learning in domains beyond language.\n\n| | |\n| --- | --- |\n| Comments: | Accepted at NeurIPS 2022 (Oral). Code is available at: [this https URL](https://github.com/deepmind/emergent_in_context_learning) |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL) |\n| Cite as: | [arXiv:2205.05055](https://arxiv.org/abs/2205.05055) \\[cs.LG\\] |\n| (or [arXiv:2205.05055v6](https://arxiv.org/abs/2205.05055v6) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2205.05055](https://doi.org/10.48550/arXiv.2205.05055) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Stephanie Chan \\[ [view email](https://arxiv.org/show-email/c28765c7/2205.05055)\\] **[\\[v1\\]](https://arxiv.org/abs/2205.05055v1)**\nFri, 22 Apr 2022 16:10:50 UTC (1,215 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2205.05055v2)**\nThu, 12 May 2022 08:11:29 UTC (1,215 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2205.05055v3)**\nMon, 23 May 2022 09:58:47 UTC (2,148 KB)\n**[\\[v4\\]](https://arxiv.org/abs/2205.05055v4)**\nMon, 30 May 2022 11:39:30 UTC (2,148 KB)\n**[\\[v5\\]](https://arxiv.org/abs/2205.05055v5)**\nMon, 10 Oct 2022 18:57:58 UTC (1,250 KB)\n**\\[v6\\]**\nThu, 17 Nov 2022 18:42:14 UTC (1,250 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Data Distributional Properties Drive Emergent In-Context Learning in Transformers, by Stephanie C.Y. Chan and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2205.05055)\n- [TeX Source](https://arxiv.org/src/2205.05055)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2205.05055&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2205.05055&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2022-05](https://arxiv.org/list/cs.LG/2022-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2205.05055?context=cs) [cs.AI](https://arxiv.org/abs/2205.05055?context=cs.AI) [cs.CL](https://arxiv.org/abs/2205.05055?context=cs.CL)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2205.05055)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2205.05055)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2205.05055)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2205.05055) \\|\n[Disable MathJ",
          "original_query": "Data distributional properties drive emergent in\u2011context learning in transformers",
          "cleaned_query": "Data distributional properties drive emergent in\u2011context learning in transformers"
        },
        {
          "success": true,
          "title": "Attention layers provably solve single-location regression - arXiv",
          "url": "https://arxiv.org/abs/2410.01537",
          "content": "# Statistics > Machine Learning\n\n**arXiv:2410.01537** (stat)\n\n\\[Submitted on 2 Oct 2024 ( [v1](https://arxiv.org/abs/2410.01537v1)), last revised 25 Feb 2025 (this version, v2)\\]\n\n# Title:Attention layers provably solve single-location regression\n\nAuthors: [Pierre Marion](https://arxiv.org/search/stat?searchtype=author&query=Marion,+P), [Rapha\u00ebl Berthier](https://arxiv.org/search/stat?searchtype=author&query=Berthier,+R), [G\u00e9rard Biau](https://arxiv.org/search/stat?searchtype=author&query=Biau,+G), [Claire Boyer](https://arxiv.org/search/stat?searchtype=author&query=Boyer,+C)\n\nView a PDF of the paper titled Attention layers provably solve single-location regression, by Pierre Marion and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2410.01537) [HTML (experimental)](https://arxiv.org/html/2410.01537v2)\n\n> Abstract:Attention-based models, such as Transformer, excel across various tasks but lack a comprehensive theoretical understanding, especially regarding token-wise sparsity and internal linear representations. To address this gap, we introduce the single-location regression task, where only one token in a sequence determines the output, and its position is a latent random variable, retrievable via a linear projection of the input. To solve this task, we propose a dedicated predictor, which turns out to be a simplified version of a non-linear self-attention layer. We study its theoretical properties, by showing its asymptotic Bayes optimality and analyzing its training dynamics. In particular, despite the non-convex nature of the problem, the predictor effectively learns the underlying structure. This work highlights the capacity of attention mechanisms to handle sparse token information and internal linear structures.\n\n| | |\n| --- | --- |\n| Comments: | 42 pages, 10 figures. Accepted to ICLR 2025 |\n| Subjects: | Machine Learning (stat.ML); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2410.01537](https://arxiv.org/abs/2410.01537) \\[stat.ML\\] |\n| | (or [arXiv:2410.01537v2](https://arxiv.org/abs/2410.01537v2) \\[stat.ML\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2410.01537](https://doi.org/10.48550/arXiv.2410.01537) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Pierre Marion \\[ [view email](https://arxiv.org/show-email/ca21f5a3/2410.01537)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2410.01537v1)**\nWed, 2 Oct 2024 13:28:02 UTC (1,501 KB)\n\n**\\[v2\\]**\nTue, 25 Feb 2025 23:15:41 UTC (2,165 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Attention layers provably solve single-location regression, by Pierre Marion and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2410.01537)\n- [HTML (experimental)](https://arxiv.org/html/2410.01537v2)\n- [TeX Source](https://arxiv.org/src/2410.01537)\n- [Other Formats](https://arxiv.org/format/2410.01537)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2410.01537&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2410.01537&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2024-10](https://arxiv.org/list/stat.ML/2024-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2410.01537?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2410.01537?context=cs.LG)\n\n[stat](https://arxiv.org/abs/2410.01537?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2410.01537)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2410.01537)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2410.01537)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2410.01537&description=Attention layers provably solve single-location regression) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2410.01537&title=Attention layers provably solve single-location regression)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2410.01537) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Attention layers provably solve single\u2011location regression",
          "cleaned_query": "Attention layers provably solve single\u2011location regression"
        },
        {
          "success": true,
          "title": "[2410.07041] Emergent properties with repeated examples - arXiv",
          "url": "https://arxiv.org/abs/2410.07041",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2410.07041** (cs)\n\n\\[Submitted on 9 Oct 2024\\]\n\n# Title:Emergent properties with repeated examples\n\nAuthors: [Fran\u00e7ois Charton](https://arxiv.org/search/cs?searchtype=author&query=Charton,+F), [Julia Kempe](https://arxiv.org/search/cs?searchtype=author&query=Kempe,+J)\n\nView a PDF of the paper titled Emergent properties with repeated examples, by Fran\\\\c{c}ois Charton and Julia Kempe\n\n[View PDF](https://arxiv.org/pdf/2410.07041) [HTML (experimental)](https://arxiv.org/html/2410.07041v1)\n\n> Abstract:We study the performance of transformers as a function of the number of repetitions of training examples with algorithmically generated datasets. On three problems of mathematics: the greatest common divisor, modular multiplication, and matrix eigenvalues, we show that for a fixed number of training steps, models trained on smaller sets of repeated examples outperform models trained on larger sets of single-use examples. We also demonstrate that two-set training - repeated use of a small random subset of examples, along normal sampling on the rest of the training set - provides for faster learning and better performance. This highlights that the benefits of repetition can outweigh those of data diversity. These datasets and problems provide a controlled setting to shed light on the still poorly understood interplay between generalization and memorization in deep learning.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2410.07041](https://arxiv.org/abs/2410.07041) \\[cs.LG\\] |\n| | (or [arXiv:2410.07041v1](https://arxiv.org/abs/2410.07041v1) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2410.07041](https://doi.org/10.48550/arXiv.2410.07041) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Fran\u00e7ois Charton \\[ [view email](https://arxiv.org/show-email/80175a4c/2410.07041)\\]\n\n**\\[v1\\]**\nWed, 9 Oct 2024 16:28:23 UTC (3,077 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Emergent properties with repeated examples, by Fran\\\\c{c}ois Charton and Julia Kempe\n\n- [View PDF](https://arxiv.org/pdf/2410.07041)\n- [HTML (experimental)](https://arxiv.org/html/2410.07041v1)\n- [TeX Source](https://arxiv.org/src/2410.07041)\n- [Other Formats](https://arxiv.org/format/2410.07041)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2410.07041&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2410.07041&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2024-10](https://arxiv.org/list/cs.LG/2024-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2410.07041?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2410.07041?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2410.07041)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2410.07041)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2410.07041)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2410.07041&description=Emergent properties with repeated examples) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2410.07041&title=Emergent properties with repeated examples)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2410.07041) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Emergent properties with repeated examples",
          "cleaned_query": "Emergent properties with repeated examples"
        }
      ],
      "generated_ideas": [
        "1. **Provable Induction Heads for Zipfian, Bursty Streams**  \n   Build a theoretical model that extends single-location regression (Paper 5) to Zipfian and bursty token streams (Paper 4), proving when self-attention learns an \u201cinduction-head-like\u201d copy rule (Paper 2). Derive sample-complexity and convergence guarantees showing how burstiness and rare-class mass change the phase transition point for induction behavior.",
        "**Two-Set Curriculum to Control the Induction-Head Phase Transition**  \n   Design and test a training curriculum that mixes (i) a small repeatedly sampled subset and (ii) a large normally sampled set (Paper 6), explicitly targeting earlier or later emergence of induction heads (Paper 2). Measure causal effects via head ablations and training-loss \u201cbumps,\u201d and map how repetition rate shifts in-context learning vs in-weights learning trade-offs (Paper 4).",
        "**Multi-Location Regression: Sparse-Set Retrieval Beyond a Single Token**  \n   Generalize single-location regression (Paper 5) to *k*-location regression where the label depends on a sparse set of latent tokens with linear projections. Create a family of synthetic benchmarks and prove when multi-head attention recovers all *k* locations, including conditions where different heads specialize to different latent slots (Paper 1).",
        "**Induction-Head Development Predicted by Deep-Linear Learning Dynamics**  \n   Construct a simplified Transformer surrogate where attention updates are approximated by deep linear dynamics (Paper 3) to analytically predict plateaus and rapid transitions corresponding to induction-head formation (Paper 2). Fit the theory to real training curves by extracting an \u201ceffective singular spectrum\u201d of attention maps over time, then validate predictions on controlled data distributions (Paper 4).",
        "**Mechanistic Link Between Repetition and Copy-Circuit Formation**  \n   Hypothesize and test that repeated examples (Paper 6) increase the signal-to-noise ratio of the exact key-query alignment needed for induction circuits (Paper 2). Implement controlled experiments varying repetition while holding token marginal frequencies fixed, and use causal interventions (e.g., patching/ablating candidate heads) to quantify whether repetition preferentially strengthens induction vs other heuristics.",
        "**Dynamic-Meaning Training to Induce Induction Heads in Non-Language Domains**  \n   Extend \u201cdynamic interpretation\u201d conditions (Paper 4) to algorithmic tasks (e.g., modular arithmetic, eigenvalue prompts; Paper 6) by randomizing symbol-to-value mappings per sequence. Evaluate whether such dynamic semantics reliably elicit induction heads (Paper 2) and whether the resulting models transfer better to unseen symbol systems than i.i.d.-trained baselines.",
        "**Architectural Minimality Study: How Much Transformer Is Needed for In-Context Learning?**  \n   Starting from attention-only cores (Paper 1) and the simplified predictor in single-location regression (Paper 5), systematically remove components (MLPs, layernorm, positional encodings variants) to identify minimal circuits that still exhibit induction-head-driven in-context learning (Paper 2). Produce a component-by-component causal map linking each architectural element to the onset and robustness of induction.",
        "**Induction Heads Under Distribution Shift: Burstiness vs Uniformization at Test Time**  \n   Create a benchmark where training is bursty/Zipfian (Paper 4) but evaluation ranges from bursty to fully uniform i.i.d., to test whether induction heads (Paper 2) are a brittle adaptation to specific temporal clustering. Use mechanistic diagnostics (attention pattern templates, head patching) to determine whether performance drops correspond to failures of the learned \u201ccopy-from-previous-match\u201d algorithm.",
        "**Optimization Interventions to Prefer In-Context Over In-Weights Learning**  \n   Develop training-time regularizers that bias attention toward token-to-token algorithmic reuse (induction) rather than memorization in weights, leveraging the observed trade-off (Paper 4). Examples: penalize attention entropy collapse outside repeated-pattern contexts, or explicitly reward attention that links repeated key-query matches; evaluate impact on induction-head prevalence (Paper 2) and on algorithmic generalization with repetition (Paper 6).",
        "**A Unified Synthetic Suite: From Single-Location Regression to Algorithmic ICL**  \n   Build a standardized suite of tasks spanning (i) single-location regression (Paper 5), (ii) sparse multi-location retrieval, and (iii) repeated-example algorithmic problems (Paper 6), each parameterized by burstiness/Zipfianity/dynamic meaning (Paper 4). Release protocols and metrics that quantify emergence points (loss bumps), mechanistic signatures (induction heads), and generalization modes, enabling reproducible comparisons across Transformer variants (Paper 1) and training regimes."
      ],
      "generated_ideas_raw": "1. **Provable Induction Heads for Zipfian, Bursty Streams**  \n   Build a theoretical model that extends single-location regression (Paper 5) to Zipfian and bursty token streams (Paper 4), proving when self-attention learns an \u201cinduction-head-like\u201d copy rule (Paper 2). Derive sample-complexity and convergence guarantees showing how burstiness and rare-class mass change the phase transition point for induction behavior.\n\n2. **Two-Set Curriculum to Control the Induction-Head Phase Transition**  \n   Design and test a training curriculum that mixes (i) a small repeatedly sampled subset and (ii) a large normally sampled set (Paper 6), explicitly targeting earlier or later emergence of induction heads (Paper 2). Measure causal effects via head ablations and training-loss \u201cbumps,\u201d and map how repetition rate shifts in-context learning vs in-weights learning trade-offs (Paper 4).\n\n3. **Multi-Location Regression: Sparse-Set Retrieval Beyond a Single Token**  \n   Generalize single-location regression (Paper 5) to *k*-location regression where the label depends on a sparse set of latent tokens with linear projections. Create a family of synthetic benchmarks and prove when multi-head attention recovers all *k* locations, including conditions where different heads specialize to different latent slots (Paper 1).\n\n4. **Induction-Head Development Predicted by Deep-Linear Learning Dynamics**  \n   Construct a simplified Transformer surrogate where attention updates are approximated by deep linear dynamics (Paper 3) to analytically predict plateaus and rapid transitions corresponding to induction-head formation (Paper 2). Fit the theory to real training curves by extracting an \u201ceffective singular spectrum\u201d of attention maps over time, then validate predictions on controlled data distributions (Paper 4).\n\n5. **Mechanistic Link Between Repetition and Copy-Circuit Formation**  \n   Hypothesize and test that repeated examples (Paper 6) increase the signal-to-noise ratio of the exact key-query alignment needed for induction circuits (Paper 2). Implement controlled experiments varying repetition while holding token marginal frequencies fixed, and use causal interventions (e.g., patching/ablating candidate heads) to quantify whether repetition preferentially strengthens induction vs other heuristics.\n\n6. **Dynamic-Meaning Training to Induce Induction Heads in Non-Language Domains**  \n   Extend \u201cdynamic interpretation\u201d conditions (Paper 4) to algorithmic tasks (e.g., modular arithmetic, eigenvalue prompts; Paper 6) by randomizing symbol-to-value mappings per sequence. Evaluate whether such dynamic semantics reliably elicit induction heads (Paper 2) and whether the resulting models transfer better to unseen symbol systems than i.i.d.-trained baselines.\n\n7. **Architectural Minimality Study: How Much Transformer Is Needed for In-Context Learning?**  \n   Starting from attention-only cores (Paper 1) and the simplified predictor in single-location regression (Paper 5), systematically remove components (MLPs, layernorm, positional encodings variants) to identify minimal circuits that still exhibit induction-head-driven in-context learning (Paper 2). Produce a component-by-component causal map linking each architectural element to the onset and robustness of induction.\n\n8. **Induction Heads Under Distribution Shift: Burstiness vs Uniformization at Test Time**  \n   Create a benchmark where training is bursty/Zipfian (Paper 4) but evaluation ranges from bursty to fully uniform i.i.d., to test whether induction heads (Paper 2) are a brittle adaptation to specific temporal clustering. Use mechanistic diagnostics (attention pattern templates, head patching) to determine whether performance drops correspond to failures of the learned \u201ccopy-from-previous-match\u201d algorithm.\n\n9. **Optimization Interventions to Prefer In-Context Over In-Weights Learning**  \n   Develop training-time regularizers that bias attention toward token-to-token algorithmic reuse (induction) rather than memorization in weights, leveraging the observed trade-off (Paper 4). Examples: penalize attention entropy collapse outside repeated-pattern contexts, or explicitly reward attention that links repeated key-query matches; evaluate impact on induction-head prevalence (Paper 2) and on algorithmic generalization with repetition (Paper 6).\n\n10. **A Unified Synthetic Suite: From Single-Location Regression to Algorithmic ICL**  \n   Build a standardized suite of tasks spanning (i) single-location regression (Paper 5), (ii) sparse multi-location retrieval, and (iii) repeated-example algorithmic problems (Paper 6), each parameterized by burstiness/Zipfianity/dynamic meaning (Paper 4). Release protocols and metrics that quantify emergence points (loss bumps), mechanistic signatures (induction heads), and generalization modes, enabling reproducible comparisons across Transformer variants (Paper 1) and training regimes.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "1. **Provable Induction Heads for Zipfian, Bursty Streams**  \n   Build a theoretical model that extends single-location regression (Paper 5) to Zipfian and bursty token streams (Paper 4), proving when",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "**Two-Set Curriculum to Control the Induction-Head Phase Transition**  \n   Design and test a training curriculum that mixes (i) a small repeatedly sampled subset and (ii) a large normally sampled set ",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "**Multi-Location Regression: Sparse-Set Retrieval Beyond a Single Token**  \n   Generalize single-location regression (Paper 5) to *k*-location regression where the label depends on a sparse set of lat",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "**Induction-Head Development Predicted by Deep-Linear Learning Dynamics**  \n   Construct a simplified Transformer surrogate where attention updates are approximated by deep linear dynamics (Paper 3) t",
          "is_match": true
        },
        {
          "idea_idx": 4,
          "idea_text": "**Mechanistic Link Between Repetition and Copy-Circuit Formation**  \n   Hypothesize and test that repeated examples (Paper 6) increase the signal-to-noise ratio of the exact key-query alignment needed",
          "is_match": true
        },
        {
          "idea_idx": 5,
          "idea_text": "**Dynamic-Meaning Training to Induce Induction Heads in Non-Language Domains**  \n   Extend \u201cdynamic interpretation\u201d conditions (Paper 4) to algorithmic tasks (e.g., modular arithmetic, eigenvalue prom",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "**Architectural Minimality Study: How Much Transformer Is Needed for In-Context Learning?**  \n   Starting from attention-only cores (Paper 1) and the simplified predictor in single-location regression",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "**Induction Heads Under Distribution Shift: Burstiness vs Uniformization at Test Time**  \n   Create a benchmark where training is bursty/Zipfian (Paper 4) but evaluation ranges from bursty to fully un",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "**Optimization Interventions to Prefer In-Context Over In-Weights Learning**  \n   Develop training-time regularizers that bias attention toward token-to-token algorithmic reuse (induction) rather than",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "**A Unified Synthetic Suite: From Single-Location Regression to Algorithmic ICL**  \n   Build a standardized suite of tasks spanning (i) single-location regression (Paper 5), (ii) sparse multi-location",
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 18,
      "paper_title": "ControlFusion: A Controllable Image Fusion Network with Language-Vision Degradation Prompts",
      "contribution": "Introduce a prompt-modulated restoration-and-fusion network trained on physically simulated composite degradations that uses language-vision degradation prompts plus a spatial-frequency visual adapter to produce controllable, degradation-robust infrared-visible image fusion.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 13028,
      "output_tokens": 968,
      "predecessor_details": [
        {
          "success": true,
          "title": "Learning Transferable Visual Models From Natural Language ...",
          "url": "https://arxiv.org/abs/2103.00020",
          "content": "[2103.00020] Learning Transferable Visual Models From Natural Language Supervision\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2103.00020\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2103.00020**(cs)\n[Submitted on 26 Feb 2021]\n# Title:Learning Transferable Visual Models From Natural Language Supervision\nAuthors:[Alec Radford](https://arxiv.org/search/cs?searchtype=author&amp;query=Radford,+A),[Jong Wook Kim](https://arxiv.org/search/cs?searchtype=author&amp;query=Kim,+J+W),[Chris Hallacy](https://arxiv.org/search/cs?searchtype=author&amp;query=Hallacy,+C),[Aditya Ramesh](https://arxiv.org/search/cs?searchtype=author&amp;query=Ramesh,+A),[Gabriel Goh](https://arxiv.org/search/cs?searchtype=author&amp;query=Goh,+G),[Sandhini Agarwal](https://arxiv.org/search/cs?searchtype=author&amp;query=Agarwal,+S),[Girish Sastry](https://arxiv.org/search/cs?searchtype=author&amp;query=Sastry,+G),[Amanda Askell](https://arxiv.org/search/cs?searchtype=author&amp;query=Askell,+A),[Pamela Mishkin](https://arxiv.org/search/cs?searchtype=author&amp;query=Mishkin,+P),[Jack Clark](https://arxiv.org/search/cs?searchtype=author&amp;query=Clark,+J),[Gretchen Krueger](https://arxiv.org/search/cs?searchtype=author&amp;query=Krueger,+G),[Ilya Sutskever](https://arxiv.org/search/cs?searchtype=author&amp;query=Sutskever,+I)\nView a PDF of the paper titled Learning Transferable Visual Models From Natural Language Supervision, by Alec Radford and 11 other authors\n[View PDF](https://arxiv.org/pdf/2103.00020)> > Abstract:\n> State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at [> this https URL\n](https://github.com/OpenAI/CLIP)> . Subjects:|Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)|\nCite as:|[arXiv:2103.00020](https://arxiv.org/abs/2103.00020)[cs.CV]|\n|(or[arXiv:2103.00020v1](https://arxiv.org/abs/2103.00020v1)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2103.00020](https://doi.org/10.48550/arXiv.2103.00020)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jong Wook Kim [[view email](https://arxiv.org/show-email/6c157f7d/2103.00020)]\n**[v1]**Fri, 26 Feb 2021 19:04:58 UTC (6,174 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Learning Transferable Visual Models From Natural Language Supervision, by Alec Radford and 11 other authors\n* [View PDF](https://arxiv.org/pdf/2103.00020)\n* [TeX Source](https://arxiv.org/src/2103.00020)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2103.00020&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2103.00020&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2021-03](https://arxiv.org/list/cs.CV/2021-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2103.00020?context=cs)\n[cs.LG](https://arxiv.org/abs/2103.00020?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2103.00020)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2103.00020)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2103.00020)\n### [16 blog links](https://arxiv.org/tb/2103.00020)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2103.html#abs-2103-00020)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2103-00020)\n[Alec Radford]()\n[Jong Wook Kim]()\n[Aditya Ramesh]()\n[Gabriel Goh]()\n[Girish Sastry]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommende",
          "original_query": "Learning transferable visual models from natural language supervision",
          "cleaned_query": "Learning transferable visual models from natural language supervision"
        },
        {
          "success": true,
          "title": "Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware ...",
          "url": "https://arxiv.org/abs/2403.16387",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2403.16387** (cs)\n\n\\[Submitted on 25 Mar 2024\\]\n\n# Title:Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion\n\nAuthors: [Xunpeng Yi](https://arxiv.org/search/cs?searchtype=author&query=Yi,+X), [Han Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+H), [Hao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+H), [Linfeng Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+L), [Jiayi Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+J)\n\nView a PDF of the paper titled Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion, by Xunpeng Yi and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2403.16387) [HTML (experimental)](https://arxiv.org/html/2403.16387v1)\n\n> Abstract:Image fusion aims to combine information from different source images to create a comprehensively representative image. Existing fusion methods are typically helpless in dealing with degradations in low-quality source images and non-interactive to multiple subjective and objective needs. To solve them, we introduce a novel approach that leverages semantic text guidance image fusion model for degradation-aware and interactive image fusion task, termed as Text-IF. It innovatively extends the classical image fusion to the text guided image fusion along with the ability to harmoniously address the degradation and interaction issues during fusion. Through the text semantic encoder and semantic interaction fusion decoder, Text-IF is accessible to the all-in-one infrared and visible image degradation-aware processing and the interactive flexible fusion outcomes. In this way, Text-IF achieves not only multi-modal image fusion, but also multi-modal information fusion. Extensive experiments prove that our proposed text guided image fusion strategy has obvious advantages over SOTA methods in the image fusion performance and degradation treatment. The code is available at [this https URL](https://github.com/XunpengYi/Text-IF).\n\n| | |\n| --- | --- |\n| Comments: | Accepted by CVPR 2024 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2403.16387](https://arxiv.org/abs/2403.16387) \\[cs.CV\\] |\n| | (or [arXiv:2403.16387v1](https://arxiv.org/abs/2403.16387v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2403.16387](https://doi.org/10.48550/arXiv.2403.16387) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Xunpeng Yi \\[ [view email](https://arxiv.org/show-email/9573e6fd/2403.16387)\\]\n\n**\\[v1\\]**\nMon, 25 Mar 2024 03:06:45 UTC (12,949 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion, by Xunpeng Yi and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2403.16387)\n- [HTML (experimental)](https://arxiv.org/html/2403.16387v1)\n- [TeX Source](https://arxiv.org/src/2403.16387)\n- [Other Formats](https://arxiv.org/format/2403.16387)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2403.16387&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2403.16387&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-03](https://arxiv.org/list/cs.CV/2024-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2403.16387?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2403.16387)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2403.16387)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2403.16387)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2403.16387&description=Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2403.16387&title=Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2403.16387) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Text-IF: Leveraging semantic text guidance for degradation-aware and interactive image fusion",
          "cleaned_query": "Text-IF: Leveraging semantic text guidance for degradation-aware and interactive image fusion"
        },
        {
          "success": true,
          "title": "PromptIR: Prompting for All-in-One Blind Image Restoration - arXiv",
          "url": "https://arxiv.org/abs/2306.13090",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2306.13090** (cs)\n\n\\[Submitted on 22 Jun 2023\\]\n\n# Title:PromptIR: Prompting for All-in-One Blind Image Restoration\n\nAuthors: [Vaishnav Potlapalli](https://arxiv.org/search/cs?searchtype=author&query=Potlapalli,+V), [Syed Waqas Zamir](https://arxiv.org/search/cs?searchtype=author&query=Zamir,+S+W), [Salman Khan](https://arxiv.org/search/cs?searchtype=author&query=Khan,+S), [Fahad Shahbaz Khan](https://arxiv.org/search/cs?searchtype=author&query=Khan,+F+S)\n\nView a PDF of the paper titled PromptIR: Prompting for All-in-One Blind Image Restoration, by Vaishnav Potlapalli and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2306.13090)\n\n> Abstract:Image restoration involves recovering a high-quality clean image from its degraded version. Deep learning-based methods have significantly improved image restoration performance, however, they have limited generalization ability to different degradation types and levels. This restricts their real-world application since it requires training individual models for each specific degradation and knowing the input degradation type to apply the relevant model. We present a prompt-based learning approach, PromptIR, for All-In-One image restoration that can effectively restore images from various types and levels of degradation. In particular, our method uses prompts to encode degradation-specific information, which is then used to dynamically guide the restoration network. This allows our method to generalize to different degradation types and levels, while still achieving state-of-the-art results on image denoising, deraining, and dehazing. Overall, PromptIR offers a generic and efficient plugin module with few lightweight prompts that can be used to restore images of various types and levels of degradation with no prior information on the corruptions present in the image. Our code and pretrained models are available here: [this https URL](https://github.com/va1shn9v/PromptIR)\n\n| | |\n| --- | --- |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2306.13090](https://arxiv.org/abs/2306.13090) \\[cs.CV\\] |\n| | (or [arXiv:2306.13090v1](https://arxiv.org/abs/2306.13090v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2306.13090](https://doi.org/10.48550/arXiv.2306.13090) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Vaishnav Potlapalli \\[ [view email](https://arxiv.org/show-email/e858f0dc/2306.13090)\\]\n\n**\\[v1\\]**\nThu, 22 Jun 2023 17:59:52 UTC (39,008 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled PromptIR: Prompting for All-in-One Blind Image Restoration, by Vaishnav Potlapalli and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2306.13090)\n- [TeX Source](https://arxiv.org/src/2306.13090)\n- [Other Formats](https://arxiv.org/format/2306.13090)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2306.13090&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2306.13090&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2023-06](https://arxiv.org/list/cs.CV/2023-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2306.13090?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2306.13090)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2306.13090)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2306.13090)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2306.13090&description=PromptIR: Prompting for All-in-One Blind Image Restoration) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2306.13090&title=PromptIR: Prompting for All-in-One Blind Image Restoration)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2306.13090) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "PromptIR: Prompting for all-in-one image restoration",
          "cleaned_query": "PromptIR: Prompting for all-in-one image restoration"
        },
        {
          "success": true,
          "title": "DRMF: Degradation-Robust Multi-Modal Image Fusion via ...",
          "url": "https://dl.acm.org/doi/10.1145/3664647.3681064",
          "content": "DRMF: Degradation-Robust Multi-Modal Image Fusion via Composable Diffusion Prior | Proceedings of the 32nd ACM International Conference on Multimedia[skip to main content](#skip-to-main-content)\n[](#global-menu)\nSearch ACM Digital Library\nSearchSearch\n[Advanced Search](https://dl.acm.org/search/advanced)\n10.1145/3664647.3681064acmconferencesArticle/Chapter ViewAbstractPublication PagesmmConference Proceedingsconference-collections\n[mm](#)\n**## Export Citations\nSelect Citation formatBibTeXEndNoteACM Ref**\n* Please download or close your previous search result export first before starting a new bulk export.\nPreview is not available.\nBy clicking download,**a status dialog**will open to start the export process. The process may take**a few minutes**but once it finishes a file will be downloadable from your browser. You may continue to browse the DL while the export process is in progress.\n* ```\n```\n* [Download citation**](javascript:void(0))\n* [Copy citation**](javascript:void(0))\nresearch-article\nShare on\n* **\n* **\n* **\n* **\n* **\n# DRMF: Degradation-Robust Multi-Modal Image Fusion via Composable Diffusion Prior\nAuthors:[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)LinfengTang](#artseq-00001),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)YuxinDeng](#artseq-00002),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)XunpengYi](#artseq-00003),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)QinglongYan](#artseq-00004),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)YixuanYuan](#artseq-00005),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)JiayiMa](#artseq-00006)[Authors Info &amp; Claims](#tab-contributors)\n[MM '24: Proceedings of the 32nd ACM International Conference on Multimedia](https://dl.acm.org/doi/proceedings/10.1145/3664647)\nPages8546-8555\n[https://doi.org/10.1145/3664647.3681064](https://doi.org/10.1145/3664647.3681064)\nPublished:28 October 2024[Publication History](#core-history)[](#)\n**15citation**2,871Downloads\nMetrics\n[\nTotal Citations15\n](#tab-citations)[\nTotal Downloads2,871\n](#tab-metrics-inner)\nLast 12 Months2,727\nLast 6 weeks244\n**Get Citation Alerts\n**## New Citation Alert added!\nThis alert has been successfully added and will be sent to:\nYou will be notified whenever a record that you have chosen has been cited.\nTo manage your alert preferences, click on the button below.\n[Manage my Alerts](https://dl.acm.org/action/showPreferences?menuTab=Alerts)\n**## New Citation Alert!\nPlease[log in to your account](https://dl.acm.org/action/showLogin?redirectUri=/doi/10.1145/3664647.3681064)\n**\n**\n[**Get Access](#core-collateral-purchase-access)\n**Contents\n## Abstract\nExisting multi-modal image fusion algorithms are typically designed for high-quality images and fail to tackle degradation (*e.g.,*low light, low resolution, and noise), which restricts image fusion from unleashing the potential in practice. In this work, we present**D**egradation-**R**obust**M**ulti-modality image**F**usion (**DRMF**), leveraging the powerful generative properties of diffusion models to counteract various degradations during image fusion. Our critical insight is that generative diffusion models driven by different modalities and degradation are inherently complementary during the denoising process. Specifically, we pre-train multiple degradation-robust conditional diffusion models for different modalities to handle degradations. Subsequently, the diffusion priori combination module is devised to integrate generative priors from pre-trained uni-modal models, enabling effective multi-modal image fusion. Extensive experiments demonstrate that DRMF excels in infrared-visible and medical image fusion, even under complex degradations. Our code is available at https://github.com/Linfeng-Tang/DRMF.\n## Supplemental Material\nMP4 File - DRMF: Degradation-Robust Multi-Modal Image Fusion via Composable Diffusion Prior\nVideo presentation of our conference paper, DRMF: Degradation-Robust Multi-Modal Image Fusion via Composable Diffusion Prior, presented at ACM MM24. Video showing the motivation, methodology, and experimentation of our DRMF.\n* [Download](https://dl.acm.org/doi/suppl/10.1145/3664647.3681064/suppl_file/ftp2367-video.mp4)\n* 87.35 MB\n## References\n[1]\nFanglin Bao, Xueji Wang, Shree Hari Sureshbabu, Gautam Sreekumar, Liping Yang, Vaneet Aggarwal, Vishnu N Boddeti, and Zubin Jacob. 2023. Heat-assisted detection and ranging. Nature, Vol. 619, 7971 (2023), 743--748.\n[Google Scholar](https://scholar.google.com/scholar?q=Fanglin+Bao,+Xueji+Wang,+Shree+Hari+Sureshbabu,+Gautam+Sreekumar,+Liping+Yang,+Vaneet+Aggarwal,+Vishnu+N+Boddeti,+and+Zubin+Jacob.+2023.+Heat-assisted+detection+and+ranging.+Nature,+Vol.+619,+7971+(2023),+743--748.)\n[2]\nZheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xin Yuan, et al. 2022. Cross aggregation transformer for image restoration. Advances in Neural Information Processing Systems, Vol. 35 (2022), 25478--25490.\n[Google Scholar](https://scholar.google.com/scholar?q=Zheng+Chen,+Yulun+Zhang,+Jinjin+Gu,+Linghe+Kong,+Xin+Yuan,+et+al.+2022.+Cross+aggregation+transformer+for+image+restoration.+Advances+in+Neural+Information+Processing+Systems,+Vol.+35+(2022),+25478--25490.)\n[3]\nPrafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, Vol. 34 (2021), 8780--8794.\n[Google Scholar](https://scholar.google.com/scholar?q=Prafulla+Dhariwal+and+Alexander+Nichol.+2021.+Diffusion+models+beat+gans+on+image+synthesis.+Advances+in+Neural+Information+Processing+Systems,+Vol.+34+(2021),+8780--8794.)\n[4]\nAhmet M Eskicioglu and Paul S Fisher. 1995. Image quality measures and their performance. IEEE Transactions on Communications, Vol. 43, 12 (1995), 2959--2965.\n[Crossref](https://doi.org/10.1109/26.477498)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1109/26.477498)\n[5]\nBen Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, and Bo Dai. 2023. Generative diffusion prior for unified image restoration and enhancement. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 9935--9946.\n[Crossref](https://doi.org/10.1109/CVPR52729.2023.00958)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1109/CVPR52729.2023.00958)\n[6]\nYu Han, Yunze Cai, Yin Cao, and Xiaoming Xu. 2013. A new image fusion performance metric based on visual information fidelity. Information Fusion, Vol. 14, 2 (2013), 127--135.\n[Digital Library](https://dl.acm.org/doi/10.1016/j.inffus.2011.08.002)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1016/j.inffus.2011.08.002)\n[7]\nHaithem Hermessi, Olfa Mourali, and Ezzeddine Zagrouba. 2021. Multimodal medical image fusion review: Theoretical background and recent advances. Signal Processing, Vol. 183 (2021), 108036.\n[Crossref](https://doi.org/10.1016/j.sigpro.2021.108036)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1016/j.sigpro.2021.108036)\n[8]\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, Vol. 33 (2020), 6840--6851.\n[Google Scholar](https://scholar.google.com/scholar?q=Jonathan+Ho,+Ajay+Jain,+and+Pieter+Abbeel.+2020.+Denoising+diffusion+probabilistic+models.+Advances+in+Neural+Information+Processing+Systems,+Vol.+33+(2020),+6840--6851.)\n[9]\nZiqi Huang, Kelvin CK Chan, Yuming Jiang, and Ziwei Liu. 2023. Collaborative diffusion for multi-modal face generation and editing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 6080--6090.\n[Crossref](https://doi.org/10.1109/CVPR52729.2023.00589)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1109/CVPR52729.2023.00589)\n[10]\nDeepak Kumar Jain, Xudong Zhao, Germ\u00e1n Gonz\u00e1lez-Almagro, Chenquan",
          "original_query": "DRMF: Degradation-robust multi-modal image fusion via composable diffusion prior",
          "cleaned_query": "DRMF: Degradation-robust multi-modal image fusion via composable diffusion prior"
        },
        {
          "success": true,
          "title": "Text-DiFuse: Interactive Multi-Modal Image Fusion Framework",
          "url": "https://arxiv.org/abs/2410.23905",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2410.23905** (cs)\n\n\\[Submitted on 31 Oct 2024\\]\n\n# Title:Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model\n\nAuthors: [Hao Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+H), [Lei Cao](https://arxiv.org/search/cs?searchtype=author&query=Cao,+L), [Jiayi Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+J)\n\nView a PDF of the paper titled Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model, by Hao Zhang and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2410.23905) [HTML (experimental)](https://arxiv.org/html/2410.23905v1)\n\n> Abstract:Existing multi-modal image fusion methods fail to address the compound degradations presented in source images, resulting in fusion images plagued by noise, color bias, improper exposure, \\\\textit{etc}. Additionally, these methods often overlook the specificity of foreground objects, weakening the salience of the objects of interest within the fused images. To address these challenges, this study proposes a novel interactive multi-modal image fusion framework based on the text-modulated diffusion model, called Text-DiFuse. First, this framework integrates feature-level information integration into the diffusion process, allowing adaptive degradation removal and multi-modal information fusion. This is the first attempt to deeply and explicitly embed information fusion within the diffusion process, effectively addressing compound degradation in image fusion. Second, by embedding the combination of the text and zero-shot location model into the diffusion fusion process, a text-controlled fusion re-modulation strategy is developed. This enables user-customized text control to improve fusion performance and highlight foreground objects in the fused images. Extensive experiments on diverse public datasets show that our Text-DiFuse achieves state-of-the-art fusion performance across various scenarios with complex degradation. Moreover, the semantic segmentation experiment validates the significant enhancement in semantic performance achieved by our text-controlled fusion re-modulation strategy. The code is publicly available at [this https URL](https://github.com/Leiii-Cao/Text-DiFuse).\n\n| | |\n| --- | --- |\n| Comments: | Accepted by the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2410.23905](https://arxiv.org/abs/2410.23905) \\[cs.CV\\] |\n| | (or [arXiv:2410.23905v1](https://arxiv.org/abs/2410.23905v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2410.23905](https://doi.org/10.48550/arXiv.2410.23905) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Hao Zhang \\[ [view email](https://arxiv.org/show-email/2cceca29/2410.23905)\\]\n\n**\\[v1\\]**\nThu, 31 Oct 2024 13:10:50 UTC (22,971 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model, by Hao Zhang and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2410.23905)\n- [HTML (experimental)](https://arxiv.org/html/2410.23905v1)\n- [TeX Source](https://arxiv.org/src/2410.23905)\n- [Other Formats](https://arxiv.org/format/2410.23905)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2410.23905&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2410.23905&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-10](https://arxiv.org/list/cs.CV/2024-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2410.23905?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2410.23905)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2410.23905)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2410.23905)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2410.23905&description=Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2410.23905&title=Text-DiFuse: An Interactive Multi-Modal Image Fusion Framework based on Text-modulated Diffusion Model)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2410.23905) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Text-diffuse: An interactive multi-modal image fusion framework based on text-modulated diffusion model",
          "cleaned_query": "Text-diffuse: An interactive multi-modal image fusion framework based on text-modulated diffusion model"
        },
        {
          "success": true,
          "title": "Denoising Diffusion Model for Multi-Modality Image Fusion",
          "url": "https://arxiv.org/abs/2303.06840",
          "content": "[2303.06840] DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2303.06840\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2303.06840**(cs)\n[Submitted on 13 Mar 2023 ([v1](https://arxiv.org/abs/2303.06840v1)), last revised 22 Aug 2023 (this version, v2)]\n# Title:DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion\nAuthors:[Zixiang Zhao](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+Z),[Haowen Bai](https://arxiv.org/search/cs?searchtype=author&amp;query=Bai,+H),[Yuanzhi Zhu](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+Y),[Jiangshe Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+J),[Shuang Xu](https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+S),[Yulun Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Y),[Kai Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+K),[Deyu Meng](https://arxiv.org/search/cs?searchtype=author&amp;query=Meng,+D),[Radu Timofte](https://arxiv.org/search/cs?searchtype=author&amp;query=Timofte,+R),[Luc Van Gool](https://arxiv.org/search/cs?searchtype=author&amp;query=Van+Gool,+L)\nView a PDF of the paper titled DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion, by Zixiang Zhao and 9 other authors\n[View PDF](https://arxiv.org/pdf/2303.06840)> > Abstract:\n> Multi-modality image fusion aims to combine different modalities to produce fused images that retain the complementary features of each modality, such as functional highlights and texture details. To leverage strong generative priors and address challenges such as unstable training and lack of interpretability for GAN-based generative methods, we propose a novel fusion algorithm based on the denoising diffusion probabilistic model (DDPM). The fusion task is formulated as a conditional generation problem under the DDPM sampling framework, which is further divided into an unconditional generation subproblem and a maximum likelihood subproblem. The latter is modeled in a hierarchical Bayesian manner with latent variables and inferred by the expectation-maximization (EM) algorithm. By integrating the inference solution into the diffusion sampling iteration, our method can generate high-quality fused images with natural image generative priors and cross-modality information from source images. Note that all we required is an unconditional pre-trained generative model, and no fine-tuning is needed. Our extensive experiments indicate that our approach yields promising fusion results in infrared-visible image fusion and medical image fusion. The code is available at \\url{\n[> this https URL\n](https://github.com/Zhaozixiang1228/MMIF-DDFM)> }. Comments:|Accepted by ICCV 2023 (Oral)|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:2303.06840](https://arxiv.org/abs/2303.06840)[cs.CV]|\n|(or[arXiv:2303.06840v2](https://arxiv.org/abs/2303.06840v2)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2303.06840](https://doi.org/10.48550/arXiv.2303.06840)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Zixiang Zhao [[view email](https://arxiv.org/show-email/ea390005/2303.06840)]\n**[[v1]](https://arxiv.org/abs/2303.06840v1)**Mon, 13 Mar 2023 04:06:42 UTC (13,496 KB)\n**[v2]**Tue, 22 Aug 2023 23:19:03 UTC (17,490 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion, by Zixiang Zhao and 9 other authors\n* [View PDF](https://arxiv.org/pdf/2303.06840)\n* [TeX Source](https://arxiv.org/src/2303.06840)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2303.06840&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2303.06840&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2023-03](https://arxiv.org/list/cs.CV/2023-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2303.06840?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2303.06840)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2303.06840)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2303.06840)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLab",
          "original_query": "DDFM: Denoising diffusion model for multi-modality image fusion",
          "cleaned_query": "DDFM: Denoising diffusion model for multi-modality image fusion"
        }
      ],
      "generated_ideas": [
        "CLIP-Conditioned Degradation Prompts for All-in-One Restoration\nTrain a PromptIR-style prompt generator whose prompt embeddings are explicitly aligned to CLIP text/image space (e.g., \u201clow-light + motion blur\u201d, \u201cthermal sensor noise\u201d) using contrastive losses. This would enable zero-shot restoration control via natural-language descriptions while preserving PromptIR\u2019s blind restoration capability on unseen mixtures of degradations.",
        "Text-Grounded Fusion With Region-Level Semantic Guarantees\nExtend Text-DiFuse by adding a CLIP-driven region constraint: enforce that user-specified phrases (e.g., \u201cpedestrian\u201d, \u201ctumor boundary\u201d) have higher similarity on the fused image within predicted regions than in the unfused sources. Implement via differentiable attention maps or segmentation priors and optimize within the diffusion sampling loop to explicitly boost object-of-interest salience without over-enhancing background.",
        "Composable Multi-Modal Priors Beyond Two Modalities (RGB\u2013IR\u2013Depth\u2013Event)\nGeneralize DRMF\u2019s \u201ccomposable diffusion prior\u201d to N modalities by learning modality-specific conditional diffusion experts and a lightweight gating network that selects/weights experts per timestep and per spatial location. Validate on tri-modal setups (e.g., RGB+IR+Depth) and show improved robustness when one modality is degraded or missing.",
        "Uncertainty-Aware Diffusion Fusion Under Unknown/Compound Degradations\nBuild on DDFM\u2019s probabilistic framing by augmenting the latent-variable model to estimate per-modality, per-pixel uncertainty maps (noise level, blur confidence, exposure confidence). Use these uncertainties to adapt the EM updates and to weight guidance during diffusion sampling, yielding calibrated fusion outputs that avoid amplifying corrupted regions.",
        "Interactive Multi-Objective Fusion via Natural-Language Preference Learning\nCreate a dataset of user pairwise preferences for fused outputs described by text (e.g., \u201cprefer sharper edges but keep thermal highlights subtle\u201d), then learn a reward model in CLIP space. Use that reward to guide diffusion-based fusion (Text-DiFuse/DRMF) at inference, enabling controllable trade-offs (detail vs. naturalness vs. target salience) without retraining the core model.",
        "Cross-Modal Degradation Simulation With Text Labels for Scalable Training\nDevelop a synthetic degradation pipeline that applies physically motivated corruptions per modality (IR fixed-pattern noise, visible low-light shot noise, medical acquisition artifacts) and auto-generates matched text descriptions. Use it to train Text-IF-style semantic encoders and PromptIR prompts on large-scale, diverse degradations, improving generalization to real compound degradations.",
        "Fusion-to-Downstream Task Co-Optimization With Zero-Shot Text Queries\nInstead of optimizing fusion purely for pixel/quality metrics, incorporate a downstream objective like open-vocabulary detection/segmentation using CLIP queries (e.g., \u201ccar\u201d, \u201clesion\u201d). Train/guide diffusion fusion so the fused image maximizes downstream CLIP-based separability and localization performance, demonstrating that \u201ctask-aware fusion\u201d beats traditional fusion for robotics and medical triage.",
        "Test-Time Adaptation for Fusion Using Text as a Self-Supervision Signal\nPropose a training-free (or minimal) test-time adaptation method where the fusion model updates a small set of prompts/adapters by maximizing alignment between the fused image and user-provided text while maintaining cycle/consistency with inputs. This would allow rapid personalization to new sensors/environments (new IR cameras, new hospital scanners) without curated ground truth.",
        "Faithfulness and Hallucination Benchmarks for Diffusion-Based Fusion\nIntroduce a benchmark and metrics to quantify when diffusion fusion \u201challucinates\u201d structures not supported by any modality, a key risk in medical/thermal use. Combine provenance-style constraints (can each fused structure be traced to at least one source modality?) with CLIP-based semantic drift scores, and evaluate DDFM/DRMF/Text-DiFuse under heavy degradations.",
        "Spatially Local Text Control for Fine-Grained Fusion Editing\nExtend Text-IF\u2019s interaction to support localized prompts (\u201cenhance text on sign\u201d, \u201csuppress sky noise\u201d, \u201chighlight warm objects near road\u201d) by coupling text embeddings with user scribbles/boxes and producing spatially varying fusion weights inside the decoder or diffusion guidance. This enables precise, editor-like control over what information is fused where, particularly useful when degradations are localized (glare patches, sensor dead pixels)."
      ],
      "generated_ideas_raw": "1. **CLIP-Conditioned Degradation Prompts for All-in-One Restoration**\n   Train a PromptIR-style prompt generator whose prompt embeddings are explicitly aligned to CLIP text/image space (e.g., \u201clow-light + motion blur\u201d, \u201cthermal sensor noise\u201d) using contrastive losses. This would enable zero-shot restoration control via natural-language descriptions while preserving PromptIR\u2019s blind restoration capability on unseen mixtures of degradations.\n\n2. **Text-Grounded Fusion With Region-Level Semantic Guarantees**\n   Extend Text-DiFuse by adding a CLIP-driven region constraint: enforce that user-specified phrases (e.g., \u201cpedestrian\u201d, \u201ctumor boundary\u201d) have higher similarity on the fused image within predicted regions than in the unfused sources. Implement via differentiable attention maps or segmentation priors and optimize within the diffusion sampling loop to explicitly boost object-of-interest salience without over-enhancing background.\n\n3. **Composable Multi-Modal Priors Beyond Two Modalities (RGB\u2013IR\u2013Depth\u2013Event)**\n   Generalize DRMF\u2019s \u201ccomposable diffusion prior\u201d to N modalities by learning modality-specific conditional diffusion experts and a lightweight gating network that selects/weights experts per timestep and per spatial location. Validate on tri-modal setups (e.g., RGB+IR+Depth) and show improved robustness when one modality is degraded or missing.\n\n4. **Uncertainty-Aware Diffusion Fusion Under Unknown/Compound Degradations**\n   Build on DDFM\u2019s probabilistic framing by augmenting the latent-variable model to estimate per-modality, per-pixel uncertainty maps (noise level, blur confidence, exposure confidence). Use these uncertainties to adapt the EM updates and to weight guidance during diffusion sampling, yielding calibrated fusion outputs that avoid amplifying corrupted regions.\n\n5. **Interactive Multi-Objective Fusion via Natural-Language Preference Learning**\n   Create a dataset of user pairwise preferences for fused outputs described by text (e.g., \u201cprefer sharper edges but keep thermal highlights subtle\u201d), then learn a reward model in CLIP space. Use that reward to guide diffusion-based fusion (Text-DiFuse/DRMF) at inference, enabling controllable trade-offs (detail vs. naturalness vs. target salience) without retraining the core model.\n\n6. **Cross-Modal Degradation Simulation With Text Labels for Scalable Training**\n   Develop a synthetic degradation pipeline that applies physically motivated corruptions per modality (IR fixed-pattern noise, visible low-light shot noise, medical acquisition artifacts) and auto-generates matched text descriptions. Use it to train Text-IF-style semantic encoders and PromptIR prompts on large-scale, diverse degradations, improving generalization to real compound degradations.\n\n7. **Fusion-to-Downstream Task Co-Optimization With Zero-Shot Text Queries**\n   Instead of optimizing fusion purely for pixel/quality metrics, incorporate a downstream objective like open-vocabulary detection/segmentation using CLIP queries (e.g., \u201ccar\u201d, \u201clesion\u201d). Train/guide diffusion fusion so the fused image maximizes downstream CLIP-based separability and localization performance, demonstrating that \u201ctask-aware fusion\u201d beats traditional fusion for robotics and medical triage.\n\n8. **Test-Time Adaptation for Fusion Using Text as a Self-Supervision Signal**\n   Propose a training-free (or minimal) test-time adaptation method where the fusion model updates a small set of prompts/adapters by maximizing alignment between the fused image and user-provided text while maintaining cycle/consistency with inputs. This would allow rapid personalization to new sensors/environments (new IR cameras, new hospital scanners) without curated ground truth.\n\n9. **Faithfulness and Hallucination Benchmarks for Diffusion-Based Fusion**\n   Introduce a benchmark and metrics to quantify when diffusion fusion \u201challucinates\u201d structures not supported by any modality, a key risk in medical/thermal use. Combine provenance-style constraints (can each fused structure be traced to at least one source modality?) with CLIP-based semantic drift scores, and evaluate DDFM/DRMF/Text-DiFuse under heavy degradations.\n\n10. **Spatially Local Text Control for Fine-Grained Fusion Editing**\n   Extend Text-IF\u2019s interaction to support localized prompts (\u201cenhance text on sign\u201d, \u201csuppress sky noise\u201d, \u201chighlight warm objects near road\u201d) by coupling text embeddings with user scribbles/boxes and producing spatially varying fusion weights inside the decoder or diffusion guidance. This enables precise, editor-like control over what information is fused where, particularly useful when degradations are localized (glare patches, sensor dead pixels).",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "CLIP-Conditioned Degradation Prompts for All-in-One Restoration\nTrain a PromptIR-style prompt generator whose prompt embeddings are explicitly aligned to CLIP text/image space (e.g., \u201clow-light + moti",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Text-Grounded Fusion With Region-Level Semantic Guarantees\nExtend Text-DiFuse by adding a CLIP-driven region constraint: enforce that user-specified phrases (e.g., \u201cpedestrian\u201d, \u201ctumor boundary\u201d) have",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Composable Multi-Modal Priors Beyond Two Modalities (RGB\u2013IR\u2013Depth\u2013Event)\nGeneralize DRMF\u2019s \u201ccomposable diffusion prior\u201d to N modalities by learning modality-specific conditional diffusion experts and ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Uncertainty-Aware Diffusion Fusion Under Unknown/Compound Degradations\nBuild on DDFM\u2019s probabilistic framing by augmenting the latent-variable model to estimate per-modality, per-pixel uncertainty map",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Interactive Multi-Objective Fusion via Natural-Language Preference Learning\nCreate a dataset of user pairwise preferences for fused outputs described by text (e.g., \u201cprefer sharper edges but keep ther",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Cross-Modal Degradation Simulation With Text Labels for Scalable Training\nDevelop a synthetic degradation pipeline that applies physically motivated corruptions per modality (IR fixed-pattern noise, v",
          "is_match": true
        },
        {
          "idea_idx": 6,
          "idea_text": "Fusion-to-Downstream Task Co-Optimization With Zero-Shot Text Queries\nInstead of optimizing fusion purely for pixel/quality metrics, incorporate a downstream objective like open-vocabulary detection/s",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Test-Time Adaptation for Fusion Using Text as a Self-Supervision Signal\nPropose a training-free (or minimal) test-time adaptation method where the fusion model updates a small set of prompts/adapters ",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Faithfulness and Hallucination Benchmarks for Diffusion-Based Fusion\nIntroduce a benchmark and metrics to quantify when diffusion fusion \u201challucinates\u201d structures not supported by any modality, a key ",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Spatially Local Text Control for Fine-Grained Fusion Editing\nExtend Text-IF\u2019s interaction to support localized prompts (\u201cenhance text on sign\u201d, \u201csuppress sky noise\u201d, \u201chighlight warm objects near road\u201d",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 19,
      "paper_title": "Identifiability of Deep Polynomial Neural Networks",
      "contribution": "Provides a comprehensive, constructive characterization of when deep polynomial neural networks are (finitely and/or globally) identifiable by reducing identifiability to low-rank polynomial/tensor decomposition uniqueness and settling open dimension and degree-threshold conjectures for neurovarieties.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 5,
      "input_tokens": 10345,
      "output_tokens": 1146,
      "predecessor_details": [
        {
          "success": true,
          "title": "Recovering a feed-forward net from its output",
          "url": "https://dl.acm.org/doi/10.5555/2987189.2987232",
          "content": "Recovering a feed-forward net from its output | Proceedings of the 7th International Conference on Neural Information Processing Systems[skip to main content](#skip-to-main-content)\n[](#global-menu)\nSearch ACM Digital Library\nSearchSearch\n[Advanced Search](https://dl.acm.org/search/advanced)\n10.5555/2987189.2987232guideproceedingsArticle/Chapter ViewAbstractPublication PagesnipsConference Proceedingsconference-collections\n[nips](#)\n**## Export Citations\nSelect Citation formatBibTeXEndNoteACM Ref**\n* Please download or close your previous search result export first before starting a new bulk export.\nPreview is not available.\nBy clicking download,**a status dialog**will open to start the export process. The process may take**a few minutes**but once it finishes a file will be downloadable from your browser. You may continue to browse the DL while the export process is in progress.\n* ```\n```\n* [Download citation**](javascript:void(0))\n* [Copy citation**](javascript:void(0))\nArticle\nShare on\n* **\n* **\n* **\n* **\n* **\n# Recovering a feed-forward net from its output\nAuthors:[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)CharlesFefferman](#artseq-00001),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)ScottMarkel](#artseq-00002)[Authors Info &amp; Claims](#tab-contributors)\n[NIPS'93: Proceedings of the 7th International Conference on Neural Information Processing Systems](https://dl.acm.org/doi/proceedings/10.5555/2987189)\nPages335-342\nPublished:29 November 1993[Publication History](#core-history)\n**0citation**0Downloads\nMetrics\n[\nTotal Citations0\n](#tab-citations)[\nTotal Downloads0\n](#tab-metrics-inner)\nLast 12 Months0\nLast 6 weeks0\n**Get Citation Alerts\n**## New Citation Alert added!\nThis alert has been successfully added and will be sent to:\nYou will be notified whenever a record that you have chosen has been cited.\nTo manage your alert preferences, click on the button below.\n[Manage my Alerts](https://dl.acm.org/action/showPreferences?menuTab=Alerts)\n**## New Citation Alert!\nPlease[log in to your account](https://dl.acm.org/action/showLogin?redirectUri=/doi/10.5555/2987189.2987232)\n**\n**\n[**Publisher Site](http://papers.nips.cc/paper/748-recovering-a-feed-forward-net-from-its-output.pdf)\n**Contents\n## Abstract\nWe study feed-forward nets with arbitrarily many layers, using the standard sigmoid, tanh x. Aside from technicalities, our theorems are: 1. Complete knowledge of the output of a neural net for arbitrary inputs uniquely specifies the architecture, weights and thresholds; and 2. There are only finitely many critical points on the error surface for a generic training problem.\n## References\n[1]\nR. Hecht-Nielson, et al.,*On the geometry of feedforward neural network error surfaces*. (to appear).\n[Google Scholar](https://scholar.google.com/scholar?q=R.+Hecht-Nielson,+et+al.,+On+the+geometry+of+feedforward+neural+network+error+surfaces.+(to+appear).)\n[2]\nC. Fefferman,*Reconstructing a neural network from its output*, Revista Mathem\u00e1tica Iberoamericana. (to appear).\n[Google Scholar](https://scholar.google.com/scholar?q=C.+Fefferman,+Reconstructing+a+neural+network+from+its+output,+Revista+Mathem\u00e1tica+Iberoamericana.+(to+appear).)\n[3]\nF. Albertini and E. Sontag,*Uniqueness of weights for neural networks.*(to appear).\n[Google Scholar](https://scholar.google.com/scholar?q=F.+Albertini+and+E.+Sontag,+Uniqueness+of+weights+for+neural+networks.+(to+appear).)\n[4]\nH. Sussman,*Uniqueness of the weights for minimal feedforward nets with a given input-output map*, Neural Networks**5**(1992), pp. 589-593.\n[Digital Library](https://dl.acm.org/doi/10.1016/S0893-6080(05)80037-1)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1016/S0893-6080(05)80037-1)\n## Index Terms\n1. Recovering a feed-forward net from its output\n1. [Computing methodologies](https://dl.acm.org/topic/ccs2012/10010147?ContentGroupKey=10.5555/2987189&amp;expand=all)\n1. [Machine learning](https://dl.acm.org/topic/ccs2012/10010147.10010257?ContentGroupKey=10.5555/2987189&amp;expand=all)\n1. [Machine learning approaches](https://dl.acm.org/topic/ccs2012/10010147.10010257.10010293?ContentGroupKey=10.5555/2987189&amp;expand=all)\n1. [Neural networks](https://dl.acm.org/topic/ccs2012/10010147.10010257.10010293.10010294?ContentGroupKey=10.5555/2987189&amp;expand=all)\n1. [Hardware](https://dl.acm.org/topic/ccs2012/10010583?ContentGroupKey=10.5555/2987189&amp;expand=all)\n1. [Emerging technologies](https://dl.acm.org/topic/ccs2012/10010583.10010786?ContentGroupKey=10.5555/2987189&amp;expand=all)\n1. [Circuit substrates](https://dl.acm.org/topic/ccs2012/10010583.10010786.10010799?ContentGroupKey=10.5555/2987189&amp;expand=all)\n1. [Cellular neural networks](https://dl.acm.org/topic/ccs2012/10010583.10010786.10010799.10010802?ContentGroupKey=10.5555/2987189&amp;expand=all)\n1. [Mathematics of computing](https://dl.acm.org/topic/ccs2012/10002950?ContentGroupKey=10.5555/2987189&amp;expand=all)\n1. [Mathematical analysis](https://dl.acm.org/topic/ccs2012/10002950.10003714?ContentGroupKey=10.5555/2987189&amp;expand=all)\n1. [Mathematical optimization](https://dl.acm.org/topic/ccs2012/10002950.10003714.10003716?ContentGroupKey=10.5555/2987189&amp;expand=all)\n1. [Theory of computation](https://dl.acm.org/topic/ccs2012/10003752?ContentGroupKey=10.5555/2987189&amp;expand=all)\n1. [Design and analysis of algorithms](https://dl.acm.org/topic/ccs2012/10003752.10003809?ContentGroupKey=10.5555/2987189&amp;expand=all)\n1. [Mathematical optimization](https://dl.acm.org/topic/ccs2012/10003752.10003809.10003716?ContentGroupKey=10.5555/2987189&amp;expand=all)\n1. [Theory and algorithms for application domains](https://dl.acm.org/topic/ccs2012/10003752.10010070?ContentGroupKey=10.5555/2987189&amp;expand=all)\n1. [Machine learning theory](https://dl.acm.org/topic/ccs2012/10003752.10010070.10010071?ContentGroupKey=10.5555/2987189&amp;expand=all)\n## Recommendations\n* [### Feed-Forward neural networks using hermite polynomial activation functions\n](https://dl.acm.org/doi/10.1007/11752912_33)\nSETN'06: Proceedings of the 4th Helenic conference on Advances in Artificial Intelligence\nIn this paper feed-forward neural networks are introduced where hidden units employ\northogonal Hermite polynomials for their activation functions. The proposed neural\nnetworks have some interesting properties: (i) the basis functions are invariant under\n...\n[Read More](https://dl.acm.org/doi/10.1007/11752912_33)\n* [### Feed-Forward Neural Networks: Vector Decomposition Analysis, Modelling and Analog Implementation\n](https://dl.acm.org/doi/10.5555/2601758)\n[Read More](https://dl.acm.org/doi/10.5555/2601758)\n* [### The errors in simultaneous approximation by feed-forward neural networks\n](https://dl.acm.org/doi/10.1016/j.neucom.2009.09.014)\nThere have been many studies on the simultaneous approximation capability of feed-forward\nneural networks (FNNs). Most of these, however, are only concerned with the density\nor feasibility of performing simultaneous approximations. This paper considers ...\n[Read More](https://dl.acm.org/doi/10.1016/j.neucom.2009.09.014)\n## Comments\n[](https://dl.acm.org/doi/10.5555/2987189.2987232#disqus_thread)\n## **Information &amp; Contributors\n### Information\n#### Published In\n![cover image Guide Proceedings](https://dl.acm.org/specs/products/acm/releasedAssets/images/Default_image_lazy-0687af31f0f1c8d4b7a22b686995ab9b.svg)\nNIPS'93: Proceedings of the 7th International Conference on Neural Information Processing Systems\nNovember 1993\n1189 pages\n* **Editors:**\n* [![Author Picture](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)J. D. Cowan](https://dl.acm.org/profile/81453639898),\n* [![Author Picture](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)G. Tesauro](https://dl.acm.org/profile/99660947629),\n* [![Author Picture](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)J. Alspector](https://dl.acm.org/",
          "original_query": "Reconstructing a neural net from its output [46]",
          "cleaned_query": "Reconstructing a neural net from its output"
        },
        {
          "success": true,
          "title": "On the Expressive Power of Deep Polynomial Neural Networks - arXiv",
          "url": "https://arxiv.org/abs/1905.12207",
          "content": "\n \n \n \n \n We gratefully acknowledge support from \nthe Simons Foundation and member institutions. \n \n \n \n \n \n \n",
          "original_query": "On the expressive power of deep polynomial neural networks [7]",
          "cleaned_query": "On the expressive power of deep polynomial neural networks"
        },
        {
          "success": true,
          "title": "Identifiability of an X-rank decomposition of polynomial maps",
          "url": "https://arxiv.org/abs/1603.01566",
          "content": "[1603.01566] Identifiability of an X-rank decomposition of polynomial maps\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1603.01566\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Information Theory\n**arXiv:1603.01566**(cs)\n[Submitted on 4 Mar 2016 ([v1](https://arxiv.org/abs/1603.01566v1)), last revised 5 Apr 2017 (this version, v3)]\n# Title:Identifiability of an X-rank decomposition of polynomial maps\nAuthors:[Pierre Comon](https://arxiv.org/search/cs?searchtype=author&amp;query=Comon,+P),[Yang Qi](https://arxiv.org/search/cs?searchtype=author&amp;query=Qi,+Y),[Konstantin Usevich](https://arxiv.org/search/cs?searchtype=author&amp;query=Usevich,+K)\nView a PDF of the paper titled Identifiability of an X-rank decomposition of polynomial maps, by Pierre Comon and 2 other authors\n[View PDF](https://arxiv.org/pdf/1603.01566)> > Abstract:\n> In this paper, we study a polynomial decomposition model that arises in problems of system identification, signal processing and machine learning. We show that this decomposition is a special case of the X-rank decomposition --- a powerful novel concept in algebraic geometry that generalizes the tensor CP decomposition. We prove new results on generic/maximal rank and on identifiability of a particular polynomial decomposition model. In the paper, we try to make results and basic tools accessible for general audience (assuming no knowledge of algebraic geometry or its prerequisites). Comments:|26 pages|\nSubjects:|Information Theory (cs.IT); Numerical Analysis (math.NA); Machine Learning (stat.ML)|\nMSCclasses:|12E05, 14M12, 15A21, 15A69|\nCite as:|[arXiv:1603.01566](https://arxiv.org/abs/1603.01566)[cs.IT]|\n|(or[arXiv:1603.01566v3](https://arxiv.org/abs/1603.01566v3)[cs.IT]for this version)|\n|[https://doi.org/10.48550/arXiv.1603.01566](https://doi.org/10.48550/arXiv.1603.01566)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Konstantin Usevich [[view email](https://arxiv.org/show-email/65dc2a0f/1603.01566)]\n**[[v1]](https://arxiv.org/abs/1603.01566v1)**Fri, 4 Mar 2016 18:50:40 UTC (29 KB)\n**[[v2]](https://arxiv.org/abs/1603.01566v2)**Tue, 7 Feb 2017 20:29:09 UTC (171 KB)\n**[v3]**Wed, 5 Apr 2017 20:20:21 UTC (167 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Identifiability of an X-rank decomposition of polynomial maps, by Pierre Comon and 2 other authors\n* [View PDF](https://arxiv.org/pdf/1603.01566)\n* [TeX Source](https://arxiv.org/src/1603.01566)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.IT\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1603.01566&amp;function=prev&amp;context=cs.IT) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1603.01566&amp;function=next&amp;context=cs.IT)\n[new](https://arxiv.org/list/cs.IT/new)|[recent](https://arxiv.org/list/cs.IT/recent)|[2016-03](https://arxiv.org/list/cs.IT/2016-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/1603.01566?context=cs)\n[math](https://arxiv.org/abs/1603.01566?context=math)\n[math.IT](https://arxiv.org/abs/1603.01566?context=math.IT)\n[math.NA](https://arxiv.org/abs/1603.01566?context=math.NA)\n[stat](https://arxiv.org/abs/1603.01566?context=stat)\n[stat.ML](https://arxiv.org/abs/1603.01566?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1603.01566)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1603.01566)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1603.01566)\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1603.html#ComonQU16)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/ComonQU16)\n[Pierre Comon]()\n[Yang Qi]()\n[Konstantin Usevich]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1603.01566)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Identifiability of an X-rank decomposition of polynomial maps []",
          "cleaned_query": "Identifiability of an X-rank decomposition of polynomial maps []"
        },
        {
          "success": true,
          "title": "Tensor decompositions for learning latent variable models",
          "url": "https://arxiv.org/abs/1210.7559",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1210.7559** (cs)\n\n\\[Submitted on 29 Oct 2012 ( [v1](https://arxiv.org/abs/1210.7559v1)), last revised 13 Nov 2014 (this version, v4)\\]\n\n# Title:Tensor decompositions for learning latent variable models\n\nAuthors: [Anima Anandkumar](https://arxiv.org/search/cs?searchtype=author&query=Anandkumar,+A), [Rong Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge,+R), [Daniel Hsu](https://arxiv.org/search/cs?searchtype=author&query=Hsu,+D), [Sham M. Kakade](https://arxiv.org/search/cs?searchtype=author&query=Kakade,+S+M), [Matus Telgarsky](https://arxiv.org/search/cs?searchtype=author&query=Telgarsky,+M)\n\nView a PDF of the paper titled Tensor decompositions for learning latent variable models, by Anima Anandkumar and Rong Ge and Daniel Hsu and Sham M. Kakade and Matus Telgarsky\n\n[View PDF](https://arxiv.org/pdf/1210.7559)\n\n> Abstract:This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models---including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation---which exploits a certain tensor structure in their low-order observable moments (typically, of second- and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Numerical Analysis (math.NA); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1210.7559](https://arxiv.org/abs/1210.7559) \\[cs.LG\\] |\n| | (or [arXiv:1210.7559v4](https://arxiv.org/abs/1210.7559v4) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1210.7559](https://doi.org/10.48550/arXiv.1210.7559) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | Journal of Machine Learning Research, 15(Aug):2773-2832, 2014 |\n\n## Submission history\n\nFrom: Daniel Hsu \\[ [view email](https://arxiv.org/show-email/4e158393/1210.7559)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1210.7559v1)**\nMon, 29 Oct 2012 04:38:41 UTC (56 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1210.7559v2)**\nSun, 9 Dec 2012 00:59:17 UTC (57 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/1210.7559v3)**\nSat, 1 Mar 2014 19:06:31 UTC (62 KB)\n\n**\\[v4\\]**\nThu, 13 Nov 2014 22:43:15 UTC (57 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Tensor decompositions for learning latent variable models, by Anima Anandkumar and Rong Ge and Daniel Hsu and Sham M. Kakade and Matus Telgarsky\n\n- [View PDF](https://arxiv.org/pdf/1210.7559)\n- [TeX Source](https://arxiv.org/src/1210.7559)\n- [Other Formats](https://arxiv.org/format/1210.7559)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1210.7559&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1210.7559&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2012-10](https://arxiv.org/list/cs.LG/2012-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1210.7559?context=cs)\n\n[math](https://arxiv.org/abs/1210.7559?context=math)\n\n[math.NA](https://arxiv.org/abs/1210.7559?context=math.NA)\n\n[stat](https://arxiv.org/abs/1210.7559?context=stat)\n\n[stat.ML](https://arxiv.org/abs/1210.7559?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1210.7559)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1210.7559)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1210.7559)\n\n### [1 blog link](https://arxiv.org/tb/1210.7559)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1210.html#abs-1210-7559) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1210-7559)\n\n[Anima Anandkumar](https://dblp.uni-trier.de/search/author?author=Anima%20Anandkumar)\n\n[Rong Ge](https://dblp.uni-trier.de/search/author?author=Rong%20Ge)\n\n[Daniel Hsu](https://dblp.uni-trier.de/search/author?author=Daniel%20Hsu)\n\n[Daniel J. Hsu](https://dblp.uni-trier.de/search/author?author=Daniel%20J.%20Hsu)\n\n[Sham M. Kakade](https://dblp.uni-trier.de/search/author?author=Sham%20M.%20Kakade)\n\n\u2026\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1210.7559&description=Tensor decompositions for learning latent variable models) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1210.7559&title=Tensor decompositions for learning latent variable models)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a p",
          "original_query": "Tensor decompositions for learning latent variable models [10]",
          "cleaned_query": "Tensor decompositions for learning latent variable models"
        },
        {
          "success": true,
          "title": "[2402.00949] Geometry of Polynomial Neural Networks - arXiv",
          "url": "https://arxiv.org/abs/2402.00949",
          "content": "# Mathematics > Algebraic Geometry\n\n**arXiv:2402.00949** (math)\n\n\\[Submitted on 1 Feb 2024 ( [v1](https://arxiv.org/abs/2402.00949v1)), last revised 4 Nov 2024 (this version, v2)\\]\n\n# Title:Geometry of Polynomial Neural Networks\n\nAuthors: [Kaie Kubjas](https://arxiv.org/search/math?searchtype=author&query=Kubjas,+K), [Jiayi Li](https://arxiv.org/search/math?searchtype=author&query=Li,+J), [Maximilian Wiesmann](https://arxiv.org/search/math?searchtype=author&query=Wiesmann,+M)\n\nView a PDF of the paper titled Geometry of Polynomial Neural Networks, by Kaie Kubjas and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2402.00949) [HTML (experimental)](https://arxiv.org/html/2402.00949v2)\n\n> Abstract:We study the expressivity and learning process for polynomial neural networks (PNNs) with monomial activation functions. The weights of the network parametrize the neuromanifold. In this paper, we study certain neuromanifolds using tools from algebraic geometry: we give explicit descriptions as semialgebraic sets and characterize their Zariski closures, called neurovarieties. We study their dimension and associate an algebraic degree, the learning degree, to the neurovariety. The dimension serves as a geometric measure for the expressivity of the network, the learning degree is a measure for the complexity of training the network and provides upper bounds on the number of learnable functions. These theoretical results are accompanied with experiments.\n\n| | |\n| --- | --- |\n| Comments: | 34 pages, 3 figures. Comments are welcome! |\n| Subjects: | Algebraic Geometry (math.AG); Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| MSC classes: | 68T07, 14P10, 14N07, 14M12 |\n| Cite as: | [arXiv:2402.00949](https://arxiv.org/abs/2402.00949) \\[math.AG\\] |\n| (or [arXiv:2402.00949v2](https://arxiv.org/abs/2402.00949v2) \\[math.AG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2402.00949](https://doi.org/10.48550/arXiv.2402.00949) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | Alg. Stat. 15 (2024) 295-328 |\n| Related DOI: | [https://doi.org/10.2140/astat.2024.15.295](https://doi.org/10.2140/astat.2024.15.295) Focus to learn more DOI(s) linking to related resources |\n\n## Submission history\n\nFrom: Maximilian Wiesmann \\[ [view email](https://arxiv.org/show-email/13ac06f2/2402.00949)\\] **[\\[v1\\]](https://arxiv.org/abs/2402.00949v1)**\nThu, 1 Feb 2024 19:06:06 UTC (780 KB)\n**\\[v2\\]**\nMon, 4 Nov 2024 17:39:35 UTC (844 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Geometry of Polynomial Neural Networks, by Kaie Kubjas and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2402.00949)\n- [HTML (experimental)](https://arxiv.org/html/2402.00949v2)\n- [TeX Source](https://arxiv.org/src/2402.00949)\n- [Other Formats](https://arxiv.org/format/2402.00949)\n\n[view license](http://creativecommons.org/licenses/by-nc-nd/4.0/)\n\nCurrent browse context:\n\nmath.AG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2402.00949&function=prev&context=math.AG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2402.00949&function=next&context=math.AG)\n\n[new](https://arxiv.org/list/math.AG/new) \\| [recent](https://arxiv.org/list/math.AG/recent) \\| [2024-02](https://arxiv.org/list/math.AG/2024-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2402.00949?context=cs) [cs.LG](https://arxiv.org/abs/2402.00949?context=cs.LG) [math](https://arxiv.org/abs/2402.00949?context=math) [stat](https://arxiv.org/abs/2402.00949?context=stat) [stat.ML](https://arxiv.org/abs/2402.00949?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2402.00949)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2402.00949)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2402.00949)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2402.00949) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Geometry of polynomial neural networks [35]",
          "cleaned_query": "Geometry of polynomial neural networks"
        },
        {
          "success": true,
          "title": "Activation degree thresholds and expressiveness of polynomial ...",
          "url": "https://arxiv.org/abs/2408.04569",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "Activation degree thresholds and expressiveness of polynomial neural networks [38]",
          "cleaned_query": "Activation degree thresholds and expressiveness of polynomial neural networks"
        }
      ],
      "generated_ideas": [
        "Query-Efficient Recovery of Polynomial Neural Networks from Black-Box Outputs\nDevelop an active-query algorithm that reconstructs a polynomial neural network\u2019s architecture and weights from function evaluations (and optionally gradients), extending the \u201cunique recovery from full output knowledge\u201d result to a practical, sample-efficient setting. Use polynomial interpolation plus structural constraints from PNN neurovarieties to provably reduce queries from dense-grid sampling to near-parameter-count scaling. Benchmark recovery accuracy vs. noise and depth, and derive identifiability conditions in terms of degree and width.",
        "Identifiability Limits Under Noisy Moment Observations for Polynomial Maps\nBridge X-rank identifiability of polynomial maps with robust tensor power methods by analyzing what happens when the polynomial map is only observed through estimated low-order moments (as in latent-variable learning). Provide finite-sample error bounds that translate moment estimation error into uncertainty sets over decompositions, explicitly characterizing when decomposition remains unique \u201cup to symmetry.\u201d Deliver a practical diagnostic that flags when data are insufficient to identify a PNN-generated polynomial map.",
        "Learning Degree as a Predictor of Optimization Hardness in Training PNNs\nOperationalize the \u201clearning degree\u201d (algebraic degree of the neurovariety) as a quantitative predictor of training difficulty: construct families of PNNs where learning degree is varied while parameter count is held fixed. Measure correlations with critical point counts, gradient-based convergence time, and frequency of bad local minima, tying back to finiteness/genericity claims about critical points. The output would be both empirical evidence and a theoretical explanation linking learning degree to landscape complexity.",
        "Neurovariety-Guided Architecture Selection via Dimension Matching\nCreate an architecture-selection method that chooses PNN depth/width by matching the neurovariety dimension to an estimated \u201cintrinsic dimension\u201d of the target function class (e.g., via local polynomial fits or moment rank estimates). The key contribution is a concrete pipeline: estimate target polynomial complexity \u2192 choose candidate PNNs whose neurovariety dimensions minimally exceed it \u2192 train and compare. This turns geometric expressivity measures into an actionable, data-driven model selection criterion.",
        "Hybrid Tensor\u2013Algebraic Training: Initialize PNNs from CP/X-Rank Decompositions\nDesign a training algorithm that first fits a low-order moment tensor of the data (or of labels conditioned on inputs) and extracts a CP/X-rank decomposition, then maps the factors into an initial PNN parameterization consistent with the polynomial decomposition model. The contribution is an explicit initialization-to-weights procedure plus convergence guarantees under identifiability conditions (genericity, rank bounds). Evaluate against random init on synthetic polynomial targets and latent-variable benchmarks.",
        "Degree Threshold Phase Transitions in Expressiveness for Deep PNNs\nFormally characterize \u201cactivation degree thresholds\u201d as phase transitions in representable function sets: identify critical degrees where the neurovariety dimension jumps or where generic identifiability fails. Provide computable criteria (in terms of depth, width, and monomial degree) predicting when adding one more layer changes the Zariski closure or increases maximal rank. Validate with experiments that map empirical fit capacity to predicted thresholds.",
        "Symmetry-Aware Uniqueness: Classifying All Equivalent Parameterizations of a Given Polynomial Map\nExtend classic uniqueness results (recovering a net from outputs) by explicitly cataloging the full group of parameter symmetries for polynomial neural networks (permutations, scalings, and polynomial-specific invariances). The deliverable is a theorem and algorithm: given a recovered decomposition, enumerate (or describe) all equivalent weight settings, and define canonical representatives. This aids both identifiability theory and practical recovery/compression by avoiding redundant solutions.",
        "Robust Recovery of PNNs from Partial Output Knowledge via Compressed Sampling\nInvestigate whether one can recover PNN parameters when outputs are known only on a low-dimensional random subspace or via random linear measurements of the output function (compressed sensing style). Use algebraic geometry of neurovarieties to derive conditions under which such partial observations intersect the neurovariety in finitely many points (generic identifiability). Provide a reconstruction algorithm and quantify how many measurements suffice as a function of dimension/degree.",
        "Neurovariety-Based Generalization Bounds for Polynomial Networks Using Algebraic Complexity\nDerive generalization bounds where capacity is controlled by algebraic-geometric invariants (dimension, degree/learning degree) rather than only norm-based or VC-style measures. The key step is to relate the number of distinct labelings/fit functions to degree bounds on semialgebraic sets associated with the network image. Validate whether these bounds better predict test error for PNNs trained on polynomial-like tasks than standard complexity measures.",
        "From Latent Variable Models to Deep Polynomial Nets: End-to-End Identifiable Architectures\nConstruct deep PNN architectures whose intermediate representations correspond to identifiable latent-variable models (e.g., mixture components or HMM states) recoverable by tensor decomposition, enabling interpretable hidden layers. The contribution is a concrete design recipe: specify layers so that observable moments yield orthogonal/symmetric tensors amenable to robust power methods, then prove identifiability of both latent factors and final predictor. Demonstrate on synthetic mixtures and real topic-model-like data where learned hidden units align with ground-truth factors."
      ],
      "generated_ideas_raw": "1. **Query-Efficient Recovery of Polynomial Neural Networks from Black-Box Outputs**  \nDevelop an active-query algorithm that reconstructs a polynomial neural network\u2019s architecture and weights from function evaluations (and optionally gradients), extending the \u201cunique recovery from full output knowledge\u201d result to a practical, sample-efficient setting. Use polynomial interpolation plus structural constraints from PNN neurovarieties to provably reduce queries from dense-grid sampling to near-parameter-count scaling. Benchmark recovery accuracy vs. noise and depth, and derive identifiability conditions in terms of degree and width.\n\n2. **Identifiability Limits Under Noisy Moment Observations for Polynomial Maps**  \nBridge X-rank identifiability of polynomial maps with robust tensor power methods by analyzing what happens when the polynomial map is only observed through estimated low-order moments (as in latent-variable learning). Provide finite-sample error bounds that translate moment estimation error into uncertainty sets over decompositions, explicitly characterizing when decomposition remains unique \u201cup to symmetry.\u201d Deliver a practical diagnostic that flags when data are insufficient to identify a PNN-generated polynomial map.\n\n3. **Learning Degree as a Predictor of Optimization Hardness in Training PNNs**  \nOperationalize the \u201clearning degree\u201d (algebraic degree of the neurovariety) as a quantitative predictor of training difficulty: construct families of PNNs where learning degree is varied while parameter count is held fixed. Measure correlations with critical point counts, gradient-based convergence time, and frequency of bad local minima, tying back to finiteness/genericity claims about critical points. The output would be both empirical evidence and a theoretical explanation linking learning degree to landscape complexity.\n\n4. **Neurovariety-Guided Architecture Selection via Dimension Matching**  \nCreate an architecture-selection method that chooses PNN depth/width by matching the neurovariety dimension to an estimated \u201cintrinsic dimension\u201d of the target function class (e.g., via local polynomial fits or moment rank estimates). The key contribution is a concrete pipeline: estimate target polynomial complexity \u2192 choose candidate PNNs whose neurovariety dimensions minimally exceed it \u2192 train and compare. This turns geometric expressivity measures into an actionable, data-driven model selection criterion.\n\n5. **Hybrid Tensor\u2013Algebraic Training: Initialize PNNs from CP/X-Rank Decompositions**  \nDesign a training algorithm that first fits a low-order moment tensor of the data (or of labels conditioned on inputs) and extracts a CP/X-rank decomposition, then maps the factors into an initial PNN parameterization consistent with the polynomial decomposition model. The contribution is an explicit initialization-to-weights procedure plus convergence guarantees under identifiability conditions (genericity, rank bounds). Evaluate against random init on synthetic polynomial targets and latent-variable benchmarks.\n\n6. **Degree Threshold Phase Transitions in Expressiveness for Deep PNNs**  \nFormally characterize \u201cactivation degree thresholds\u201d as phase transitions in representable function sets: identify critical degrees where the neurovariety dimension jumps or where generic identifiability fails. Provide computable criteria (in terms of depth, width, and monomial degree) predicting when adding one more layer changes the Zariski closure or increases maximal rank. Validate with experiments that map empirical fit capacity to predicted thresholds.\n\n7. **Symmetry-Aware Uniqueness: Classifying All Equivalent Parameterizations of a Given Polynomial Map**  \nExtend classic uniqueness results (recovering a net from outputs) by explicitly cataloging the full group of parameter symmetries for polynomial neural networks (permutations, scalings, and polynomial-specific invariances). The deliverable is a theorem and algorithm: given a recovered decomposition, enumerate (or describe) all equivalent weight settings, and define canonical representatives. This aids both identifiability theory and practical recovery/compression by avoiding redundant solutions.\n\n8. **Robust Recovery of PNNs from Partial Output Knowledge via Compressed Sampling**  \nInvestigate whether one can recover PNN parameters when outputs are known only on a low-dimensional random subspace or via random linear measurements of the output function (compressed sensing style). Use algebraic geometry of neurovarieties to derive conditions under which such partial observations intersect the neurovariety in finitely many points (generic identifiability). Provide a reconstruction algorithm and quantify how many measurements suffice as a function of dimension/degree.\n\n9. **Neurovariety-Based Generalization Bounds for Polynomial Networks Using Algebraic Complexity**  \nDerive generalization bounds where capacity is controlled by algebraic-geometric invariants (dimension, degree/learning degree) rather than only norm-based or VC-style measures. The key step is to relate the number of distinct labelings/fit functions to degree bounds on semialgebraic sets associated with the network image. Validate whether these bounds better predict test error for PNNs trained on polynomial-like tasks than standard complexity measures.\n\n10. **From Latent Variable Models to Deep Polynomial Nets: End-to-End Identifiable Architectures**  \nConstruct deep PNN architectures whose intermediate representations correspond to identifiable latent-variable models (e.g., mixture components or HMM states) recoverable by tensor decomposition, enabling interpretable hidden layers. The contribution is a concrete design recipe: specify layers so that observable moments yield orthogonal/symmetric tensors amenable to robust power methods, then prove identifiability of both latent factors and final predictor. Demonstrate on synthetic mixtures and real topic-model-like data where learned hidden units align with ground-truth factors.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Query-Efficient Recovery of Polynomial Neural Networks from Black-Box Outputs\nDevelop an active-query algorithm that reconstructs a polynomial neural network\u2019s architecture and weights from function e",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Identifiability Limits Under Noisy Moment Observations for Polynomial Maps\nBridge X-rank identifiability of polynomial maps with robust tensor power methods by analyzing what happens when the polynomi",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Learning Degree as a Predictor of Optimization Hardness in Training PNNs\nOperationalize the \u201clearning degree\u201d (algebraic degree of the neurovariety) as a quantitative predictor of training difficulty:",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Neurovariety-Guided Architecture Selection via Dimension Matching\nCreate an architecture-selection method that chooses PNN depth/width by matching the neurovariety dimension to an estimated \u201cintrinsic",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Hybrid Tensor\u2013Algebraic Training: Initialize PNNs from CP/X-Rank Decompositions\nDesign a training algorithm that first fits a low-order moment tensor of the data (or of labels conditioned on inputs) a",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Degree Threshold Phase Transitions in Expressiveness for Deep PNNs\nFormally characterize \u201cactivation degree thresholds\u201d as phase transitions in representable function sets: identify critical degrees w",
          "is_match": true
        },
        {
          "idea_idx": 6,
          "idea_text": "Symmetry-Aware Uniqueness: Classifying All Equivalent Parameterizations of a Given Polynomial Map\nExtend classic uniqueness results (recovering a net from outputs) by explicitly cataloging the full gr",
          "is_match": true
        },
        {
          "idea_idx": 7,
          "idea_text": "Robust Recovery of PNNs from Partial Output Knowledge via Compressed Sampling\nInvestigate whether one can recover PNN parameters when outputs are known only on a low-dimensional random subspace or via",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Neurovariety-Based Generalization Bounds for Polynomial Networks Using Algebraic Complexity\nDerive generalization bounds where capacity is controlled by algebraic-geometric invariants (dimension, degr",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "From Latent Variable Models to Deep Polynomial Nets: End-to-End Identifiable Architectures\nConstruct deep PNN architectures whose intermediate representations correspond to identifiable latent-variabl",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 20,
      "paper_title": "Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference",
      "contribution": "A systematic diagnosis showing that GPU/kernel-level floating-point non\u2011associativity and reduction ordering produce large, reproducibility\u2011breaking output differences in LLM inference, and a lightweight inference pipeline to mitigate these numerical sources of nondeterminism.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "input_tokens": 12495,
      "output_tokens": 1015,
      "predecessor_details": [
        {
          "success": true,
          "title": "Impacts of floating-point non-associativity on reproducibility for HPC ...",
          "url": "https://arxiv.org/abs/2408.05148",
          "content": "[2408.05148] Impacts of floating-point non-associativity on reproducibility for HPC and deep learning applications\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2408.05148\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Distributed, Parallel, and Cluster Computing\n**arXiv:2408.05148**(cs)\n[Submitted on 9 Aug 2024 ([v1](https://arxiv.org/abs/2408.05148v1)), last revised 30 Oct 2024 (this version, v3)]\n# Title:Impacts of floating-point non-associativity on reproducibility for HPC and deep learning applications\nAuthors:[Sanjif Shanmugavelu](https://arxiv.org/search/cs?searchtype=author&amp;query=Shanmugavelu,+S),[Mathieu Taillefumier](https://arxiv.org/search/cs?searchtype=author&amp;query=Taillefumier,+M),[Christopher Culver](https://arxiv.org/search/cs?searchtype=author&amp;query=Culver,+C),[Oscar Hernandez](https://arxiv.org/search/cs?searchtype=author&amp;query=Hernandez,+O),[Mark Coletti](https://arxiv.org/search/cs?searchtype=author&amp;query=Coletti,+M),[Ada Sedova](https://arxiv.org/search/cs?searchtype=author&amp;query=Sedova,+A)\nView a PDF of the paper titled Impacts of floating-point non-associativity on reproducibility for HPC and deep learning applications, by Sanjif Shanmugavelu and 4 other authors\n[View PDF](https://arxiv.org/pdf/2408.05148)[HTML (experimental)](https://arxiv.org/html/2408.05148v3)> > Abstract:\n> Run to run variability in parallel programs caused by floating-point non-associativity has been known to significantly affect reproducibility in iterative algorithms, due to accumulating errors. Non-reproducibility can critically affect the efficiency and effectiveness of correctness testing for stochastic programs. Recently, the sensitivity of deep learning training and inference pipelines to floating-point non-associativity has been found to sometimes be extreme. It can prevent certification for commercial applications, accurate assessment of robustness and sensitivity, and bug detection. New approaches in scientific computing applications have coupled deep learning models with high-performance computing, leading to an aggravation of debugging and testing challenges. Here we perform an investigation of the statistical properties of floating-point non-associativity within modern parallel programming models, and analyze performance and productivity impacts of replacing atomic operations with deterministic alternatives on GPUs. We examine the recently-added deterministic options in PyTorch within the context of GPU deployment for deep learning, uncovering and quantifying the impacts of input parameters triggering run to run variability and reporting on the reliability and completeness of the documentation. Finally, we evaluate the strategy of exploiting automatic determinism that could be provided by deterministic hardware, using the Groq accelerator for inference portions of the deep learning pipeline. We demonstrate the benefits that a hardware-based strategy can provide within reproducibility and correctness efforts. Subjects:|Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)|\nCite as:|[arXiv:2408.05148](https://arxiv.org/abs/2408.05148)[cs.DC]|\n|(or[arXiv:2408.05148v3](https://arxiv.org/abs/2408.05148v3)[cs.DC]for this version)|\n|[https://doi.org/10.48550/arXiv.2408.05148](https://doi.org/10.48550/arXiv.2408.05148)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Ada Sedova [[view email](https://arxiv.org/show-email/a929034b/2408.05148)]\n**[[v1]](https://arxiv.org/abs/2408.05148v1)**Fri, 9 Aug 2024 16:07:37 UTC (2,037 KB)\n**[[v2]](https://arxiv.org/abs/2408.05148v2)**Fri, 23 Aug 2024 17:40:15 UTC (2,037 KB)\n**[v3]**Wed, 30 Oct 2024 16:52:42 UTC (1,923 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Impacts of floating-point non-associativity on reproducibility for HPC and deep learning applications, by Sanjif Shanmugavelu and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2408.05148)\n* [HTML (experimental)](https://arxiv.org/html/2408.05148v3)\n* [TeX Source](https://arxiv.org/src/2408.05148)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.DC\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2408.05148&amp;function=prev&amp;context=cs.DC) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2408.05148&amp;function=next&amp;context=cs.DC)\n[new](https://arxiv.org/list/cs.DC/new)|[recent](https://arxiv.org/list/cs.DC/recent)|[2024-08](https://arxiv.org/list/cs.DC/2024-08)\nChange to browse by:\n[cs](https://arxiv.org/abs/2408.05148?context=cs)\n[cs.LG](https://arxiv.org/abs/2408.05148?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2408.05148)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2408.05148)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2408.05148)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs ",
          "original_query": "Impacts of floating-point non-associativity on reproducibility for hpc and deep learning applications (2024)",
          "cleaned_query": "Impacts of floating-point non-associativity on reproducibility for hpc and deep learning applications"
        },
        {
          "success": true,
          "title": "cutlass/media/docs/cpp/efficient_gemm.md at main - GitHub",
          "url": "https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/efficient_gemm.md",
          "content": "cutlass/media/docs/cpp/efficient\\_gemm.md at main \u00b7NVIDIA/cutlass \u00b7GitHub\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/efficient_gemm.md)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n \nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n \nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/efficient_gemm.md)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E/blob/show&amp;source=header-repo&amp;source_repo=NVIDIA/cutlass)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[NVIDIA](https://github.com/NVIDIA)/**[cutlass](https://github.com/NVIDIA/cutlass)**Public\n* [Notifications](https://github.com/login?return_to=/NVIDIA/cutlass)You must be signed in to change notification settings\n* [Fork1.6k](https://github.com/login?return_to=/NVIDIA/cutlass)\n* [Star9k](https://github.com/login?return_to=/NVIDIA/cutlass)\n \nYou can\u2019t perform that action at this time.\n",
          "original_query": "Efficient GEMM in CUTLASS (NVIDIA Developer Documentation)",
          "cleaned_query": "Efficient GEMM in CUTLASS (NVIDIA Developer Documentation)"
        },
        {
          "success": true,
          "title": "[PDF] Orca: A Distributed Serving System for Transformer-Based ...",
          "url": "https://www.semanticscholar.org/paper/Orca%3A-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6",
          "content": "[PDF] Orca: A Distributed Serving System for Transformer-Based Generative Models | Semantic Scholar\n[Skip to search form](#search-form)[Skip to main content](#main-content)[Skip to account menu](#account-menu)\n[Semantic ScholarSemantic Scholar's Logo](https://www.semanticscholar.org/)\nSearch 231,260,004 papers from all fields of science\nSearch\n* Corpus ID: 251734964# Orca: A Distributed Serving System for Transformer-Based Generative Models\n```\n@inproceedings{Yu2022OrcaAD,\ntitle={Orca: A Distributed Serving System for Transformer-Based Generative Models},\nauthor={Gyeong-In Yu and Joo Seong Jeong},\nbooktitle={USENIX Symposium on Operating Systems Design and Implementation},\nyear={2022},\nurl={https://api.semanticscholar.org/CorpusID:251734964}\n}\n```\n* [Gyeong-In Yu](https://www.semanticscholar.org/author/Gyeong-In-Yu/40928126),[Joo Seong Jeong](https://www.semanticscholar.org/author/Joo-Seong-Jeong/13705521)\n* Publishedin[USENIX Symposium on Operating\u2026](https://www.semanticscholar.org/venue?name=USENIX%20Symposium%20on%20Operating%20Systems%20Design%20and%20Implementation)2022\n* Computer Science\nTLDR\nThis paper proposes iteration-level scheduling, a new scheduling mechanism that schedules execution at the granularity of iteration (instead of request) where the scheduler invokes the execution engine to run only a single iteration of the model on the batch.Expand\n[View Paper](https://www.usenix.org/conference/osdi22/presentation/yu)\n[usenix.org](https://www.usenix.org/system/files/osdi22-yu.pdf)\nSave to LibrarySave\nCreate AlertAlert\nCite\nShare\n507 Citations\n[\nHighly Influential Citations\n](#citing-papers)[](https://www.semanticscholar.org/faq#influential-citations)\n73\n[\nBackground Citations\n](#citing-papers)\n100\n[\nMethods Citations\n](#citing-papers)\n81\n[\nResults Citations\n](#citing-papers)\n3\n[View All](#citing-papers)\n## Figures and Tables from this paper\n* [\n![figure 1](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/4-Figure1-1.png)\nfigure 1](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/0)\n* [\n![table 1](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/10-Table1-1.png)\ntable 1](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/1)\n* [\n![figure 2](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/5-Figure2-1.png)\nfigure 2](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/2)\n* [\n![figure 3](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/5-Figure3-1.png)\nfigure 3](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/3)\n* [\n![figure 4](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/6-Figure4-1.png)\nfigure 4](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/4)\n* [\n![figure 5](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/7-Figure5-1.png)\nfigure 5](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/5)\n* [\n![figure 6](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/8-Figure6-1.png)\nfigure 6](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/6)\n* [\n![figure 7](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/8-Figure7-1.png)\nfigure 7](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/7)\n* [\n![figure 8](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/10-Figure8-1.png)\nfigure 8](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/8)\n* [\n![figure 9](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/12-Figure9-1.png)\nfigure 9](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/9)\n* [\n![figure 10](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/12-Figure10-1.png)\nfigure 10](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/10)\n* [\n![figure 11](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/13-Figure11-1.png)\nfigure 11](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/11)\n## Topics\nAI-Generated\n[Iteration-level Scheduling(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/68263405938?corpusId=251734964)[FasterTransformer(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/23316458425?corpusId=251734964)[Serving System(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/40452359079?corpusId=251734964)[BatchMaker(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/1421560912?corpusId=251734964)[TensorFlow-Serving(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/19718902336?corpusId=251734964)[LightSeq(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/29493777737?corpusId=251734964)[TurboTransformers(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/24086097315?corpusId=251734964)[Generative Pre-trained Transformer 3(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/30153973311?corpusId=251734964)[Optimal Reciprocal Collision Avoidance(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/46371741145?corpusId=251734964)[Latency(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/32350302981?corpusId=251734964)\n## 507 Citations\nCitation Type\nHas PDF\nAuthor\nMore Filters\nMore Filters\nFilters\nSort by Most Influenced PapersSort by Citation CountSort by Recency\n[### A Survey of LLM Inference Systems\n](https://www.semanticscholar.org/paper/A-Survey-of-LLM-Inference-Systems-Pan-Li/5814cca92325e0644a15e1c7e8df5ad2922b9f77)[J. Pan](https://www.semanticscholar.org/author/J.-Pan/2261352681)[Guoliang Li](https://www.semanticscholar.org/author/Guoliang-Li/2371088227)\nComputer Science\n[ArXiv](https://www.semanticscholar.org/venue?name=ArXiv)\n* 2025\nTLDR\nThis survey reviews techniques for large language model inference, starting from operators and algorithms for request processing, then moving on to techniques for model optimization and execution, including kernel design, batching, and scheduling, before ending with techniques for memory management.Expand\n* [\n4\n](https://www.semanticscholar.org/paper/5814cca92325e0644a15e1c7e8df5ad2922b9f77#citing-papers)\n* [Highly Influenced](https://www.semanticscholar.org/paper/5814cca92325e0644a15e1c7e8df5ad2922b9f77?sort=is-influential#citing-papers)\n[[PDF]](https://www.semanticscholar.org/reader/5814cca92325e0644a15e1c7e8df5ad2922b9f77)\n* 5 Excerpts\nSave\n[### Andes: Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming Services\n](https://www.semanticscholar.org/paper/Andes:-Defining-and-Enhancing-Quality-of-Experience-Liu-Wu/b397af271897328a3111ea406c63553fb963e883)[Jiachen Liu](https://www.semanticscholar.org/author/Jiachen-Liu/2257350256)[Zhiyu Wu](https://www.semanticscholar.org/author/Zhiyu-Wu/2298290649)[Jae-Won Chung](https://www.semanticscholar.org/author/",
          "original_query": "Orca: A distributed serving system for Transformer-Based generative models (OSDI 2022)",
          "cleaned_query": "Orca: A distributed serving system for Transformer-Based generative models"
        },
        {
          "success": true,
          "title": "Nondeterminism and Instability in Neural Network Optimization - arXiv",
          "url": "https://arxiv.org/abs/2103.04514",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2103.04514** (cs)\n\n\\[Submitted on 8 Mar 2021 ( [v1](https://arxiv.org/abs/2103.04514v1)), last revised 10 Jul 2021 (this version, v3)\\]\n\n# Title:Nondeterminism and Instability in Neural Network Optimization\n\nAuthors: [Cecilia Summers](https://arxiv.org/search/cs?searchtype=author&query=Summers,+C), [Michael J. Dinneen](https://arxiv.org/search/cs?searchtype=author&query=Dinneen,+M+J)\n\nView a PDF of the paper titled Nondeterminism and Instability in Neural Network Optimization, by Cecilia Summers and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2103.04514)\n\n> Abstract:Nondeterminism in neural network optimization produces uncertainty in performance, making small improvements difficult to discern from run-to-run variability. While uncertainty can be reduced by training multiple model copies, doing so is time-consuming, costly, and harms reproducibility. In this work, we establish an experimental protocol for understanding the effect of optimization nondeterminism on model diversity, allowing us to isolate the effects of a variety of sources of nondeterminism. Surprisingly, we find that all sources of nondeterminism have similar effects on measures of model diversity. To explain this intriguing fact, we identify the instability of model training, taken as an end-to-end procedure, as the key determinant. We show that even one-bit changes in initial parameters result in models converging to vastly different values. Last, we propose two approaches for reducing the effects of instability on run-to-run variability.\n\n| | |\n| --- | --- |\n| Comments: | ICML 2021 |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2103.04514](https://arxiv.org/abs/2103.04514) \\[cs.LG\\] |\n| (or [arXiv:2103.04514v3](https://arxiv.org/abs/2103.04514v3) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2103.04514](https://doi.org/10.48550/arXiv.2103.04514) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Cecilia Summers \\[ [view email](https://arxiv.org/show-email/21e7c999/2103.04514)\\] **[\\[v1\\]](https://arxiv.org/abs/2103.04514v1)**\nMon, 8 Mar 2021 02:28:18 UTC (103 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2103.04514v2)**\nTue, 1 Jun 2021 00:54:16 UTC (2,268 KB)\n**\\[v3\\]**\nSat, 10 Jul 2021 21:58:40 UTC (2,268 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Nondeterminism and Instability in Neural Network Optimization, by Cecilia Summers and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2103.04514)\n- [TeX Source](https://arxiv.org/src/2103.04514)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2103.04514&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2103.04514&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-03](https://arxiv.org/list/cs.LG/2021-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2103.04514?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2103.04514)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2103.04514)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2103.04514)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2103.html#abs-2103-04514) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2103-04514)\n\n[Cecilia Summers](https://dblp.uni-trier.de/search/author?author=Cecilia%20Summers) [Michael J. Dinneen](https://dblp.uni-trier.de/search/author?author=Michael%20J.%20Dinneen)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2103.04514) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Nondeterminism and instability in neural network optimization (ICML 2021)",
          "cleaned_query": "Nondeterminism and instability in neural network optimization"
        },
        {
          "success": true,
          "title": "Benchmarking Large Language Model Volatility",
          "url": "https://arxiv.org/abs/2311.15180",
          "content": "[2311.15180] Benchmarking Large Language Model Volatility\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[q-fin](https://arxiv.org/list/q-fin/recent)&gt;arXiv:2311.15180\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Quantitative Finance \\> Trading and Market Microstructure\n**arXiv:2311.15180**(q-fin)\n[Submitted on 26 Nov 2023]\n# Title:Benchmarking Large Language Model Volatility\nAuthors:[Boyang Yu](https://arxiv.org/search/q-fin?searchtype=author&amp;query=Yu,+B)\nView a PDF of the paper titled Benchmarking Large Language Model Volatility, by Boyang Yu\n[View PDF](https://arxiv.org/pdf/2311.15180)> > Abstract:\n> The impact of non-deterministic outputs from Large Language Models (LLMs) is not well examined for financial text understanding tasks. Through a compelling case study on investing in the US equity market via news sentiment analysis, we uncover substantial variability in sentence-level sentiment classification results, underscoring the innate volatility of LLM outputs. These uncertainties cascade downstream, leading to more significant variations in portfolio construction and return. While tweaking the temperature parameter in the language model decoder presents a potential remedy, it comes at the expense of stifled creativity. Similarly, while ensembling multiple outputs mitigates the effect of volatile outputs, it demands a notable computational investment. This work furnishes practitioners with invaluable insights for adeptly navigating uncertainty in the integration of LLMs into financial decision-making, particularly in scenarios dictated by non-deterministic information. Comments:|7 pages, 2 figures, Workshop on AI Safety and Robustness In Finance, ICAIF 2023|\nSubjects:|Trading and Market Microstructure (q-fin.TR); Computation and Language (cs.CL)|\nCite as:|[arXiv:2311.15180](https://arxiv.org/abs/2311.15180)[q-fin.TR]|\n|(or[arXiv:2311.15180v1](https://arxiv.org/abs/2311.15180v1)[q-fin.TR]for this version)|\n|[https://doi.org/10.48550/arXiv.2311.15180](https://doi.org/10.48550/arXiv.2311.15180)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Boyang Yu [[view email](https://arxiv.org/show-email/6edd252f/2311.15180)]\n**[v1]**Sun, 26 Nov 2023 03:54:03 UTC (1,616 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Benchmarking Large Language Model Volatility, by Boyang Yu\n* [View PDF](https://arxiv.org/pdf/2311.15180)\n* [TeX Source](https://arxiv.org/src/2311.15180)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\nq-fin.TR\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2311.15180&amp;function=prev&amp;context=q-fin.TR) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2311.15180&amp;function=next&amp;context=q-fin.TR)\n[new](https://arxiv.org/list/q-fin.TR/new)|[recent](https://arxiv.org/list/q-fin.TR/recent)|[2023-11](https://arxiv.org/list/q-fin.TR/2023-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/2311.15180?context=cs)\n[cs.CL](https://arxiv.org/abs/2311.15180?context=cs.CL)\n[q-fin](https://arxiv.org/abs/2311.15180?context=q-fin)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2311.15180)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2311.15180)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2311.15180)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2311.15180)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Benchmarking large language model volatility (arXiv 2023)",
          "cleaned_query": "Benchmarking large language model volatility"
        },
        {
          "success": true,
          "title": "Efficient Memory Management for Large Language Model ...",
          "url": "https://arxiv.org/abs/2309.06180",
          "content": "[2309.06180] Efficient Memory Management for Large Language Model Serving with PagedAttention\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nIn just 5 minutes help us improve arXiv:\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2309.06180\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2309.06180**(cs)\n[Submitted on 12 Sep 2023]\n# Title:Efficient Memory Management for Large Language Model Serving with PagedAttention\nAuthors:[Woosuk Kwon](https://arxiv.org/search/cs?searchtype=author&amp;query=Kwon,+W),[Zhuohan Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Z),[Siyuan Zhuang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhuang,+S),[Ying Sheng](https://arxiv.org/search/cs?searchtype=author&amp;query=Sheng,+Y),[Lianmin Zheng](https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+L),[Cody Hao Yu](https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+C+H),[Joseph E. Gonzalez](https://arxiv.org/search/cs?searchtype=author&amp;query=Gonzalez,+J+E),[Hao Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+H),[Ion Stoica](https://arxiv.org/search/cs?searchtype=author&amp;query=Stoica,+I)\nView a PDF of the paper titled Efficient Memory Management for Large Language Model Serving with PagedAttention, by Woosuk Kwon and 8 other authors\n[View PDF](https://arxiv.org/pdf/2309.06180)> > Abstract:\n> High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM&#39;s source code is publicly available at [> this https URL\n](https://github.com/vllm-project/vllm)> Comments:|SOSP 2023|\nSubjects:|Machine Learning (cs.LG); Distributed, Parallel, and Cluster Computing (cs.DC)|\nCite as:|[arXiv:2309.06180](https://arxiv.org/abs/2309.06180)[cs.LG]|\n|(or[arXiv:2309.06180v1](https://arxiv.org/abs/2309.06180v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2309.06180](https://doi.org/10.48550/arXiv.2309.06180)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Woosuk Kwon [[view email](https://arxiv.org/show-email/2fbc22fc/2309.06180)]\n**[v1]**Tue, 12 Sep 2023 12:50:04 UTC (831 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Efficient Memory Management for Large Language Model Serving with PagedAttention, by Woosuk Kwon and 8 other authors\n* [View PDF](https://arxiv.org/pdf/2309.06180)\n* [TeX Source](https://arxiv.org/src/2309.06180)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2309.06180&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2309.06180&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2023-09](https://arxiv.org/list/cs.LG/2023-09)\nChange to browse by:\n[cs](https://arxiv.org/abs/2309.06180?context=cs)\n[cs.DC](https://arxiv.org/abs/2309.06180?context=cs.DC)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2309.06180)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2309.06180)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2309.06180)\n### [1 blog link](https://arxiv.org/tb/2309.06180)\n([what is this?](https://info.arxiv.org/help/trackback.html))\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add ",
          "original_query": "Efficient memory management for large language model serving with pagedattention (vLLM) (SOSP/OSDI 2023)",
          "cleaned_query": "Efficient memory management for large language model serving with pagedattention (vLLM)"
        }
      ],
      "generated_ideas": [
        "1. **Deterministic-by-Construction GPU Reductions for DL Training**\n   Develop a CUDA/CUTLASS-based library of deterministic reduction primitives (sum, softmax stats, layernorm stats, gradient aggregation) that replaces atomics with reproducible tree-based reductions while minimizing throughput loss. Evaluate on PyTorch\u2019s deterministic modes across GPUs, quantifying accuracy deltas, performance overhead, and which ops dominate run-to-run variability (as in Paper 1).",
        "**Reproducibility-Aware Operator Scheduler for Transformer Serving**\n   Extend Orca\u2019s iteration-level scheduling to include a \u201cnumerical determinism budget\u201d that controls which kernels run in deterministic mode per iteration (e.g., attention softmax, GEMMs with specific epilogues). The contribution is a scheduler that trades slight throughput reductions for provable bounds on output variance, tested on multi-GPU serving with mixed batch sizes (Paper 3 + Paper 1).",
        "**Volatility Propagation Model from Kernel Nondeterminism to LLM Outputs**\n   Build a statistical pipeline that measures how low-level floating-point non-associativity and parallel reduction order propagate to token-level distribution changes and downstream task metrics (e.g., sentiment-to-portfolio volatility in Paper 5). Provide an empirical \u201cvariance decomposition\u201d that attributes output volatility to specific kernels/ops, seeds, and decoding settings (Papers 1, 4, 5).",
        "**PagedAttention with Deterministic KV-Cache Sharing Semantics**\n   Design a deterministic KV-cache paging and sharing protocol for vLLM where page allocation, eviction, and sharing decisions are made reproducible across runs and replicas. The key contribution is eliminating \u201cmemory-management-induced nondeterminism\u201d in latency/throughput and potentially in outputs for speculative/complex decoding, validated under high churn workloads (Paper 6 + Paper 1).",
        "**Numerically Stable, Reproducible Softmax/Attention for Long Contexts**\n   Create a long-context attention implementation that combines paging (PagedAttention) with reproducible log-sum-exp accumulation (e.g., fixed-order block reductions) to reduce both overflow/underflow and run-to-run drift. Benchmark on long-sequence decoding where small probability shifts can change sampling outcomes, reporting variance in perplexity and generation (Papers 6, 1, 5).",
        "**Training Instability Diagnostics Driven by One-Bit Perturbation on Modern Accelerators**\n   Reproduce and extend the \u201cone-bit change\u201d protocol (Paper 4) to quantify how GPU kernel nondeterminism and deterministic modes alter basin selection in optimization. The output is a diagnostic toolkit that separates (a) algorithmic instability from (b) floating-point execution variability, across optimizers, AMP settings, and distributed strategies (Papers 4 + 1).",
        "**Deterministic Mixed-Precision GEMM Recipes with CUTLASS Autotuning Constraints**\n   Define a set of \u201creproducibility-safe\u201d GEMM configurations (tile sizes, accumulation types, epilogues) and integrate them into an autotuner that selects only deterministic or variance-bounded kernels. Evaluate the performance gap versus unrestricted autotuning and identify where determinism costs are minimal for transformer inference/training workloads (Paper 2 + Paper 1).",
        "**Serving-Time Ensemble Compression: Variance Reduction Without Linear Cost**\n   Propose a method to approximate multi-sample/ensemble variance reduction (as suggested in Paper 5) using shared-prefix KV-cache and branching only after divergence, leveraging vLLM KV sharing. The contribution is a serving algorithm that achieves near-ensemble stability in outputs with sublinear extra compute/memory, evaluated on finance sentiment pipelines and general text tasks (Papers 5 + 6).",
        "**Cross-Hardware Determinism Certification Suite for LLM Inference**\n   Create a certification harness that runs identical inference graphs across GPU and deterministic accelerators (e.g., Groq-style determinism from Paper 1) and reports (i) bitwise agreement, (ii) distributional agreement, and (iii) decision-level agreement under sampling. This produces actionable \u201cdeterminism compliance\u201d reports for deployment settings, including which ops break reproducibility and how to patch them (Paper 1 + LLM serving from Papers 3/6).",
        "**Adaptive Decoding that Targets Output Volatility Constraints**\n   Design a decoder that dynamically adjusts temperature/top-p or switches to deterministic decoding when measured output volatility exceeds a user-defined threshold (e.g., for finance decisions). The key contribution is a closed-loop controller that uses lightweight online variance estimates (across repeated partial decodes or logit perturbation probes) to stabilize outputs while preserving creativity when safe (Papers 5 + 3/6)."
      ],
      "generated_ideas_raw": "1. **Deterministic-by-Construction GPU Reductions for DL Training**\n   Develop a CUDA/CUTLASS-based library of deterministic reduction primitives (sum, softmax stats, layernorm stats, gradient aggregation) that replaces atomics with reproducible tree-based reductions while minimizing throughput loss. Evaluate on PyTorch\u2019s deterministic modes across GPUs, quantifying accuracy deltas, performance overhead, and which ops dominate run-to-run variability (as in Paper 1).\n\n2. **Reproducibility-Aware Operator Scheduler for Transformer Serving**\n   Extend Orca\u2019s iteration-level scheduling to include a \u201cnumerical determinism budget\u201d that controls which kernels run in deterministic mode per iteration (e.g., attention softmax, GEMMs with specific epilogues). The contribution is a scheduler that trades slight throughput reductions for provable bounds on output variance, tested on multi-GPU serving with mixed batch sizes (Paper 3 + Paper 1).\n\n3. **Volatility Propagation Model from Kernel Nondeterminism to LLM Outputs**\n   Build a statistical pipeline that measures how low-level floating-point non-associativity and parallel reduction order propagate to token-level distribution changes and downstream task metrics (e.g., sentiment-to-portfolio volatility in Paper 5). Provide an empirical \u201cvariance decomposition\u201d that attributes output volatility to specific kernels/ops, seeds, and decoding settings (Papers 1, 4, 5).\n\n4. **PagedAttention with Deterministic KV-Cache Sharing Semantics**\n   Design a deterministic KV-cache paging and sharing protocol for vLLM where page allocation, eviction, and sharing decisions are made reproducible across runs and replicas. The key contribution is eliminating \u201cmemory-management-induced nondeterminism\u201d in latency/throughput and potentially in outputs for speculative/complex decoding, validated under high churn workloads (Paper 6 + Paper 1).\n\n5. **Numerically Stable, Reproducible Softmax/Attention for Long Contexts**\n   Create a long-context attention implementation that combines paging (PagedAttention) with reproducible log-sum-exp accumulation (e.g., fixed-order block reductions) to reduce both overflow/underflow and run-to-run drift. Benchmark on long-sequence decoding where small probability shifts can change sampling outcomes, reporting variance in perplexity and generation (Papers 6, 1, 5).\n\n6. **Training Instability Diagnostics Driven by One-Bit Perturbation on Modern Accelerators**\n   Reproduce and extend the \u201cone-bit change\u201d protocol (Paper 4) to quantify how GPU kernel nondeterminism and deterministic modes alter basin selection in optimization. The output is a diagnostic toolkit that separates (a) algorithmic instability from (b) floating-point execution variability, across optimizers, AMP settings, and distributed strategies (Papers 4 + 1).\n\n7. **Deterministic Mixed-Precision GEMM Recipes with CUTLASS Autotuning Constraints**\n   Define a set of \u201creproducibility-safe\u201d GEMM configurations (tile sizes, accumulation types, epilogues) and integrate them into an autotuner that selects only deterministic or variance-bounded kernels. Evaluate the performance gap versus unrestricted autotuning and identify where determinism costs are minimal for transformer inference/training workloads (Paper 2 + Paper 1).\n\n8. **Serving-Time Ensemble Compression: Variance Reduction Without Linear Cost**\n   Propose a method to approximate multi-sample/ensemble variance reduction (as suggested in Paper 5) using shared-prefix KV-cache and branching only after divergence, leveraging vLLM KV sharing. The contribution is a serving algorithm that achieves near-ensemble stability in outputs with sublinear extra compute/memory, evaluated on finance sentiment pipelines and general text tasks (Papers 5 + 6).\n\n9. **Cross-Hardware Determinism Certification Suite for LLM Inference**\n   Create a certification harness that runs identical inference graphs across GPU and deterministic accelerators (e.g., Groq-style determinism from Paper 1) and reports (i) bitwise agreement, (ii) distributional agreement, and (iii) decision-level agreement under sampling. This produces actionable \u201cdeterminism compliance\u201d reports for deployment settings, including which ops break reproducibility and how to patch them (Paper 1 + LLM serving from Papers 3/6).\n\n10. **Adaptive Decoding that Targets Output Volatility Constraints**\n   Design a decoder that dynamically adjusts temperature/top-p or switches to deterministic decoding when measured output volatility exceeds a user-defined threshold (e.g., for finance decisions). The key contribution is a closed-loop controller that uses lightweight online variance estimates (across repeated partial decodes or logit perturbation probes) to stabilize outputs while preserving creativity when safe (Papers 5 + 3/6).",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "1. **Deterministic-by-Construction GPU Reductions for DL Training**\n   Develop a CUDA/CUTLASS-based library of deterministic reduction primitives (sum, softmax stats, layernorm stats, gradient aggrega",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "**Reproducibility-Aware Operator Scheduler for Transformer Serving**\n   Extend Orca\u2019s iteration-level scheduling to include a \u201cnumerical determinism budget\u201d that controls which kernels run in determin",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "**Volatility Propagation Model from Kernel Nondeterminism to LLM Outputs**\n   Build a statistical pipeline that measures how low-level floating-point non-associativity and parallel reduction order pro",
          "is_match": true
        },
        {
          "idea_idx": 3,
          "idea_text": "**PagedAttention with Deterministic KV-Cache Sharing Semantics**\n   Design a deterministic KV-cache paging and sharing protocol for vLLM where page allocation, eviction, and sharing decisions are made",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "**Numerically Stable, Reproducible Softmax/Attention for Long Contexts**\n   Create a long-context attention implementation that combines paging (PagedAttention) with reproducible log-sum-exp accumulat",
          "is_match": true
        },
        {
          "idea_idx": 5,
          "idea_text": "**Training Instability Diagnostics Driven by One-Bit Perturbation on Modern Accelerators**\n   Reproduce and extend the \u201cone-bit change\u201d protocol (Paper 4) to quantify how GPU kernel nondeterminism and",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "**Deterministic Mixed-Precision GEMM Recipes with CUTLASS Autotuning Constraints**\n   Define a set of \u201creproducibility-safe\u201d GEMM configurations (tile sizes, accumulation types, epilogues) and integra",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "**Serving-Time Ensemble Compression: Variance Reduction Without Linear Cost**\n   Propose a method to approximate multi-sample/ensemble variance reduction (as suggested in Paper 5) using shared-prefix ",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "**Cross-Hardware Determinism Certification Suite for LLM Inference**\n   Create a certification harness that runs identical inference graphs across GPU and deterministic accelerators (e.g., Groq-style ",
          "is_match": true
        },
        {
          "idea_idx": 9,
          "idea_text": "**Adaptive Decoding that Targets Output Volatility Constraints**\n   Design a decoder that dynamically adjusts temperature/top-p or switches to deterministic decoding when measured output volatility ex",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 21,
      "paper_title": "PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models",
      "contribution": "PRIMT reduces human labeling and improves reward learning in preference-based RL by using a hierarchical fusion of multimodal foundation models for synthetic feedback together with foresight and hindsight trajectory synthesis (including SCM-based counterfactuals) to reduce query ambiguity and improve credit assignment.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12050,
      "output_tokens": 1063,
      "predecessor_details": [
        {
          "success": true,
          "title": "Deep reinforcement learning from human preferences",
          "url": "https://arxiv.org/abs/1706.03741",
          "content": "[1706.03741] Deep reinforcement learning from human preferences\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:1706.03741\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:1706.03741**(stat)\n[Submitted on 12 Jun 2017 ([v1](https://arxiv.org/abs/1706.03741v1)), last revised 17 Feb 2023 (this version, v4)]\n# Title:Deep reinforcement learning from human preferences\nAuthors:[Paul Christiano](https://arxiv.org/search/stat?searchtype=author&amp;query=Christiano,+P),[Jan Leike](https://arxiv.org/search/stat?searchtype=author&amp;query=Leike,+J),[Tom B. Brown](https://arxiv.org/search/stat?searchtype=author&amp;query=Brown,+T+B),[Miljan Martic](https://arxiv.org/search/stat?searchtype=author&amp;query=Martic,+M),[Shane Legg](https://arxiv.org/search/stat?searchtype=author&amp;query=Legg,+S),[Dario Amodei](https://arxiv.org/search/stat?searchtype=author&amp;query=Amodei,+D)\nView a PDF of the paper titled Deep reinforcement learning from human preferences, by Paul Christiano and 5 other authors\n[View PDF](https://arxiv.org/pdf/1706.03741)> > Abstract:\n> For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent&#39;s interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback. Subjects:|Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)|\nCite as:|[arXiv:1706.03741](https://arxiv.org/abs/1706.03741)[stat.ML]|\n|(or[arXiv:1706.03741v4](https://arxiv.org/abs/1706.03741v4)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.1706.03741](https://doi.org/10.48550/arXiv.1706.03741)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Paul Christiano [[view email](https://arxiv.org/show-email/9bdfcc4b/1706.03741)]\n**[[v1]](https://arxiv.org/abs/1706.03741v1)**Mon, 12 Jun 2017 17:23:59 UTC (3,355 KB)\n**[[v2]](https://arxiv.org/abs/1706.03741v2)**Sun, 2 Jul 2017 20:25:56 UTC (3,355 KB)\n**[[v3]](https://arxiv.org/abs/1706.03741v3)**Thu, 13 Jul 2017 20:18:41 UTC (3,355 KB)\n**[v4]**Fri, 17 Feb 2023 17:00:34 UTC (3,356 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Deep reinforcement learning from human preferences, by Paul Christiano and 5 other authors\n* [View PDF](https://arxiv.org/pdf/1706.03741)\n* [TeX Source](https://arxiv.org/src/1706.03741)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1706.03741&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1706.03741&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2017-06](https://arxiv.org/list/stat.ML/2017-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1706.03741?context=cs)\n[cs.AI](https://arxiv.org/abs/1706.03741?context=cs.AI)\n[cs.HC](https://arxiv.org/abs/1706.03741?context=cs.HC)\n[cs.LG](https://arxiv.org/abs/1706.03741?context=cs.LG)\n[stat](https://arxiv.org/abs/1706.03741?context=stat)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1706.03741)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1706.03741)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1706.03741)\n### [2 blog links](https://arxiv.org/tb/1706.03741)\n([what is this?](https://info.arxiv.org/help/trackback.html))\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1706.03741)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://inf",
          "original_query": "Deep Reinforcement Learning from Human Preferences",
          "cleaned_query": "Deep Reinforcement Learning from Human Preferences"
        },
        {
          "success": true,
          "title": "[1707.01495] Hindsight Experience Replay - arXiv",
          "url": "https://arxiv.org/abs/1707.01495",
          "content": "[1707.01495] Hindsight Experience Replay[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1707.01495\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1707.01495**(cs)\n[Submitted on 5 Jul 2017 ([v1](https://arxiv.org/abs/1707.01495v1)), last revised 23 Feb 2018 (this version, v3)]\n# Title:Hindsight Experience Replay\nAuthors:[Marcin Andrychowicz](https://arxiv.org/search/cs?searchtype=author&amp;query=Andrychowicz,+M),[Filip Wolski](https://arxiv.org/search/cs?searchtype=author&amp;query=Wolski,+F),[Alex Ray](https://arxiv.org/search/cs?searchtype=author&amp;query=Ray,+A),[Jonas Schneider](https://arxiv.org/search/cs?searchtype=author&amp;query=Schneider,+J),[Rachel Fong](https://arxiv.org/search/cs?searchtype=author&amp;query=Fong,+R),[Peter Welinder](https://arxiv.org/search/cs?searchtype=author&amp;query=Welinder,+P),[Bob McGrew](https://arxiv.org/search/cs?searchtype=author&amp;query=McGrew,+B),[Josh Tobin](https://arxiv.org/search/cs?searchtype=author&amp;query=Tobin,+J),[Pieter Abbeel](https://arxiv.org/search/cs?searchtype=author&amp;query=Abbeel,+P),[Wojciech Zaremba](https://arxiv.org/search/cs?searchtype=author&amp;query=Zaremba,+W)\nView a PDF of the paper titled Hindsight Experience Replay, by Marcin Andrychowicz and 9 other authors\n[View PDF](https://arxiv.org/pdf/1707.01495)> > Abstract:\n> Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum.\n> We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. Subjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Robotics (cs.RO)|\nCite as:|[arXiv:1707.01495](https://arxiv.org/abs/1707.01495)[cs.LG]|\n|(or[arXiv:1707.01495v3](https://arxiv.org/abs/1707.01495v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1707.01495](https://doi.org/10.48550/arXiv.1707.01495)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Marcin Andrychowicz [[view email](https://arxiv.org/show-email/c7a00ded/1707.01495)]\n**[[v1]](https://arxiv.org/abs/1707.01495v1)**Wed, 5 Jul 2017 17:55:53 UTC (1,023 KB)\n**[[v2]](https://arxiv.org/abs/1707.01495v2)**Mon, 10 Jul 2017 18:35:33 UTC (1,023 KB)\n**[v3]**Fri, 23 Feb 2018 10:04:20 UTC (1,238 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Hindsight Experience Replay, by Marcin Andrychowicz and 9 other authors\n* [View PDF](https://arxiv.org/pdf/1707.01495)\n* [TeX Source](https://arxiv.org/src/1707.01495)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1707.01495&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1707.01495&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2017-07](https://arxiv.org/list/cs.LG/2017-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/1707.01495?context=cs)\n[cs.AI](https://arxiv.org/abs/1707.01495?context=cs.AI)\n[cs.NE](https://arxiv.org/abs/1707.01495?context=cs.NE)\n[cs.RO](https://arxiv.org/abs/1707.01495?context=cs.RO)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1707.01495)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1707.01495)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1707.01495)\n### [4 blog links](https://arxiv.org/tb/1707.01495)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1707.html#AndrychowiczWRS17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/AndrychowiczWRS17)\n[Marcin Andrychowicz]()\n[Filip Wolski]()\n[Alex Ray]()\n[Jonas Schneider]()\n[Rachel Fong]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*(",
          "original_query": "Hindsight Experience Replay",
          "cleaned_query": "Hindsight Experience Replay"
        },
        {
          "success": true,
          "title": "[2009.01325] Learning to summarize from human feedback - arXiv",
          "url": "https://arxiv.org/abs/2009.01325",
          "content": "[2009.01325] Learning to summarize from human feedback\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2009.01325\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2009.01325**(cs)\n[Submitted on 2 Sep 2020 ([v1](https://arxiv.org/abs/2009.01325v1)), last revised 15 Feb 2022 (this version, v3)]\n# Title:Learning to summarize from human feedback\nAuthors:[Nisan Stiennon](https://arxiv.org/search/cs?searchtype=author&amp;query=Stiennon,+N),[Long Ouyang](https://arxiv.org/search/cs?searchtype=author&amp;query=Ouyang,+L),[Jeff Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+J),[Daniel M. Ziegler](https://arxiv.org/search/cs?searchtype=author&amp;query=Ziegler,+D+M),[Ryan Lowe](https://arxiv.org/search/cs?searchtype=author&amp;query=Lowe,+R),[Chelsea Voss](https://arxiv.org/search/cs?searchtype=author&amp;query=Voss,+C),[Alec Radford](https://arxiv.org/search/cs?searchtype=author&amp;query=Radford,+A),[Dario Amodei](https://arxiv.org/search/cs?searchtype=author&amp;query=Amodei,+D),[Paul Christiano](https://arxiv.org/search/cs?searchtype=author&amp;query=Christiano,+P)\nView a PDF of the paper titled Learning to summarize from human feedback, by Nisan Stiennon and 8 other authors\n[View PDF](https://arxiv.org/pdf/2009.01325)> > Abstract:\n> As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want. Comments:|NeurIPS 2020|\nSubjects:|Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)|\nCite as:|[arXiv:2009.01325](https://arxiv.org/abs/2009.01325)[cs.CL]|\n|(or[arXiv:2009.01325v3](https://arxiv.org/abs/2009.01325v3)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2009.01325](https://doi.org/10.48550/arXiv.2009.01325)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Ryan Lowe T. [[view email](https://arxiv.org/show-email/0f0b6052/2009.01325)]\n**[[v1]](https://arxiv.org/abs/2009.01325v1)**Wed, 2 Sep 2020 19:54:41 UTC (7,777 KB)\n**[[v2]](https://arxiv.org/abs/2009.01325v2)**Tue, 27 Oct 2020 22:19:53 UTC (7,781 KB)\n**[v3]**Tue, 15 Feb 2022 19:09:36 UTC (3,889 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Learning to summarize from human feedback, by Nisan Stiennon and 8 other authors\n* [View PDF](https://arxiv.org/pdf/2009.01325)\n* [TeX Source](https://arxiv.org/src/2009.01325)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2009.01325&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2009.01325&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2020-09](https://arxiv.org/list/cs.CL/2020-09)\nChange to browse by:\n[cs](https://arxiv.org/abs/2009.01325?context=cs)\n[cs.AI](https://arxiv.org/abs/2009.01325?context=cs.AI)\n[cs.LG](https://arxiv.org/abs/2009.01325?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2009.01325)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2009.01325)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2009.01325)\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2009.html#abs-2009-01325)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2009-01325)\n[Long Ouyang]()\n[Ryan Lowe]()\n[Alec Radford]()\n[Dario Amodei]()\n[Paul F. Christiano]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* I",
          "original_query": "Learning to Summarize with Human Feedback (RLHF pipeline applied to language models)",
          "cleaned_query": "Learning to Summarize with Human Feedback (RLHF pipeline applied to language models)"
        },
        {
          "success": true,
          "title": "Constitutional AI: Harmlessness from AI Feedback - arXiv",
          "url": "https://arxiv.org/abs/2212.08073",
          "content": "[2212.08073] Constitutional AI: Harmlessness from AI Feedback[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## arXiv Is Hiring a DevOps Engineer\nWork on one of the world's most important websites and make an impact on open science.\n[**View Jobs**](https://info.arxiv.org/hiring/index.html)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\narXiv Is Hiring a DevOps Engineer\n[View Jobs](https://info.arxiv.org/hiring/index.html)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2212.08073\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2212.08073**(cs)\n[Submitted on 15 Dec 2022]\n# Title:Constitutional AI: Harmlessness from AI Feedback\nAuthors:[Yuntao Bai](https://arxiv.org/search/cs?searchtype=author&amp;query=Bai,+Y),[Saurav Kadavath](https://arxiv.org/search/cs?searchtype=author&amp;query=Kadavath,+S),[Sandipan Kundu](https://arxiv.org/search/cs?searchtype=author&amp;query=Kundu,+S),[Amanda Askell](https://arxiv.org/search/cs?searchtype=author&amp;query=Askell,+A),[Jackson Kernion](https://arxiv.org/search/cs?searchtype=author&amp;query=Kernion,+J),[Andy Jones](https://arxiv.org/search/cs?searchtype=author&amp;query=Jones,+A),[Anna Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+A),[Anna Goldie](https://arxiv.org/search/cs?searchtype=author&amp;query=Goldie,+A),[Azalia Mirhoseini](https://arxiv.org/search/cs?searchtype=author&amp;query=Mirhoseini,+A),[Cameron McKinnon](https://arxiv.org/search/cs?searchtype=author&amp;query=McKinnon,+C),[Carol Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+C),[Catherine Olsson](https://arxiv.org/search/cs?searchtype=author&amp;query=Olsson,+C),[Christopher Olah](https://arxiv.org/search/cs?searchtype=author&amp;query=Olah,+C),[Danny Hernandez](https://arxiv.org/search/cs?searchtype=author&amp;query=Hernandez,+D),[Dawn Drain](https://arxiv.org/search/cs?searchtype=author&amp;query=Drain,+D),[Deep Ganguli](https://arxiv.org/search/cs?searchtype=author&amp;query=Ganguli,+D),[Dustin Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+D),[Eli Tran-Johnson](https://arxiv.org/search/cs?searchtype=author&amp;query=Tran-Johnson,+E),[Ethan Perez](https://arxiv.org/search/cs?searchtype=author&amp;query=Perez,+E),[Jamie Kerr](https://arxiv.org/search/cs?searchtype=author&amp;query=Kerr,+J),[Jared Mueller](https://arxiv.org/search/cs?searchtype=author&amp;query=Mueller,+J),[Jeffrey Ladish](https://arxiv.org/search/cs?searchtype=author&amp;query=Ladish,+J),[Joshua Landau](https://arxiv.org/search/cs?searchtype=author&amp;query=Landau,+J),[Kamal Ndousse](https://arxiv.org/search/cs?searchtype=author&amp;query=Ndousse,+K),[Kamile Lukosuite](https://arxiv.org/search/cs?searchtype=author&amp;query=Lukosuite,+K),[Liane Lovitt](https://arxiv.org/search/cs?searchtype=author&amp;query=Lovitt,+L),[Michael Sellitto](https://arxiv.org/search/cs?searchtype=author&amp;query=Sellitto,+M),[Nelson Elhage](https://arxiv.org/search/cs?searchtype=author&amp;query=Elhage,+N),[Nicholas Schiefer](https://arxiv.org/search/cs?searchtype=author&amp;query=Schiefer,+N),[Noemi Mercado](https://arxiv.org/search/cs?searchtype=author&amp;query=Mercado,+N),[Nova DasSarma](https://arxiv.org/search/cs?searchtype=author&amp;query=DasSarma,+N),[Robert Lasenby](https://arxiv.org/search/cs?searchtype=author&amp;query=Lasenby,+R),[Robin Larson](https://arxiv.org/search/cs?searchtype=author&amp;query=Larson,+R),[Sam Ringer](https://arxiv.org/search/cs?searchtype=author&amp;query=Ringer,+S),[Scott Johnston](https://arxiv.org/search/cs?searchtype=author&amp;query=Johnston,+S),[Shauna Kravec](https://arxiv.org/search/cs?searchtype=author&amp;query=Kravec,+S),[Sheer El Showk](https://arxiv.org/search/cs?searchtype=author&amp;query=Showk,+S+E),[Stanislav Fort](https://arxiv.org/search/cs?searchtype=author&amp;query=Fort,+S),[Tamera Lanham](https://arxiv.org/search/cs?searchtype=author&amp;query=Lanham,+T),[Timothy Telleen-Lawton](https://arxiv.org/search/cs?searchtype=author&amp;query=Telleen-Lawton,+T),[Tom Conerly](https://arxiv.org/search/cs?searchtype=author&amp;query=Conerly,+T),[Tom Henighan](https://arxiv.org/search/cs?searchtype=author&amp;query=Henighan,+T),[Tristan Hume](https://arxiv.org/search/cs?searchtype=author&amp;query=Hume,+T),[Samuel R. Bowman](https://arxiv.org/search/cs?searchtype=author&amp;query=Bowman,+S+R),[Zac Hatfield-Dodds](https://arxiv.org/search/cs?searchtype=author&amp;query=Hatfield-Dodds,+Z),[Ben Mann](https://arxiv.org/search/cs?searchtype=author&amp;query=Mann,+B),[Dario Amodei](https://arxiv.org/search/cs?searchtype=author&amp;query=Amodei,+D),[Nicholas Joseph](https://arxiv.org/search/cs?searchtype=author&amp;query=Joseph,+N),[Sam McCandlish](https://arxiv.org/search/cs?searchtype=author&amp;query=McCandlish,+S),[Tom Brown](https://arxiv.org/search/cs?searchtype=author&amp;query=Brown,+T),[Jared Kaplan](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaplan,+J)\nView a PDF of the paper titled Constitutional AI: Harmlessness from AI Feedback, by Yuntao Bai and 50 other authors\n[View PDF](https://arxiv.org/pdf/2212.08073)> > Abstract:\n> As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as &#39;Constitutional AI&#39;. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use &#39;RL from AI Feedback&#39; (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels. Subjects:|Computation and Language (cs.CL); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2212.08073](https://arxiv.org/abs/2212.08073)[cs.CL]|\n|(or[arXiv:2212.08073v1](https://arxiv.org/abs/2212.08073v1)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2212.08073](https://doi.org/10.48550/arXiv.2212.08073)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jared K",
          "original_query": "Constitutional AI / LM self-critique (LM-to-LM critique for alignment without humans)",
          "cleaned_query": "Constitutional AI"
        },
        {
          "success": true,
          "title": "PaLM-E: An Embodied Multimodal Language Model - arXiv",
          "url": "https://arxiv.org/abs/2303.03378",
          "content": "[2303.03378] PaLM-E: An Embodied Multimodal Language Model\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2303.03378\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2303.03378**(cs)\n[Submitted on 6 Mar 2023]\n# Title:PaLM-E: An Embodied Multimodal Language Model\nAuthors:[Danny Driess](https://arxiv.org/search/cs?searchtype=author&amp;query=Driess,+D),[Fei Xia](https://arxiv.org/search/cs?searchtype=author&amp;query=Xia,+F),[Mehdi S. M. Sajjadi](https://arxiv.org/search/cs?searchtype=author&amp;query=Sajjadi,+M+S+M),[Corey Lynch](https://arxiv.org/search/cs?searchtype=author&amp;query=Lynch,+C),[Aakanksha Chowdhery](https://arxiv.org/search/cs?searchtype=author&amp;query=Chowdhery,+A),[Brian Ichter](https://arxiv.org/search/cs?searchtype=author&amp;query=Ichter,+B),[Ayzaan Wahid](https://arxiv.org/search/cs?searchtype=author&amp;query=Wahid,+A),[Jonathan Tompson](https://arxiv.org/search/cs?searchtype=author&amp;query=Tompson,+J),[Quan Vuong](https://arxiv.org/search/cs?searchtype=author&amp;query=Vuong,+Q),[Tianhe Yu](https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+T),[Wenlong Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+W),[Yevgen Chebotar](https://arxiv.org/search/cs?searchtype=author&amp;query=Chebotar,+Y),[Pierre Sermanet](https://arxiv.org/search/cs?searchtype=author&amp;query=Sermanet,+P),[Daniel Duckworth](https://arxiv.org/search/cs?searchtype=author&amp;query=Duckworth,+D),[Sergey Levine](https://arxiv.org/search/cs?searchtype=author&amp;query=Levine,+S),[Vincent Vanhoucke](https://arxiv.org/search/cs?searchtype=author&amp;query=Vanhoucke,+V),[Karol Hausman](https://arxiv.org/search/cs?searchtype=author&amp;query=Hausman,+K),[Marc Toussaint](https://arxiv.org/search/cs?searchtype=author&amp;query=Toussaint,+M),[Klaus Greff](https://arxiv.org/search/cs?searchtype=author&amp;query=Greff,+K),[Andy Zeng](https://arxiv.org/search/cs?searchtype=author&amp;query=Zeng,+A),[Igor Mordatch](https://arxiv.org/search/cs?searchtype=author&amp;query=Mordatch,+I),[Pete Florence](https://arxiv.org/search/cs?searchtype=author&amp;query=Florence,+P)\nView a PDF of the paper titled PaLM-E: An Embodied Multimodal Language Model, by Danny Driess and 21 other authors\n[View PDF](https://arxiv.org/pdf/2303.03378)> > Abstract:\n> Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale. Subjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)|\nCite as:|[arXiv:2303.03378](https://arxiv.org/abs/2303.03378)[cs.LG]|\n|(or[arXiv:2303.03378v1](https://arxiv.org/abs/2303.03378v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2303.03378](https://doi.org/10.48550/arXiv.2303.03378)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Danny Driess [[view email](https://arxiv.org/show-email/4520d8fc/2303.03378)]\n**[v1]**Mon, 6 Mar 2023 18:58:06 UTC (10,064 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled PaLM-E: An Embodied Multimodal Language Model, by Danny Driess and 21 other authors\n* [View PDF](https://arxiv.org/pdf/2303.03378)\n* [TeX Source](https://arxiv.org/src/2303.03378)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2303.03378&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2303.03378&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2023-03](https://arxiv.org/list/cs.LG/2023-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2303.03378?context=cs)\n[cs.AI](https://arxiv.org/abs/2303.03378?context=cs.AI)\n[cs.RO](https://arxiv.org/abs/2303.03378?context=cs.RO)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2303.03378)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2303.03378)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2303.03378)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://t",
          "original_query": "PaLM-E: An Embodied Multimodal Language Model for Robotic Perception and Action",
          "cleaned_query": "PaLM-E: An Embodied Multimodal Language Model for Robotic Perception and Action"
        },
        {
          "success": true,
          "title": "Models, Reasoning, and Inference, by Judea Pearl - ScienceDirect",
          "url": "https://www.sciencedirect.com/science/article/pii/S0022249601914075",
          "content": "Abstract Judea Pearl is Professor Emeritus of Computer Science and Statistics and Director of the Cognitive Systems Laboratory at University of California, Los Angeles. He has authored the books Heuristics (1984) and Probabilistic Reasoning in Intelligent Systems (1988). He has also published close to 300 articles on automated reasoning, learning, and inference. In 1999, he was awarded the IJCAI Research Excellence Award in Artificial Intelligence fundamental work in heuristic search, reasoning under uncertainty, and causality. Susan F. Butler is a doctoral student at Tufts University. Her work is in modeling of judgment and decision making, stochastic processes, and the examination of choice behavior under uncertainty.",
          "original_query": "Causality: Models, Reasoning, and Inference (Judea Pearl)",
          "cleaned_query": "Causality: Models, Reasoning, and Inference (Judea Pearl)"
        }
      ],
      "generated_ideas": [
        "Causal Reward Modeling from Human Preferences\nBuild a preference-based reward model that explicitly separates causal features (that should influence reward) from spurious correlates (that happen to co-occur) using Pearl-style causal graphs and interventions. Concretely, train a reward model on trajectory-segment comparisons plus targeted \u201ccounterfactual\u201d segment edits (e.g., changing background objects/visual textures) to enforce invariance to non-causal factors, then evaluate robustness under distribution shift in Atari and robot simulation.",
        "Hindsight Preference Replay (HPR) for Sparse-Feedback Alignment\nExtend Hindsight Experience Replay by relabeling not only goals but also *preference queries*: for failed episodes, generate alternative \u201cachieved-goal\u201d segments and ask (or simulate via a reward model) which behaviors are preferred given that achieved goal. Implement an off-policy pipeline where a preference-trained reward model provides dense learning signal, while hindsight relabeling ensures the model sees informative near-miss behavior, improving sample efficiency on binary-reward manipulation.",
        "Constitutional Preference Models for Embodied Agents\nCombine Constitutional AI with preference-based RL by encoding \u201crobot constitutions\u201d (e.g., safety, non-destruction, privacy, human comfort) as self-critique rules that generate synthetic preference labels over robot trajectories. Train a reward model on a mixture of human comparisons and constitution-generated comparisons, then fine-tune a policy for manipulation/navigation; measure reduced harmful behaviors and lower human oversight costs.",
        "Multimodal Trajectory Summarization to Reduce Human Feedback Cost\nAdapt \u201clearning to summarize from human feedback\u201d to robotics by training a model to produce short, faithful *trajectory summaries* (language + keyframes + state highlights) that enable humans to give preferences faster and more consistently. The core experiment: compare preference-label quality and throughput when humans view raw videos vs. learned summaries, and quantify downstream gains in preference-RL performance per unit human-minute.",
        "PaLM-E\u2013Guided Goal Proposal for Hindsight RL\nUse an embodied multimodal language model (PaLM-E-style) to propose semantically meaningful alternative goals for HER relabeling (e.g., \u201cplace the mug near the sink\u201d vs. arbitrary achieved end-states). Implement a goal-proposal module that maps observations to a set of language-grounded candidate goals, then relabel replay with these goals; evaluate whether semantic relabeling improves transfer to new object layouts and novel instructions.",
        "Counterfactual Preference Explanations for Human-in-the-Loop Debugging\nCreate a system that, given a learned reward model from preferences, generates counterfactual explanations: \u201cYou preferred A over B because object X remained intact; if X were intact in B, preference would flip.\u201d This operationalizes Pearl-style counterfactuals for reward models and enables targeted data collection (ask the next preference query where the explanation is most uncertain), aiming to detect reward hacking and misgeneralization early.",
        "Preference-Based RL Under Annotator Disagreement via Causal Mixtures\nModel multiple human preference \u201ctypes\u201d as latent causal reward functions (mixture model) rather than label noise, learning when disagreements reflect different causal priorities (e.g., speed vs. safety). Train separate reward heads with a shared causal feature encoder and infer annotator clusters; then deploy policies that can either personalize behavior or optimize a socially-aggregated constitution, tested on summarization and embodied tasks.",
        "Constitutional Safety Shields for Off-Policy Preference RL\nBuild a two-layer controller: an off-policy RL agent optimized on a learned preference reward model, plus a constitutional \u201cshield\u201d that filters actions/trajectories violating explicit safety principles derived from Constitutional AI. Implement the shield as a learned classifier over (state, action) with synthetic AI-feedback labels plus a small set of human-labeled violations, and evaluate improved safety with minimal performance loss in robot manipulation.",
        "Interventional Benchmark Suite for Alignment in Embodied and Text Domains\nCreate a shared benchmark where the same underlying \u201cvalues\u201d are tested under interventions in both text (summarization) and robotics (trajectory choice), enabling measurement of causal generalization of preference-trained reward models. Concretely, define paired tasks with controlled spurious cues (style tokens in text; background textures in vision) and require models trained on preferences/constitutions to remain invariant under these interventions, reporting standardized causal robustness metrics."
      ],
      "generated_ideas_raw": "1. **Causal Reward Modeling from Human Preferences**\n   Build a preference-based reward model that explicitly separates causal features (that should influence reward) from spurious correlates (that happen to co-occur) using Pearl-style causal graphs and interventions. Concretely, train a reward model on trajectory-segment comparisons plus targeted \u201ccounterfactual\u201d segment edits (e.g., changing background objects/visual textures) to enforce invariance to non-causal factors, then evaluate robustness under distribution shift in Atari and robot simulation.\n\n2. **Hindsight Preference Replay (HPR) for Sparse-Feedback Alignment**\n   Extend Hindsight Experience Replay by relabeling not only goals but also *preference queries*: for failed episodes, generate alternative \u201cachieved-goal\u201d segments and ask (or simulate via a reward model) which behaviors are preferred given that achieved goal. Implement an off-policy pipeline where a preference-trained reward model provides dense learning signal, while hindsight relabeling ensures the model sees informative near-miss behavior, improving sample efficiency on binary-reward manipulation.\n\n3. **Active Querying with Causal Uncertainty for Preference Learning**\n   Develop an active learning algorithm that selects which trajectory pairs to show humans by maximizing expected reduction in *causal* uncertainty of the reward model (e.g., uncertainty over edges/parents in a learned causal feature graph). The actionable contribution is a query policy that mixes disagreement sampling with targeted interventions (swap variables, occlude objects) to disambiguate what humans care about, aiming to cut human labeling time below the \u201c<1% interactions\u201d regime.\n\n4. **Constitutional Preference Models for Embodied Agents**\n   Combine Constitutional AI with preference-based RL by encoding \u201crobot constitutions\u201d (e.g., safety, non-destruction, privacy, human comfort) as self-critique rules that generate synthetic preference labels over robot trajectories. Train a reward model on a mixture of human comparisons and constitution-generated comparisons, then fine-tune a policy for manipulation/navigation; measure reduced harmful behaviors and lower human oversight costs.\n\n5. **Multimodal Trajectory Summarization to Reduce Human Feedback Cost**\n   Adapt \u201clearning to summarize from human feedback\u201d to robotics by training a model to produce short, faithful *trajectory summaries* (language + keyframes + state highlights) that enable humans to give preferences faster and more consistently. The core experiment: compare preference-label quality and throughput when humans view raw videos vs. learned summaries, and quantify downstream gains in preference-RL performance per unit human-minute.\n\n6. **PaLM-E\u2013Guided Goal Proposal for Hindsight RL**\n   Use an embodied multimodal language model (PaLM-E-style) to propose semantically meaningful alternative goals for HER relabeling (e.g., \u201cplace the mug near the sink\u201d vs. arbitrary achieved end-states). Implement a goal-proposal module that maps observations to a set of language-grounded candidate goals, then relabel replay with these goals; evaluate whether semantic relabeling improves transfer to new object layouts and novel instructions.\n\n7. **Counterfactual Preference Explanations for Human-in-the-Loop Debugging**\n   Create a system that, given a learned reward model from preferences, generates counterfactual explanations: \u201cYou preferred A over B because object X remained intact; if X were intact in B, preference would flip.\u201d This operationalizes Pearl-style counterfactuals for reward models and enables targeted data collection (ask the next preference query where the explanation is most uncertain), aiming to detect reward hacking and misgeneralization early.\n\n8. **Preference-Based RL Under Annotator Disagreement via Causal Mixtures**\n   Model multiple human preference \u201ctypes\u201d as latent causal reward functions (mixture model) rather than label noise, learning when disagreements reflect different causal priorities (e.g., speed vs. safety). Train separate reward heads with a shared causal feature encoder and infer annotator clusters; then deploy policies that can either personalize behavior or optimize a socially-aggregated constitution, tested on summarization and embodied tasks.\n\n9. **Constitutional Safety Shields for Off-Policy Preference RL**\n   Build a two-layer controller: an off-policy RL agent optimized on a learned preference reward model, plus a constitutional \u201cshield\u201d that filters actions/trajectories violating explicit safety principles derived from Constitutional AI. Implement the shield as a learned classifier over (state, action) with synthetic AI-feedback labels plus a small set of human-labeled violations, and evaluate improved safety with minimal performance loss in robot manipulation.\n\n10. **Interventional Benchmark Suite for Alignment in Embodied and Text Domains**\n   Create a shared benchmark where the same underlying \u201cvalues\u201d are tested under interventions in both text (summarization) and robotics (trajectory choice), enabling measurement of causal generalization of preference-trained reward models. Concretely, define paired tasks with controlled spurious cues (style tokens in text; background textures in vision) and require models trained on preferences/constitutions to remain invariant under these interventions, reporting standardized causal robustness metrics.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Causal Reward Modeling from Human Preferences\nBuild a preference-based reward model that explicitly separates causal features (that should influence reward) from spurious correlates (that happen to co",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hindsight Preference Replay (HPR) for Sparse-Feedback Alignment\nExtend Hindsight Experience Replay by relabeling not only goals but also *preference queries*: for failed episodes, generate alternative",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Constitutional Preference Models for Embodied Agents\nCombine Constitutional AI with preference-based RL by encoding \u201crobot constitutions\u201d (e.g., safety, non-destruction, privacy, human comfort) as sel",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Multimodal Trajectory Summarization to Reduce Human Feedback Cost\nAdapt \u201clearning to summarize from human feedback\u201d to robotics by training a model to produce short, faithful *trajectory summaries* (l",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "PaLM-E\u2013Guided Goal Proposal for Hindsight RL\nUse an embodied multimodal language model (PaLM-E-style) to propose semantically meaningful alternative goals for HER relabeling (e.g., \u201cplace the mug near",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Counterfactual Preference Explanations for Human-in-the-Loop Debugging\nCreate a system that, given a learned reward model from preferences, generates counterfactual explanations: \u201cYou preferred A over",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Preference-Based RL Under Annotator Disagreement via Causal Mixtures\nModel multiple human preference \u201ctypes\u201d as latent causal reward functions (mixture model) rather than label noise, learning when di",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Constitutional Safety Shields for Off-Policy Preference RL\nBuild a two-layer controller: an off-policy RL agent optimized on a learned preference reward model, plus a constitutional \u201cshield\u201d that filt",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Interventional Benchmark Suite for Alignment in Embodied and Text Domains\nCreate a shared benchmark where the same underlying \u201cvalues\u201d are tested under interventions in both text (summarization) and r",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 22,
      "paper_title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders",
      "contribution": "Identifies and formalizes 'feature absorption'\u2014a systematic failure mode of Sparse Autoencoders (SAEs) where seemingly monosemantic latents are suppressed by their hierarchical children under sparsity pressure\u2014introduces a metric to detect it, and empirically shows it is pervasive and not remedied by simple SAE size or sparsity tuning.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10740,
      "output_tokens": 1012,
      "predecessor_details": [
        {
          "success": true,
          "title": "Sparse coding with an over-complete basis set: A strategy employed ...",
          "url": "https://www.researchgate.net/publication/13803865_Sparse_coding_with_an_over-complete_basis_set_A_strategy_employed_by_V1_Vision_Research",
          "content": "Article PDF Available Abstract The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete--i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli. Discover the world's research 25+ million members 160+ million publication pages 2.3+ billion citations Join for free... Here, approximate orthogonality is argued to be necessary for circumventing the problem of packing more unique vectors (concepts) than the dimensionality of the space allows (a.k.a. the superposition problem [24]), hence enabling reliable estimation of a concept's presence by a linear operator (e.g., the linear map in an MLP) [29,30]. Grounded in this idea, and inspired by its relation to the well-known problem of sparse coding [31, 32], SAEs have been proposed to learn, in an unsupervised manner, a sparse, overcomplete dictionary of directions that (ideally) map onto abstract, interpretable concepts encoded in a neural network, hence enabling its interpretability [33,14]. Figure 1: Conceptual organization in neural representations. ...... Specifically, noting that the model described in Def. 2.1 closely aligns with the generative model assumed in classical work on sparse coding [32, 31,81], wherein one seeks to express data as sparse linear combinations over an overcomplete dictionary, SAEs were recently proposed to re-contextualize that literature's tools for identifying concepts encoded in a neural network's representations [14][15][16][17][18][19][20][21][22][23]. ...... Sparse coding Sparse linear generative models aim to find a sparse representations z \u2208 R p that explains the data x \u2208 R m via a collection of atoms, forming an overcomplete dictionary D \u2208 R m\u00d7p (p \u226b m.) [106, 32]. Sparse representations are ubiquitous in science and engineering, with origins from computational neuroscience [106,32] and applications in medical imaging [107][108][109], image restorations [110][111][112], radar sensing [113], transcriptomics [114,115], and genomics [116,117]. ... Motivated by the hypothesis that neural network representations encode abstract, interpretable features as linearly accessible, approximately orthogonal directions, sparse autoencoders (SAEs) have become a popular tool in interpretability. However, recent work has demonstrated phenomenology of model representations that lies outside the scope of this hypothesis, showing signatures of hierarchical, nonlinear, and multi-dimensional features. This raises the question: do SAEs represent features that possess structure at odds with their motivating hypothesis? If not, does avoiding this mismatch help identify said features and gain further insights into neural network representations? To answer these questions, we take a construction-based approach and re-contextualize the popular matching pursuits (MP) algorithm from sparse coding to design MP-SAE -- an SAE that unrolls its encoder into a sequence of residual-guided steps, allowing it to capture hierarchical and nonlinearly accessible features. Comparing this architecture with existing SAEs on a mixture of synthetic and natural data settings, we show: (i) hierarchical concepts induce conditionally orthogonal features, which existing SAEs are unable to faithfully capture, and (ii) the nonlinear encoding step of MP-SAE recovers highly meaningful features, helping us unravel shared structure in the seemingly dichotomous representation spaces of different modalities in a vision-language model, hence demonstrating the assumption that useful features are solely linearly accessible is insufficient. We also show that the sequential encoder principle of MP-SAE affords an additional benefit of adaptive sparsity at inference time, which may be of independent interest. Overall, we argue our results provide credence to the idea that interpretability should begin with the phenomenology of representations, with methods emerging from assumptions that fit it.... Sparse coding is inspired by the behavior of neurons in the primary visual cortex (V1) [46]. This method, when applied to an input \u2208 \u211d generates a simpler representation in the form of a sparse vector \u2208 \u211d , where most coefficients are zeros. ... Spike sorting is a crucial step in decoding multichannel extracellular neural signals, enabling the identification of individual neuronal activity. A key challenge in brain-machine interfaces (BMIs) is achieving real-time, low-power spike sorting at the edge while keeping high neural decoding performance. This study introduces the Neuromorphic Sparse Sorter (NSS), a compact two-layer spiking neural network optimized for efficient spike sorting. NSS leverages the Locally Competitive Algorithm (LCA) for sparse coding to extract relevant features from noisy events with reduced computational demands. NSS learns to sort detected spike waveforms in an online fashion and operates entirely unsupervised. To exploit multi-bit spike coding capabilities of neuromorphic platforms like Intel's Loihi 2, a custom neuron model was implemented, enabling flexible power-performance trade-offs via adjustable spike bit-widths. Evaluations on simulated and real-world tetrode signals with biological drift showed NSS outperformed established pipelines such as WaveClus3 and PCA+KMeans. With 2-bit graded spikes, NSS on Loihi 2 outperformed NSS implemented with leaky integrate-and-fire neuron and achieved an F1-score of 77% (+10% improvement) while consuming 8.6mW (+1.65mW) when tested on a drifting recording, with a computational processing time of 0.25ms (+60 us) per inference.... The linear representation hypothesis proposes that semantic concepts are encoded in linear subspaces of neural representations (Mikolov et al., 2013). Additionally, the sparse coding hypothesis assumes that semantic concepts in neural representations can be effectively captured using sparsity priors to identify meaningful features in an unsupervised manner (Olshausen &amp; Field, 1997; Arora et al., 2018;Elhage et al., 2022). Together, these foundational works inspired the development of the SAE architecture and its variants, which produce highly interpretable and semantically meaningful features from neural representations (Cunningham et al., 2023;Bricken et al., 2023;Gao et al., 2024;Rajamanoharan et al., 2024a;b). ... Miles Wang Tom Dupr\u00e9 la Tour Olivia Watkins Dan Mossing Understanding how language models generalize behaviors from their training to a broader deployment distribution is an important problem in AI safety. Betley et al. discovered that fine-tuning GPT-4o on intentionally insecure code causes \"emergent misalignment,\" where models give stereotypically malicious responses to unrelated prompts. We extend this work, demonstrating emergent misalignment across diverse conditions, including reinforcement learning on reasoning models, fine-tuning on various synthetic datasets, and in ",
          "original_query": "Sparse coding with an overcomplete basis set: A strategy employed by v1?",
          "cleaned_query": "Sparse coding with an overcomplete basis set: A strategy employed by v1?"
        },
        {
          "success": true,
          "title": "[2209.10652] Toy Models of Superposition - arXiv",
          "url": "https://arxiv.org/abs/2209.10652",
          "content": "[2209.10652] Toy Models of Superposition\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2209.10652\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2209.10652**(cs)\n[Submitted on 21 Sep 2022]\n# Title:Toy Models of Superposition\nAuthors:[Nelson Elhage](https://arxiv.org/search/cs?searchtype=author&amp;query=Elhage,+N),[Tristan Hume](https://arxiv.org/search/cs?searchtype=author&amp;query=Hume,+T),[Catherine Olsson](https://arxiv.org/search/cs?searchtype=author&amp;query=Olsson,+C),[Nicholas Schiefer](https://arxiv.org/search/cs?searchtype=author&amp;query=Schiefer,+N),[Tom Henighan](https://arxiv.org/search/cs?searchtype=author&amp;query=Henighan,+T),[Shauna Kravec](https://arxiv.org/search/cs?searchtype=author&amp;query=Kravec,+S),[Zac Hatfield-Dodds](https://arxiv.org/search/cs?searchtype=author&amp;query=Hatfield-Dodds,+Z),[Robert Lasenby](https://arxiv.org/search/cs?searchtype=author&amp;query=Lasenby,+R),[Dawn Drain](https://arxiv.org/search/cs?searchtype=author&amp;query=Drain,+D),[Carol Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+C),[Roger Grosse](https://arxiv.org/search/cs?searchtype=author&amp;query=Grosse,+R),[Sam McCandlish](https://arxiv.org/search/cs?searchtype=author&amp;query=McCandlish,+S),[Jared Kaplan](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaplan,+J),[Dario Amodei](https://arxiv.org/search/cs?searchtype=author&amp;query=Amodei,+D),[Martin Wattenberg](https://arxiv.org/search/cs?searchtype=author&amp;query=Wattenberg,+M),[Christopher Olah](https://arxiv.org/search/cs?searchtype=author&amp;query=Olah,+C)\nView a PDF of the paper titled Toy Models of Superposition, by Nelson Elhage and 15 other authors\n[View PDF](https://arxiv.org/pdf/2209.10652)> > Abstract:\n> Neural networks often pack many unrelated concepts into a single neuron - a puzzling phenomenon known as &#39;polysemanticity&#39; which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in &#34;superposition.&#34; We demonstrate the existence of a phase change, a surprising connection to the geometry of uniform polytopes, and evidence of a link to adversarial examples. We also discuss potential implications for mechanistic interpretability. Comments:|Also available at[this https URL](https://transformer-circuits.pub/2022/toy_model/index.html)|\nSubjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:2209.10652](https://arxiv.org/abs/2209.10652)[cs.LG]|\n|(or[arXiv:2209.10652v1](https://arxiv.org/abs/2209.10652v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2209.10652](https://doi.org/10.48550/arXiv.2209.10652)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Samuel McCandlish [[view email](https://arxiv.org/show-email/df0fa1c4/2209.10652)]\n**[v1]**Wed, 21 Sep 2022 20:49:26 UTC (4,804 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Toy Models of Superposition, by Nelson Elhage and 15 other authors\n* [View PDF](https://arxiv.org/pdf/2209.10652)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2209.10652&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2209.10652&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2022-09](https://arxiv.org/list/cs.LG/2022-09)\nChange to browse by:\n[cs](https://arxiv.org/abs/2209.10652?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2209.10652)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2209.10652)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2209.10652)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2209.10652)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Toy models of superposition",
          "cleaned_query": "Toy models of superposition"
        },
        {
          "success": true,
          "title": "Decomposing Language Models With Dictionary Learning",
          "url": "https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning",
          "content": "[Skip to main content](#main-content)[Skip to footer](#footer)\n[\n](https://www.anthropic.com/)\n[Try Claude](https://claude.ai/)\nInterpretabilityResearch\n# Towards Monosemanticity: Decomposing Language Models With Dictionary Learning\nOct 5, 2023\n[Read Paper](https://transformer-circuits.pub/2023/monosemantic-features/index.html)\n## Abstract\nIn our latest paper,[*Towards Monosemanticity: Decomposing Language Models With Dictionary Learning*](https://transformer-circuits.pub/2023/monosemantic-features), we outline evidence that there are better units of analysis than individual neurons, and we have built machinery that lets us find these units in small transformer models. These units, called features, correspond to patterns (linear combinations) of neuron activations. This provides a path to breaking down complex neural networks into parts we can understand, and builds on previous efforts to interpret high-dimensional systems in neuroscience, machine learning, and statistics. In a transformer language model, we decompose a layer with 512 neurons into more than 4000 features which separately represent things like DNA sequences, legal language, HTTP requests, Hebrew text, nutrition statements, and much, much more. Most of these model properties are invisible when looking at the activations of individual neurons in isolation.\n[](https://twitter.com/intent/tweet?text=https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning)[](https://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning)\n## Related content\n### Introducing Anthropic Interviewer: What 1,250 professionals told us about working with AI\nWe built an interview tool called Anthropic Interviewer. Powered by Claude, Anthropic Interviewer runs detailed interviews automatically and at unprecedented scale.\n[Read more](https://www.anthropic.com/research/anthropic-interviewer)\n### How AI is transforming work at Anthropic\nWe surveyed Anthropic engineers and researchers, conducted in-depth qualitative interviews, and studied internal Claude Code usage data to find out how AI use is changing how we do our jobs. We found that AI use is radically changing the nature of work for software developers.\n[Read more](https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic)\n### Estimating AI productivity gains from Claude conversations\nAnalyzing 100,000 Claude conversations, this research finds AI reduces task time by 80% on average. If universally adopted over 10 years, current models could increase US labor productivity growth by 1.8% annually\u2014doubling recent rates. Knowledge work like software development and management see the largest gains.\n[Read more](https://www.anthropic.com/research/estimating-productivity-gains)\nTowards Monosemanticity: Decomposing Language Models With Dictionary Learning \\\\ Anthropic",
          "original_query": "Towards monosemanticity: Decomposing language models with dictionary learning",
          "cleaned_query": "Towards monosemanticity: Decomposing language models with dictionary learning",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "Sparse Autoencoders Find Highly Interpretable Features in ...",
          "url": "https://arxiv.org/abs/2309.08600",
          "content": "[2309.08600] Sparse Autoencoders Find Highly Interpretable Features in Language Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2309.08600\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2309.08600**(cs)\n[Submitted on 15 Sep 2023 ([v1](https://arxiv.org/abs/2309.08600v1)), last revised 4 Oct 2023 (this version, v3)]\n# Title:Sparse Autoencoders Find Highly Interpretable Features in Language Models\nAuthors:[Hoagy Cunningham](https://arxiv.org/search/cs?searchtype=author&amp;query=Cunningham,+H),[Aidan Ewart](https://arxiv.org/search/cs?searchtype=author&amp;query=Ewart,+A),[Logan Riggs](https://arxiv.org/search/cs?searchtype=author&amp;query=Riggs,+L),[Robert Huben](https://arxiv.org/search/cs?searchtype=author&amp;query=Huben,+R),[Lee Sharkey](https://arxiv.org/search/cs?searchtype=author&amp;query=Sharkey,+L)\nView a PDF of the paper titled Sparse Autoencoders Find Highly Interpretable Features in Language Models, by Hoagy Cunningham and 4 other authors\n[View PDF](https://arxiv.org/pdf/2309.08600)> > Abstract:\n> One of the roadblocks to a better understanding of neural networks&#39; internals is \\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability. Comments:|20 pages, 18 figures, 2 tables|\nSubjects:|Machine Learning (cs.LG); Computation and Language (cs.CL)|\nCite as:|[arXiv:2309.08600](https://arxiv.org/abs/2309.08600)[cs.LG]|\n|(or[arXiv:2309.08600v3](https://arxiv.org/abs/2309.08600v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2309.08600](https://doi.org/10.48550/arXiv.2309.08600)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Robert Huben [[view email](https://arxiv.org/show-email/d274585a/2309.08600)]\n**[[v1]](https://arxiv.org/abs/2309.08600v1)**Fri, 15 Sep 2023 17:56:55 UTC (3,437 KB)\n**[[v2]](https://arxiv.org/abs/2309.08600v2)**Tue, 19 Sep 2023 17:20:52 UTC (3,597 KB)\n**[v3]**Wed, 4 Oct 2023 13:17:38 UTC (3,684 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Sparse Autoencoders Find Highly Interpretable Features in Language Models, by Hoagy Cunningham and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2309.08600)\n* [TeX Source](https://arxiv.org/src/2309.08600)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2309.08600&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2309.08600&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2023-09](https://arxiv.org/list/cs.LG/2023-09)\nChange to browse by:\n[cs](https://arxiv.org/abs/2309.08600?context=cs)\n[cs.CL](https://arxiv.org/abs/2309.08600?context=cs.CL)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2309.08600)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2309.08600)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2309.08600)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will a",
          "original_query": "Sparse autoencoders find highly interpretable features in language models",
          "cleaned_query": "Sparse autoencoders find highly interpretable features in language models"
        },
        {
          "success": true,
          "title": "Sparse Autoencoders Do Not Find Canonical Units of Analysis - arXiv",
          "url": "https://arxiv.org/abs/2502.04878",
          "content": "[2502.04878] Sparse Autoencoders Do Not Find Canonical Units of Analysis\n[Skip to main content](#content)\n[![Cornell University](https://www.arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://www.arxiv.org/IgnoreMe)\n[![arxiv logo](https://www.arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://www.arxiv.org/)&gt;[cs](https://www.arxiv.org/list/cs/recent)&gt;arXiv:2502.04878\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://www.arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://www.arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2502.04878**(cs)\n[Submitted on 7 Feb 2025]\n# Title:Sparse Autoencoders Do Not Find Canonical Units of Analysis\nAuthors:[Patrick Leask](https://arxiv.org/search/cs?searchtype=author&amp;query=Leask,+P),[Bart Bussmann](https://arxiv.org/search/cs?searchtype=author&amp;query=Bussmann,+B),[Michael Pearce](https://arxiv.org/search/cs?searchtype=author&amp;query=Pearce,+M),[Joseph Bloom](https://arxiv.org/search/cs?searchtype=author&amp;query=Bloom,+J),[Curt Tigges](https://arxiv.org/search/cs?searchtype=author&amp;query=Tigges,+C),[Noura Al Moubayed](https://arxiv.org/search/cs?searchtype=author&amp;query=Moubayed,+N+A),[Lee Sharkey](https://arxiv.org/search/cs?searchtype=author&amp;query=Sharkey,+L),[Neel Nanda](https://arxiv.org/search/cs?searchtype=author&amp;query=Nanda,+N)\nView a PDF of the paper titled Sparse Autoencoders Do Not Find Canonical Units of Analysis, by Patrick Leask and Bart Bussmann and 6 other authors\n[View PDF](https://www.arxiv.org/pdf/2502.04878)[HTML (experimental)](https://arxiv.org/html/2502.04878v1)> > Abstract:\n> A common goal of mechanistic interpretability is to decompose the activations of neural networks into features: interpretable properties of the input computed by the model. Sparse autoencoders (SAEs) are a popular method for finding these features in LLMs, and it has been postulated that they can be used to find a \\textit{canonical} set of units: a unique and complete list of atomic features. We cast doubt on this belief using two novel techniques: SAE stitching to show they are incomplete, and meta-SAEs to show they are not atomic. SAE stitching involves inserting or swapping latents from a larger SAE into a smaller one. Latents from the larger SAE can be divided into two categories: \\emph{novel latents}, which improve performance when added to the smaller SAE, indicating they capture novel information, and \\emph{reconstruction latents}, which can replace corresponding latents in the smaller SAE that have similar behavior. The existence of novel features indicates incompleteness of smaller SAEs. Using meta-SAEs -- SAEs trained on the decoder matrix of another SAE -- we find that latents in SAEs often decompose into combinations of latents from a smaller SAE, showing that larger SAE latents are not atomic. The resulting decompositions are often interpretable; e.g. a latent representing ``Einstein&#39;&#39; decomposes into ``scientist&#39;&#39;, ``Germany&#39;&#39;, and ``famous person&#39;&#39;. Even if SAEs do not find canonical units of analysis, they may still be useful tools. We suggest that future research should either pursue different approaches for identifying such units, or pragmatically choose the SAE size suited to their task. We provide an interactive dashboard to explore meta-SAEs: [> this https URL\n](https://metasaes.streamlit.app/)> Comments:|Accepted to ICLR 2025|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2502.04878](https://arxiv.org/abs/2502.04878)[cs.LG]|\n|(or[arXiv:2502.04878v1](https://arxiv.org/abs/2502.04878v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2502.04878](https://doi.org/10.48550/arXiv.2502.04878)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Patrick Leask [[view email](https://www.arxiv.org/show-email/54f8c211/2502.04878)]\n**[v1]**Fri, 7 Feb 2025 12:33:08 UTC (4,505 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Sparse Autoencoders Do Not Find Canonical Units of Analysis, by Patrick Leask and Bart Bussmann and 6 other authors\n* [View PDF](https://www.arxiv.org/pdf/2502.04878)\n* [HTML (experimental)](https://arxiv.org/html/2502.04878v1)\n* [TeX Source](https://www.arxiv.org/src/2502.04878)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://www.arxiv.org/prevnext?id=2502.04878&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://www.arxiv.org/prevnext?id=2502.04878&amp;function=next&amp;context=cs.LG)\n[new](https://www.arxiv.org/list/cs.LG/new)|[recent](https://www.arxiv.org/list/cs.LG/recent)|[2025-02](https://www.arxiv.org/list/cs.LG/2025-02)\nChange to browse by:\n[cs](https://www.arxiv.org/abs/2502.04878?context=cs)\n[cs.AI](https://www.arxiv.org/abs/2502.04878?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2502.04878)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2502.04878)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2502.04878)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://www.arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://www.arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# a",
          "original_query": "Showing SAE latents are not atomic using meta-SAEs",
          "cleaned_query": "Showing SAE latents are not atomic using meta-SAEs"
        },
        {
          "success": true,
          "title": "Proximal Methods for Hierarchical Sparse Coding",
          "url": "https://arxiv.org/abs/1009.2139",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: Sparse coding consists in representing signals as sparse linear combinations\nof atoms selected from a dictionary. We consider an extension of this framework\nwhere the atoms are further assumed to be embedded in a tree. This is achieved\nusing a recently introduced tree-structured sparse regularization norm, which\nhas proven useful in several applications. This norm leads to regularized\nproblems that are difficult to optimize, and we propose in this paper efficient\nalgorithms for solving them. More precisely, we show that the proximal operator\nassociated with this norm is computable exactly via a dual approach that can be\nviewed as the composition of elementary proximal operators. Our procedure has a\ncomplexity linear, or close to linear, in the number of atoms, and allows the\nuse of accelerated gradient techniques to solve the tree-structured sparse\napproximation problem at the same computational cost as traditional ones using\nthe L1-norm. Our method is efficient and scales gracefully to millions of\nvariables, which we illustrate in two types of applications: first, we consider\nfixed hierarchical dictionaries of wavelets to denoise natural images. Then, we\napply our optimization tools in the context of dictionary learning, where\nlearned dictionary elements naturally organize in a prespecified arborescent\nstructure, leading to a better performance in reconstruction of natural image\npatches. When applied to text documents, our method learns hierarchies of\ntopics, thus providing a competitive alternative to probabilistic topic models.\n \n \n \n \n Submission history From: Rodolphe Jenatton [ view email]\u00a0[via CCSD proxy]\n \n [v1] \n Sat, 11 Sep 2010 05:46:55 UTC (445 KB) \n [v2] \n Thu, 16 Sep 2010 18:22:27 UTC (433 KB) \n [v3] \n Wed, 9 Mar 2011 16:21:37 UTC (387 KB) [v4] \nTue, 5 Jul 2011 15:04:02 UTC (347 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > stat > arXiv:1009.2139\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Statistics > Machine Learning\n\n arXiv:1009.2139 (stat)\n [Submitted on 11 Sep 2010 (v1), last revised 5 Jul 2011 (this version, v4)]\n\n Title: Proximal Methods for Hierarchical Sparse Coding\n\n Authors: Rodolphe Jenatton (INRIA Paris - Rocquencourt, LIENS), Julien Mairal (INRIA Paris - Rocquencourt, LIENS), Guillaume Obozinski (INRIA Paris - Rocquencourt, LIENS), Francis Bach (INRIA Paris - Rocquencourt, LIENS)\n Download PDF\n Abstract: Sparse coding consists in representing signals as sparse linear combinations of atoms selected from a dictionary. We consider an extension of this framework where the atoms are further assumed to be embedded in a tree. This is achieved using a recently introduced tree-structured sparse regularization norm, which has proven useful in several applications. This norm leads to regularized problems that are difficult to optimize, and we propose in this paper efficient algorithms for solving them. More precisely, we show that the proximal operator associated with this norm is computable exactly via a dual approach that can be viewed as the composition of elementary proximal operators. Our procedure has a complexity linear, or close to linear, in the number of atoms, and allows the use of accelerated gradient techniques to solve the tree-structured sparse approximation problem at the same computational cost as traditional ones using the L1-norm. Our method is efficient and scales gracefully to millions of variables, which we illustrate in two types of applications: first, we consider fixed hierarchical dictionaries of wavelets to denoise natural images. Then, we apply our optimization tools in the context of dictionary learning, where learned dictionary elements naturally organize in a prespecified arborescent structure, leading to a better performance in reconstruction of natural image patches. When applied to text documents, our method learns hierarchies of topics, thus providing a competitive alternative to probabilistic topic models.\n Subjects: Machine Learning (stat.ML) \n Cite as: arXiv:1009.2139 [stat.ML] \n (or arXiv:1009.2139v4 [stat.ML] for this version) \n https://doi.org/10.48550/arXiv.1009.2139 \n Focus to learn more \n arXiv-issued DOI via DataCite \n Journal reference: Journal of Machine Learning Research, 12 (2011) 2297-2334\n \n\n Submission history\n\n From: Rodolphe Jenatton [view email] [via CCSD proxy]\n [v1] Sat, 11 Sep 2010 05:46:55 UTC (445 KB)\n [v2] Thu, 16 Sep 2010 18:22:27 UTC (433 KB)\n [v3] Wed, 9 Mar 2011 16:21:37 UTC (387 KB)\n [v4] Tue, 5 Jul 2011 15:04:02 UTC (347 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * PostScript\n * Other formats\n (license)\n Current browse context:\n stat.ML\n < prev | next >\n new | recent | 1009\n Change to browse by:\n stat\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs and how to get involved.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Proximal methods for hierarchical sparse coding",
          "cleaned_query": "Proximal methods for hierarchical sparse coding"
        }
      ],
      "generated_ideas": [
        "Hierarchical Sparse Autoencoders for Multi-Scale \u201cFeature Trees\u201d in LLM Activations\nCombine SAE-style dictionary learning with tree-structured sparsity (hierarchical proximal norms) so that activating a specific feature implies activating its ancestors (coarser concepts). Train on residual stream/MLP activations and evaluate whether learned trees align with semantic hierarchies (e.g., \u201cscientist \u2192 physicist \u2192 Einstein\u201d) while improving faithfulness on causal tests (activation patching / feature ablations).",
        "Phase-Transition Map of Superposition vs. Sparsity Across Model Scale and Data Regimes\nExtend \u201cToy Models of Superposition\u201d into an empirical study: sweep model width, dataset entropy, regularization strength, and training compute, and measure when representations flip from polysemantic to more separable (using SAE/MP-SAE reconstructions and interference metrics). Produce a predictive \u201cphase diagram\u201d that forecasts feature crowding and polysemanticity from measurable training conditions.",
        "Residual-Guided Sequential Encoders (MP-SAE) as a Standard Decomposition Primitive for Transformers\nImplement MP-SAE-style matching-pursuit encoders for transformer layers and test whether sequential residual steps recover nonlinearly accessible or compositional features missed by standard SAEs. Quantify gains on (i) interpretability metrics, (ii) causal localization of known circuits (e.g., IOI), and (iii) cross-modal shared structure in vision-language models.",
        "Meta-SAE-Guided Feature Refinement: Turning Non-Atomic Latents into Explicit Compositions\nUse meta-SAEs (trained on SAE decoder matrices) to automatically replace \u201cnon-atomic\u201d latents with small, explicit sparse sums of more primitive latents, yielding a compositional feature grammar. Validate by showing improved stability across random seeds and better transfer of features across model checkpoints, while preserving reconstruction and causal influence.",
        "Stitching-Based Completeness Benchmarks for Feature Dictionaries\nGeneralize SAE stitching into a rigorous benchmark: define \u201ccoverage\u201d as the marginal causal/reconstruction improvement when adding latents from a larger dictionary into a smaller one under a fixed compute budget. Use this to compare SAEs, MP-SAEs, and hierarchical sparse coders, and to derive practical rules for \u201chow many features are enough\u201d for tasks like circuit finding or safety monitoring.",
        "Adversarial Superposition Probes Using Learned Feature Dictionaries\nTest the hypothesized link between superposition and adversarial examples by constructing perturbations that selectively activate SAE/MP-SAE features while minimally changing inputs. Measure whether models with higher feature interference (from the superposition phase diagram) are more vulnerable, and whether enforcing structured sparsity (e.g., tree constraints) reduces adversarial susceptibility.",
        "Neuro-inspired Online Sparse Coding for Real-Time Interpretability Telemetry\nAdapt locally competitive algorithm (LCA)-style online sparse coding to stream activations from running LLMs, producing low-latency \u201cfeature event\u201d logs without full SAE inference. Use this to build real-time monitors for rare, safety-relevant features (e.g., self-harm instructions, credential theft patterns) and compare accuracy/latency/compute to standard offline SAE decompositions."
      ],
      "generated_ideas_raw": "1. **Hierarchical Sparse Autoencoders for Multi-Scale \u201cFeature Trees\u201d in LLM Activations**  \n   Combine SAE-style dictionary learning with tree-structured sparsity (hierarchical proximal norms) so that activating a specific feature implies activating its ancestors (coarser concepts). Train on residual stream/MLP activations and evaluate whether learned trees align with semantic hierarchies (e.g., \u201cscientist \u2192 physicist \u2192 Einstein\u201d) while improving faithfulness on causal tests (activation patching / feature ablations).\n\n2. **Phase-Transition Map of Superposition vs. Sparsity Across Model Scale and Data Regimes**  \n   Extend \u201cToy Models of Superposition\u201d into an empirical study: sweep model width, dataset entropy, regularization strength, and training compute, and measure when representations flip from polysemantic to more separable (using SAE/MP-SAE reconstructions and interference metrics). Produce a predictive \u201cphase diagram\u201d that forecasts feature crowding and polysemanticity from measurable training conditions.\n\n3. **Residual-Guided Sequential Encoders (MP-SAE) as a Standard Decomposition Primitive for Transformers**  \n   Implement MP-SAE-style matching-pursuit encoders for transformer layers and test whether sequential residual steps recover nonlinearly accessible or compositional features missed by standard SAEs. Quantify gains on (i) interpretability metrics, (ii) causal localization of known circuits (e.g., IOI), and (iii) cross-modal shared structure in vision-language models.\n\n4. **Meta-SAE-Guided Feature Refinement: Turning Non-Atomic Latents into Explicit Compositions**  \n   Use meta-SAEs (trained on SAE decoder matrices) to automatically replace \u201cnon-atomic\u201d latents with small, explicit sparse sums of more primitive latents, yielding a compositional feature grammar. Validate by showing improved stability across random seeds and better transfer of features across model checkpoints, while preserving reconstruction and causal influence.\n\n5. **Stitching-Based Completeness Benchmarks for Feature Dictionaries**  \n   Generalize SAE stitching into a rigorous benchmark: define \u201ccoverage\u201d as the marginal causal/reconstruction improvement when adding latents from a larger dictionary into a smaller one under a fixed compute budget. Use this to compare SAEs, MP-SAEs, and hierarchical sparse coders, and to derive practical rules for \u201chow many features are enough\u201d for tasks like circuit finding or safety monitoring.\n\n6. **Adversarial Superposition Probes Using Learned Feature Dictionaries**  \n   Test the hypothesized link between superposition and adversarial examples by constructing perturbations that selectively activate SAE/MP-SAE features while minimally changing inputs. Measure whether models with higher feature interference (from the superposition phase diagram) are more vulnerable, and whether enforcing structured sparsity (e.g., tree constraints) reduces adversarial susceptibility.\n\n7. **Approximate Orthogonality Constraints that Preserve Compositionality (Beyond Linear Monosemanticity)**  \n   Introduce a training objective for dictionary learning that encourages *conditional* orthogonality (features are orthogonal given an active parent/cluster) rather than global orthogonality, directly targeting the mismatch highlighted in recent SAE phenomenology. Evaluate whether this yields more \u201cusable\u201d features for mechanistic interventions (steering, refusal triggers, tool-use behaviors) than classic L1-regularized SAEs.\n\n8. **Cross-Layer Feature Tracking via Shared Dictionaries and Temporal Coherence**  \n   Train coupled dictionaries across multiple transformer layers with a constraint that encourages feature identity persistence (a feature at layer \u2113 should map to a small set of related features at layer \u2113+1). This creates an actionable pipeline for tracing how concepts transform through depth and for identifying where superposition is created or resolved during computation.\n\n9. **Neuro-inspired Online Sparse Coding for Real-Time Interpretability Telemetry**  \n   Adapt locally competitive algorithm (LCA)-style online sparse coding to stream activations from running LLMs, producing low-latency \u201cfeature event\u201d logs without full SAE inference. Use this to build real-time monitors for rare, safety-relevant features (e.g., self-harm instructions, credential theft patterns) and compare accuracy/latency/compute to standard offline SAE decompositions.\n\n10. **Unified Sparse-Coding Lens on V1 Gabor-Like Dictionaries and Transformer Feature Dictionaries**  \n   Construct a controlled comparison study where the same sparse-coding/hierarchical-sparse framework is applied to (i) natural image patches (recovering V1-like bases) and (ii) LLM activations (recovering monosemantic features). The key contribution is identifying shared geometric signatures (overcompleteness, interference patterns, hierarchical organization) and using them to propose architecture-agnostic metrics for \u201cfeature quality\u201d and superposition across domains.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Hierarchical Sparse Autoencoders for Multi-Scale \u201cFeature Trees\u201d in LLM Activations\nCombine SAE-style dictionary learning with tree-structured sparsity (hierarchical proximal norms) so that activating",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Phase-Transition Map of Superposition vs. Sparsity Across Model Scale and Data Regimes\nExtend \u201cToy Models of Superposition\u201d into an empirical study: sweep model width, dataset entropy, regularization ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Residual-Guided Sequential Encoders (MP-SAE) as a Standard Decomposition Primitive for Transformers\nImplement MP-SAE-style matching-pursuit encoders for transformer layers and test whether sequential ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Meta-SAE-Guided Feature Refinement: Turning Non-Atomic Latents into Explicit Compositions\nUse meta-SAEs (trained on SAE decoder matrices) to automatically replace \u201cnon-atomic\u201d latents with small, expl",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Stitching-Based Completeness Benchmarks for Feature Dictionaries\nGeneralize SAE stitching into a rigorous benchmark: define \u201ccoverage\u201d as the marginal causal/reconstruction improvement when adding lat",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Adversarial Superposition Probes Using Learned Feature Dictionaries\nTest the hypothesized link between superposition and adversarial examples by constructing perturbations that selectively activate SA",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Neuro-inspired Online Sparse Coding for Real-Time Interpretability Telemetry\nAdapt locally competitive algorithm (LCA)-style online sparse coding to stream activations from running LLMs, producing low",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 23,
      "paper_title": "EvoLM: In Search of Lost Language Model Training Dynamics",
      "contribution": "EvoLM builds a transparent, end-to-end model suite and experimental pipeline (100+ 1B/4B decoder-only LMs trained from scratch on open data) to systematically trace training dynamics across pre-training, continued pre-training, supervised fine-tuning, and RL, revealing practical trade-offs (diminishing returns, forgetting, bridging roles of continued pre-training, and SFT/RL trade-offs) and releasing all models, data, and code for reproducible study.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10342,
      "output_tokens": 1003,
      "predecessor_details": [
        {
          "success": true,
          "title": "Relative Scaling Laws for LLMs",
          "url": "https://arxiv.org/abs/2510.24626",
          "content": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
          "original_query": "Scaling laws for neural language models",
          "cleaned_query": "Scaling laws for neural language models"
        },
        {
          "success": true,
          "title": "Training Compute-Optimal Large Language Models - arXiv",
          "url": "https://arxiv.org/abs/2203.15556",
          "content": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
          "original_query": "Training compute-optimal large language models",
          "cleaned_query": "Training compute-optimal large language models"
        },
        {
          "success": true,
          "title": "[2212.09803] Training Trajectories of Language Models Across Scales",
          "url": "https://arxiv.org/abs/2212.09803",
          "content": "[2212.09803] Training Trajectories of Language Models Across Scales\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2212.09803\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2212.09803**(cs)\n[Submitted on 19 Dec 2022 ([v1](https://arxiv.org/abs/2212.09803v1)), last revised 30 May 2023 (this version, v3)]\n# Title:Training Trajectories of Language Models Across Scales\nAuthors:[Mengzhou Xia](https://arxiv.org/search/cs?searchtype=author&amp;query=Xia,+M),[Mikel Artetxe](https://arxiv.org/search/cs?searchtype=author&amp;query=Artetxe,+M),[Chunting Zhou](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+C),[Xi Victoria Lin](https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+X+V),[Ramakanth Pasunuru](https://arxiv.org/search/cs?searchtype=author&amp;query=Pasunuru,+R),[Danqi Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+D),[Luke Zettlemoyer](https://arxiv.org/search/cs?searchtype=author&amp;query=Zettlemoyer,+L),[Ves Stoyanov](https://arxiv.org/search/cs?searchtype=author&amp;query=Stoyanov,+V)\nView a PDF of the paper titled Training Trajectories of Language Models Across Scales, by Mengzhou Xia and 7 other authors\n[View PDF](https://arxiv.org/pdf/2212.09803)> > Abstract:\n> Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al.,2022)--from 125M to 175B parameters--on next-token prediction, sequence-level generation, and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior; 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation. Comments:|Accepted to ACL 2023; The code and analysis results are available at[this https URL](https://github.com/xiamengzhou/training_trajectory_analysis)|\nSubjects:|Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)|\nCite as:|[arXiv:2212.09803](https://arxiv.org/abs/2212.09803)[cs.CL]|\n|(or[arXiv:2212.09803v3](https://arxiv.org/abs/2212.09803v3)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2212.09803](https://doi.org/10.48550/arXiv.2212.09803)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Mengzhou Xia [[view email](https://arxiv.org/show-email/f5df37a1/2212.09803)]\n**[[v1]](https://arxiv.org/abs/2212.09803v1)**Mon, 19 Dec 2022 19:16:29 UTC (3,432 KB)\n**[[v2]](https://arxiv.org/abs/2212.09803v2)**Mon, 29 May 2023 02:25:22 UTC (3,394 KB)\n**[v3]**Tue, 30 May 2023 03:33:27 UTC (3,394 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Training Trajectories of Language Models Across Scales, by Mengzhou Xia and 7 other authors\n* [View PDF](https://arxiv.org/pdf/2212.09803)\n* [TeX Source](https://arxiv.org/src/2212.09803)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2212.09803&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2212.09803&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2022-12](https://arxiv.org/list/cs.CL/2022-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/2212.09803?context=cs)\n[cs.AI](https://arxiv.org/abs/2212.09803?context=cs.AI)\n[cs.LG](https://arxiv.org/abs/2212.09803?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2212.09803)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2212.09803)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2212.09803)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them",
          "original_query": "Training trajectories of language models across scales",
          "cleaned_query": "Training trajectories of language models across scales"
        },
        {
          "success": true,
          "title": "Language models scale reliably with over-training and on ... - arXiv",
          "url": "https://arxiv.org/abs/2403.08540",
          "content": "[2403.08540] Language models scale reliably with over-training and on downstream tasks\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nIn just 5 minutes help us improve arXiv:\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2403.08540\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2403.08540**(cs)\n[Submitted on 13 Mar 2024 ([v1](https://arxiv.org/abs/2403.08540v1)), last revised 14 Jun 2024 (this version, v2)]\n# Title:Language models scale reliably with over-training and on downstream tasks\nAuthors:[Samir Yitzhak Gadre](https://arxiv.org/search/cs?searchtype=author&amp;query=Gadre,+S+Y),[Georgios Smyrnis](https://arxiv.org/search/cs?searchtype=author&amp;query=Smyrnis,+G),[Vaishaal Shankar](https://arxiv.org/search/cs?searchtype=author&amp;query=Shankar,+V),[Suchin Gururangan](https://arxiv.org/search/cs?searchtype=author&amp;query=Gururangan,+S),[Mitchell Wortsman](https://arxiv.org/search/cs?searchtype=author&amp;query=Wortsman,+M),[Rulin Shao](https://arxiv.org/search/cs?searchtype=author&amp;query=Shao,+R),[Jean Mercat](https://arxiv.org/search/cs?searchtype=author&amp;query=Mercat,+J),[Alex Fang](https://arxiv.org/search/cs?searchtype=author&amp;query=Fang,+A),[Jeffrey Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J),[Sedrick Keh](https://arxiv.org/search/cs?searchtype=author&amp;query=Keh,+S),[Rui Xin](https://arxiv.org/search/cs?searchtype=author&amp;query=Xin,+R),[Marianna Nezhurina](https://arxiv.org/search/cs?searchtype=author&amp;query=Nezhurina,+M),[Igor Vasiljevic](https://arxiv.org/search/cs?searchtype=author&amp;query=Vasiljevic,+I),[Jenia Jitsev](https://arxiv.org/search/cs?searchtype=author&amp;query=Jitsev,+J),[Luca Soldaini](https://arxiv.org/search/cs?searchtype=author&amp;query=Soldaini,+L),[Alexandros G. Dimakis](https://arxiv.org/search/cs?searchtype=author&amp;query=Dimakis,+A+G),[Gabriel Ilharco](https://arxiv.org/search/cs?searchtype=author&amp;query=Ilharco,+G),[Pang Wei Koh](https://arxiv.org/search/cs?searchtype=author&amp;query=Koh,+P+W),[Shuran Song](https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+S),[Thomas Kollar](https://arxiv.org/search/cs?searchtype=author&amp;query=Kollar,+T),[Yair Carmon](https://arxiv.org/search/cs?searchtype=author&amp;query=Carmon,+Y),[Achal Dave](https://arxiv.org/search/cs?searchtype=author&amp;query=Dave,+A),[Reinhard Heckel](https://arxiv.org/search/cs?searchtype=author&amp;query=Heckel,+R),[Niklas Muennighoff](https://arxiv.org/search/cs?searchtype=author&amp;query=Muennighoff,+N),[Ludwig Schmidt](https://arxiv.org/search/cs?searchtype=author&amp;query=Schmidt,+L)\nView a PDF of the paper titled Language models scale reliably with over-training and on downstream tasks, by Samir Yitzhak Gadre and Georgios Smyrnis and Vaishaal Shankar and Suchin Gururangan and Mitchell Wortsman and Rulin Shao and Jean Mercat and Alex Fang and Jeffrey Li and Sedrick Keh and Rui Xin and Marianna Nezhurina and Igor Vasiljevic and Jenia Jitsev and Luca Soldaini and Alexandros G. Dimakis and Gabriel Ilharco and Pang Wei Koh and Shuran Song and Thomas Kollar and Yair Carmon and Achal Dave and Reinhard Heckel and Niklas Muennighoff and Ludwig Schmidt\n[View PDF](https://arxiv.org/pdf/2403.08540)[HTML (experimental)](https://arxiv.org/html/2403.08540v2)> > Abstract:\n> Scaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments. However, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e., &#34;Chinchilla optimal&#34; regime). In contrast, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but models are usually compared on downstream task performance. To address both shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we fit scaling laws that extrapolate in both the amount of over-training and the number of model parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B parameter, 138B token run (i.e., a compute-optimal run)$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute. Second, we relate the perplexity of a language model to its downstream task performance by proposing a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models, using experiments that take 20$\\times$ less compute. Our experiments are available at [> this https URL\n](https://github.com/mlfoundations/scaling)> . Subjects:|Computation and Language (cs.CL); Machine Learning (cs.LG)|\nCite as:|[arXiv:2403.08540](https://arxiv.org/abs/2403.08540)[cs.CL]|\n|(or[arXiv:2403.08540v2](https://arxiv.org/abs/2403.08540v2)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2403.08540](https://doi.org/10.48550/arXiv.2403.08540)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Samir Yitzhak Gadre [[view email](https://arxiv.org/show-email/bfeb7c92/2403.08540)]\n**[[v1]](https://arxiv.org/abs/2403.08540v1)**Wed, 13 Mar 2024 13:54:00 UTC (361 KB)\n**[v2]**Fri, 14 Jun 2024 20:21:05 UTC (424 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Language models scale reliably with over-training and on downstream tasks, by Samir Yitzhak Gadre and Georgios Smyrnis and Vaishaal Shankar and Suchin Gururangan and Mitchell Wortsman and Rulin Shao and Jean Mercat and Alex Fang and Jeffrey Li and Sedrick Keh and Rui Xin and Marianna Nezhurina and Igor Vasiljevic and Jenia Jitsev and Luca Soldaini and Alexandros G. Dimakis and Gabriel Ilharco and Pang Wei Koh and Shuran Song and Thomas Kollar and Yair Carmon and Achal Dave and Reinhard Heckel and Niklas Muennighoff and Ludwig Schmidt\n* [View PDF](https://arxiv.org/pdf/2403.08540)\n* [HTML (experimental)](https://arxiv.org/html/2403.08540v2)\n* [TeX Source](https://arxiv.org/src/2403.08540)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2403.08540&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2403.08540&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2024-03](https://arxiv.org/list/cs.CL/2024-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2403.08540?context=cs)\n[cs.LG](https://arxiv.org/abs/2403.08540?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2403.08540)\n* [Google Scholar]",
          "original_query": "Language models scale reliably with over-training and on downstream tasks",
          "cleaned_query": "Language models scale reliably with over-training and on downstream tasks"
        },
        {
          "success": true,
          "title": "The FineWeb Datasets: Decanting the Web for the Finest Text Data ...",
          "url": "https://arxiv.org/abs/2406.17557",
          "content": "[2406.17557] The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2406.17557\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2406.17557**(cs)\n[Submitted on 25 Jun 2024 ([v1](https://arxiv.org/abs/2406.17557v1)), last revised 31 Oct 2024 (this version, v2)]\n# Title:The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale\nAuthors:[Guilherme Penedo](https://arxiv.org/search/cs?searchtype=author&amp;query=Penedo,+G),[Hynek Kydl\u00ed\u010dek](https://arxiv.org/search/cs?searchtype=author&amp;query=Kydl\u00ed\u010dek,+H),[Loubna Ben allal](https://arxiv.org/search/cs?searchtype=author&amp;query=allal,+L+B),[Anton Lozhkov](https://arxiv.org/search/cs?searchtype=author&amp;query=Lozhkov,+A),[Margaret Mitchell](https://arxiv.org/search/cs?searchtype=author&amp;query=Mitchell,+M),[Colin Raffel](https://arxiv.org/search/cs?searchtype=author&amp;query=Raffel,+C),[Leandro Von Werra](https://arxiv.org/search/cs?searchtype=author&amp;query=Von+Werra,+L),[Thomas Wolf](https://arxiv.org/search/cs?searchtype=author&amp;query=Wolf,+T)\nView a PDF of the paper titled The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale, by Guilherme Penedo and 7 other authors\n[View PDF](https://arxiv.org/pdf/2406.17557)[HTML (experimental)](https://arxiv.org/html/2406.17557v2)> > Abstract:\n> The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly available and very little is known about how they were created. In this work, we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. To advance the understanding of how best to curate high-quality pretraining datasets, we carefully document and ablate all of the design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. LLMs pretrained on FineWeb-Edu exhibit dramatically better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we publicly release our data curation codebase and all of the models trained during our ablation experiments. Subjects:|Computation and Language (cs.CL)|\nCite as:|[arXiv:2406.17557](https://arxiv.org/abs/2406.17557)[cs.CL]|\n|(or[arXiv:2406.17557v2](https://arxiv.org/abs/2406.17557v2)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2406.17557](https://doi.org/10.48550/arXiv.2406.17557)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Guilherme Penedo [[view email](https://arxiv.org/show-email/2c18eaa7/2406.17557)]\n**[[v1]](https://arxiv.org/abs/2406.17557v1)**Tue, 25 Jun 2024 13:50:56 UTC (6,266 KB)\n**[v2]**Thu, 31 Oct 2024 11:37:49 UTC (6,347 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale, by Guilherme Penedo and 7 other authors\n* [View PDF](https://arxiv.org/pdf/2406.17557)\n* [HTML (experimental)](https://arxiv.org/html/2406.17557v2)\n* [TeX Source](https://arxiv.org/src/2406.17557)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2406.17557&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2406.17557&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2024-06](https://arxiv.org/list/cs.CL/2024-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/2406.17557?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2406.17557)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2406.17557)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2406.17557)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2406.17557)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.ar",
          "original_query": "The fineweb datasets: Decanting the web for the finest text data at scale",
          "cleaned_query": "The fineweb datasets: Decanting the web for the finest text data at scale"
        },
        {
          "success": true,
          "title": "LLaMA: Open and Efficient Foundation Language Models - arXiv",
          "url": "https://arxiv.org/abs/2302.13971",
          "content": "[2302.13971] LLaMA: Open and Efficient Foundation Language Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2302.13971\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2302.13971**(cs)\n[Submitted on 27 Feb 2023]\n# Title:LLaMA: Open and Efficient Foundation Language Models\nAuthors:[Hugo Touvron](https://arxiv.org/search/cs?searchtype=author&amp;query=Touvron,+H),[Thibaut Lavril](https://arxiv.org/search/cs?searchtype=author&amp;query=Lavril,+T),[Gautier Izacard](https://arxiv.org/search/cs?searchtype=author&amp;query=Izacard,+G),[Xavier Martinet](https://arxiv.org/search/cs?searchtype=author&amp;query=Martinet,+X),[Marie-Anne Lachaux](https://arxiv.org/search/cs?searchtype=author&amp;query=Lachaux,+M),[Timoth\u00e9e Lacroix](https://arxiv.org/search/cs?searchtype=author&amp;query=Lacroix,+T),[Baptiste Rozi\u00e8re](https://arxiv.org/search/cs?searchtype=author&amp;query=Rozi\u00e8re,+B),[Naman Goyal](https://arxiv.org/search/cs?searchtype=author&amp;query=Goyal,+N),[Eric Hambro](https://arxiv.org/search/cs?searchtype=author&amp;query=Hambro,+E),[Faisal Azhar](https://arxiv.org/search/cs?searchtype=author&amp;query=Azhar,+F),[Aurelien Rodriguez](https://arxiv.org/search/cs?searchtype=author&amp;query=Rodriguez,+A),[Armand Joulin](https://arxiv.org/search/cs?searchtype=author&amp;query=Joulin,+A),[Edouard Grave](https://arxiv.org/search/cs?searchtype=author&amp;query=Grave,+E),[Guillaume Lample](https://arxiv.org/search/cs?searchtype=author&amp;query=Lample,+G)\nView a PDF of the paper titled LLaMA: Open and Efficient Foundation Language Models, by Hugo Touvron and 13 other authors\n[View PDF](https://arxiv.org/pdf/2302.13971)> > Abstract:\n> We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community. Subjects:|Computation and Language (cs.CL)|\nCite as:|[arXiv:2302.13971](https://arxiv.org/abs/2302.13971)[cs.CL]|\n|(or[arXiv:2302.13971v1](https://arxiv.org/abs/2302.13971v1)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2302.13971](https://doi.org/10.48550/arXiv.2302.13971)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Gautier Izacard [[view email](https://arxiv.org/show-email/4f64b855/2302.13971)]\n**[v1]**Mon, 27 Feb 2023 17:11:15 UTC (364 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled LLaMA: Open and Efficient Foundation Language Models, by Hugo Touvron and 13 other authors\n* [View PDF](https://arxiv.org/pdf/2302.13971)\n* [TeX Source](https://arxiv.org/src/2302.13971)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2302.13971&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2302.13971&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2023-02](https://arxiv.org/list/cs.CL/2023-02)\nChange to browse by:\n[cs](https://arxiv.org/abs/2302.13971?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2302.13971)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2302.13971)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2302.13971)\n### [5 blog links](https://arxiv.org/tb/2302.13971)\n([what is this?](https://info.arxiv.org/help/trackback.html))\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2302.13971)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Llama: Open and efficient foundation language models",
          "cleaned_query": "Llama: Open and efficient foundation language models"
        }
      ],
      "generated_ideas": [
        "Per-Token Scaling Laws via Loss-Reduction Trajectories\nBuild scaling laws not just for average validation loss, but for *per-token* loss-reduction rates using intermediate checkpoints (as in training-trajectory analyses). Fit a mixture model that predicts which token subsets will improve, stagnate, or exhibit double-descent as a function of model size and token budget, and validate by forecasting \u201chard token\u201d behavior on held-out corpora.",
        "Curriculum Scheduling from Trajectory-Identified \u201cStagnating Tokens\u201d\nUse checkpoint-level token loss histories to automatically identify stagnating/high-variance token groups (e.g., rare syntax, long-range dependencies, citations). Implement an online curriculum that upweights these groups only after the model reaches a perplexity threshold, and test whether this reduces hallucination-prone grammatical sequences earlier in training compared to fixed sampling.",
        "Compute-Optimal Over-Training Under Data Quality Constraints\nExtend compute-optimal training (Chinchilla-style) to a tri-objective optimization over parameters, tokens, and *data quality* (e.g., FineWeb vs FineWeb-Edu vs noisier web). Concretely, derive and empirically fit a law of the form loss = f(N_params, N_tokens, q) where q is a measurable quality score from filtering/dedup pipelines, enabling budget allocation decisions like \u201cmore tokens of lower quality vs fewer tokens of higher quality.\u201d",
        "Hallucination-Likelihood Scaling Law Using \u201cGrammatical Hallucination\u201d Probes\nOperationalize the trajectory finding that models first learn fluent but wrong sequences and only later suppress them by constructing a standardized probe set of grammatical hallucinations (e.g., plausible fake citations, invented entities with consistent syntax). Fit a scaling law that predicts the checkpoint (tokens seen) at which hallucination probability drops below a target, as a function of model size, over-training factor, and data quality.",
        "Deduplication Strength as a Control Knob for In-Context Learning Emergence\nUsing FineWeb\u2019s documented dedup variants, train matched model families where only dedup aggressiveness changes while compute is fixed. Measure how ICL performance on multi-choice tasks correlates with perplexity *and* dedup level at equal perplexity points, testing whether \u201cICL-at-a-given-perplexity\u201d invariance holds or breaks when memorization opportunities are reduced.",
        "Checkpoint-Efficient Scaling: Early-Stop Extrapolation for Over-Trained Regimes\nDevelop a method that fits over-training-aware scaling curves from the first X% of training tokens and extrapolates the final loss and downstream error (building on over-training scaling work). The actionable contribution is a training-time controller that decides to stop/continue or adjust token budgets mid-run with bounded risk, validated on multiple model sizes and data distributions.",
        "FineWeb-Edu Adaptive Mixing Triggered by Perplexity Milestones\nImplement a dynamic data-mixing policy where FineWeb-Edu proportion increases when the model hits specified perplexity thresholds (rather than fixed ratios). Evaluate whether this improves reasoning/knowledge benchmarks at constant compute versus static mixtures, and analyze trajectories to see whether the \u201csubset of tokens that improve most\u201d shifts when the mix changes.",
        "Relative Scaling Laws for Architecture Efficiency Across Public LLaMA-Style Recipes\nReplicate \u201crelative scaling\u201d comparisons across multiple efficient transformer variants (e.g., different width/depth ratios, attention variants) trained on the same FineWeb slices. Produce actionable guidance in the form of pairwise relative-scaling curves that predict which architecture wins at a given compute *and* over-training factor, using small models to forecast larger ones.",
        "Token-Budget Allocation for Downstream Robustness (Not Just Mean Accuracy)\nExtend downstream scaling evaluation to robustness axes (domain shift, adversarial paraphrase, long-context degradation) and fit separate power laws relating perplexity to each robustness metric. Use the resulting model to allocate training tokens toward specific FineWeb subdomains that most improve the *worst-case* downstream performance, yielding a compute-budgeting method targeted at reliability rather than average benchmark scores."
      ],
      "generated_ideas_raw": "1. **Per-Token Scaling Laws via Loss-Reduction Trajectories**\n   Build scaling laws not just for average validation loss, but for *per-token* loss-reduction rates using intermediate checkpoints (as in training-trajectory analyses). Fit a mixture model that predicts which token subsets will improve, stagnate, or exhibit double-descent as a function of model size and token budget, and validate by forecasting \u201chard token\u201d behavior on held-out corpora.\n\n2. **Curriculum Scheduling from Trajectory-Identified \u201cStagnating Tokens\u201d**\n   Use checkpoint-level token loss histories to automatically identify stagnating/high-variance token groups (e.g., rare syntax, long-range dependencies, citations). Implement an online curriculum that upweights these groups only after the model reaches a perplexity threshold, and test whether this reduces hallucination-prone grammatical sequences earlier in training compared to fixed sampling.\n\n3. **Compute-Optimal Over-Training Under Data Quality Constraints**\n   Extend compute-optimal training (Chinchilla-style) to a tri-objective optimization over parameters, tokens, and *data quality* (e.g., FineWeb vs FineWeb-Edu vs noisier web). Concretely, derive and empirically fit a law of the form loss = f(N_params, N_tokens, q) where q is a measurable quality score from filtering/dedup pipelines, enabling budget allocation decisions like \u201cmore tokens of lower quality vs fewer tokens of higher quality.\u201d\n\n4. **Predicting Downstream Performance from Perplexity *and* Data-Mix Metadata**\n   Generalize the perplexity\u2192downstream power law by conditioning on dataset mixture proportions (e.g., fraction educational, code, forum, news) and dedup strength. Train dozens of small-to-mid models across controlled FineWeb-derived mixtures, then fit a calibrated predictor that forecasts average benchmark error and *per-benchmark* performance from (perplexity, mix vector, model size), enabling data-centric scaling forecasts.\n\n5. **Hallucination-Likelihood Scaling Law Using \u201cGrammatical Hallucination\u201d Probes**\n   Operationalize the trajectory finding that models first learn fluent but wrong sequences and only later suppress them by constructing a standardized probe set of grammatical hallucinations (e.g., plausible fake citations, invented entities with consistent syntax). Fit a scaling law that predicts the checkpoint (tokens seen) at which hallucination probability drops below a target, as a function of model size, over-training factor, and data quality.\n\n6. **Deduplication Strength as a Control Knob for In-Context Learning Emergence**\n   Using FineWeb\u2019s documented dedup variants, train matched model families where only dedup aggressiveness changes while compute is fixed. Measure how ICL performance on multi-choice tasks correlates with perplexity *and* dedup level at equal perplexity points, testing whether \u201cICL-at-a-given-perplexity\u201d invariance holds or breaks when memorization opportunities are reduced.\n\n7. **Checkpoint-Efficient Scaling: Early-Stop Extrapolation for Over-Trained Regimes**\n   Develop a method that fits over-training-aware scaling curves from the first X% of training tokens and extrapolates the final loss and downstream error (building on over-training scaling work). The actionable contribution is a training-time controller that decides to stop/continue or adjust token budgets mid-run with bounded risk, validated on multiple model sizes and data distributions.\n\n8. **FineWeb-Edu Adaptive Mixing Triggered by Perplexity Milestones**\n   Implement a dynamic data-mixing policy where FineWeb-Edu proportion increases when the model hits specified perplexity thresholds (rather than fixed ratios). Evaluate whether this improves reasoning/knowledge benchmarks at constant compute versus static mixtures, and analyze trajectories to see whether the \u201csubset of tokens that improve most\u201d shifts when the mix changes.\n\n9. **Relative Scaling Laws for Architecture Efficiency Across Public LLaMA-Style Recipes**\n   Replicate \u201crelative scaling\u201d comparisons across multiple efficient transformer variants (e.g., different width/depth ratios, attention variants) trained on the same FineWeb slices. Produce actionable guidance in the form of pairwise relative-scaling curves that predict which architecture wins at a given compute *and* over-training factor, using small models to forecast larger ones.\n\n10. **Token-Budget Allocation for Downstream Robustness (Not Just Mean Accuracy)**\n   Extend downstream scaling evaluation to robustness axes (domain shift, adversarial paraphrase, long-context degradation) and fit separate power laws relating perplexity to each robustness metric. Use the resulting model to allocate training tokens toward specific FineWeb subdomains that most improve the *worst-case* downstream performance, yielding a compute-budgeting method targeted at reliability rather than average benchmark scores.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Per-Token Scaling Laws via Loss-Reduction Trajectories\nBuild scaling laws not just for average validation loss, but for *per-token* loss-reduction rates using intermediate checkpoints (as in training-",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Curriculum Scheduling from Trajectory-Identified \u201cStagnating Tokens\u201d\nUse checkpoint-level token loss histories to automatically identify stagnating/high-variance token groups (e.g., rare syntax, long-",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Compute-Optimal Over-Training Under Data Quality Constraints\nExtend compute-optimal training (Chinchilla-style) to a tri-objective optimization over parameters, tokens, and *data quality* (e.g., FineW",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Hallucination-Likelihood Scaling Law Using \u201cGrammatical Hallucination\u201d Probes\nOperationalize the trajectory finding that models first learn fluent but wrong sequences and only later suppress them by c",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Deduplication Strength as a Control Knob for In-Context Learning Emergence\nUsing FineWeb\u2019s documented dedup variants, train matched model families where only dedup aggressiveness changes while compute",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Checkpoint-Efficient Scaling: Early-Stop Extrapolation for Over-Trained Regimes\nDevelop a method that fits over-training-aware scaling curves from the first X% of training tokens and extrapolates the ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "FineWeb-Edu Adaptive Mixing Triggered by Perplexity Milestones\nImplement a dynamic data-mixing policy where FineWeb-Edu proportion increases when the model hits specified perplexity thresholds (rather",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Relative Scaling Laws for Architecture Efficiency Across Public LLaMA-Style Recipes\nReplicate \u201crelative scaling\u201d comparisons across multiple efficient transformer variants (e.g., different width/depth",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Token-Budget Allocation for Downstream Robustness (Not Just Mean Accuracy)\nExtend downstream scaling evaluation to robustness axes (domain shift, adversarial paraphrase, long-context degradation) and ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 24,
      "paper_title": "Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions",
      "contribution": "A theoretical and algorithmic treatment of gradient-based training on AIMC devices with general, asymmetric and nonlinear pulse-response functions, proving that residual-learning updates (a bilevel formulation) remove the implicit bias caused by asymmetric responses and recover convergence to true critical points while also handling limited response granularity.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 1,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 7313,
      "output_tokens": 1029,
      "predecessor_details": [
        {
          "success": true,
          "title": "Towards Exact Gradient-based Training on Analog In-memory ...",
          "url": "https://arxiv.org/abs/2406.12774",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2406.12774** (cs)\n\n\\[Submitted on 18 Jun 2024\\]\n\n# Title:Towards Exact Gradient-based Training on Analog In-memory Computing\n\nAuthors: [Zhaoxian Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu,+Z), [Tayfun Gokmen](https://arxiv.org/search/cs?searchtype=author&query=Gokmen,+T), [Malte J. Rasch](https://arxiv.org/search/cs?searchtype=author&query=Rasch,+M+J), [Tianyi Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+T)\n\nView a PDF of the paper titled Towards Exact Gradient-based Training on Analog In-memory Computing, by Zhaoxian Wu and Tayfun Gokmen and Malte J. Rasch and Tianyi Chen\n\n[View PDF](https://arxiv.org/pdf/2406.12774) [HTML (experimental)](https://arxiv.org/html/2406.12774v1)\n\n> Abstract:Given the high economic and environmental costs of using large vision or language models, analog in-memory accelerators present a promising solution for energy-efficient AI. While inference on analog accelerators has been studied recently, the training perspective is underexplored. Recent studies have shown that the \"workhorse\" of digital AI training - stochastic gradient descent (SGD) algorithm converges inexactly when applied to model training on non-ideal devices. This paper puts forth a theoretical foundation for gradient-based training on analog devices. We begin by characterizing the non-convergent issue of SGD, which is caused by the asymmetric updates on the analog devices. We then provide a lower bound of the asymptotic error to show that there is a fundamental performance limit of SGD-based analog training rather than an artifact of our analysis. To address this issue, we study a heuristic analog algorithm called Tiki-Taka that has recently exhibited superior empirical performance compared to SGD and rigorously show its ability to exactly converge to a critical point and hence eliminates the asymptotic error. The simulations verify the correctness of the analyses.\n\n| | |\n| --- | --- |\n| Comments: | 10 pages, 5 figures,2 tables |\n| Subjects: | Machine Learning (cs.LG); Hardware Architecture (cs.AR); Optimization and Control (math.OC) |\n| Cite as: | [arXiv:2406.12774](https://arxiv.org/abs/2406.12774) \\[cs.LG\\] |\n| | (or [arXiv:2406.12774v1](https://arxiv.org/abs/2406.12774v1) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2406.12774](https://doi.org/10.48550/arXiv.2406.12774) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Zhaoxian Wu \\[ [view email](https://arxiv.org/show-email/e9e3b88b/2406.12774)\\]\n\n**\\[v1\\]**\nTue, 18 Jun 2024 16:43:59 UTC (390 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Towards Exact Gradient-based Training on Analog In-memory Computing, by Zhaoxian Wu and Tayfun Gokmen and Malte J. Rasch and Tianyi Chen\n\n- [View PDF](https://arxiv.org/pdf/2406.12774)\n- [HTML (experimental)](https://arxiv.org/html/2406.12774v1)\n- [TeX Source](https://arxiv.org/src/2406.12774)\n- [Other Formats](https://arxiv.org/format/2406.12774)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2406.12774&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2406.12774&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2024-06](https://arxiv.org/list/cs.LG/2024-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2406.12774?context=cs)\n\n[cs.AR](https://arxiv.org/abs/2406.12774?context=cs.AR)\n\n[math](https://arxiv.org/abs/2406.12774?context=math)\n\n[math.OC](https://arxiv.org/abs/2406.12774?context=math.OC)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2406.12774)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2406.12774)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2406.12774)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2406.12774&description=Towards Exact Gradient-based Training on Analog In-memory Computing) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2406.12774&title=Towards Exact Gradient-based Training on Analog In-memory Computing)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2406.12774) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Towards exact gradient-based training on analog in-memory computing",
          "cleaned_query": "Towards exact gradient-based training on analog in-memory computing"
        },
        {
          "success": true,
          "title": "In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning",
          "url": "https://arxiv.org/abs/2510.02516",
          "content": "[2510.02516] In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nIn just 5 minutes help us improve arXiv:\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2510.02516\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2510.02516**(cs)\n[Submitted on 2 Oct 2025]\n# Title:In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning\nAuthors:[Jindan Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J),[Zhaoxian Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+Z),[Gaowen Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+G),[Tayfun Gokmen](https://arxiv.org/search/cs?searchtype=author&amp;query=Gokmen,+T),[Tianyi Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+T)\nView a PDF of the paper titled In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning, by Jindan Li and 4 other authors\n[View PDF](https://arxiv.org/pdf/2510.02516)[HTML (experimental)](https://arxiv.org/html/2510.02516v1)> > Abstract:\n> Analog in-memory computing (AIMC) accelerators enable efficient deep neural network computation directly within memory using resistive crossbar arrays, where model parameters are represented by the conductance states of memristive devices. However, effective in-memory training typically requires at least 8-bit conductance states to match digital baselines. Realizing such fine-grained states is costly and often requires complex noise mitigation techniques that increase circuit complexity and energy consumption. In practice, many promising memristive devices such as ReRAM offer only about 4-bit resolution due to fabrication constraints, and this limited update precision substantially degrades training accuracy. To enable on-chip training with these limited-state devices, this paper proposes a \\emph{residual learning} framework that sequentially learns on multiple crossbar tiles to compensate the residual errors from low-precision weight updates. Our theoretical analysis shows that the optimality gap shrinks with the number of tiles and achieves a linear convergence rate. Experiments on standard image classification benchmarks demonstrate that our method consistently outperforms state-of-the-art in-memory analog training strategies under limited-state settings, while incurring only moderate hardware overhead as confirmed by our cost analysis. Subjects:|Machine Learning (cs.LG); Hardware Architecture (cs.AR); Optimization and Control (math.OC)|\nCite as:|[arXiv:2510.02516](https://arxiv.org/abs/2510.02516)[cs.LG]|\n|(or[arXiv:2510.02516v1](https://arxiv.org/abs/2510.02516v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2510.02516](https://doi.org/10.48550/arXiv.2510.02516)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jindan Li [[view email](https://arxiv.org/show-email/27d08587/2510.02516)]\n**[v1]**Thu, 2 Oct 2025 19:44:25 UTC (5,023 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning, by Jindan Li and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2510.02516)\n* [HTML (experimental)](https://arxiv.org/html/2510.02516v1)\n* [TeX Source](https://arxiv.org/src/2510.02516)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2510.02516&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2510.02516&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-10](https://arxiv.org/list/cs.LG/2025-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/2510.02516?context=cs)\n[cs.AR](https://arxiv.org/abs/2510.02516?context=cs.AR)\n[math](https://arxiv.org/abs/2510.02516?context=math)\n[math.OC](https://arxiv.org/abs/2510.02516?context=math.OC)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2510.02516)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2510.02516)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2510.02516)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add ",
          "original_query": "Algorithm for training neural networks on resistive device arrays",
          "cleaned_query": "Algorithm for training neural networks on resistive device arrays"
        },
        {
          "success": true,
          "title": "Acceleration of Deep Neural Network Training with ...",
          "url": "https://arxiv.org/abs/1603.07341",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "Acceleration of deep neural network training with resistive cross-point devices: Design considerations",
          "cleaned_query": "Acceleration of deep neural network training with resistive cross-point devices: Design considerations"
        },
        {
          "success": true,
          "title": "Mitigating methodology of hardware non-ideal characteristics ...",
          "url": "https://link.springer.com/article/10.1007/s11432-023-4021-y",
          "content": "References Yao P, Wu H, Gao B, et al. Fully hardware-implemented memristor convolutional neural network. Nature, 2020, 577: 641\u2013646 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Wan W, Kubendran R, Schaefer C, et al. A compute-in-memory chip based on resistive random-access memory. Nature, 2022, 608: 504\u2013512 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Xue C X, Hung J M, Kao H Y, et al. A 22nm 4Mb 8b-precision ReRAM computing-in-memory macro with 11.91 to 195.7 TOPS/W for tiny AI edge devices. In: Proceedings of IEEE International Solid-State Circuits Conference (ISSCC), 2021. 245\u2013247 MATH \u00a0\n \n Google Scholar \u00a0\n Hung J M, Huang T H, Huang S P, et al. An 8-Mb DC-current-free binary-to-8b precision ReRAM nonvolatile computing-inmemory macro using time-space-readout with 1286.4-21.6 TOPS/W for edge-AI devices. In: Proceedings of IEEE International Solid-State Circuits Conference (ISSCC), 2022 MATH \u00a0\n \n Google Scholar \u00a0\n Song S Y, Huang P, Shen W S, et al. A 3.3-Mbit/s true random number generator based on resistive random access memory. Sci China Inf Sci, 2023, 66: 219402 Article \u00a0\n \n Google Scholar \u00a0\n Khaddam-Aljameh R, Stanisavljevic M, Fornt Mas J, et al. HERMES core-A 14nm CMOS and PCM-based in-memory compute core using an array of 300ps/LSB linearized CCO-based ADCs and local digital processing. In: Proceedings of Symposium on VLSI Circuits, 2021 \n Google Scholar \u00a0\n Khwa W S, Chiu Y C, Jhang C J, et al. A 40-nm, 2M-Cell, 8b-precision, hybrid SLC-MLC PCM computing-in-memory Macro with 20.5\u201365.0 TOPS/W for tiny-Al edge devices. In: Proceedings of IEEE International Solid-State Circuits Conference (ISSCC), 2022 MATH \u00a0\n \n Google Scholar \u00a0\n Chiu Y C, Yang C S, Teng S H, et al. A 22nm 4Mb STT-MRAM data-encrypted near-memory computation Macro with a 192GB/s read-and-decryption bandwidth and 25.1\u201355.1 TOPS/W 8b MAC for AI operations. In: Proceedings of IEEE International Solid-State Circuits Conference (ISSCC), 2022 MATH \u00a0\n \n Google Scholar \u00a0\n Guo Z, Yin J, Bai Y, et al. Spintronics for energy-efficient computing: an overview and outlook. Proc IEEE, 2021, 109: 1398\u20131417 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Xiang Y C, Huang P, Yang H Z, et al. Storage reliability of multi-bit flash oriented to deep neural network. In: Proceedings of IEEE International Electron Devices Meeting (IEDM), 2019 MATH \u00a0\n \n Google Scholar \u00a0\n Zhang D, Wang H, Feng Y, et al. Implementation of image compression by using high-precision in-memory computing scheme based on NOR flash memory. IEEE Electron Dev Lett, 2021, 42: 1603\u20131606 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Yu G H, Huang P, Han R Z, et al. Co-optimization strategy between array operation and weight mapping for flash computing arrays to achieve high computing efficiency and accuracy. Sci China Inf Sci, 2023, 66: 129403 Article \u00a0\n \n Google Scholar \u00a0\n Yang H Z, Huang P, Han R Z, et al. An ultra-high-density and energy-efficient content addressable memory design based on 3D-NAND flash. Sci China Inf Sci, 2023, 66: 142402 Article \u00a0\n \n Google Scholar \u00a0\n Yayla M, Thomann S, Buschjager S, et al. Reliable binarized neural networks on unreliable beyond von-Neumann architecture. IEEE Trans Circ Syst I, 2022, 69: 2516\u20132528 MATH \u00a0\n \n Google Scholar \u00a0\n Soliman T, M\u00fcller F, Kirchner T, et al. Ultra-low power flexible precision FeFET based analog in-memory computing. In: Proceedings of IEEE International Electron Devices Meeting (IEDM), 2020 MATH \u00a0\n \n Google Scholar \u00a0\n Jeong Y J, Zidan M A, Lu W D. Parasitic effect analysis in memristor-array-based neuromorphic systems. IEEE Trans Nanotechnol, 2018, 17: 184\u2013193 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Chen L R, Li J W, Chen Y R, et al. Accelerator-friendly neural-network training: learning variations and defects in RRAM crossbar. In: Proceedings of Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), 2017 MATH \u00a0\n \n Google Scholar \u00a0\n Woo J, Moon K, Song J, et al. Improved synaptic behavior under identical pulses using AlO x /HfO 2 bilayer RRAM array for neuromorphic systems. IEEE Electron Dev Lett, 2016, 37: 994\u2013997 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Mao R, Wen B, Jiang M, et al. Experimentally-validated crossbar model for defect-aware training of neural networks. IEEE Trans Circ Syst II, 2022, 69: 2468\u20132472 MATH \u00a0\n \n Google Scholar \u00a0\n Li H, Jiang Z, Huang P, et al. Variation-aware, reliability-emphasized design and optimization of RRAM using SPICE model. In: Proceedings of Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE), 2015 MATH \u00a0\n \n Google Scholar \u00a0\n Xiang Y C, Huang P, Zhou Z, et al. Analog deep neural network based on nor flash computing array for high speed/energy efficiency computation. In: Proceedings of IEEE International Symposium on Circuits and Systems (ISCAS), 2019 MATH \u00a0\n \n Google Scholar \u00a0\n Wu W, Wu H Q, Gao B, et al. A methodology to improve linearity of analog RRAM for neuromorphic computing. In: Proceedings of IEEE Symposium on VLSI Technology, 2018 MATH \u00a0\n \n Google Scholar \u00a0\n Sun X, Yu S. Impact of non-ideal characteristics of resistive synaptic devices on implementing convolutional neural networks. IEEE J Emerg Sel Top Circ Syst, 2019, 9: 570\u2013579 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Jain S, Sengupta A, Roy K, et al. RxNN: a framework for evaluating deep neural networks on resistive crossbars. IEEE Trans Comput-Aided Des Integr Circ Syst, 2021, 40: 326\u2013338 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Peng X, Huang S, Jiang H, et al. DNN+NeuroSim V2.0: an end-to-end benchmarking framework for compute-in-memory accelerators for on-chip training. IEEE Trans Comput-Aided Des Integr Circ Syst, 2021, 40: 2306\u20132319 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Zhu Z, Sun H, Xie T, et al. MNSIM 2.0: a behavior-level modeling tool for processing-in-memory architectures. IEEE Trans Comput-Aided Des Integr Circ Syst, 2023, 42: 4112\u20134125 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Chakraborty I, Ali M F, Kim D E, et al. GENIEx: a generalized approach to emulating non-ideality in memristive Xbars using neural networks. In: Proceedings of the 57th ACM/IEEE Design Automation Conference (DAC), 2020 MATH \u00a0\n \n Google Scholar \u00a0\n Liu C C, Hu M, Strachan J P, et al. Rescuing memristor-based neuromorphic design with high defects. In: Proceedings of the 54th Annual Design Automation Conference, 2017 MATH \u00a0\n \n Google Scholar \u00a0\n Yan B N, Yang J H, Wu Q, et al. A closed-loop design to enhance weight stability of memristor based neural network chips. In: Proceedings of IEEE/ACM International Conference on Computer-Aided Design (ICCAD), 2017 MATH \u00a0\n \n Google Scholar \u00a0\n Liu B Y, Hu M, Li H, et al. Digital-assisted noise-eliminating training for memristor crossbar-based analog neuromorphic computing engine. In: Proceedings of the 50th ACM/EDAC/IEEE Design Automation Conference (DAC), 2013 MATH \u00a0\n \n Google Scholar \u00a0\n He Z Z, Lin J, Ewetz R, et al. Noise injection adaption: end-to-end ReRAM crossbar non-ideal effect adaption for neural network mapping. In: Proceedings of the 56th Annual Design Automation Conference, 2019 MATH \u00a0\n \n Google Scholar \u00a0\n Chen P Y, Lin B B, Wang I T, et al. Mitigating effects of non-ideal synaptic device characteristics for on-chip learning. In: Proceedings of IEEE/ACM International Conference on Computer-Aided Design (ICCAD), 2015 MATH \u00a0\n \n Google Scholar \u00a0\n Han L X, Xiang Y C, Huang P, et al. Novel weight mapping method for reliable NVM based neural network. In: Proceedings of IEEE International Reliability Physics Symposium (IRPS), 2021 MATH \u00a0\n \n Google Scholar \u00a0\n Liao Y, Gao B, Yao P, et al. Diagonal matrix regression layer: training neural networks on resistive crossbars with interconnect resistance effect. IEEE Trans Comput-Aided Des Integr Circ Syst, 2021, 40: 1662\u20131671 Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Sung C, Lim S, Kim H, et al. Effect of conductance linearity and multi-level cell characteristics of TaO x -based synapse device on pattern recognition accuracy of neuromorphic system. Nanotechnology, 2018, 29: 115203 Article \u00a0\n MATH \u00a0\n \n",
          "original_query": "Mitigating effects of non-ideal synaptic device characteristics for on-chip learning",
          "cleaned_query": "Mitigating effects of non-ideal synaptic device characteristics for on-chip learning"
        },
        {
          "success": true,
          "title": "Accurate deep neural network inference using ...",
          "url": "https://www.nature.com/articles/s41467-020-16108-9",
          "content": "Accurate deep neural network inference using computational phase-change memory | Nature Communications\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature Communications](https://media.springernature.com/full/nature-cms/uploads/product/ncomms/header-7001f06bc3fe2437048388e9f2f44215.svg)](https://www.nature.com/ncomms)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41467-020-16108-9?error=cookies_not_supported&code=f761899c-58ee-4227-8655-448e3295cfe0)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41467)\n* [RSS feed](https://www.nature.com/ncomms.rss)\nAccurate deep neural network inference using computational phase-change memory\n[Download PDF](https://www.nature.com/articles/s41467-020-16108-9.pdf)\n[Download PDF](https://www.nature.com/articles/s41467-020-16108-9.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:18 May 2020# Accurate deep neural network inference using computational phase-change memory\n* [Vinay Joshi](#auth-Vinay-Joshi-Aff1-Aff2)[ORCID:orcid.org/0000-0001-6031-1669](https://orcid.org/0000-0001-6031-1669)[1](#Aff1),[2](#Aff2),\n* [Manuel Le Gallo](#auth-Manuel-Le_Gallo-Aff1)[ORCID:orcid.org/0000-0003-1600-6151](https://orcid.org/0000-0003-1600-6151)[1](#Aff1),\n* [Simon Haefeli](#auth-Simon-Haefeli-Aff1-Aff3)[1](#Aff1),[3](#Aff3),\n* [Irem Boybat](#auth-Irem-Boybat-Aff1-Aff4)[ORCID:orcid.org/0000-0002-4255-8622](https://orcid.org/0000-0002-4255-8622)[1](#Aff1),[4](#Aff4),\n* [S. R. Nandakumar](#auth-S__R_-Nandakumar-Aff1)[ORCID:orcid.org/0000-0002-7930-508X](https://orcid.org/0000-0002-7930-508X)[1](#Aff1),\n* [Christophe Piveteau](#auth-Christophe-Piveteau-Aff1-Aff3)[1](#Aff1),[3](#Aff3),\n* [Martino Dazzi](#auth-Martino-Dazzi-Aff1-Aff3)[1](#Aff1),[3](#Aff3),\n* [Bipin Rajendran](#auth-Bipin-Rajendran-Aff2)[2](#Aff2),\n* [Abu Sebastian](#auth-Abu-Sebastian-Aff1)[ORCID:orcid.org/0000-0001-5603-5243](https://orcid.org/0000-0001-5603-5243)[1](#Aff1)&amp;\n* \u2026* [Evangelos Eleftheriou](#auth-Evangelos-Eleftheriou-Aff1)[1](#Aff1)Show authors\n[*Nature Communications*](https://www.nature.com/ncomms)**volume11**, Article\u00a0number:2473(2020)[Cite this article](#citeas)\n* 42kAccesses\n* 444Citations\n* 62Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41467-020-16108-9/metrics)\n### Subjects\n* [Computer science](https://www.nature.com/subjects/computer-science)\n* [Electronic devices](https://www.nature.com/subjects/electronic-devices)\n## Abstract\nIn-memory computing using resistive memory devices is a promising non-von Neumann approach for making energy-efficient deep learning inference hardware. However, due to device variability and noise, the network needs to be trained in a specific way so that transferring the digitally trained weights to the analog resistive memory devices will not result in significant loss of accuracy. Here, we introduce a methodology to train ResNet-type convolutional neural networks that results in no appreciable accuracy loss when transferring weights to phase-change memory (PCM) devices. We also propose a compensation technique that exploits the batch normalization parameters to improve the accuracy retention over time. We achieve a classification accuracy of 93.7% on CIFAR-10 and a top-1 accuracy of 71.6% on ImageNet benchmarks after mapping the trained weights to PCM. Our hardware results on CIFAR-10 with ResNet-32 demonstrate an accuracy above 93.5% retained over a one-day period, where each of the 361,722 synaptic weights is programmed on just two PCM devices organized in a differential configuration.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-022-31405-1/MediaObjects/41467_2022_31405_Fig1_HTML.png)\n### [Optimised weight programming for analogue memory-based deep neural networks](https://www.nature.com/articles/s41467-022-31405-1?fromPaywallRec=false)\nArticleOpen access30 June 2022\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-025-59815-x/MediaObjects/41467_2025_59815_Fig1_HTML.png)\n### [Two-dimensional materials based two-transistor-two-resistor synaptic kernel for efficient neuromorphic computing](https://www.nature.com/articles/s41467-025-59815-x?fromPaywallRec=false)\nArticleOpen access09 May 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-025-23303-5/MediaObjects/41598_2025_23303_Fig1_HTML.png)\n### [An energy efficient processor array and memory controller for accurate processing of convolutional neural network-based inference engines](https://www.nature.com/articles/s41598-025-23303-5?fromPaywallRec=false)\nArticleOpen access12 November 2025\n## Introduction\nDeep neural networks (DNNs) have revolutionized the field of artificial intelligence and have achieved unprecedented success in cognitive tasks such as image and speech recognition. Platforms for deploying the trained model of such networks and performing inference in an energy-efficient manner are highly attractive for edge computing applications. In particular, internet-of-things battery-powered devices and autonomous cars could especially benefit from fast, low-power, and reliably accurate DNN inference engines. Significant progress in this direction has been made with the introduction of specialized hardware for inference operating at reduced digital precision (4\u20138-bit), such as Google\u2019s tensor processing unit[1](https://www.nature.com/articles/s41467-020-16108-9#ref-CR1)and low-power graphical processing units such as NVIDIA T4[2](https://www.nature.com/articles/s41467-020-16108-9#ref-CR2). While these platforms are very flexible, they are based on architectures where there is a physical separation between memory and processing units. The models are typically stored in off-chip memory, leading to constant shuttling of data between memory and processing units, which limits the maximum achievable energy efficiency.\nIn order to reduce the data transfers to a minimum in inference accelerators, a promising avenue is to employ in-memory computing using non-volatile memory devices[3](#ref-CR3),[4](#ref-CR4),[5](https://www.nature.com/articles/s41467-020-16108-9#ref-CR5). Both charge-based storage devices, such as Flash memory[6](https://www.nature.com/articles/s41467-020-16108-9#ref-CR6), and resistance-based (memristive) storage devices, such as metal-oxide resistive random-access memory[7](#ref-CR7),[8](#ref-CR8),[9](#ref-CR9),[10](https://www.nature.com/articles/s41467-020-16108-9#ref-CR10)and phase-change memory (PCM)[11](#ref-CR11),[12](#ref-CR12),[13](#ref-CR13),[14](https://www.nature.com/articles/s41467-020-16108-9#ref-CR14)are being investigated for this. In this approach, the network weights are encoded as the analog charge state or conductance state of these devices organized in crossbar arrays, and the matrix-vector multiplications during inference can be performed in-situ in a single time step by exploiting Kirchhoff\u2019s circuit laws. The fact that these devices are non-volatile (the weights will be retained when the power supply is turned off) and have multi-level storage capability (a single device can encode an analog range of values as opposed to 1 bit) is very attractive for inference applications. However, due to the analog nature of ",
          "original_query": "Accurate deep neural network inference using computational phase-change memory",
          "cleaned_query": "Accurate deep neural network inference using computational phase-change memory"
        }
      ],
      "generated_ideas": [
        "Design a training algorithm that routes residual capacity preferentially to defect-free subarrays and prove a recovery bound showing how many residual tiles are needed to match a target accuracy under a given defect rate.\n"
      ],
      "generated_ideas_raw": "1. **Tiki-Taka with Quantized Multi-tile Residual Updates for 4-bit AIMC Training**  \nCombine Paper 1\u2019s exact-convergence Tiki-Taka dynamics with Paper 2\u2019s multi-tile residual learning by running Tiki-Taka on a \u201cbase tile\u201d and allocating residual tiles to store the mismatch induced by 4-bit conductance quantization. Develop convergence guarantees that explicitly include both update asymmetry and limited conductance states, and validate on CIFAR-10/ImageNet-scale ResNets in a NeuroSim/MNSIM-style simulator.\n\n2. **Tile-Adaptive Residual Allocation via Online Optimality-Gap Estimation**  \nExtend multi-tile residual learning (Paper 2) with an online controller that decides, per layer, how many tiles to allocate and when to stop adding tiles based on an estimated optimality gap or gradient norm measured from analog readouts. The key contribution is an actionable scheme that trades hardware overhead for accuracy dynamically during training, with provable bounds on regret/optimality gap under device noise and drift.\n\n3. **Asymmetry-Aware Optimizer Design Beyond SGD: Analog Adam/Adagrad with Exact-Critical-Point Guarantees**  \nGeneralize Paper 1\u2019s analysis of asymmetric updates causing SGD\u2019s asymptotic error to adaptive methods (Adam/Adagrad/RMSProp) implemented under realistic AIMC update rules. Propose a modified \u201cbias-corrected, symmetry-enforced\u201d analog adaptive optimizer and prove conditions under which it converges exactly to critical points, then benchmark against Tiki-Taka and vanilla analog SGD.\n\n4. **BatchNorm-as-Calibration During Training: Joint Weight + BN Drift Compensation**  \nBuild on Paper 5\u2019s use of batch normalization parameters for post-mapping compensation by integrating BN-based calibration directly into analog training loops. Specifically, treat BN affine parameters as fast digital variables updated to counter device drift/noise, while weights are trained in-memory; quantify how much this reduces required conductance precision and number of residual tiles.\n\n5. **Differential-Pair + Residual-Tile Co-design to Reduce Tile Count and Drift Sensitivity**  \nPaper 5 shows differential PCM mapping improves inference robustness; Paper 2 uses multiple tiles for residual correction. Develop a co-design where the primary weight is stored as a differential pair (to linearize/center updates) and residual tiles store only low-amplitude corrections, reducing both tile count and sensitivity to temporal drift; provide a hardware-cost/accuracy Pareto analysis.\n\n6. **Defect- and Stuck-Device-Aware Residual Learning with Provable Recovery**  \nIncorporate defect models (e.g., stuck-at, limited dynamic range, line resistance/parasitics referenced in Paper 4\u2019s ecosystem) into the residual learning framework of Paper 2. Design a training algorithm that routes residual capacity preferentially to defect-free subarrays and prove a recovery bound showing how many residual tiles are needed to match a target accuracy under a given defect rate.\n\n7. **Energy-Optimal Training Schedules: When to Program, When to Accumulate Digitally**  \nUsing the cost analyses implied in Paper 2 and the training-vs-inference concerns in Paper 1, formulate an energy model that includes ADC/DAC cost, programming pulses, and tile reads. Propose an actionable schedule that alternates between (i) cheap analog accumulation steps and (ii) infrequent programming/refresh steps, and prove convergence while minimizing joules-per-accuracy at fixed wall-clock time.\n\n8. **Stochastic Device-Noise as Regularization: Controlled Noise Injection with Exact Convergence**  \nLeverage Paper 1\u2019s exact-convergence framework to introduce deliberate, controllable noise (matching device stochasticity) as an implicit regularizer during analog training. The contribution is a noise-injection policy (variance schedule tied to gradient norms and update asymmetry) that improves generalization while retaining guaranteed convergence to critical points, tested across varying conductance resolutions.\n\n9. **Layerwise Mixed-Precision AIMC Training: Residual Tiles Only Where Needed**  \nExtend Paper 2 by making residual learning selective: allocate multi-tile residuals only to layers that are most sensitive to limited states (e.g., first/last layers or attention projections) while training other layers with single-tile Tiki-Taka or standard AIMC updates. Provide an actionable sensitivity metric (based on Hessian/gradient statistics obtainable from analog measurements) and demonstrate large hardware savings with minimal accuracy loss.\n\n10. **Closed-loop Programming with On-chip Verification: \u201cExact Update Emulation\u201d for AIMC Training**  \nMotivated by Paper 1\u2019s emphasis on exact gradient-based training and Paper 5\u2019s emphasis on accurate weight transfer, design a closed-loop scheme that verifies each programmed conductance change via quick read-and-correct pulses to emulate a desired symmetric update. The contribution is a practical algorithm-circuit interface (verification frequency, tolerance bands, and correction rules) plus theory showing how verification reduces asymptotic error and lowers the required number of residual tiles.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Design a training algorithm that routes residual capacity preferentially to defect-free subarrays and prove a recovery bound showing how many residual tiles are needed to match a target accuracy under",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 25,
      "paper_title": "Discovering Opinion Intervals from Conflicts in Signed Graphs",
      "contribution": "Introduce and study the problem of recovering a small set of interpretable opinion intervals on a line that explain the positive/negative edges of a signed graph, prove hardness results, derive a polynomial-time approximation scheme by connecting the model to interval/indifference graphs and correlation clustering, and provide scalable heuristics with empirical validation.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "input_tokens": 8858,
      "output_tokens": 895,
      "predecessor_details": [
        {
          "success": true,
          "title": "On the notion of balance of a signed graph. - Semantic Scholar",
          "url": "https://www.semanticscholar.org/paper/On-the-notion-of-balance-of-a-signed-graph.-Harary/f7706ee104cc9f7d708cddccb476bf693e0b7637",
          "content": "[PDF] On the notion of balance of a signed graph. | Semantic Scholar\n[Skip to search form](#search-form)[Skip to main content](#main-content)[Skip to account menu](#account-menu)\n[Semantic ScholarSemantic Scholar's Logo](https://www.semanticscholar.org/)\nSearch 231,065,249 papers from all fields of science\nSearch\n* DOI:[10.1307/MMJ/1028989917](https://doi.org/10.1307/MMJ/1028989917)\n* Corpus ID: 120368252# On the notion of balance of a signed graph.\n```\n@article{Harary1953OnTN,\ntitle={On the notion of balance of a signed graph.},\nauthor={Frank Harary},\njournal={Michigan Mathematical Journal},\nyear={1953},\nvolume={2},\npages={143-146},\nurl={https://api.semanticscholar.org/CorpusID:120368252}\n}\n```\n* [F. Harary](https://www.semanticscholar.org/author/F.-Harary/1750484)\n* Published1953\n* Mathematics\n* Michigan Mathematical Journal\n[View via Publisher](https://doi.org/10.1307/MMJ/1028989917)\n[projecteuclid.org](https://projecteuclid.org/journals/michigan-mathematical-journal/volume-2/issue-2/On-the-notion-of-balance-of-a-signed-graph/10.1307/mmj/1028989917.pdf)\nSave to LibrarySave\nCreate AlertAlert\nCite\nShare\n1,121 Citations\n[\nHighly Influential Citations\n](#citing-papers)[](https://www.semanticscholar.org/faq#influential-citations)\n96\n[\nBackground Citations\n](#citing-papers)\n474\n[\nMethods Citations\n](#citing-papers)\n111\n[\nResults Citations\n](#citing-papers)\n3\n[View All](#citing-papers)\n## 1,121 Citations\nCitation Type\nHas PDF\nAuthor\nMore Filters\nMore Filters\nFilters\nSort by Most Influenced PapersSort by Citation CountSort by Recency\n[### Social balance - a signed detour distance analysis\n](https://www.semanticscholar.org/paper/Social-balance-a-signed-detour-distance-analysis-Mathew-Shijin/b21e5cf6413ee9177b6b499a12ecbecb529146fc)[Albin Mathew](https://www.semanticscholar.org/author/Albin-Mathew/2148617957)[T. Shijin](https://www.semanticscholar.org/author/T.-Shijin/115225265)[Roshni T. Roy](https://www.semanticscholar.org/author/Roshni-T.-Roy/2067860642)[P. Soorya](https://www.semanticscholar.org/author/P.-Soorya/71927235)[Shahul K. Hameed](https://www.semanticscholar.org/author/Shahul-K.-Hameed/2124784001)[K. A. Germina](https://www.semanticscholar.org/author/K.-A.-Germina/2028882692)\nMathematics, Sociology\n[The Journal of Mathematical Sociology](https://www.semanticscholar.org/venue?name=The%20Journal%20of%20Mathematical%20Sociology)\n* 2021\nTLDR\nBy defining two types of signed detour distances and correspondingdetour distance matrices, the notion of detour distance compatibility for signed graphs is introduced and later applied, it is given yet another characterization of balance in signed graphs.Expand\n* [Highly Influenced](https://www.semanticscholar.org/paper/b21e5cf6413ee9177b6b499a12ecbecb529146fc?sort=is-influential#citing-papers)\n* 5 Excerpts\nSave\n[### Algebraic criteria for structure identification and behaviour analysis of signed networks\n](https://www.semanticscholar.org/paper/Algebraic-criteria-for-structure-identification-and-Du-Ma/3319fd2b0d5b5038c1aecf962e7fdb9717efae8a)[Mingjun Du](https://www.semanticscholar.org/author/Mingjun-Du/50593595)[B. Ma](https://www.semanticscholar.org/author/B.-Ma/3126976)[De-yuan Meng](https://www.semanticscholar.org/author/De-yuan-Meng/143723433)\nMathematics\n[Int. J. Syst. Sci.](https://www.semanticscholar.org/venue?name=Int.%20J.%20Syst.%20Sci.)\n* 2019\nTLDR\nAn algorithm via algebraic manipulations is proposed to identify the structural balance and unbalance of signed digraphs, which contributes to achieving behaviour analysis of signed networks.Expand\n* [\n4\n](https://www.semanticscholar.org/paper/3319fd2b0d5b5038c1aecf962e7fdb9717efae8a#citing-papers)\n* [Highly Influenced](https://www.semanticscholar.org/paper/3319fd2b0d5b5038c1aecf962e7fdb9717efae8a?sort=is-influential#citing-papers)\n* 9 Excerpts\nSave\n[### Curvature and Higher Order Buser Inequalities for the Graph Connection Laplacian\n](https://www.semanticscholar.org/paper/Curvature-and-Higher-Order-Buser-Inequalities-for-Liu-M%C3%BCnch/6b1070fa50ae257e8fb300145ce208391f9aad08)[Shiping Liu](https://www.semanticscholar.org/author/Shiping-Liu/48641995)[Florentin M\u00fcnch](https://www.semanticscholar.org/author/Florentin-M%C3%BCnch/32512490)[N. Peyerimhoff](https://www.semanticscholar.org/author/N.-Peyerimhoff/2403211)\nMathematics\n[SIAM J. Discret. Math.](https://www.semanticscholar.org/venue?name=SIAM%20J.%20Discret.%20Math.)\n* 2019\nTLDR\nIn this process, the concepts of Cheeger type constants and a discrete Ricci curvature for connection Laplacians are discussed and their properties systematically are studied.Expand\n* [\n31\n](https://www.semanticscholar.org/paper/6b1070fa50ae257e8fb300145ce208391f9aad08#citing-papers)\n* [Highly Influenced](https://www.semanticscholar.org/paper/6b1070fa50ae257e8fb300145ce208391f9aad08?sort=is-influential#citing-papers)\n[[PDF]](https://www.semanticscholar.org/reader/6b1070fa50ae257e8fb300145ce208391f9aad08)\n* 9 Excerpts\nSave\n[### An application of Cubical Cohomology to Adinkras and Supersymmetry Representations\n](https://www.semanticscholar.org/paper/An-application-of-Cubical-Cohomology-to-Adinkras-Doran-Iga/2019f03616c9d3ab85c655c0096e544c0d8484b7)[C. Doran](https://www.semanticscholar.org/author/C.-Doran/35072488)[K. Iga](https://www.semanticscholar.org/author/K.-Iga/2252983)[G. Landweber](https://www.semanticscholar.org/author/G.-Landweber/1919877)\nPhysics, Mathematics\n* 2012\nAn Adinkra is a class of graphs with certain signs marking its vertices and edges, which encodes off-shell representations of the super Poincar\\\\'e algebra. The markings on the vertices and edges of\u2026Expand\n* [\n22\n](https://www.semanticscholar.org/paper/2019f03616c9d3ab85c655c0096e544c0d8484b7#citing-papers)\n* [Highly Influenced](https://www.semanticscholar.org/paper/2019f03616c9d3ab85c655c0096e544c0d8484b7?sort=is-influential#citing-papers)\n[[PDF]](https://www.semanticscholar.org/reader/2019f03616c9d3ab85c655c0096e544c0d8484b7)\n* 4 Excerpts\nSave\n[### From Signed Networks to Group Graphs\n](https://www.semanticscholar.org/paper/From-Signed-Networks-to-Group-Graphs-Evans/3fc984d82f793dd04b7ab4083d21f194d55ee50e)[T. S. Evans](https://www.semanticscholar.org/author/T.-S.-Evans/2364031247)\nMathematics, Computer Science\n[ArXiv](https://www.semanticscholar.org/venue?name=ArXiv)\n* 2025\nTLDR\nIt is shown that for processes on a balanced group graph the time evolution is completely determined by the network topology, not by the group structure.Expand\n* [\n1\n](https://www.semanticscholar.org/paper/3fc984d82f793dd04b7ab4083d21f194d55ee50e#citing-papers)\n* [Highly Influenced](https://www.semanticscholar.org/paper/3fc984d82f793dd04b7ab4083d21f194d55ee50e?sort=is-influential#citing-papers)\n* [\nPDF\n](https://www.semanticscholar.org/paper/3fc984d82f793dd04b7ab4083d21f194d55ee50e)\n* 7 Excerpts\nSave\n[### Optimal Network Pairwise Comparison\n](https://www.semanticscholar.org/paper/Optimal-Network-Pairwise-Comparison-Jin-Ke/91a0082ba34caa4d83bb49801be2a992d045a52d)[Jiashun Jin](https://www.semanticscholar.org/author/Jiashun-Jin/2268982168)[Z. Ke](https://www.semanticscholar.org/author/Z.-Ke/39622714)[Shengming Luo](https://www.semanticscholar.org/author/Shengming-Luo/51128577)[Yucong Ma](https://www.semanticscholar.org/author/Yucong-Ma/2109326109)\nComputer Science, Mathematics\n[Journal of the American Statistical Association](https://www.semanticscholar.org/venue?name=Journal%20of%20the%20American%20Statistical%20Association)\n* 2024\nTLDR\nIt is shown that IBM is a test: in a broad DCMM setting with only mild regularity conditions, IBM has N(0,1) as the limiting null and achieves the optimal phase transition and continues to achieve the optimal phase transition.Expand\n* [\n2\n](https://www.semanticscholar.org/paper/91a0082ba34caa4d83bb49801be2a992d045a52d#citing-papers)\n* [Highly Influenced](https://www.semanticscholar.org/paper/91a0082ba34caa4d83bb49801be2a992d045a52d?sort=is-influential#citing-papers)\n* [\nPDF\n](https://www.semanticscholar.org/paper/91a0082ba34caa4d83bb49801be2a9",
          "original_query": "On the notion of balance of a signed graph",
          "cleaned_query": "On the notion of balance of a signed graph"
        },
        {
          "success": true,
          "title": "Correlation Clustering and (De)Sparsification: Graph Sketches Can ...",
          "url": "https://arxiv.org/abs/2504.04258",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "Correlation Clustering",
          "cleaned_query": "Correlation Clustering"
        },
        {
          "success": true,
          "title": "[cs/0504023] Correlation Clustering with a Fixed Number of ...",
          "url": "https://arxiv.org/abs/cs/0504023",
          "content": "[cs/0504023] Correlation Clustering with a Fixed Number of Clusters\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:cs/0504023\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Data Structures and Algorithms\n**arXiv:cs/0504023**(cs)\n[Submitted on 6 Apr 2005]\n# Title:Correlation Clustering with a Fixed Number of Clusters\nAuthors:[Ioannis Giotis](https://arxiv.org/search/cs?searchtype=author&amp;query=Giotis,+I),[Venkatesan Guruswami](https://arxiv.org/search/cs?searchtype=author&amp;query=Guruswami,+V)\nView a PDF of the paper titled Correlation Clustering with a Fixed Number of Clusters, by Ioannis Giotis and Venkatesan Guruswami\n[View PDF](https://arxiv.org/pdf/cs/0504023)> > Abstract:\n> We continue the investigation of problems concerning correlation clustering or clustering with qualitative information, which is a clustering formulation that has been studied recently. The basic setup here is that we are given as input a complete graph on n nodes (which correspond to nodes to be clustered) whose edges are labeled + (for similar pairs of items) and - (for dissimilar pairs of items). Thus we have only as input qualitative information on similarity and no quantitative distance measure between items. The quality of a clustering is measured in terms of its number of agreements, which is simply the number of edges it correctly classifies, that is the sum of number of - edges whose endpoints it places in different clusters plus the number of + edges both of whose endpoints it places within the same cluster.\n> In this paper, we study the problem of finding clusterings that maximize the number of agreements, and the complementary minimization version where we seek clusterings that minimize the number of disagreements. We focus on the situation when the number of clusters is stipulated to be a small constant k. Our main result is that for every k, there is a polynomial time approximation scheme for both maximizing agreements and minimizing disagreements. (The problems are NP-hard for every k &gt;= 2.) The main technical work is for the minimization version, as the PTAS for maximizing agreements follows along the lines of the property tester for Max k-CUT.\n> In contrast, when the number of clusters is not specified, the problem of minimizing disagreements was shown to be APX-hard, even though the maximization version admits a PTAS. Comments:|16 pages|\nSubjects:|Data Structures and Algorithms (cs.DS)|\nACMclasses:|F.2.2; G.1.2; G.1.6|\nCite as:|[arXiv:cs/0504023](https://arxiv.org/abs/cs/0504023)[cs.DS]|\n|(or[arXiv:cs/0504023v1](https://arxiv.org/abs/cs/0504023v1)[cs.DS]for this version)|\n|[https://doi.org/10.48550/arXiv.cs/0504023](https://doi.org/10.48550/arXiv.cs/0504023)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Ioannis Giotis [[view email](https://arxiv.org/show-email/e893880f/cs/0504023)]\n**[v1]**Wed, 6 Apr 2005 22:36:03 UTC (18 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Correlation Clustering with a Fixed Number of Clusters, by Ioannis Giotis and Venkatesan Guruswami\n* [View PDF](https://arxiv.org/pdf/cs/0504023)\n* [TeX Source](https://arxiv.org/src/cs/0504023)\n[view license](http://arxiv.org/licenses/assumed-1991-2003/)\nCurrent browse context:\ncs.DS\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=cs/0504023&amp;function=prev&amp;context=cs.DS) | [next&gt;&gt;](https://arxiv.org/prevnext?id=cs/0504023&amp;function=next&amp;context=cs.DS)\n[new](https://arxiv.org/list/cs.DS/new)|[recent](https://arxiv.org/list/cs.DS/recent)|[2005-04](https://arxiv.org/list/cs.DS/2005-04)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:cs/0504023)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=cs/0504023)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:cs/0504023)\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr0504.html#abs-cs-0504023)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-cs-0504023)\n[Ioannis Giotis]()\n[Venkatesan Guruswami]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/cs/0504023)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Correlation clustering with a fixed number of clusters",
          "cleaned_query": "Correlation clustering with a fixed number of clusters"
        },
        {
          "success": true,
          "title": "Can Everybody Sit Closer to Their Friends Than Their Enemies?",
          "url": "https://link.springer.com/chapter/10.1007/978-3-642-22993-0_36",
          "content": "\n \n \n Abstract \n Signed graphs are graphs with signed edges. They are commonly used to represent positive and negative relationships in social networks. While balance theory and clusterizable graphs deal with signed graphs, recent empirical studies have proved that they fail to reflect some current practices in real social networks. In this paper we address the issue of drawing signed graphs and capturing such social interactions. We relax the previous assumptions to define a drawing as a model in which every vertex has to be placed closer to its neighbors connected through a positive edge than its neighbors connected through a negative edge in the resulting space. Based on this definition, we address the problem of deciding whether a given signed graph has a drawing in a given \u2113-dimensional Euclidean space. We focus on the 1-dimensional case, where we provide a polynomial time algorithm that decides if a given complete signed graph has a drawing, and provides it when applicable. This work has been supported by the ERC Starting research grant GOSSPLE number 204742, Comunidad de Madrid grant S2009TIC-1692 and Spanish MICINN grant TIN2008\u201306735-C02-01. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n Preview \n \n Unable to display preview.\u00a0 Download preview\n PDF. \n \n \n \n \n \n \n \n Similar content being viewed by others \n \n \n \n \n \n \n \n References Antal, T., Krapivsky, P.L., Redner, S.: Dynamics of social balance on networks. Phys. Rev. E\u00a072(3), 036121 (2005) Article \u00a0\n MathSciNet \u00a0\n \n Google Scholar \u00a0\n Bansal, N., Blum, A., Chawla, S.: Correlation clustering. Machine Learning\u00a056(1-3), 89\u2013113 (2004) Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Brandes, U., Fleischer, D., Lerner, J.: Summarizing dynamic bipolar conflict structures. IEEE Trans. Vis. Comput. Graph.\u00a012(6), 1486\u20131499 (2006) Article \u00a0\n \n Google Scholar \u00a0\n Cartwright, D., Harary, F.: Structural balance: a generalization of heider\u2019s theory. Psychological Review\u00a063(5), 277\u2013293 (1956) Article \u00a0\n \n Google Scholar \u00a0\n Davis, J.A.: Clustering and structural balance in graphs. Human Relations\u00a020(2), 181 (1967) Article \u00a0\n \n Google Scholar \u00a0\n Habib, M., McConnell, R.M., Paul, C., Viennot, L.: Lex-bfs and partition refinement, with applications to transitive orientation, interval graph recognition and consecutive ones testing. Theor. Comput. Sci.\u00a0234(1-2), 59\u201384 (2000) Article \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Harary, F.: On the notion of balance of a signed graph. Michigan Mathematical Journal\u00a02(2), 143 (1953) Article \u00a0\n MathSciNet \u00a0\n \n Google Scholar \u00a0\n Harary, F., Kabell, J.A.: A simple algorithm to detect balance in signed graphs. Mathematical Social Sciences\u00a01(1), 131\u2013136 (1980) Article \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n Harary, F., Kabell, J.A.: Counting balanced signed graphs using marked graphs. In: Proceedings of the Edinburgh Mathematical Society, vol.\u00a024(2), pp. 99\u2013104 (1981) \n Google Scholar \u00a0\n Harary, F., Palmer, E.: On the number of balanced signed graphs. Bulletin of Mathematical Biology\u00a029, 759\u2013765 (1967) MATH \u00a0\n \n Google Scholar \u00a0\n Kunegis, J., Schmidt, S., Lommatzsch, A., Lerner, J., De Luca, E.W., Albayrak, S.: Spectral analysis of signed graphs for clustering, prediction and visualization. In: SDM, page 559 (2010) \n Google Scholar \u00a0\n Lauterbach, D., Truong, H., Shah, T., Adamic, L.A.: Surfing a web of trust: Reputation and reciprocity on couchsurfing.com. In: CSE (4), pp. 346\u2013353 (2009) \n Google Scholar \u00a0\n Leskovec, J., Huttenlocher, D.P., Kleinberg, J.M.: Governance in social media: A case study of the wikipedia promotion process. In: ICWSM 2010 (2010) \n Google Scholar \u00a0\n Leskovec, J., Huttenlocher, D.P., Kleinberg, J.M.: Predicting positive and negative links in online social networks. In: WWW 2010, pp. 641\u2013650 (2010) \n Google Scholar \u00a0\n Leskovec, J., Huttenlocher, D.P., Kleinberg, J.M.: Signed networks in social media. In: CHI 2010, pp. 1361\u20131370 (2010) \n Google Scholar \u00a0\n Szell, M., Lambiotte, R., Thurner, S.: Multirelational organization of large-scale social networks in an online world. PNAS\u00a0107(31), 13636\u201313641 (2010) Article \u00a0\n \n Google Scholar \u00a0\n Download references Author information Authors and Affiliations INRIA Rennes \u2013 Bretagne Atlantique, France Anne-Marie Kermarrec LADyR, GSyC, Universidad Rey Juan Carlos, Spain Christopher Thraves Authors Anne-Marie Kermarrec Christopher Thraves Editor information Editors and Affiliations Institute of Informatics, University of Warsaw, ul. Banacha 2, 02-097, Warsaw, Poland Filip Murlak \u00a0&amp;\u00a0Piotr Sankowski \u00a0&amp;\u00a0 Rights and permissions Copyright information \u00a9 2011 Springer-Verlag GmbH Berlin Heidelberg About this paper Cite this paper Kermarrec, AM., Thraves, C. (2011). Can Everybody Sit Closer to Their Friends Than Their Enemies?.\n In: Murlak, F., Sankowski, P. (eds) Mathematical Foundations of Computer Science 2011. MFCS 2011. Lecture Notes in Computer Science, vol 6907. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-22993-0_36 Download citation.RIS.ENW.BIB DOI: https://doi.org/10.1007/978-3-642-22993-0_36 \n Publisher Name: Springer, Berlin, Heidelberg \n Print ISBN: 978-3-642-22992-3 \n Online ISBN: 978-3-642-22993-0 eBook Packages: Computer Science Computer Science (R0) Publish with us \n \n \n \n",
          "original_query": "Can everybody sit closer to their friends than their enemies?",
          "cleaned_query": "Can everybody sit closer to their friends than their enemies?"
        },
        {
          "success": true,
          "title": "Gridline indifference graphs - ScienceDirect",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S0165489606000655",
          "content": "[Skip to main content](https://www.sciencedirect.com/science/article/abs/pii/S0165489606000655#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/science/article/abs/pii/S0165489606000655#screen-reader-main-title)\n\n- [Access through\u00a0**your institution**](https://www.sciencedirect.com/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS0165489606000655)\n- [Purchase PDF](https://www.sciencedirect.com/getaccess/pii/S0165489606000655/purchase)\n\nSearch ScienceDirect\n\n## Article preview\n\n- [Abstract](https://www.sciencedirect.com/science/article/abs/pii/S0165489606000655#preview-section-abstract)\n- [Introduction](https://www.sciencedirect.com/science/article/abs/pii/S0165489606000655#preview-section-introduction)\n- [Section snippets](https://www.sciencedirect.com/science/article/abs/pii/S0165489606000655#preview-section-snippets)\n- [References (14)](https://www.sciencedirect.com/science/article/abs/pii/S0165489606000655#preview-section-references)\n\n[![Elsevier](https://sdfestaticassets-us-east-1.sciencedirectassets.com/prod/558f6b3505d331efa27a89a25731aa712b0662a4/image/elsevier-non-solus.png)](https://www.sciencedirect.com/journal/mathematical-social-sciences)\n\n## [Mathematical Social Sciences](https://www.sciencedirect.com/journal/mathematical-social-sciences)\n\n[Volume 53, Issue 1](https://www.sciencedirect.com/journal/mathematical-social-sciences/vol/53/issue/1), January 2007, Pages 69-92\n\n[![Mathematical Social Sciences](https://ars.els-cdn.com/content/image/1-s2.0-S0165489606X01592-cov150h.gif)](https://www.sciencedirect.com/journal/mathematical-social-sciences/vol/53/issue/1)\n\n# Gridline indifference graphs\n\nAuthor links open overlay panelDalePeterson\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/j.mathsocsci.2006.08.003](https://doi.org/10.1016/j.mathsocsci.2006.08.003) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S0165489606000655&orderBeanReset=true)\n\n## Abstract\n\nIndifference graphs can be realized on a line with vertices adjacent whenever they are within a given distance. These well-studied graphs have applications to many fields including ecology, cluster theory, and psychology, in the placement of objects in a single dimension. The extension to the grid and [higher dimensions](https://www.sciencedirect.com/topics/mathematics/higher-dimensions) has been considered in e.g. Goodman's study of perception (1977); we introduce _gridline indifference graphs_, which can be realized in the plane with vertices adjacent whenever they are within a given distance and on a common vertical or [horizontal line](https://www.sciencedirect.com/topics/mathematics/horizontal-line). We obtain full and partial characterizations when these graphs are triangulated in terms of forbidden subgraphs, extreme points, and tree-clique graphs. These results are extended to higher dimensions.\n\n## Introduction\n\nAn _indifference graph_ (Roberts, 1969) is one that can be realized on the line with vertices adjacent whenever they are at most some fixed distance apart. There are applications of indifference graphs in many areas including biology, communication, economics, psychology, archaeology, transportation, and ecology (see Roberts, 1976, Roberts, 1978). Another area of application is perception. Goodman (1977) studies predicates that can be used to order stimuli such as sight or sound. Roberts (1969) shows that some of the predicates studied by Goodman lead precisely to indifference graphs. Roberts (1973) discusses how indifference graphs can model visual perception of physical space and _subjective_ visual space; changes that are close in some sense are indistinguishable. Goodman (1977) considers the extension to two dimensions natural and applies some of his predicates to the grid. This suggests extending indifference graphs to two and higher dimensional _p_ grids; we call these _gridline indifference graphs_. One application to perception is where an object has not just one but _p_ characteristics, such as position in multiple dimensions, color, intensity, texture, sound and taste.\n\nMuch of this paper deals with realizing gridline indifference graphs in _p_ dimensions. A realization gives insight and \u201cvisualization\u201d into the relative changes in the _p_ characteristics.\n\nAnother application is in the confoundability of sequences of symbols, such as words or molecular chains, where symbols may be confused. An example is the set of words of length _p_ where, for each position _i_, certain letters can be confused. This may be modeled by a gridline indifference graph where each letter is a point in _p_-space, and any line in dimension _i_ contains an indifference graph representing letters that can be confused in position _i_. If letters that can be confused are within, say, distance one on an indifference graph, then the set of points inside a _p_-dimensional unit box corresponds to a set of words that can be confused. (See also Imrich (2000) for the related concept of _Shannon capacity_ of a graph.)\n\nStill another motivation for studying gridline indifference graphs is _cluster theory_. Often one wishes to aggregate data into groups that share some common property. If each datum corresponds to a vertex and edges are between data sharing the property, then we seek a group of pairwise adjacent vertices. When vertices are along a line and all vertices in any such group are within a fixed distance of one another, then we obtain an indifference graph (Roberts, 1978). If clustered vertices are grouped in the same way along a tree structure rather than a line, then we obtain a _tree-clique graph_ (Batbedat (1990) and Gutierrez and Oubi\u00f1a (1996)); these graphs turn out to be closely related to gridline indifference graphs.\n\nFor most of this paper we restrict ourselves to the class of gridline indifference graphs in which none can \u201ccycle back into itself\u201d \u2014 where two indifference graphs along different dimensions intersect at a vertex and then rejoin elsewhere. In some applications this is natural. For example, an indifference graph may model archaeological finds along time. If a second dimension were to correspond to, say, culture, then we would not expect cultures to separate, then independently develop similar objects and later merge back together. Moreover, this restriction lends itself to analogous results to those for indifference graphs, and to connections with tree-clique graphs.\n\nBefore stating three important characterizations of indifference graphs, we present some graph theory definitions. We follow the terminology of Bondy and Murty (1976); any undefined terms can be found there. A graph is a pair _G_\u00a0=\u00a0( _V_, _E_) where _V_ (or _V_( _G_)) is the vertex set and the edge set _E_ (or _E_( _G_)) is a subset of (V2):={uv(=vu):uandvaredistinctvertices}. All graphs are finite and simple, that is, they have no multiple edges or loops, and are undirected. With an abuse of language, and when no confusion is possible, we often refer to a vertex or edge as being in a graph _G_, and write for example _v_\u00a0\u2208\u00a0_G_ or _uw_\u00a0\u2208\u00a0_G_ instead of _v_\u00a0\u2208\u00a0_V_( _G_) or _uw_\u00a0\u2208\u00a0_E_( _G_). As defined in Bondy and Murty, the _degree_ of a vertex is the number of edges incident to it; a _pendant vertex_ is a vertex with degree one.\n\nA _subgraph G_\u2032 of a graph _G_\u00a0=\u00a0( _V_, _E_) is a pair ( _V_\u2032, _E_\u2032) where _V_\u2032\u00a0\u2286\u00a0_V_ and E\u2032\u2286(V\u20322)\u2229E, i.e. _E_\u2032 is a subset of the edges in _E_ restricted to _V_\u2032. An _induced_ subgraph _G_\u2032 of _G_ is a subgraph where E\u2032=(V\u20322)\u2229E, i.e. _E_\u2032 is all of the edges of _E_ restricted to _V_\u2032. If _G_ and _H_ are graphs, then _G_ is _H_- _free_ means no induced subgraph of _G_ is isomorphic to _H_. (A graph is a subgraph, and in fact an induced subgraph, of itself.)\n\nAn _indifference graph_ is a graph that can be realized on the line such that two vertices are adjacent whenever the distance between them is at most and _\u03f5_, where \u03f5\u2208R+ is a fixed number. A _triangulated graph_",
          "original_query": "Indifference graphs",
          "cleaned_query": "Indifference graphs"
        }
      ],
      "generated_ideas": [
        "Balance Testing via Graph Sketches for Signed Networks\nDevelop a linear-/near-linear-space sketch that estimates the frustration index (minimum number of edge sign flips or deletions to make a signed graph balanced) and distinguishes balanced vs. \u03b5-far-from-balanced graphs with provable guarantees. Build on (de)sparsification/sketching ideas from correlation clustering sketches and Harary\u2019s characterization of balance through cycle parity.",
        "Dimension of Social Embeddings: Minimal \u2113 for Signed-Graph Drawings\nStudy the optimization problem: given a (complete) signed graph, find the minimum Euclidean dimension \u2113 such that a \u201cfriends-closer-than-enemies\u201d drawing exists. Provide approximation algorithms and hardness results, and relate \u2113 to structural balance distance and correlation clustering cost.",
        "Correlation Clustering on Gridline Indifference Constraints (p-Dimensional Qualitative Similarity)\nDefine a correlation clustering variant where + edges must be explainable by a gridline indifference realization (same coordinate line and within threshold) while \u2212 edges violate it. Design recognition + clustering algorithms that jointly infer a gridline realization and clusters, leveraging forbidden-subgraph characterizations from gridline indifference graphs.",
        "Cycle-Localized Explanations for Disagreements in Correlation Clustering\nBuild an interpretable post-hoc method that decomposes correlation clustering disagreements into a small set of \u201cwitness\u201d unbalanced cycles (odd number of \u2212 edges) using Harary\u2019s balance criterion. The contribution is an algorithm that outputs a minimal/near-minimal certificate set of cycles explaining most objective loss, aiding debugging of signed data.",
        "Streaming Correlation Clustering with Fixed k Using Signed-Graph Balance Structure\nCreate a one-pass streaming algorithm for correlation clustering with fixed \\(k\\) that maintains compact summaries of +/\u2212 edge patterns and approximates agreements/disagreements. Combine the PTAS framework for fixed \\(k\\) with sketching primitives, and incorporate balance-aware constraints to improve accuracy on nearly balanced instances.",
        "Active Querying for Signed Edges Guided by Balance and Clustering Uncertainty\nDesign an active learning strategy that adaptively queries unknown edge signs to most reduce uncertainty in (i) whether the graph is (approximately) balanced and (ii) the optimal fixed-\\(k\\) correlation clustering. Use cycle-based balance certificates to choose queries that resolve many implied constraints at once.",
        "Signed Graph \u201cSparsifiers\u201d that Preserve Clusterability and Drawability\nConstruct edge sparsification methods that preserve (within \u03b5) both correlation clustering objective values and the feasibility/near-feasibility of friends-closer-than-enemies drawings. The deliverable is a theoretical guarantee plus an algorithm that outputs a much smaller signed graph suitable for fast clustering and visualization without losing key signed structure.",
        "Hybrid Geometric\u2013Combinatorial Clustering: Enforcing Gridline Realizations Inside Clusters\nPropose a model where each cluster must admit a low-dimensional gridline indifference realization (capturing within-cluster qualitative similarity structure), while between clusters edges can be arbitrary or mostly negative. Develop an algorithm that alternates between correlation-clustering-style reassignment and per-cluster gridline realization/recognition, yielding clusters with interpretable internal geometry."
      ],
      "generated_ideas_raw": "1. **Balance Testing via Graph Sketches for Signed Networks**  \n   Develop a linear-/near-linear-space sketch that estimates the frustration index (minimum number of edge sign flips or deletions to make a signed graph balanced) and distinguishes balanced vs. \u03b5-far-from-balanced graphs with provable guarantees. Build on (de)sparsification/sketching ideas from correlation clustering sketches and Harary\u2019s characterization of balance through cycle parity.\n\n2. **PTAS for Correlation Clustering under 1D \u201cFriends Closer than Enemies\u201d Embeddability**  \n   Use the polynomial-time recognizer for 1D signed-drawing feasibility on complete signed graphs to derive a PTAS (or exact algorithm) for correlation clustering restricted to instances that admit such an ordering. The actionable goal is to exploit the induced vertex order to design dynamic programming that beats general-case approximations for fixed \\(k\\).\n\n3. **Dimension of Social Embeddings: Minimal \u2113 for Signed-Graph Drawings**  \n   Study the optimization problem: given a (complete) signed graph, find the minimum Euclidean dimension \u2113 such that a \u201cfriends-closer-than-enemies\u201d drawing exists. Provide approximation algorithms and hardness results, and relate \u2113 to structural balance distance and correlation clustering cost.\n\n4. **Correlation Clustering on Gridline Indifference Constraints (p-Dimensional Qualitative Similarity)**  \n   Define a correlation clustering variant where + edges must be explainable by a gridline indifference realization (same coordinate line and within threshold) while \u2212 edges violate it. Design recognition + clustering algorithms that jointly infer a gridline realization and clusters, leveraging forbidden-subgraph characterizations from gridline indifference graphs.\n\n5. **Cycle-Localized Explanations for Disagreements in Correlation Clustering**  \n   Build an interpretable post-hoc method that decomposes correlation clustering disagreements into a small set of \u201cwitness\u201d unbalanced cycles (odd number of \u2212 edges) using Harary\u2019s balance criterion. The contribution is an algorithm that outputs a minimal/near-minimal certificate set of cycles explaining most objective loss, aiding debugging of signed data.\n\n6. **Streaming Correlation Clustering with Fixed k Using Signed-Graph Balance Structure**  \n   Create a one-pass streaming algorithm for correlation clustering with fixed \\(k\\) that maintains compact summaries of +/\u2212 edge patterns and approximates agreements/disagreements. Combine the PTAS framework for fixed \\(k\\) with sketching primitives, and incorporate balance-aware constraints to improve accuracy on nearly balanced instances.\n\n7. **Robust 1D Signed-Graph Drawing with Edge Noise (Minimum Violations Ordering)**  \n   Extend the 1D feasibility algorithm to an optimization problem: find an ordering (and 1D coordinates) minimizing the number (or total weight) of violated friend/enemy distance constraints. This would connect directly to minimizing disagreements in correlation clustering while producing a geometric layout usable for visualization and downstream inference.\n\n8. **Active Querying for Signed Edges Guided by Balance and Clustering Uncertainty**  \n   Design an active learning strategy that adaptively queries unknown edge signs to most reduce uncertainty in (i) whether the graph is (approximately) balanced and (ii) the optimal fixed-\\(k\\) correlation clustering. Use cycle-based balance certificates to choose queries that resolve many implied constraints at once.\n\n9. **Signed Graph \u201cSparsifiers\u201d that Preserve Clusterability and Drawability**  \n   Construct edge sparsification methods that preserve (within \u03b5) both correlation clustering objective values and the feasibility/near-feasibility of friends-closer-than-enemies drawings. The deliverable is a theoretical guarantee plus an algorithm that outputs a much smaller signed graph suitable for fast clustering and visualization without losing key signed structure.\n\n10. **Hybrid Geometric\u2013Combinatorial Clustering: Enforcing Gridline Realizations Inside Clusters**  \n   Propose a model where each cluster must admit a low-dimensional gridline indifference realization (capturing within-cluster qualitative similarity structure), while between clusters edges can be arbitrary or mostly negative. Develop an algorithm that alternates between correlation-clustering-style reassignment and per-cluster gridline realization/recognition, yielding clusters with interpretable internal geometry.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Balance Testing via Graph Sketches for Signed Networks\nDevelop a linear-/near-linear-space sketch that estimates the frustration index (minimum number of edge sign flips or deletions to make a signed ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Dimension of Social Embeddings: Minimal \u2113 for Signed-Graph Drawings\nStudy the optimization problem: given a (complete) signed graph, find the minimum Euclidean dimension \u2113 such that a \u201cfriends-closer-",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Correlation Clustering on Gridline Indifference Constraints (p-Dimensional Qualitative Similarity)\nDefine a correlation clustering variant where + edges must be explainable by a gridline indifference ",
          "is_match": true
        },
        {
          "idea_idx": 3,
          "idea_text": "Cycle-Localized Explanations for Disagreements in Correlation Clustering\nBuild an interpretable post-hoc method that decomposes correlation clustering disagreements into a small set of \u201cwitness\u201d unbal",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Streaming Correlation Clustering with Fixed k Using Signed-Graph Balance Structure\nCreate a one-pass streaming algorithm for correlation clustering with fixed \\(k\\) that maintains compact summaries of",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Active Querying for Signed Edges Guided by Balance and Clustering Uncertainty\nDesign an active learning strategy that adaptively queries unknown edge signs to most reduce uncertainty in (i) whether th",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Signed Graph \u201cSparsifiers\u201d that Preserve Clusterability and Drawability\nConstruct edge sparsification methods that preserve (within \u03b5) both correlation clustering objective values and the feasibility/",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Hybrid Geometric\u2013Combinatorial Clustering: Enforcing Gridline Realizations Inside Clusters\nPropose a model where each cluster must admit a low-dimensional gridline indifference realization (capturing ",
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 26,
      "paper_title": "A Clean Slate for Offline Reinforcement Learning",
      "contribution": "They introduce a rigorous, budget-aware evaluation and a set of minimal single-file implementations, unify prior algorithmic choices into a single hyperparameterized family (Unifloral), and\u2014using that clean infrastructure\u2014develop two new algorithms (TD3-AWR and MoBRAC) that outperform prior baselines under transparent, quantified offline evaluation budgets.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": true,
      "matching_idea_idx": 6,
      "input_tokens": 11693,
      "output_tokens": 1025,
      "predecessor_details": [
        {
          "success": true,
          "title": "[2110.04156] Showing Your Offline Reinforcement Learning Work",
          "url": "https://arxiv.org/abs/2110.04156",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2110.04156** (cs)\n\n\\[Submitted on 8 Oct 2021 ( [v1](https://arxiv.org/abs/2110.04156v1)), last revised 5 Jun 2022 (this version, v3)\\]\n\n# Title:Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters\n\nAuthors: [Vladislav Kurenkov](https://arxiv.org/search/cs?searchtype=author&query=Kurenkov,+V), [Sergey Kolesnikov](https://arxiv.org/search/cs?searchtype=author&query=Kolesnikov,+S)\n\nView a PDF of the paper titled Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters, by Vladislav Kurenkov and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2110.04156)\n\n> Abstract:In this work, we argue for the importance of an online evaluation budget for a reliable comparison of deep offline RL algorithms. First, we delineate that the online evaluation budget is problem-dependent, where some problems allow for less but others for more. And second, we demonstrate that the preference between algorithms is budget-dependent across a diverse range of decision-making domains such as Robotics, Finance, and Energy Management. Following the points above, we suggest reporting the performance of deep offline RL algorithms under varying online evaluation budgets. To facilitate this, we propose to use a reporting tool from the NLP field, Expected Validation Performance. This technique makes it possible to reliably estimate expected maximum performance under different budgets while not requiring any additional computation beyond hyperparameter search. By employing this tool, we also show that Behavioral Cloning is often more favorable to offline RL algorithms when working within a limited budget.\n\n| | |\n| --- | --- |\n| Comments: | ICML 2022, Spotlight; [this https URL](https://tinkoff-ai.github.io/eop/) |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2110.04156](https://arxiv.org/abs/2110.04156) \\[cs.LG\\] |\n| (or [arXiv:2110.04156v3](https://arxiv.org/abs/2110.04156v3) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2110.04156](https://doi.org/10.48550/arXiv.2110.04156) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Vladislav Kurenkov \\[ [view email](https://arxiv.org/show-email/5157f0ee/2110.04156)\\] **[\\[v1\\]](https://arxiv.org/abs/2110.04156v1)**\nFri, 8 Oct 2021 14:25:21 UTC (1,087 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2110.04156v2)**\nFri, 4 Feb 2022 15:52:03 UTC (1,194 KB)\n**\\[v3\\]**\nSun, 5 Jun 2022 17:03:50 UTC (1,194 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters, by Vladislav Kurenkov and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2110.04156)\n- [TeX Source](https://arxiv.org/src/2110.04156)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2110.04156&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2110.04156&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-10](https://arxiv.org/list/cs.LG/2021-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2110.04156?context=cs) [cs.AI](https://arxiv.org/abs/2110.04156?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2110.04156)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2110.04156)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2110.04156)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2110.html#abs-2110-04156) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2110-04156)\n\n[Sergey Kolesnikov](https://dblp.uni-trier.de/search/author?author=Sergey%20Kolesnikov)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2110.04156) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters",
          "cleaned_query": "Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters"
        },
        {
          "success": true,
          "title": "Hyperparameter Tuning in Offline Reinforcement Learning",
          "url": "https://ieeexplore.ieee.org/document/10069868/",
          "content": "### IEEE Account\n\n- [Change Username/Password](https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Update Address](https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n\n### Purchase Details\n\n- [Payment Options](https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Order History](https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [View Purchased Documents](https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp)\n\n### Profile Information\n\n- [Communications Preferences](https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Profession and Education](https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Technical Interests](https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n\n### Need Help?\n\n- **US & Canada:** +1 800 678 4333\n- **Worldwide:** +1 732 981 0060\n- [Contact & Support](https://ieeexplore.ieee.org/xpl/contact)\n\n- [About IEEE _Xplore_](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-ieee-xplore)\n- [Contact Us](https://ieeexplore.ieee.org/xpl/contact)\n- [Help](https://ieeexplore.ieee.org/Xplorehelp)\n- [Accessibility](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/accessibility-statement)\n- [Terms of Use](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/terms-of-use)\n- [Nondiscrimination Policy](http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html)\n- [Sitemap](https://ieeexplore.ieee.org/xpl/sitemap.jsp)\n- [Privacy & Opting Out of Cookies](http://www.ieee.org/about/help/security_privacy.html)\n\nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.\u00a9 Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.",
          "original_query": "Hyperparameter Selection for Offline Reinforcement Learning",
          "cleaned_query": "Hyperparameter Selection for Offline Reinforcement Learning"
        },
        {
          "success": true,
          "title": "A Minimalist Approach to Offline Reinforcement Learning - arXiv",
          "url": "https://arxiv.org/abs/2106.06860",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2106.06860** (cs)\n\n\\[Submitted on 12 Jun 2021 ( [v1](https://arxiv.org/abs/2106.06860v1)), last revised 3 Dec 2021 (this version, v2)\\]\n\n# Title:A Minimalist Approach to Offline Reinforcement Learning\n\nAuthors: [Scott Fujimoto](https://arxiv.org/search/cs?searchtype=author&query=Fujimoto,+S), [Shixiang Shane Gu](https://arxiv.org/search/cs?searchtype=author&query=Gu,+S+S)\n\nView a PDF of the paper titled A Minimalist Approach to Offline Reinforcement Learning, by Scott Fujimoto and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2106.06860)\n\n> Abstract:Offline reinforcement learning (RL) defines the task of learning from a fixed batch of data. Due to errors in value estimation from out-of-distribution actions, most offline RL algorithms take the approach of constraining or regularizing the policy with the actions contained in the dataset. Built on pre-existing RL algorithms, modifications to make an RL algorithm work offline comes at the cost of additional complexity. Offline RL algorithms introduce new hyperparameters and often leverage secondary components such as generative models, while adjusting the underlying RL algorithm. In this paper we aim to make a deep RL algorithm work while making minimal changes. We find that we can match the performance of state-of-the-art offline RL algorithms by simply adding a behavior cloning term to the policy update of an online RL algorithm and normalizing the data. The resulting algorithm is a simple to implement and tune baseline, while more than halving the overall run time by removing the additional computational overhead of previous methods.\n\n| | |\n| --- | --- |\n| Comments: | NeurIPS 2021 Spotlight |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2106.06860](https://arxiv.org/abs/2106.06860) \\[cs.LG\\] |\n| | (or [arXiv:2106.06860v2](https://arxiv.org/abs/2106.06860v2) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2106.06860](https://doi.org/10.48550/arXiv.2106.06860) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Scott Fujimoto \\[ [view email](https://arxiv.org/show-email/e905ddc9/2106.06860)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2106.06860v1)**\nSat, 12 Jun 2021 20:38:59 UTC (348 KB)\n\n**\\[v2\\]**\nFri, 3 Dec 2021 18:58:09 UTC (921 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled A Minimalist Approach to Offline Reinforcement Learning, by Scott Fujimoto and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2106.06860)\n- [TeX Source](https://arxiv.org/src/2106.06860)\n- [Other Formats](https://arxiv.org/format/2106.06860)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2106.06860&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2106.06860&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-06](https://arxiv.org/list/cs.LG/2021-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2106.06860?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2106.06860?context=cs.AI)\n\n[stat](https://arxiv.org/abs/2106.06860?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2106.06860?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2106.06860)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2106.06860)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2106.06860)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2106.html#abs-2106-06860) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2106-06860)\n\n[Scott Fujimoto](https://dblp.uni-trier.de/search/author?author=Scott%20Fujimoto)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2106.06860&description=A Minimalist Approach to Offline Reinforcement Learning) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2106.06860&title=A Minimalist Approach to Offline Reinforcement Learning)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2106.06860) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "A Minimalist Approach to Offline Reinforcement Learning",
          "cleaned_query": "A Minimalist Approach to Offline Reinforcement Learning"
        },
        {
          "success": true,
          "title": "CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy Optimization",
          "url": "https://arxiv.org/abs/2506.15654",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2506.15654** (cs)\n\n\\[Submitted on 18 Jun 2025\\]\n\n# Title:CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy Optimization\n\nAuthors: [Ranting Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu,+R)\n\nView a PDF of the paper titled CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy Optimization, by Ranting Hu\n\n[View PDF](https://arxiv.org/pdf/2506.15654) [HTML (experimental)](https://arxiv.org/html/2506.15654v1)\n\n> Abstract:Offline reinforcement learning (offline RL) algorithms often require additional constraints or penalty terms to address distribution shift issues, such as adding implicit or explicit policy constraints during policy optimization to reduce the estimation bias of functions. This paper focuses on a limitation of the Advantage-Weighted Regression family (AWRs), i.e., the potential for learning over-conservative policies due to data corruption, specifically the poor explorations in suboptimal offline data. We study it from two perspectives: (1) how poor explorations impact the theoretically optimal policy based on KL divergence, and (2) how such poor explorations affect the approximation of the theoretically optimal policy. We prove that such over-conservatism is mainly caused by the sensitivity of the loss function for policy optimization to poor explorations, and the proportion of poor explorations in offline datasets. To address this concern, we propose Corruption-Averse Advantage-Weighted Regression (CAWR), which incorporates a set of robust loss functions during policy optimization and an advantage-based prioritized experience replay method to filter out poor explorations. Numerical experiments on the D4RL benchmark show that our method can learn superior policies from suboptimal offline data, significantly enhancing the performance of policy optimization.\n\n| | |\n| --- | --- |\n| Comments: | 23 pages, 14 figures |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2506.15654](https://arxiv.org/abs/2506.15654) \\[cs.LG\\] |\n| (or [arXiv:2506.15654v1](https://arxiv.org/abs/2506.15654v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2506.15654](https://doi.org/10.48550/arXiv.2506.15654) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Ranting Hu \\[ [view email](https://arxiv.org/show-email/32f8c31f/2506.15654)\\] **\\[v1\\]**\nWed, 18 Jun 2025 17:31:26 UTC (663 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy Optimization, by Ranting Hu\n\n- [View PDF](https://arxiv.org/pdf/2506.15654)\n- [HTML (experimental)](https://arxiv.org/html/2506.15654v1)\n- [TeX Source](https://arxiv.org/src/2506.15654)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2506.15654&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2506.15654&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-06](https://arxiv.org/list/cs.LG/2025-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2506.15654?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2506.15654)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2506.15654)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2506.15654)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2506.15654) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Advantage-weighted Regression: Simple and Scalable Off-policy Reinforcement Learning",
          "cleaned_query": "Advantage-weighted Regression: Simple and Scalable Off-policy Reinforcement Learning"
        },
        {
          "success": true,
          "title": "[2005.13239] MOPO: Model-based Offline Policy Optimization - arXiv",
          "url": "https://arxiv.org/abs/2005.13239",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2005.13239** (cs)\n\n\\[Submitted on 27 May 2020 ( [v1](https://arxiv.org/abs/2005.13239v1)), last revised 22 Nov 2020 (this version, v6)\\]\n\n# Title:MOPO: Model-based Offline Policy Optimization\n\nAuthors: [Tianhe Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+T), [Garrett Thomas](https://arxiv.org/search/cs?searchtype=author&query=Thomas,+G), [Lantao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+L), [Stefano Ermon](https://arxiv.org/search/cs?searchtype=author&query=Ermon,+S), [James Zou](https://arxiv.org/search/cs?searchtype=author&query=Zou,+J), [Sergey Levine](https://arxiv.org/search/cs?searchtype=author&query=Levine,+S), [Chelsea Finn](https://arxiv.org/search/cs?searchtype=author&query=Finn,+C), [Tengyu Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+T)\n\nView a PDF of the paper titled MOPO: Model-based Offline Policy Optimization, by Tianhe Yu and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2005.13239)\n\n> Abstract:Offline reinforcement learning (RL) refers to the problem of learning policies entirely from a large batch of previously collected data. This problem setting offers the promise of utilizing such datasets to acquire policies without any costly or dangerous active exploration. However, it is also challenging, due to the distributional shift between the offline training data and those states visited by the learned policy. Despite significant recent progress, the most successful prior methods are model-free and constrain the policy to the support of data, precluding generalization to unseen states. In this paper, we first observe that an existing model-based RL algorithm already produces significant gains in the offline setting compared to model-free approaches. However, standard model-based RL methods, designed for the online setting, do not provide an explicit mechanism to avoid the offline setting's distributional shift issue. Instead, we propose to modify the existing model-based RL methods by applying them with rewards artificially penalized by the uncertainty of the dynamics. We theoretically show that the algorithm maximizes a lower bound of the policy's return under the true MDP. We also characterize the trade-off between the gain and risk of leaving the support of the batch data. Our algorithm, Model-based Offline Policy Optimization (MOPO), outperforms standard model-based RL algorithms and prior state-of-the-art model-free offline RL algorithms on existing offline RL benchmarks and two challenging continuous control tasks that require generalizing from data collected for a different task. The code is available at [this https URL](https://github.com/tianheyu927/mopo).\n\n| | |\n| --- | --- |\n| Comments: | NeurIPS 2020. First two authors contributed equally. Last two authors advised equally |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2005.13239](https://arxiv.org/abs/2005.13239) \\[cs.LG\\] |\n| | (or [arXiv:2005.13239v6](https://arxiv.org/abs/2005.13239v6) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2005.13239](https://doi.org/10.48550/arXiv.2005.13239) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Tianhe Yu \\[ [view email](https://arxiv.org/show-email/aeaf7671/2005.13239)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2005.13239v1)**\nWed, 27 May 2020 08:46:41 UTC (1,763 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2005.13239v2)**\nWed, 1 Jul 2020 06:01:54 UTC (377 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/2005.13239v3)**\nSun, 6 Sep 2020 23:25:56 UTC (746 KB)\n\n**[\\[v4\\]](https://arxiv.org/abs/2005.13239v4)**\nWed, 23 Sep 2020 07:08:58 UTC (377 KB)\n\n**[\\[v5\\]](https://arxiv.org/abs/2005.13239v5)**\nTue, 29 Sep 2020 23:49:26 UTC (377 KB)\n\n**\\[v6\\]**\nSun, 22 Nov 2020 07:04:17 UTC (379 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled MOPO: Model-based Offline Policy Optimization, by Tianhe Yu and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2005.13239)\n- [TeX Source](https://arxiv.org/src/2005.13239)\n- [Other Formats](https://arxiv.org/format/2005.13239)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2005.13239&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2005.13239&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2020-05](https://arxiv.org/list/cs.LG/2020-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2005.13239?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2005.13239?context=cs.AI)\n\n[stat](https://arxiv.org/abs/2005.13239?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2005.13239?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2005.13239)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2005.13239)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2005.13239)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2005.html#abs-2005-13239) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2005-13239)\n\n[Tianhe Yu](https://dblp.uni-trier.de/search/author?author=Tianhe%20Yu)\n\n[Garrett Thomas](https://dblp.uni-trier.de/search/author?author=Garrett%20Thomas)\n\n[Lantao Yu](https://dblp.uni-trier.de/search/author?author=Lantao%20Yu)\n\n[Stefano Ermon](https://dblp.uni-trier.de/search/author?author=Stefano%20Ermon)\n\n[James Zou](https://dblp.uni-trier.de/search/author?author=James%20Zou)\n\n\u2026\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2005.13239&description=MOPO: Model-based Offline Policy Optimization) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2005.13239&title=MOPO: Model-based Offline Policy Optimization)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/service",
          "original_query": "MOPO: Model-based Offline Policy Optimization",
          "cleaned_query": "MOPO: Model-based Offline Policy Optimization"
        },
        {
          "success": true,
          "title": "Farama-Foundation/D4RL: A collection of reference ... - GitHub",
          "url": "https://github.com/Farama-Foundation/D4RL",
          "content": "GitHub - Farama-Foundation/D4RL: A collection of reference environments for offline reinforcement learning\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/Farama-Foundation/D4RL)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n \nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n \nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/Farama-Foundation/D4RL)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=Farama-Foundation/D4RL)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[Farama-Foundation](https://github.com/Farama-Foundation)/**[D4RL](https://github.com/Farama-Foundation/D4RL)**Public\n* ### Uh oh!\nThere was an error while loading.[Please reload this page]().\n* [Notifications](https://github.com/login?return_to=/Farama-Foundation/D4RL)You must be signed in to change notification settings\n* [Fork303](https://github.com/login?return_to=/Farama-Foundation/D4RL)\n* [Star1.6k](https://github.com/login?return_to=/Farama-Foundation/D4RL)\nA collection of reference environments for offline reinforcement learning\n### License\n[Apache-2.0 license](https://github.com/Farama-Foundation/D4RL/blob/master/LICENSE)\n[1.6kstars](https://github.com/Farama-Foundation/D4RL/stargazers)[303forks](https://github.com/Farama-Foundation/D4RL/forks)[Branches](https://github.com/Farama-Foundation/D4RL/branches)[Tags](https://github.com/Farama-Foundation/D4RL/tags)[Activity](https://github.com/Farama-Foundation/D4RL/activity)\n[Star](https://github.com/login?return_to=/Farama-Foundation/D4RL)\n[Notifications](https://github.com/login?return_to=/Farama-Foundation/D4RL)You must be signed in to change notification settings\n# Farama-Foundation/D4RL\nmaster\n[Branches](https://github.com/Farama-Foundation/D4RL/branches)[Tags](https://github.com/Farama-Foundation/D4RL/tags)\n[](https://github.com/Farama-Foundation/D4RL/branches)[](https://github.com/Farama-Foundation/D4RL/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[192 Commits](https://github.com/Farama-Foundation/D4RL/commits/master/)\n[](https://github.com/Farama-Foundation/D4RL/commits/master/)\n|\n[.github](https://github.com/Farama-Foundation/D4RL/tree/master/.github)\n|\n[.github](https://github.com/Farama-Foundation/D4RL/tree/master/.github)\n|\n|\n|\n[d4rl](https://github.com/Farama-Foundation/D4RL/tree/master/d4rl)\n|\n[d4rl](https://github.com/Farama-Foundation/D4RL/tree/master/d4rl)\n|\n|\n|\n[scripts](https://github.com/Farama-Foundation/D4RL/tree/master/scripts)\n|\n[scripts](https://github.com/Farama-Foundation/D4RL/tree/master/scripts)\n|\n|\n|\n[.gitignore](https://github.com/Farama-Foundation/D4RL/blob/master/.gitignore)\n|\n[.gitignore](https://github.com/Farama-Foundation/D4RL/blob/master/.gitignore)\n|\n|\n|\n[CODE\\_OF\\_CONDUCT.rst](https://github.com/Farama-Foundation/D4RL/blob/master/CODE_OF_CONDUCT.rst)\n|\n[CODE\\_OF\\_CONDUCT.rst](https://github.com/Farama-Foundation/D4RL/blob/master/CODE_OF_CONDUCT.rst)\n|\n|\n|\n[LICENSE](https://github.com/Farama-Foundation/D4RL/blob/master/LICENSE)\n|\n[LICENSE](https://github.com/Farama-Foundation/D4RL/blob/master/LICENSE)\n|\n|\n|\n[MANIFEST.in](https://github.com/Farama-Foundation/D4RL/blob/master/MANIFEST.in)\n|\n[MANIFEST.in](https://github.com/Farama-Foundation/D4RL/blob/master/MANIFEST.in)\n|\n|\n|\n[README.md](https://github.com/Farama-Foundation/D4RL/blob/master/README.md)\n|\n[README.md](https://github.com/Farama-Foundation/D4RL/blob/master/README.md)\n|\n|\n|\n[d4rl-text.png](https://github.com/Farama-Foundation/D4RL/blob/master/d4rl-text.png)\n|\n[d4rl-text.png](https://github.com/Farama-Foundation/D4RL/blob/master/d4rl-text.png)\n|\n|\n|\n[setup.py](https://github.com/Farama-Foundation/D4RL/blob/master/setup.py)\n|\n[setup.py](https://github.com/Farama-Foundation/D4RL/blob/master/setup.py)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n## Important Notice\n[](#important-notice)\n### All of online environments libraries in D4RL have been moved[Gymnasium](https://github.com/Farama-Foundation/Gymnasium),[MiniGrid](https://github.com/Farama-Foundation/MiniGrid)and[Gymnasium-Robotics](https://github.com/Farama-Foundation/Gymnasium-Robotics), and all offline datasets in DR4L have been moved to[Minari](https://github.com/Farama-Foundation/Minari). These new versions include large bug fixes, new versions of Python, and are where all new development will continue. Please upgrade these libraries as soon as you're able to do so. If you'd like to read more about the story behind this switch, please check out[this blog post](https://farama.org/Announcing-Minari).\n[](#all-of-online-environments-libraries-in-d4rl-have-been-moved-gymnasium-minigrid-and-gymnasium-robotics-and-all-offline-datasets-in-dr4l-have-been-moved-to-minari-these-new-versions-include-large-bug-fixes-new-versions-of-python-and-are-where-all-new-development-will-continue-please-upgrade-these-libraries-as-soon-as-youre-able-to-do-so-if-youd-like-to-read-more-about-the-story-behind-this-switch-please-check-out-this-blog-post)\n[![](https://raw.githubusercontent.com/jjshoots/D4RL/master/d4rl-text.png)](https://raw.githubusercontent.com/jjshoots/D4RL/master/d4rl-text.png)\nD4RL is an open-source benchmark for offline reinforcement learning. It provides standardized environments and datasets for training and benchmarking algorithms. A supplementary[whitepaper](https://arxiv.org/abs/2004.07219)and[website](https://sites.google.com/view/d4rl/home)are also available.\nThe current maintenance plan for this library is:\n1. Pull the majority of the environments out of D4RL, fix the long standing bugs, and have them depend on the new MuJoCo bindings. The majority of the environments housed in D4RL were already maintained projects in Farama, and all the ones that aren't will be going into[Gymnasium-Robotics](https://github.com/Farama-Foundation/Gymnasium-Robotics), a standard library for housing many different Robotics environments. There are some envrionments that we don't plan to maintain, noteably the PyBullet ones (MuJoCo is not maintained and open source and PyBullet is now longer maintained) and Flow (it was never really used and the original author's don't view it as especially valuable).\n2. Recreate all the datasets in D4RL given the revised versions of environments, and host them in a standard offline RL dataset repostiry we're working on called[Minari](https://github.com/Farama-Foundation/Minari).\n## Setup\n[](#setup)\nD4RL can be installed by cloning the repository as follows:\n```\n`git clone https://github.com/Farama-Foundation/d4rl.git\ncd d4rl\npip install -e .`\n```\nOr, alternatively:\n```\n`pip install git+https://github.com/Farama-Foundation/d4rl@master#egg=d4rl`\n```\nThe control environments require MuJoCo as a dependency. You may need to obtain a[license](https://www.roboti.us/license.html)and follow the setup instructions for mujoco\\_py. This mostly involves copying the key to your MuJoCo installation folder.\nThe Flow and CARLA tasks als",
          "original_query": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning",
          "cleaned_query": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
        }
      ],
      "generated_ideas": [
        "Online-Evaluation-Efficient Hyperparameter Optimization for Offline RL\nDesign a hyperparameter tuner that explicitly optimizes expected best performance under a given online evaluation budget (small-N) using EVP-style estimators and multi-fidelity search (early stopping + partial dataset training). Compare against standard random/Bayesian search by measuring regret vs. budget and show when simple baselines (e.g., BC, minimalist BC+RL) dominate under realistic tuning constraints.",
        "Adaptive BC-Regularization Scheduling Based on Estimated Distribution Shift\nExtend the \u201cminimalist\u201d BC-regularized offline RL approach by making the BC weight a learned function of state-action density (or model uncertainty), rather than a fixed hyperparameter. Implement a schedule where BC dominates in low-support regions and relaxes in high-support regions, and evaluate gains in both final return and budgeted EVP on suboptimal datasets.",
        "CAWR for Model-Based Rollout Data: Robust AWR on Synthetic Transitions\nCombine MOPO-style model rollouts with CAWR\u2019s corruption-averse robust losses and advantage-prioritized replay to handle \u201cmodel corruption\u201d (hallucinated transitions) as a special case of data corruption. Concretely, train a dynamics ensemble, generate rollouts with uncertainty penalties, then apply CAWR with robustness tuned to rollout uncertainty; test whether this improves over MOPO or CAWR alone on tasks requiring generalization beyond dataset support.",
        "Joint Filtering of \u201cPoor Exploration\u201d via Advantage and Behavior-Policy Disagreement\nImprove CAWR\u2019s filtering by adding an explicit behavior-policy disagreement score (e.g., KL between learned policy and behavior model) to identify transitions likely to be misleading under distribution shift. Create an actionable algorithm: prioritized replay weight = f(advantage, disagreement, uncertainty), and ablate each component to quantify which best removes suboptimal/off-support samples without discarding useful rare trajectories.",
        "Budget-Constrained Algorithm Selection: When to Use BC vs Offline RL vs MOPO\nDevelop a meta-decision rule that, given a dataset and an online evaluation budget, selects among BC, minimalist BC+RL, CAWR, and MOPO (or mixtures) using only offline signals plus EVP estimates. The contribution is a practical selector trained on historical tasks that predicts which method will yield the highest expected best return under the specified budget, then validated on held-out environments.",
        "Uncertainty-Calibrated Reward Penalization Beyond MOPO (Task-Adaptive Penalty Learning)\nReplace MOPO\u2019s hand-tuned uncertainty penalty coefficient with a learned penalty function calibrated to predict policy regret from leaving the data support. Implement a small auxiliary model that maps (state, action, model ensemble stats) \u2192 penalty, trained to satisfy conservative lower-bound constraints, and evaluate whether it improves EVP under limited tuning budgets.",
        "Offline RL Reporting Standard for Hyperparameter Sensitivity and Budget Dependence\nPropose a standardized \u201coffline RL model card\u201d that reports (i) EVP curves over budgets, (ii) hyperparameter sensitivity surfaces (performance variance vs. key knobs like BC weight, robust-loss parameters, uncertainty penalty), and (iii) compute/time-normalized comparisons. Validate the standard by re-running representative methods and demonstrating that some apparent SOTA gains disappear under matched tuning/evaluation budgets.",
        "Advantage Estimation Robustness in Suboptimal Data: Robust Critics for AWR/CAWR\nInvestigate whether over-conservatism and instability in AWR/CAWR primarily originate from biased/heteroskedastic advantage estimates in suboptimal datasets, and design a robust critic training procedure (e.g., distributional critic with robust regression loss + uncertainty-aware advantage clipping). Implement and test whether improved advantage reliability reduces the need for heavy filtering and yields better performance under tight online evaluation budgets."
      ],
      "generated_ideas_raw": "1. **Budget-Aware Offline RL Benchmarking Suite (EVP Curves as First-Class Metrics)**\n   Build an evaluation protocol and open-source tool that reports performance as a function of online evaluation budget using Expected Validation Performance (EVP), rather than single-score leaderboards. Validate across D4RL/Minari tasks and show how algorithm rankings change under fixed \u201cbudget bands\u201d (e.g., 0, 5, 20, 100 evaluations), including BC, minimalist BC-regularized RL, CAWR, and MOPO.\n\n2. **Online-Evaluation-Efficient Hyperparameter Optimization for Offline RL**\n   Design a hyperparameter tuner that explicitly optimizes expected best performance under a given online evaluation budget (small-N) using EVP-style estimators and multi-fidelity search (early stopping + partial dataset training). Compare against standard random/Bayesian search by measuring regret vs. budget and show when simple baselines (e.g., BC, minimalist BC+RL) dominate under realistic tuning constraints.\n\n3. **Adaptive BC-Regularization Scheduling Based on Estimated Distribution Shift**\n   Extend the \u201cminimalist\u201d BC-regularized offline RL approach by making the BC weight a learned function of state-action density (or model uncertainty), rather than a fixed hyperparameter. Implement a schedule where BC dominates in low-support regions and relaxes in high-support regions, and evaluate gains in both final return and budgeted EVP on suboptimal datasets.\n\n4. **CAWR for Model-Based Rollout Data: Robust AWR on Synthetic Transitions**\n   Combine MOPO-style model rollouts with CAWR\u2019s corruption-averse robust losses and advantage-prioritized replay to handle \u201cmodel corruption\u201d (hallucinated transitions) as a special case of data corruption. Concretely, train a dynamics ensemble, generate rollouts with uncertainty penalties, then apply CAWR with robustness tuned to rollout uncertainty; test whether this improves over MOPO or CAWR alone on tasks requiring generalization beyond dataset support.\n\n5. **Joint Filtering of \u201cPoor Exploration\u201d via Advantage and Behavior-Policy Disagreement**\n   Improve CAWR\u2019s filtering by adding an explicit behavior-policy disagreement score (e.g., KL between learned policy and behavior model) to identify transitions likely to be misleading under distribution shift. Create an actionable algorithm: prioritized replay weight = f(advantage, disagreement, uncertainty), and ablate each component to quantify which best removes suboptimal/off-support samples without discarding useful rare trajectories.\n\n6. **Budget-Constrained Algorithm Selection: When to Use BC vs Offline RL vs MOPO**\n   Develop a meta-decision rule that, given a dataset and an online evaluation budget, selects among BC, minimalist BC+RL, CAWR, and MOPO (or mixtures) using only offline signals plus EVP estimates. The contribution is a practical selector trained on historical tasks that predicts which method will yield the highest expected best return under the specified budget, then validated on held-out environments.\n\n7. **Dataset \u201cCorruption Stress Tests\u201d for Offline RL (Controlled Poor-Exploration Injection)**\n   Create a systematic corruption framework for D4RL/Minari datasets by injecting controlled proportions of poor exploration (e.g., locally random actions, stuck behaviors, subgoal loops) and label corruption levels. Use it to map failure modes of AWR-family methods, BC-regularized methods, and MOPO, and to quantify how CAWR\u2019s robust losses and filtering scale with corruption severity.\n\n8. **Uncertainty-Calibrated Reward Penalization Beyond MOPO (Task-Adaptive Penalty Learning)**\n   Replace MOPO\u2019s hand-tuned uncertainty penalty coefficient with a learned penalty function calibrated to predict policy regret from leaving the data support. Implement a small auxiliary model that maps (state, action, model ensemble stats) \u2192 penalty, trained to satisfy conservative lower-bound constraints, and evaluate whether it improves EVP under limited tuning budgets.\n\n9. **Offline RL Reporting Standard for Hyperparameter Sensitivity and Budget Dependence**\n   Propose a standardized \u201coffline RL model card\u201d that reports (i) EVP curves over budgets, (ii) hyperparameter sensitivity surfaces (performance variance vs. key knobs like BC weight, robust-loss parameters, uncertainty penalty), and (iii) compute/time-normalized comparisons. Validate the standard by re-running representative methods and demonstrating that some apparent SOTA gains disappear under matched tuning/evaluation budgets.\n\n10. **Advantage Estimation Robustness in Suboptimal Data: Robust Critics for AWR/CAWR**\n   Investigate whether over-conservatism and instability in AWR/CAWR primarily originate from biased/heteroskedastic advantage estimates in suboptimal datasets, and design a robust critic training procedure (e.g., distributional critic with robust regression loss + uncertainty-aware advantage clipping). Implement and test whether improved advantage reliability reduces the need for heavy filtering and yields better performance under tight online evaluation budgets.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Online-Evaluation-Efficient Hyperparameter Optimization for Offline RL\nDesign a hyperparameter tuner that explicitly optimizes expected best performance under a given online evaluation budget (small-N",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Adaptive BC-Regularization Scheduling Based on Estimated Distribution Shift\nExtend the \u201cminimalist\u201d BC-regularized offline RL approach by making the BC weight a learned function of state-action densit",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "CAWR for Model-Based Rollout Data: Robust AWR on Synthetic Transitions\nCombine MOPO-style model rollouts with CAWR\u2019s corruption-averse robust losses and advantage-prioritized replay to handle \u201cmodel c",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Joint Filtering of \u201cPoor Exploration\u201d via Advantage and Behavior-Policy Disagreement\nImprove CAWR\u2019s filtering by adding an explicit behavior-policy disagreement score (e.g., KL between learned policy ",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Budget-Constrained Algorithm Selection: When to Use BC vs Offline RL vs MOPO\nDevelop a meta-decision rule that, given a dataset and an online evaluation budget, selects among BC, minimalist BC+RL, CAW",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Uncertainty-Calibrated Reward Penalization Beyond MOPO (Task-Adaptive Penalty Learning)\nReplace MOPO\u2019s hand-tuned uncertainty penalty coefficient with a learned penalty function calibrated to predict ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Offline RL Reporting Standard for Hyperparameter Sensitivity and Budget Dependence\nPropose a standardized \u201coffline RL model card\u201d that reports (i) EVP curves over budgets, (ii) hyperparameter sensitiv",
          "is_match": true
        },
        {
          "idea_idx": 7,
          "idea_text": "Advantage Estimation Robustness in Suboptimal Data: Robust Critics for AWR/CAWR\nInvestigate whether over-conservatism and instability in AWR/CAWR primarily originate from biased/heteroskedastic advant",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 27,
      "paper_title": "Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy",
      "contribution": "They develop novel high-probability spectral-norm perturbation bounds for the top-p low-rank approximation of a symmetric matrix under arbitrary symmetric noise, using a new 'contour bootstrapping' complex-analytic technique, and apply these bounds to give strictly sharper utility guarantees for differentially private PCA (improvements up to a factor \u221an).",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12192,
      "output_tokens": 1130,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] The approximation of one matrix by another of lower rank",
          "url": "https://www.semanticscholar.org/paper/The-approximation-of-one-matrix-by-another-of-lower-Eckart-Young/8112c4305b88d85199267e9e03d3a0aca4432059",
          "content": "[Skip to search form](https://www.semanticscholar.org/paper/The-approximation-of-one-matrix-by-another-of-lower-Eckart-Young/8112c4305b88d85199267e9e03d3a0aca4432059#search-form) [Skip to main content](https://www.semanticscholar.org/paper/The-approximation-of-one-matrix-by-another-of-lower-Eckart-Young/8112c4305b88d85199267e9e03d3a0aca4432059#main-content) [Skip to account menu](https://www.semanticscholar.org/paper/The-approximation-of-one-matrix-by-another-of-lower-Eckart-Young/8112c4305b88d85199267e9e03d3a0aca4432059#account-menu)\n\n- DOI: [10.1007/BF02288367](https://doi.org/10.1007/BF02288367)\n- Corpus ID: 10163399\n\n# The approximation of one matrix by another of lower rank\n\n```\n@article{Eckart1936TheAO,\n title={The approximation of one matrix by another of lower rank},\n author={Carl Eckart and G. Marion Young},\n journal={Psychometrika},\n year={1936},\n volume={1},\n pages={211-218},\n url={https://api.semanticscholar.org/CorpusID:10163399}\n}\n```\n\n- [C. Eckart](https://www.semanticscholar.org/author/C.-Eckart/37566996), [G. Young](https://www.semanticscholar.org/author/G.-Young/86970661)\n- Published 1 September 1936\n- Mathematics\n- Psychometrika\n\nThe mathematical problem of approximating one matrix by another of lower rank is closely related to the fundamental postulate of factor-theory. When formulated as a least-squares problem, the normal equations cannot be immediately written down, since the elements of the approximate matrix are not independent of one another. The solution of the problem is simplified by first expressing the matrices in a canonic form. It is found that the problem always has a solution which is usually unique\u2026\u00a0Expand\n\n[View on Springer](https://doi.org/10.1007/BF02288367)\n\n[ccrma.stanford.edu](https://ccrma.stanford.edu/~dattorro/eckart&young.1936.pdf)\n\nSave to LibrarySave\n\nCreate AlertAlert\n\nCite\n\nShare\n\n3,594 Citations\n\n[Highly Influential Citations](https://www.semanticscholar.org/paper/The-approximation-of-one-matrix-by-another-of-lower-Eckart-Young/8112c4305b88d85199267e9e03d3a0aca4432059#citing-papers)\n\n252\n\n[Background Citations](https://www.semanticscholar.org/paper/The-approximation-of-one-matrix-by-another-of-lower-Eckart-Young/8112c4305b88d85199267e9e03d3a0aca4432059#citing-papers)\n\n959\n\n[Methods Citations](https://www.semanticscholar.org/paper/The-approximation-of-one-matrix-by-another-of-lower-Eckart-Young/8112c4305b88d85199267e9e03d3a0aca4432059#citing-papers)\n\n1,075\n\n[Results Citations](https://www.semanticscholar.org/paper/The-approximation-of-one-matrix-by-another-of-lower-Eckart-Young/8112c4305b88d85199267e9e03d3a0aca4432059#citing-papers)\n\n23\n\n[View All](https://www.semanticscholar.org/paper/The-approximation-of-one-matrix-by-another-of-lower-Eckart-Young/8112c4305b88d85199267e9e03d3a0aca4432059#citing-papers)\n\n## 3,594 Citations\n\nCitation Type\n\nHas PDF\n\nAuthor\n\nMore Filters\n\nMore Filters\n\nFilters\n\nSort by RelevanceSort by Most Influenced PapersSort by Citation CountSort by Recency\n\n[**Grassmann algorithms for low rank approximation of\u00a0matrices with missing values**](https://www.semanticscholar.org/paper/Grassmann-algorithms-for-low-rank-approximation-Simonsson-Eld%C3%A9n/c6ed32b52b26645b1776e4f25d627ba3f9764523)\n\n[L. Simonsson](https://www.semanticscholar.org/author/L.-Simonsson/96930899)[L. Eld\u00e9n](https://www.semanticscholar.org/author/L.-Eld%C3%A9n/3281987)\n\nMathematics\n\n- 2010\n\nThe problem of approximating a matrix by another matrix of lower rank, when a modest portion of its elements are missing, is considered. The solution is obtained using Newton\u2019s algorithm to find a\u2026 Expand\n\n- [16](https://www.semanticscholar.org/paper/c6ed32b52b26645b1776e4f25d627ba3f9764523#citing-papers)\n- [PDF](https://www.semanticscholar.org/paper/c6ed32b52b26645b1776e4f25d627ba3f9764523)\n\n\n- 1 Excerpt\n\nSave\n\n[**Matrix Approximation by a Sum of Matrix Products**](https://www.semanticscholar.org/paper/Matrix-Approximation-by-a-Sum-of-Matrix-Products-Torokhti-Soto-Quiros/ed882da7b2558d93dffa51b468a197688bbee4c2)\n\n[A. Torokhti](https://www.semanticscholar.org/author/A.-Torokhti/3047057)[Pablo Soto-Quiros](https://www.semanticscholar.org/author/Pablo-Soto-Quiros/1404893929)[Vladimir Ejov](https://www.semanticscholar.org/author/Vladimir-Ejov/2260369002)\n\nMathematics, Computer Science\n\n[International Journal of Applied and\u2026](https://www.semanticscholar.org/venue?name=International%20Journal%20of%20Applied%20and%20Computational%20Mathematics)\n\n- 2023\n\nTLDR\n\nThe solution of the problem of approximation of a matrix by a sum of matrix products subject to multiple rank constraints is provided in the form of some special iterative procedure and it is shown that iterations converge to a coordinate-wise minimum of the objective function.Expand\n\n- [Highly Influenced](https://www.semanticscholar.org/paper/ed882da7b2558d93dffa51b468a197688bbee4c2?sort=is-influential#citing-papers)\n\n\n- 4 Excerpts\n\nSave\n\n[**On a Problem of Weighted Low-Rank Approximation of Matrices**](https://www.semanticscholar.org/paper/On-a-Problem-of-Weighted-Low-Rank-Approximation-of-Dutta-Li/6baba5a825238df6cacb5fee49528f5c3d9dfcde)\n\n[Aritra Dutta](https://www.semanticscholar.org/author/Aritra-Dutta/5741235)[Xin Li](https://www.semanticscholar.org/author/Xin-Li/2153897081)\n\nMathematics\n\n[SIAM J. Matrix Anal. Appl.](https://www.semanticscholar.org/venue?name=SIAM%20J.%20Matrix%20Anal.%20Appl.)\n\n- 2017\n\nTLDR\n\nAn algorithm based on the alternating direction method is proposed to solve the weighted low rank approximation problem and compare it with the state-of-art general algorithms such as the weighted total alternating least squares and the EM algorithm.Expand\n\n- [18](https://www.semanticscholar.org/paper/6baba5a825238df6cacb5fee49528f5c3d9dfcde#citing-papers)\\[PDF\\]\n\n- 1 Excerpt\n\nSave\n\n[**On a general class of matrix nearness problems**](https://www.semanticscholar.org/paper/On-a-general-class-of-matrix-nearness-problems-Watson/97765a541e19e4b08b0b05665f58231b5b3fbf71)\n\n[G. Watson](https://www.semanticscholar.org/author/G.-Watson/144365264)\n\nMathematics\n\n- 1991\n\nThe problem is considered of finding the nearest rank-deficient matrix to a given rectangular matrix. For a wide class of matrix norms, and arbitrary sparsity imposed on the matrix of perturbations,\u2026 Expand\n\n- [3](https://www.semanticscholar.org/paper/97765a541e19e4b08b0b05665f58231b5b3fbf71#citing-papers)\n\n- 1 Excerpt\n\nSave\n\n[**On the L1-Norm Approximation of a Matrix by Another of Lower Rank**](https://www.semanticscholar.org/paper/On-the-L1-Norm-Approximation-of-a-Matrix-by-Another-Tsagkarakis-Markopoulos/129493e3ec48db60db52df218c6f5d3c48ba747b)\n\n[N. Tsagkarakis](https://www.semanticscholar.org/author/N.-Tsagkarakis/2234132)[Panos P. Markopoulos](https://www.semanticscholar.org/author/Panos-P.-Markopoulos/2063043156)[D. Pados](https://www.semanticscholar.org/author/D.-Pados/2483917)\n\nComputer Science, Mathematics\n\n[2016 15th IEEE International Conference on\u2026](https://www.semanticscholar.org/venue?name=15th%20IEEE%20International%20Conference%20on%20Machine%20Learning%20and%20Applications%20%28ICMLA%29)\n\n- 2016\n\nTLDR\n\nThis paper first shows that the problem is NP-hard, then introduces a theorem on the sparsity of the residual matrix that sets the foundation for a novel algorithm that outperforms all existing counterparts in the L1-norm error minimization metric and exhibits high outlier resistance in comparison to usual L2-norm errors in machine learning applications.Expand\n\n- [15](https://www.semanticscholar.org/paper/129493e3ec48db60db52df218c6f5d3c48ba747b#citing-papers)\n- [Highly Influenced](https://www.semanticscholar.org/paper/129493e3ec48db60db52df218c6f5d3c48ba747b?sort=is-influential#citing-papers)\n\n\n- 5 Excerpts\n\nSave\n\n[**CRITICAL POINTS OF MATRIX LEAST SQUARE DISTANCE FUNCTIONS**](https://www.semanticscholar.org/paper/CRITICAL-POINTS-OF-MATRIX-LEAST-SQUARE-DISTANCE-Helmke-Shayman/dc767889a14281ef087db39e549a91e8cd3aa49b)\n\n[U. Helmke](https://www.semanticscholar.org/author/U.-Helmke/2719598)[M. Shay",
          "original_query": "The approximation of one matrix by another of lower rank [9] (Eckart\u2013Young\u2013Mirsky)",
          "cleaned_query": "The approximation of one matrix by another of lower rank (Eckart\u2013Young\u2013Mirsky)"
        },
        {
          "success": true,
          "title": "The rotation of eigenvectors by a perturbation - Semantic Scholar",
          "url": "https://www.semanticscholar.org/paper/The-rotation-of-eigenvectors-by-a-perturbation-Davis/fe88a8bf98e797d9368ee384f86543ec844826ab",
          "content": "[PDF] The rotation of eigenvectors by a perturbation | Semantic Scholar\n[Skip to search form](#search-form)[Skip to main content](#main-content)[Skip to account menu](#account-menu)\n[Semantic ScholarSemantic Scholar's Logo](https://www.semanticscholar.org/)\nSearch 231,079,891 papers from all fields of science\nSearch\n* DOI:[10.1016/0022-247X(63)90001-5](https://doi.org/10.1016/0022-247X(63)90001-5)\n* Corpus ID: 120835368# The rotation of eigenvectors by a perturbation\n```\n@article{Davis1963TheRO,\ntitle={The rotation of eigenvectors by a perturbation},\nauthor={Chandler Davis},\njournal={Journal of Mathematical Analysis and Applications},\nyear={1963},\nvolume={11},\npages={20-27},\nurl={https://api.semanticscholar.org/CorpusID:120835368}\n}\n```\n* [Chandler Davis](https://www.semanticscholar.org/author/Chandler-Davis/21456228)\n* Published1 April 1963\n* Mathematics\n* Journal of Mathematical Analysis and Applications\n[View via Publisher](https://doi.org/10.1016/0022-247X(63)90001-5)\n[doi.org](https://doi.org/10.1016/0022-247x(63)90001-5)\nSave to LibrarySave\nCreate AlertAlert\nCite\nShare\n802 Citations\n[\nHighly Influential Citations\n](#citing-papers)[](https://www.semanticscholar.org/faq#influential-citations)\n132\n[\nBackground Citations\n](#citing-papers)\n283\n[\nMethods Citations\n](#citing-papers)\n222\n[\nResults Citations\n](#citing-papers)\n27\n[View All](#citing-papers)\n## 802 Citations\nCitation Type\nHas PDF\nAuthor\nMore Filters\nMore Filters\nFilters\nSort by RelevanceSort by Most Influenced PapersSort by Citation CountSort by Recency\n[### On the variation of the spectrum of a Hermitian matrix\n](https://www.semanticscholar.org/paper/On-the-variation-of-the-spectrum-of-a-Hermitian-Li-Vong/6dde72a92de376497b921e91fddcbf4f1f5c9d51)[Wen Li](https://www.semanticscholar.org/author/Wen-Li/47113260)[Seakweng Vong](https://www.semanticscholar.org/author/Seakweng-Vong/34868807)\nMathematics\n[Appl. Math. Lett.](https://www.semanticscholar.org/venue?name=Appl.%20Math.%20Lett.)\n* 2017\n* [\n3\n](https://www.semanticscholar.org/paper/6dde72a92de376497b921e91fddcbf4f1f5c9d51#citing-papers)\n* 1 Excerpt\nSave\n[### Perturbation theory for the spectral decomposition of Hermitian matrices.\n](https://www.semanticscholar.org/paper/Perturbation-theory-for-the-spectral-decomposition-Carlsson/974827002a984fd88b7660bcc80d498e9835653a)[M. Carlsson](https://www.semanticscholar.org/author/M.-Carlsson/51008388)\nMathematics\n* 2018\nLet A and E be Hermitian self-adjoint matrices, where A is fixed and E a small perturbation. We study how the eigenvalues and eigenvectors of A+E depend on E, with the aim of obtaining first order\u2026Expand\n* [\n6\n](https://www.semanticscholar.org/paper/974827002a984fd88b7660bcc80d498e9835653a#citing-papers)[[PDF]](https://www.semanticscholar.org/reader/974827002a984fd88b7660bcc80d498e9835653a)\n* 1 Excerpt\nSave\n[### The tan \u0398theorem for definite matrix pairs\n](https://www.semanticscholar.org/paper/The-tan-%CE%98-theorem-for-definite-matrix-pairs-Ivi%C4%8Di%C4%87-Miodragovi%C4%87/67a5b69bfb522dbef0a17769f83dbf574b4a7098)[I. Ivi\u010di\u0107](https://www.semanticscholar.org/author/I.-Ivi%C4%8Di%C4%87/100695964)[Suzana Miodragovi\u0107](https://www.semanticscholar.org/author/Suzana-Miodragovi%C4%87/103229450)[Matea Ugrica](https://www.semanticscholar.org/author/Matea-Ugrica/103478926)\nMathematics\n[Linear and Multilinear Algebra](https://www.semanticscholar.org/venue?name=Linear%20and%20Multilinear%20Algebra)\n* 2022\nIn this paper, we consider the perturbation of a Hermitian matrix pair , where H and M are non-singular and positive definite Hermitian matrices, respectively. A novel upper bound on a tangent of the\u2026Expand\n* [Highly Influenced](https://www.semanticscholar.org/paper/67a5b69bfb522dbef0a17769f83dbf574b4a7098?sort=is-influential#citing-papers)\n* 6 Excerpts\nSave\n[### On a Perturbation Bound for Invariant Subspaces of Matrices\n](https://www.semanticscholar.org/paper/On-a-Perturbation-Bound-for-Invariant-Subspaces-of-Karow-Kressner/5dbdf2e45c809b97a67a8a0928f1352d311e9f3b)[M. Karow](https://www.semanticscholar.org/author/M.-Karow/1832869)[D. Kressner](https://www.semanticscholar.org/author/D.-Kressner/1708810)\nMathematics\n[SIAM J. Matrix Anal. Appl.](https://www.semanticscholar.org/venue?name=SIAM%20J.%20Matrix%20Anal.%20Appl.)\n* 2014\nTLDR\nThe result derived in this paper differs from Stewart's classical result and sometimes yields tighter bounds and norm estimates for the remainder terms in well-known perturbation expansions for invariant subspaces, eigenvectors, and eigenvalues are provided.Expand\n* [\n17\n](https://www.semanticscholar.org/paper/5dbdf2e45c809b97a67a8a0928f1352d311e9f3b#citing-papers)\n* [\nPDF\n](https://www.semanticscholar.org/paper/5dbdf2e45c809b97a67a8a0928f1352d311e9f3b)\n* 2 Excerpts\nSave\n[### Two Simple Residual Bounds for the Eigenvalues of Hermitian Matrices\n](https://www.semanticscholar.org/paper/Two-Simple-Residual-Bounds-for-the-Eigenvalues-of-Stewart/580f97e2c65df1f13cb1925ed4ddb1ec6914503f)[G. Stewart](https://www.semanticscholar.org/author/G.-Stewart/145257873)\nMathematics\n* 1989\nabstract Let A be Hermitian and let the orthonormal columns of X span an approximate invariant subspace of X. Then the residual R = AX XM (M = X H AX) will be small. The theorems of this paper bound\u2026Expand\n* [\n3\n](https://www.semanticscholar.org/paper/580f97e2c65df1f13cb1925ed4ddb1ec6914503f#citing-papers)\n* [\nPDF\n](https://www.semanticscholar.org/paper/580f97e2c65df1f13cb1925ed4ddb1ec6914503f)\n* 2 Excerpts\nSave\n[### Some new bounds on perturbation of subspaces\n](https://www.semanticscholar.org/paper/Some-new-bounds-on-perturbation-of-subspaces-Davis-Kahan/73a13861d82d2eeacbaf3bd01a476307f2e24a17)[Chandler Davis](https://www.semanticscholar.org/author/Chandler-Davis/21456228)[W. Kahan](https://www.semanticscholar.org/author/W.-Kahan/3238706)\nMathematics\n* 1969\nWhen a Hermitian linear operator A is slightly perturbed, by how much can its invariant subspaces change? Given some approximations to a cluster of neighboring eigenvalues and to the corresponding\u2026Expand\n* [\n91\n](https://www.semanticscholar.org/paper/73a13861d82d2eeacbaf3bd01a476307f2e24a17#citing-papers)\n* [\nPDF\n](https://www.semanticscholar.org/paper/73a13861d82d2eeacbaf3bd01a476307f2e24a17)\nSave\n[### SOME NEW BOUNDS ON PERTURBATION OF SUBSPACES\n](https://www.semanticscholar.org/paper/SOME-NEW-BOUNDS-ON-PERTURBATION-OF-SUBSPACES-Isaacson/29ddbad63f845cd7627a890d4493cc95c9b777af)[E. Isaacson](https://www.semanticscholar.org/author/E.-Isaacson/144800522)\nMathematics\n* 2007\nWhen a Hermitian linear operator A is slightly perturbed, by how much can its invariant subspaces change? Given some approximations to a cluster of neighboring eigenvalues and to the corresponding\u2026Expand\n* [\n4\n](https://www.semanticscholar.org/paper/29ddbad63f845cd7627a890d4493cc95c9b777af#citing-papers)\n* [\nPDF\n](https://www.semanticscholar.org/paper/29ddbad63f845cd7627a890d4493cc95c9b777af)\nSave\n[### Computing Invariant Subspaces of a General Matrix when the Eigensystem is Poorly Conditioned\n](https://www.semanticscholar.org/paper/Computing-Invariant-Subspaces-of-a-General-Matrix-Varah/e276acbda6d0df76cf52cb0383d18d8b87e1dab7)[J. Varah](https://www.semanticscholar.org/author/J.-Varah/3194259)\nMathematics\n* 1970\nTLDR\nThis paper defines a class of matrices where this is true, and proposes a technique for calculating bases for these invariant subspaces, and shows that for this class the technique provides basis vectors which are accurate and span the subspaced well.Expand\n* [\n25\n](https://www.semanticscholar.org/paper/e276acbda6d0df76cf52cb0383d18d8b87e1dab7#citing-papers)\n* [\nPDF\n](https://www.semanticscholar.org/paper/e276acbda6d0df76cf52cb0383d18d8b87e1dab7)\n* 2 Excerpts\nSave\n[### ANALYSIS OF SPECTRAL VARIATION AND SOME INEQUALITIES\n](https://www.semanticscholar.org/paper/ANALYSIS-OF-SPECTRAL-VARIATION-AND-SOME-Bhatia/987d1d652d45384ad9059802e2cb49835c5751a9)[R. Bhatia](https://www.semanticscholar.org/author/R.-Bhatia/145964449)\nMathematics\n* 1982\nA geometric method",
          "original_query": "The rotation of eigenvectors by a perturbation [6] (Davis\u2013Kahan)",
          "cleaned_query": "The rotation of eigenvectors by a perturbation (Davis\u2013Kahan)"
        },
        {
          "success": true,
          "title": "Perturbation Theory for Linear Operators - Springer Link",
          "url": "https://link.springer.com/book/10.1007/978-3-642-66282-9",
          "content": "Perturbation Theory for Linear Operators | Springer Nature Link (formerly SpringerLink)\n[Skip to main content](#main-content)\nAdvertisement\n[![Springer Nature Link](https://link.springer.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/book/10.1007/978-3-642-66282-9?)\n[![](https://media.springernature.com/w90/springer-static/cover-hires/book/978-3-642-66282-9?as=webp)](https://link.springer.com/book/10.1007/978-3-642-66282-9/cover)\n# Perturbation Theory for Linear Operators\n* Book\n* &copy;1995\n* 2nd edition\n* [View latest edition](https://link.springer.com/book/9783540586616)\n[Accessibility Information](#accessibility-information)\n## Overview\nAuthors:\n* [Tosio Kato](#author-0-0)[0](#Aff-0-0)\n1. Tosio Kato\n1. University of California, Berkeley, USA\n[View author publications](https://link.springer.com/search?dc.creator=Tosio+Kato&sortBy=newestFirst)\nSearch author on:[PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Tosio+Kato)[Google Scholar](http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=\"Tosio+Kato\"&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en)\nPart of the book series:[Classics in Mathematics](https://link.springer.com/series/3242)(CLASSICS, volume 132)\n* 105kAccesses\n* 3592Citations\n* 73[Altmetric](https://link.altmetric.com/details/4291888)\nThis is a preview of subscription content,[log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/book/10.1007/978-3-642-66282-9?error=cookies_not_supported&code=63c04109-fddf-48ac-a905-6d1f77d2d17c)to check access.\n## Access this book\n[Log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/book/10.1007/978-3-642-66282-9?error=cookies_not_supported&code=63c04109-fddf-48ac-a905-6d1f77d2d17c)\nSoftcover BookUSD59.99\nPrice excludes VAT (USA)\n* Compact, lightweight edition\n* Dispatched in 3 to 5 business days\n* Free shipping worldwide -[see info](https://support.springernature.com/en/support/solutions/articles/6000233448-coronavirus-disease-covid-19-delivery-information)Buy Softcover Book\nTax calculation will be finalised at checkout\n[Licence this eBook for your library](https://single-ebooks.springernature.com/search?query=10.1007/978-3-642-66282-9)\n[Learn about institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/ebooks)\n## Other ways to access\n[Licence this eBook for your library](https://single-ebooks.springernature.com/search?query=10.1007/978-3-642-66282-9)\n[Institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/ebooks)\n## About this book\nIn view of recent development in perturbation theory, supplementary notes and a supplementary bibliography are added at the end of the new edition. Little change has been made in the text except that the para\u00ad graphs V-\u00a7 4.5, VI-\u00a7 4.3, and VIII-\u00a7 1.4 have been completely rewritten, and a number of minor errors, mostly typographical, have been corrected. The author would like to thank many readers who brought the errors to his attention. Due to these changes, some theorems, lemmas, and formulas of the first edition are missing from the new edition while new ones are added. The new ones have numbers different from those attached to the old ones which they may have replaced. Despite considerable expansion, the bibliography i\" not intended to be complete. Berkeley, April 1976 TosIO RATO Preface to the First Edition This book is intended to give a systematic presentation of perturba\u00ad tion theory for linear operators. It is hoped that the book will be useful to students as well as to mature scientists, both in mathematics and in the physical sciences.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3Aplaceholder%2Fimages/placeholder-figure-springernature.png)\n### [On new exponential-type operators](https://link.springer.com/10.1007/s13398-022-01302-9?fromPaywallRec=true)\nArticle27 July 2022\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-030-10819-9?as&#x3D;webp)\n### [Spectrum and Pseudo-Spectrum](https://link.springer.com/10.1007/978-3-030-10819-9_2?fromPaywallRec=true)\nChapter\u00a9 2019\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-024-64985-7/MediaObjects/41598_2024_64985_Fig1_HTML.png)\n### [Investigating pseudo parabolic dynamics through phase portraits, sensitivity, chaos and soliton behavior](https://link.springer.com/10.1038/s41598-024-64985-7?fromPaywallRec=true)\nArticleOpen access02 July 2024\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects.\n* [Differential Equations](https://link.springer.com/subjects/differential-equations)\n* [Calculus of Variations and Optimization](https://link.springer.com/subjects/calculus-of-variations-and-optimization)\nSearch within this book\nSearch\n## Table of contents (10 chapters)\n1. ### Front Matter\nPages I-XXI\n[Download chapterPDF](https://link.springer.com/content/pdf/bfm:978-3-642-66282-9/1)\n2. ### [Operator theory in finite-dimensional vector spaces](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_1)\n* Tosio Kato\nPages 1-62\n* ### [Perturbation theory in a finite-dimensional space](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_2)\n* Tosio Kato\nPages 62-126\n* ### [Introduction to the theory of operators in Banach spaces](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_3)\n* Tosio Kato\nPages 126-188\n* ### [Stability theorems](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_4)\n* Tosio Kato\nPages 189-250\n* ### [Operators in Hilbert spaces](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_5)\n* Tosio Kato\nPages 251-308\n* ### [Sesquilinear forms in Hilbert spaces and associated operators](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_6)\n* Tosio Kato\nPages 308-364\n* ### [Analytic perturbation theory](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_7)\n* Tosio Kato\nPages 364-426\n* ### [Asymptotic perturbation theory](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_8)\n* Tosio Kato\nPages 426-479\n* ### [Perturbation theory for semigroups of operators](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_9)\n* Tosio Kato\nPages 479-515\n* ### [Perturbation of continuous spectra and unitary equivalence](https://link.springer.com/chapter/10.1007/978-3-642-66282-9_10)\n* Tosio Kato\nPages 516-567\n* ### Back Matter\nPages 568-623\n[Download chapterPDF](https://link.springer.com/content/pdf/bbm:978-3-642-66282-9/1)\n[Back to top](#back-to-top)\n## Reviews\n\"The monograph by T. Kato is an excellent textbook in the theory of linear operators in Banach and Hilbert spaces. It is a thoroughly worthwhile reference work both for graduate students in functional analysis as well as for researchers in perturbation, spectral, and scattering theory.\nIn chapters 1, 3, 5 operators in finite-dimensional vector spaces, Banach spaces and Hilbert spaces are introduced. Stability and perturbation theory are studied in finite-dimensional spaces (chapter 2) and in Banach spaces (chapter 4). Sesquilinear forms in Hilbert spaces are considered in detail (chapter 6), analytic and asymptotic perturbation theory is described (chapter 7 and 8). The fundamentals of semigroup theory are given in chapter 9. The supplementary notes appearing in the second edition of the book gave mainly additional information concerning scattering theory described in chapter 10.\nThe first edition is now 30 years old. The revised edition is 20 years old. Nevertheless it is a standard textbook for the theory of linear operators. It is user-friendly in the sens",
          "original_query": "Perturbation Theory for Linear Operators [18] (Kato)",
          "cleaned_query": "Perturbation Theory for Linear Operators (Kato)"
        },
        {
          "success": true,
          "title": "Functions of Matrices | SIAM Publications Library",
          "url": "https://epubs.siam.org/doi/10.1137/1.9780898717778",
          "content": "The FM Web Site (link) Errata (link) Matrix functions are of growing interest due to their fascinating theory and the many applications in which they provide insight and succinct solutions. Functions of Matrices: Theory and Computation gives a thorough treatment of the theory of matrix functions and numerical methods for computing them, as well as an overview of applications. The book is useful for advanced courses and is well-suited to self-study. The broad content\u2014including f(A)-related facts, tricks, and techniques, historical references, and an appendix of background results\u2014makes it convenient as a general reference in matrix analysis and numerical linear algebra. Key features of the book: \u2022 Elegant treatment of the theory of matrix functions, exploiting the equivalent definitions of f(A) via the Jordan form, polynomial interpolation, and the Cauchy integral formula. \u2022 Develops theory of conditioning and properties of the Fr\u00e9chet derivative. \u2022 Emphasizes Schur decomposition, block Parlett recurrence, and judicious use of Pad\u00e9 approximants. \u2022 General results on convergence and stability of matrix iterations. \u2022 Detailed treatment of the matrix sign function, matrix roots, the polar decomposition, and transcendental matrix functions (exponential, logarithm, cosine, sine). \u2022 Thorough analysis of the accuracy, stability, and computational cost of numerical methods. \u2022 A chapter devoted to the f(A)b problem. \u2022 Extensive collection of problems with solutions. \u2022 Matrix Function Toolbox provides MATLAB\u00ae implementations of key algorithms. Audience This book is for specialists in numerical analysis and applied linear algebra as well as anyone wishing to learn about the theory of matrix functions and state of the art methods for computing them. It can be used for a graduate-level course on functions of matrices and is a suitable reference for an advanced course on applied or numerical linear algebra. It is also particularly well suited for self-study. About the Author Nicholas J. Higham, FRS, is Richardson Professor of Applied Mathematics at The University of Manchester, UK. He is the author of more than 100 publications and of the books Accuracy and Stability of Numerical Algorithms (SIAM, 2nd ed., 2002), Handbook of Writing for the Mathematical Sciences, (SIAM, 2nd ed., 1998), and MATLAB Guide, (with Desmond J. Higham, SIAM, 2nd ed., 2005).",
          "original_query": "Functions of Matrices: Theory and Computation [29] (Higham)",
          "cleaned_query": "Functions of Matrices: Theory and Computation (Higham)",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "Analyze gauss: optimal bounds for privacy ... - ACM Digital Library",
          "url": "https://dl.acm.org/doi/10.1145/2591796.2591883",
          "content": "![logo](data:,)\n\n## This website uses cookies\n\nWe occasionally run membership recruitment campaigns on social media channels and use cookies to track post-clicks. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you\u2019ve provided to them or that they\u2019ve collected from your use of their services. Use the check boxes below to choose the types of cookies you consent to have stored on your device.\n\nDo not sell or share my personal information\n\n[Use necessary cookies only](https://dl.acm.org/doi/10.1145/2591796.2591883) [Allow all cookies](https://dl.acm.org/doi/10.1145/2591796.2591883) [Show details](https://dl.acm.org/doi/10.1145/2591796.2591883)\n\n[OK](https://dl.acm.org/doi/10.1145/2591796.2591883)\n\n[Use necessary cookies only](https://dl.acm.org/doi/10.1145/2591796.2591883) [Allow selected cookies](https://dl.acm.org/doi/10.1145/2591796.2591883) [Allow all cookies](https://dl.acm.org/doi/10.1145/2591796.2591883)\n\nNecessary\n\nPreferences\n\nStatistics\n\nMarketing\n\n[Show details](https://dl.acm.org/doi/10.1145/2591796.2591883)\n\n[Cookie declaration](https://dl.acm.org/doi/10.1145/2591796.2591883) [\\[#IABV2SETTINGS#\\]](https://dl.acm.org/doi/10.1145/2591796.2591883) [About](https://dl.acm.org/doi/10.1145/2591796.2591883)\n\n[Necessary (8)](https://dl.acm.org/doi/10.1145/2591796.2591883) [Preferences (5)](https://dl.acm.org/doi/10.1145/2591796.2591883) [Statistics (15)](https://dl.acm.org/doi/10.1145/2591796.2591883) [Marketing (24)](https://dl.acm.org/doi/10.1145/2591796.2591883) [Unclassified (5)](https://dl.acm.org/doi/10.1145/2591796.2591883)\n\nNecessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies. These cookies do not gather information about you that could be used for marketing purposes and do not remember where you have been on the internet.\n\n| Name | Provider | Purpose | Expiry | Type |\n| --- | --- | --- | --- | --- |\n| \\_\\_cf\\_bm\u00a0\\[x2\\] | [ACM](https://www.acm.org/privacy-policy) | This cookie is used to distinguish between humans and bots. This is beneficial for the website, in order to make valid reports on the use of their website. | 1 day | HTTP Cookie |\n| \\_\\_jid | c.disquscdn.com | Used to add comments to the website and remember the user's Disqus login credentials across websites that use said service. | Session | HTTP Cookie |\n| disqusauth | c.disquscdn.com | Registers whether the user is logged in. This allows the website owner to make parts of the website inaccessible, based on the user's log-in status. | Session | HTTP Cookie |\n| \\_cfuvid | [ACM](https://www.acm.org/privacy-policy) | This cookie is a part of the services provided by Cloudflare - Including load-balancing, deliverance of website content and serving DNS connection for website operators. | Session | HTTP Cookie |\n| CookieConsent | [Cookiebot](https://www.cookiebot.com/goto/privacy-policy/) | Stores the user's cookie consent state for the current domain | 1 year | HTTP Cookie |\n| JSESSIONID | [ACM](https://www.acm.org/privacy-policy) | Preserves users states across page requests. | Session | HTTP Cookie |\n| 1.gif | [Cookiebot](https://www.cookiebot.com/goto/privacy-policy/) | Used to count the number of sessions to the website, necessary for optimizing CMP product delivery. | Session | Pixel Tracker |\n\nPreference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\n\n| Name | Provider | Purpose | Expiry | Type |\n| --- | --- | --- | --- | --- |\n| aet-dismiss | c.disquscdn.com | Necessary for the functionality of the website's comment-system. | Persistent | HTML Local Storage |\n| drafts.queue | c.disquscdn.com | Necessary for the functionality of the website's comment-system. | Persistent | HTML Local Storage |\n| submitted\\_posts\\_cache | c.disquscdn.com | Necessary for the functionality of the website's comment-system. | Persistent | HTML Local Storage |\n| mopDeploy | [Mopinion](https://mopinion.com/privacy/) | Pending | Session | HTML Local Storage |\n| MACHINE\\_LAST\\_SEEN | [ACM](https://www.acm.org/privacy-policy) | Pending | 300 days | HTTP Cookie |\n\nStatistic cookies help website owners understand how visitors interact with websites by collecting and reporting information anonymously.\n\n| Name | Provider | Purpose | Expiry | Type |\n| --- | --- | --- | --- | --- |\n| \\_ga | [Google](https://business.safety.google/privacy/) | Registers a unique ID that is used to generate statistical data on how the visitor uses the website. | 2 years | HTTP Cookie |\n| \\_ga\\_# | [Google](https://business.safety.google/privacy/) | Used by Google Analytics to collect data on the number of times a user has visited the website as well as dates for the first and most recent visit. | 2 years | HTTP Cookie |\n| \\_gat | [Google](https://business.safety.google/privacy/) | Used by Google Analytics to throttle request rate | 1 day | HTTP Cookie |\n| \\_gid | [Google](https://business.safety.google/privacy/) | Registers a unique ID that is used to generate statistical data on how the visitor uses the website. | 1 day | HTTP Cookie |\n| \\_hjSession\\_# | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Collects statistics on the visitor's visits to the website, such as the number of visits, average time spent on the website and what pages have been read. | 1 day | HTTP Cookie |\n| \\_hjSessionUser\\_# | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Collects statistics on the visitor's visits to the website, such as the number of visits, average time spent on the website and what pages have been read. | 1 year | HTTP Cookie |\n| \\_hjTLDTest | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Registers statistical data on users' behaviour on the website. Used for internal analytics by the website operator. | Session | HTTP Cookie |\n| \\_hp2\\_# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 1 day | HTTP Cookie |\n| \\_hp2\\_hld#.# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 1 day | HTTP Cookie |\n| \\_hp2\\_id.# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 13 months | HTTP Cookie |\n| \\_hp2\\_ses\\_props.# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 1 day | HTTP Cookie |\n| disqus\\_unique | c.disquscdn.com | Collects statistics related to the user's visits to the website, such as number of visits, average time spent on the website and loaded pages. | Session | HTTP Cookie |\n| collect | [Google](https://business.safety.google/privacy/) | Used to send data to Google Analytics about the visitor's device and behavior. Tracks the visitor across devices and marketing channels. | Session | Pixel Tracker |\n| hjActiveViewportIds | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | This cookie contains an ID string on the current session. This contains non-personal information on what subpages the visitor enters \u2013 this information is used to optimize the visitor's experience. | Persistent | HTML Local Storage |\n| hjViewportId | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Saves the user's screen size in order to adjust the size of images on the website. | Session | HTML Local Storage |\n\nMarketing cookies are used to track visitors across websites. T",
          "original_query": "Analyze Gauss: Optimal bounds for privacy-preserving principal component analysis [8]",
          "cleaned_query": "Analyze Gauss: Optimal bounds for privacy-preserving principal component analysis"
        },
        {
          "success": true,
          "title": "Bounds for Private Matrix Approximation via Dyson Brownian Motion",
          "url": "https://arxiv.org/abs/2211.06418",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: Given a symmetric matrix $M$ and a vector $\\lambda$, we present new bounds on\nthe Frobenius-distance utility of the Gaussian mechanism for approximating $M$\nby a matrix whose spectrum is $\\lambda$, under\n$(\\varepsilon,\\delta)$-differential privacy. Our bounds depend on both\n$\\lambda$ and the gaps in the eigenvalues of $M$, and hold whenever the top\n$k+1$ eigenvalues of $M$ have sufficiently large gaps. When applied to the\nproblems of private rank-$k$ covariance matrix approximation and subspace\nrecovery, our bounds yield improvements over previous bounds. Our bounds are\nobtained by viewing the addition of Gaussian noise as a continuous-time matrix\nBrownian motion. This viewpoint allows us to track the evolution of eigenvalues\nand eigenvectors of the matrix, which are governed by stochastic differential\nequations discovered by Dyson. These equations allow us to bound the utility as\nthe square-root of a sum-of-squares of perturbations to the eigenvectors, as\nopposed to a sum of perturbation bounds obtained via Davis-Kahan-type theorems.\n \n \n \n \n Submission history From: Oren Mangoubi [ view email]\n [v1] \nFri, 11 Nov 2022 18:54:01 UTC (58 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > cs > arXiv:2211.06418\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Data Structures and Algorithms\n\n arXiv:2211.06418 (cs)\n [Submitted on 11 Nov 2022]\n\n Title: Re-Analyze Gauss: Bounds for Private Matrix Approximation via Dyson Brownian Motion\n\n Authors: Oren Mangoubi, Nisheeth K. Vishnoi\n Download PDF\n Abstract: Given a symmetric matrix $M$ and a vector $\\lambda$, we present new bounds on the Frobenius-distance utility of the Gaussian mechanism for approximating $M$ by a matrix whose spectrum is $\\lambda$, under $(\\varepsilon,\\delta)$-differential privacy. Our bounds depend on both $\\lambda$ and the gaps in the eigenvalues of $M$, and hold whenever the top $k+1$ eigenvalues of $M$ have sufficiently large gaps. When applied to the problems of private rank-$k$ covariance matrix approximation and subspace recovery, our bounds yield improvements over previous bounds. Our bounds are obtained by viewing the addition of Gaussian noise as a continuous-time matrix Brownian motion. This viewpoint allows us to track the evolution of eigenvalues and eigenvectors of the matrix, which are governed by stochastic differential equations discovered by Dyson. These equations allow us to bound the utility as the square-root of a sum-of-squares of perturbations to the eigenvectors, as opposed to a sum of perturbation bounds obtained via Davis-Kahan-type theorems.\n Comments: This is the full version of a paper which was accepted to NeurIPS 2022 \n Subjects: Data Structures and Algorithms (cs.DS) ; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Numerical Analysis (math.NA); Machine Learning (stat.ML)\n Cite as: arXiv:2211.06418 [cs.DS] \n (or arXiv:2211.06418v1 [cs.DS] for this version) \n https://doi.org/10.48550/arXiv.2211.06418 \n Focus to learn more \n arXiv-issued DOI via DataCite \n \n\n Submission history\n\n From: Oren Mangoubi [view email]\n [v1] Fri, 11 Nov 2022 18:54:01 UTC (58 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * Other formats\n (license)\n Current browse context:\n cs.DS\n < prev | next >\n new | recent | 2211\n Change to browse by:\n cs\n cs.CR\n cs.LG\n cs.NA\n math\n math.NA\n stat\n stat.ML\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Re-analyze Gauss: Bounds for private matrix approximation via Dyson Brownian motion [19]",
          "cleaned_query": "Re-analyze Gauss: Bounds for private matrix approximation via Dyson Brownian motion"
        }
      ],
      "generated_ideas": [
        "Gap-Adaptive Private Rank-\\(k\\) Approximation via Online Eigen-Gap Estimation\nDevelop a differentially private algorithm that first privately estimates local eigen-gaps (e.g., around the \\(k\\)th eigenvalue) and then calibrates Gaussian noise and post-processing to those gaps. The contribution is a concrete procedure that turns the Dyson Brownian motion utility analysis into an implementable \u201cgap-adaptive\u201d mechanism with provable Frobenius/subspace error guarantees that improve when gaps are large.",
        "Dyson-Brownian Utility Bounds for Weighted Low-Rank Approximation\nExtend the private matrix approximation analysis to weighted Frobenius objectives \\(\\|W\\odot(M-\\hat M)\\|_F\\), connecting to weighted low-rank approximation settings where entries have heterogeneous importance or missingness. The key contribution is deriving eigenvector/eigenspace perturbation bounds under anisotropic (entry-weighted) noise and proposing a matching DP mechanism optimized for a given weight pattern.",
        "Private Low-Rank Approximation with Missing Data Using Grassmann Geodesic Tracking\nCombine Grassmann-manifold optimization for low-rank completion (missing entries) with a privacy mechanism whose effect is modeled as a stochastic flow on the Grassmannian. The contribution is an actionable algorithm that alternates between (i) manifold steps using observed entries and (ii) calibrated Gaussian perturbations, together with a Davis\u2013Kahan/Dyson-style bound on subspace drift under partial observation.",
        "Perturbation-Stable Selection of Rank Under Differential Privacy\nDesign a DP procedure to choose \\(k\\) (rank) by privately estimating spectral decay and gaps, then proving stability/consistency guarantees using eigenvalue perturbation theory (Kato) and eigenvector rotation bounds (Davis). The contribution is a full pipeline\u2014rank selection + approximation\u2014where the final error bound explicitly accounts for uncertainty in \\(k\\) induced by privacy noise.",
        "Private Computation of Matrix Functions via Low-Rank Spectral Surrogates\nBuild DP algorithms for approximating \\(f(M)\\) (e.g., \\(\\exp(M)\\), \\(\\log(M)\\), sign, square root) by combining low-rank Eckart\u2013Young truncation with Higham-style stable evaluation (Schur/Pad\u00e9) on the reduced model. The key contribution is an end-to-end accuracy + privacy analysis that relates error in \\(f(M)\\) to (i) spectral truncation and (ii) eigenvector rotation under Gaussian perturbations.",
        "Fr\u00e9chet-Derivative Sensitivity Analysis for DP Matrix-Function Outputs\nUse the Fr\u00e9chet derivative framework for matrix functions (Higham) to compute tight local sensitivity bounds for releasing \\(f(M)\\) under DP, rather than bounding sensitivity through crude norms. The contribution is a concrete methodology to calibrate noise based on computable derivative norms (or bounds via resolvent integrals), potentially yielding significantly better utility for smooth \\(f\\) and well-separated spectra.",
        "Private \u201cSpectrum-Constrained\u201d Approximation with Post-Processing Certificates\nGeneralize the \u201capproximate \\(M\\) by a matrix with prescribed spectrum \\(\\lambda\\)\u201d setting by adding constraints such as PSD, trace, or condition-number bounds, and provide efficient solvers plus utility certificates. The contribution is an actionable constrained optimization algorithm (e.g., projected/ADMM on eigenvalues + subspace) with guarantees that combine Eckart\u2013Young optimality (unconstrained) and perturbation bounds quantifying the extra cost of constraints.",
        "Non-Asymptotic Eigenvector Rotation Bounds Under Correlated (Non-Gaussian) DP Noise\nReplace isotropic Gaussian perturbations with structured or correlated noise (e.g., for streaming, federated, or accounting reasons) and derive new eigenvector/eigenspace rotation bounds beyond classical Davis\u2013Kahan. The contribution is a set of explicit, checkable conditions (in terms of noise covariance and eigen-gaps) plus mechanism design guidance for when non-i.i.d. noise improves privacy/utility tradeoffs.",
        "Streaming Private Subspace Tracking via Discretized Dyson Dynamics\nCreate a streaming algorithm that maintains a private estimate of the top-\\(k\\) eigenspace as data arrive, interpreting each update as a small perturbation step and using discretized Dyson Brownian motion to analyze cumulative drift. The key contribution is a practical update rule with a regret-style bound on subspace error over time that scales with observed eigen-gaps and per-step privacy budget.",
        "Private Low-Rank + Residual Decomposition with Eigen-Gap-Aware Error Splitting\nDevelop a DP decomposition \\(M \\approx L_k + R\\) where \\(L_k\\) is rank-\\(k\\) (Eckart\u2013Young-inspired) and \\(R\\) captures the residual, but allocate privacy noise differently to the subspace and to the residual based on eigen-gaps. The contribution is a specific two-channel mechanism (subspace noise vs. residual noise) with theory showing improved Frobenius error when the top-\\(k\\) space is well-separated even if the tail is not."
      ],
      "generated_ideas_raw": "1. **Gap-Adaptive Private Rank-\\(k\\) Approximation via Online Eigen-Gap Estimation**  \n   Develop a differentially private algorithm that first privately estimates local eigen-gaps (e.g., around the \\(k\\)th eigenvalue) and then calibrates Gaussian noise and post-processing to those gaps. The contribution is a concrete procedure that turns the Dyson Brownian motion utility analysis into an implementable \u201cgap-adaptive\u201d mechanism with provable Frobenius/subspace error guarantees that improve when gaps are large.\n\n2. **Dyson-Brownian Utility Bounds for Weighted Low-Rank Approximation**  \n   Extend the private matrix approximation analysis to weighted Frobenius objectives \\(\\|W\\odot(M-\\hat M)\\|_F\\), connecting to weighted low-rank approximation settings where entries have heterogeneous importance or missingness. The key contribution is deriving eigenvector/eigenspace perturbation bounds under anisotropic (entry-weighted) noise and proposing a matching DP mechanism optimized for a given weight pattern.\n\n3. **Private Low-Rank Approximation with Missing Data Using Grassmann Geodesic Tracking**  \n   Combine Grassmann-manifold optimization for low-rank completion (missing entries) with a privacy mechanism whose effect is modeled as a stochastic flow on the Grassmannian. The contribution is an actionable algorithm that alternates between (i) manifold steps using observed entries and (ii) calibrated Gaussian perturbations, together with a Davis\u2013Kahan/Dyson-style bound on subspace drift under partial observation.\n\n4. **Perturbation-Stable Selection of Rank Under Differential Privacy**  \n   Design a DP procedure to choose \\(k\\) (rank) by privately estimating spectral decay and gaps, then proving stability/consistency guarantees using eigenvalue perturbation theory (Kato) and eigenvector rotation bounds (Davis). The contribution is a full pipeline\u2014rank selection + approximation\u2014where the final error bound explicitly accounts for uncertainty in \\(k\\) induced by privacy noise.\n\n5. **Private Computation of Matrix Functions via Low-Rank Spectral Surrogates**  \n   Build DP algorithms for approximating \\(f(M)\\) (e.g., \\(\\exp(M)\\), \\(\\log(M)\\), sign, square root) by combining low-rank Eckart\u2013Young truncation with Higham-style stable evaluation (Schur/Pad\u00e9) on the reduced model. The key contribution is an end-to-end accuracy + privacy analysis that relates error in \\(f(M)\\) to (i) spectral truncation and (ii) eigenvector rotation under Gaussian perturbations.\n\n6. **Fr\u00e9chet-Derivative Sensitivity Analysis for DP Matrix-Function Outputs**  \n   Use the Fr\u00e9chet derivative framework for matrix functions (Higham) to compute tight local sensitivity bounds for releasing \\(f(M)\\) under DP, rather than bounding sensitivity through crude norms. The contribution is a concrete methodology to calibrate noise based on computable derivative norms (or bounds via resolvent integrals), potentially yielding significantly better utility for smooth \\(f\\) and well-separated spectra.\n\n7. **Private \u201cSpectrum-Constrained\u201d Approximation with Post-Processing Certificates**  \n   Generalize the \u201capproximate \\(M\\) by a matrix with prescribed spectrum \\(\\lambda\\)\u201d setting by adding constraints such as PSD, trace, or condition-number bounds, and provide efficient solvers plus utility certificates. The contribution is an actionable constrained optimization algorithm (e.g., projected/ADMM on eigenvalues + subspace) with guarantees that combine Eckart\u2013Young optimality (unconstrained) and perturbation bounds quantifying the extra cost of constraints.\n\n8. **Non-Asymptotic Eigenvector Rotation Bounds Under Correlated (Non-Gaussian) DP Noise**  \n   Replace isotropic Gaussian perturbations with structured or correlated noise (e.g., for streaming, federated, or accounting reasons) and derive new eigenvector/eigenspace rotation bounds beyond classical Davis\u2013Kahan. The contribution is a set of explicit, checkable conditions (in terms of noise covariance and eigen-gaps) plus mechanism design guidance for when non-i.i.d. noise improves privacy/utility tradeoffs.\n\n9. **Streaming Private Subspace Tracking via Discretized Dyson Dynamics**  \n   Create a streaming algorithm that maintains a private estimate of the top-\\(k\\) eigenspace as data arrive, interpreting each update as a small perturbation step and using discretized Dyson Brownian motion to analyze cumulative drift. The key contribution is a practical update rule with a regret-style bound on subspace error over time that scales with observed eigen-gaps and per-step privacy budget.\n\n10. **Private Low-Rank + Residual Decomposition with Eigen-Gap-Aware Error Splitting**  \n   Develop a DP decomposition \\(M \\approx L_k + R\\) where \\(L_k\\) is rank-\\(k\\) (Eckart\u2013Young-inspired) and \\(R\\) captures the residual, but allocate privacy noise differently to the subspace and to the residual based on eigen-gaps. The contribution is a specific two-channel mechanism (subspace noise vs. residual noise) with theory showing improved Frobenius error when the top-\\(k\\) space is well-separated even if the tail is not.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Gap-Adaptive Private Rank-\\(k\\) Approximation via Online Eigen-Gap Estimation\nDevelop a differentially private algorithm that first privately estimates local eigen-gaps (e.g., around the \\(k\\)th eigen",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Dyson-Brownian Utility Bounds for Weighted Low-Rank Approximation\nExtend the private matrix approximation analysis to weighted Frobenius objectives \\(\\|W\\odot(M-\\hat M)\\|_F\\), connecting to weighted l",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Private Low-Rank Approximation with Missing Data Using Grassmann Geodesic Tracking\nCombine Grassmann-manifold optimization for low-rank completion (missing entries) with a privacy mechanism whose effe",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Perturbation-Stable Selection of Rank Under Differential Privacy\nDesign a DP procedure to choose \\(k\\) (rank) by privately estimating spectral decay and gaps, then proving stability/consistency guaran",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Private Computation of Matrix Functions via Low-Rank Spectral Surrogates\nBuild DP algorithms for approximating \\(f(M)\\) (e.g., \\(\\exp(M)\\), \\(\\log(M)\\), sign, square root) by combining low-rank Eckart",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Fr\u00e9chet-Derivative Sensitivity Analysis for DP Matrix-Function Outputs\nUse the Fr\u00e9chet derivative framework for matrix functions (Higham) to compute tight local sensitivity bounds for releasing \\(f(M)",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Private \u201cSpectrum-Constrained\u201d Approximation with Post-Processing Certificates\nGeneralize the \u201capproximate \\(M\\) by a matrix with prescribed spectrum \\(\\lambda\\)\u201d setting by adding constraints such as",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Non-Asymptotic Eigenvector Rotation Bounds Under Correlated (Non-Gaussian) DP Noise\nReplace isotropic Gaussian perturbations with structured or correlated noise (e.g., for streaming, federated, or acc",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Streaming Private Subspace Tracking via Discretized Dyson Dynamics\nCreate a streaming algorithm that maintains a private estimate of the top-\\(k\\) eigenspace as data arrive, interpreting each update a",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Private Low-Rank + Residual Decomposition with Eigen-Gap-Aware Error Splitting\nDevelop a DP decomposition \\(M \\approx L_k + R\\) where \\(L_k\\) is rank-\\(k\\) (Eckart\u2013Young-inspired) and \\(R\\) captures t",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 28,
      "paper_title": "Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization",
      "contribution": "By analyzing algorithm-dependent sample concentration and GP sample-path properties to refine information-gain estimates, the paper proves improved high-probability regret bounds for GP-UCB\u2014eO(\u221aT) under certain Mat\u00e9rn kernels and O(\u221a(T ln^2 T)) for the squared-exponential kernel.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12721,
      "output_tokens": 1126,
      "predecessor_details": [
        {
          "success": true,
          "title": "Gaussian Process Optimization in the Bandit Setting: No Regret and ...",
          "url": "https://arxiv.org/abs/0912.3995",
          "content": "[0912.3995] Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:0912.3995\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:0912.3995**(cs)\n[Submitted on 21 Dec 2009 ([v1](https://arxiv.org/abs/0912.3995v1)), last revised 9 Jun 2010 (this version, v4)]\n# Title:Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design\nAuthors:[Niranjan Srinivas](https://arxiv.org/search/cs?searchtype=author&amp;query=Niranjan),[Andreas Krause](https://arxiv.org/search/cs?searchtype=author&amp;query=Krause,+A),[Sham M. Kakade](https://arxiv.org/search/cs?searchtype=author&amp;query=Kakade,+S+M),[Matthias Seeger](https://arxiv.org/search/cs?searchtype=author&amp;query=Seeger,+M)\nView a PDF of the paper titled Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design, by Niranjan Srinivas and 2 other authors\n[View PDF](https://arxiv.org/pdf/0912.3995)> > Abstract:\n> Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches. Subjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:0912.3995](https://arxiv.org/abs/0912.3995)[cs.LG]|\n|(or[arXiv:0912.3995v4](https://arxiv.org/abs/0912.3995v4)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.0912.3995](https://doi.org/10.48550/arXiv.0912.3995)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\nRelated DOI:|[https://doi.org/10.1109/TIT.2011.2182033](https://doi.org/10.1109/TIT.2011.2182033)\nFocus to learn more\nDOI(s) linking to related resources\n|\n## Submission history\nFrom: Niranjan Srinivas [[view email](https://arxiv.org/show-email/0d3f4ab9/0912.3995)]\n**[[v1]](https://arxiv.org/abs/0912.3995v1)**Mon, 21 Dec 2009 00:08:19 UTC (476 KB)\n**[[v2]](https://arxiv.org/abs/0912.3995v2)**Thu, 4 Feb 2010 06:15:15 UTC (175 KB)\n**[[v3]](https://arxiv.org/abs/0912.3995v3)**Sat, 13 Feb 2010 18:24:43 UTC (175 KB)\n**[v4]**Wed, 9 Jun 2010 23:24:13 UTC (292 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design, by Niranjan Srinivas and 2 other authors\n* [View PDF](https://arxiv.org/pdf/0912.3995)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=0912.3995&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=0912.3995&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2009-12](https://arxiv.org/list/cs.LG/2009-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/0912.3995?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:0912.3995)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=0912.3995)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:0912.3995)\n### [2 blog links](https://arxiv.org/tb/0912.3995)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr0912.html#abs-0912-3995)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-0912-3995)\n[Niranjan Srinivas]()\n[Andreas Krause]()\n[Sham M. Kakade]()\n[Matthias Seeger]()\n[Matthias W. Seeger]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which author",
          "original_query": "Gaussian process optimization in the bandit setting: No regret and experimental design",
          "cleaned_query": "Gaussian process optimization in the bandit setting: No regret and experimental design"
        },
        {
          "success": true,
          "title": "Tight Regret Bounds for Bayesian Optimization in One Dimension",
          "url": "https://arxiv.org/abs/1805.11792",
          "content": "[1805.11792] Tight Regret Bounds for Bayesian Optimization in One Dimension\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:1805.11792\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:1805.11792**(stat)\n[Submitted on 30 May 2018 ([v1](https://arxiv.org/abs/1805.11792v1)), last revised 7 May 2025 (this version, v3)]\n# Title:Tight Regret Bounds for Bayesian Optimization in One Dimension\nAuthors:[Jonathan Scarlett](https://arxiv.org/search/stat?searchtype=author&amp;query=Scarlett,+J)\nView a PDF of the paper titled Tight Regret Bounds for Bayesian Optimization in One Dimension, by Jonathan Scarlett\n[View PDF](https://arxiv.org/pdf/1805.11792)[HTML (experimental)](https://arxiv.org/html/1805.11792v3)> > Abstract:\n> We consider the problem of Bayesian optimization (BO) in one dimension, under a Gaussian process prior and Gaussian sampling noise. We provide a theoretical analysis showing that, under fairly mild technical assumptions on the kernel, the best possible cumulative regret up to time $T$ behaves as $\\Omega(\\sqrt{T})$ and $O(\\sqrt{T\\log T})$. This gives a tight characterization up to a $\\sqrt{\\log T}$ factor, and includes the first non-trivial lower bound for noisy BO. Our assumptions are satisfied, for example, by the squared exponential and Mat\u00e9rn-$\\nu$ kernels, with the latter requiring $\\nu &gt; 2$. Our results certify the near-optimality of existing bounds (Srinivas {\\em et al.}, 2009) for the SE kernel, while proving them to be strictly suboptimal for the Mat\u00e9rn kernel with $\\nu &gt; 2$. Comments:|ICML 2018 + supplementary material. This version also includes an &#39;Errata&#39; section correcting two minor mistakes|\nSubjects:|Machine Learning (stat.ML); Information Theory (cs.IT); Machine Learning (cs.LG); Optimization and Control (math.OC)|\nCite as:|[arXiv:1805.11792](https://arxiv.org/abs/1805.11792)[stat.ML]|\n|(or[arXiv:1805.11792v3](https://arxiv.org/abs/1805.11792v3)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.1805.11792](https://doi.org/10.48550/arXiv.1805.11792)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jonathan Scarlett [[view email](https://arxiv.org/show-email/6f272cb2/1805.11792)]\n**[[v1]](https://arxiv.org/abs/1805.11792v1)**Wed, 30 May 2018 03:33:37 UTC (458 KB)\n**[[v2]](https://arxiv.org/abs/1805.11792v2)**Mon, 16 Dec 2019 08:38:20 UTC (458 KB)\n**[v3]**Wed, 7 May 2025 04:10:27 UTC (460 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Tight Regret Bounds for Bayesian Optimization in One Dimension, by Jonathan Scarlett\n* [View PDF](https://arxiv.org/pdf/1805.11792)\n* [HTML (experimental)](https://arxiv.org/html/1805.11792v3)\n* [TeX Source](https://arxiv.org/src/1805.11792)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1805.11792&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1805.11792&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2018-05](https://arxiv.org/list/stat.ML/2018-05)\nChange to browse by:\n[cs](https://arxiv.org/abs/1805.11792?context=cs)\n[cs.IT](https://arxiv.org/abs/1805.11792?context=cs.IT)\n[cs.LG](https://arxiv.org/abs/1805.11792?context=cs.LG)\n[math](https://arxiv.org/abs/1805.11792?context=math)\n[math.IT](https://arxiv.org/abs/1805.11792?context=math.IT)\n[math.OC](https://arxiv.org/abs/1805.11792?context=math.OC)\n[stat](https://arxiv.org/abs/1805.11792?context=stat)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1805.11792)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1805.11792)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1805.11792)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1805.11792)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Tight regret bounds for Bayesian optimization in one dimension",
          "cleaned_query": "Tight regret bounds for Bayesian optimization in one dimension"
        },
        {
          "success": true,
          "title": "On Information Gain and Regret Bounds in Gaussian ...",
          "url": "https://arxiv.org/abs/2009.06966",
          "content": "# Statistics > Machine Learning\n\n**arXiv:2009.06966** (stat)\n\n\\[Submitted on 15 Sep 2020 ( [v1](https://arxiv.org/abs/2009.06966v1)), last revised 9 Mar 2021 (this version, v3)\\]\n\n# Title:On Information Gain and Regret Bounds in Gaussian Process Bandits\n\nAuthors: [Sattar Vakili](https://arxiv.org/search/stat?searchtype=author&query=Vakili,+S), [Kia Khezeli](https://arxiv.org/search/stat?searchtype=author&query=Khezeli,+K), [Victor Picheny](https://arxiv.org/search/stat?searchtype=author&query=Picheny,+V)\n\nView a PDF of the paper titled On Information Gain and Regret Bounds in Gaussian Process Bandits, by Sattar Vakili and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2009.06966)\n\n> Abstract:Consider the sequential optimization of an expensive to evaluate and possibly non-convex objective function $f$ from noisy feedback, that can be considered as a continuum-armed bandit problem. Upper bounds on the regret performance of several learning algorithms (GP-UCB, GP-TS, and their variants) are known under both a Bayesian (when $f$ is a sample from a Gaussian process (GP)) and a frequentist (when $f$ lives in a reproducing kernel Hilbert space) setting. The regret bounds often rely on the maximal information gain $\\\\gamma\\_T$ between $T$ observations and the underlying GP (surrogate) model. We provide general bounds on $\\\\gamma\\_T$ based on the decay rate of the eigenvalues of the GP kernel, whose specialisation for commonly used kernels, improves the existing bounds on $\\\\gamma\\_T$, and subsequently the regret bounds relying on $\\\\gamma\\_T$ under numerous settings. For the Mat\u00e9rn family of kernels, where the lower bounds on $\\\\gamma\\_T$, and regret under the frequentist setting, are known, our results close a huge polynomial in $T$ gap between the upper and lower bounds (up to logarithmic in $T$ factors).\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (stat.ML); Information Theory (cs.IT); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2009.06966](https://arxiv.org/abs/2009.06966) \\[stat.ML\\] |\n| | (or [arXiv:2009.06966v3](https://arxiv.org/abs/2009.06966v3) \\[stat.ML\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2009.06966](https://doi.org/10.48550/arXiv.2009.06966) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Sattar Vakili \\[ [view email](https://arxiv.org/show-email/c6d9e213/2009.06966)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2009.06966v1)**\nTue, 15 Sep 2020 10:15:29 UTC (39 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2009.06966v2)**\nFri, 9 Oct 2020 14:28:46 UTC (40 KB)\n\n**\\[v3\\]**\nTue, 9 Mar 2021 22:46:52 UTC (41 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled On Information Gain and Regret Bounds in Gaussian Process Bandits, by Sattar Vakili and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2009.06966)\n- [TeX Source](https://arxiv.org/src/2009.06966)\n- [Other Formats](https://arxiv.org/format/2009.06966)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2009.06966&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2009.06966&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2020-09](https://arxiv.org/list/stat.ML/2020-09)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2009.06966?context=cs)\n\n[cs.IT](https://arxiv.org/abs/2009.06966?context=cs.IT)\n\n[cs.LG](https://arxiv.org/abs/2009.06966?context=cs.LG)\n\n[math](https://arxiv.org/abs/2009.06966?context=math)\n\n[math.IT](https://arxiv.org/abs/2009.06966?context=math.IT)\n\n[stat](https://arxiv.org/abs/2009.06966?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2009.06966)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2009.06966)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2009.06966)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2009.06966&description=On Information Gain and Regret Bounds in Gaussian Process Bandits) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2009.06966&title=On Information Gain and Regret Bounds in Gaussian Process Bandits)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2009.06966) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "On information gain and regret bounds in Gaussian process bandits",
          "cleaned_query": "On information gain and regret bounds in Gaussian process bandits"
        },
        {
          "success": true,
          "title": "Bandit optimisation of functions in the Mat\u00e9rn kernel RKHS - arXiv",
          "url": "https://arxiv.org/abs/2001.10396",
          "content": "\n Download PDF \nAbstract: We consider the problem of optimising functions in the reproducing kernel\nHilbert space (RKHS) of a Mat\u00e9rn kernel with smoothness parameter $\\nu$ over\nthe domain $[0,1]^d$ under noisy bandit feedback. Our contribution, the\n$\\pi$-GP-UCB algorithm, is the first practical approach with guaranteed\nsublinear regret for all $\\nu&gt;1$ and $d \\geq 1$. Empirical validation suggests\nbetter performance and drastically improved computational scalablity compared\nwith its predecessor, Improved GP-UCB.\n \n \n Submission history From: David Janz [ view email]\n \n [v1] \nTue, 28 Jan 2020 15:09:21 UTC (189 KB) [v2] \nMon, 2 Mar 2020 14:50:32 UTC (383 KB) ||||I|||| Skip to main content\nWe gratefully acknowledge support from\nthe Simons Foundation and member institutions.\n> cs > arXiv:2001.10396\nHelp | Advanced Search\nAll fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\nSearch\nGO\nquick links\n* Login\n* Help Pages\n* About\nComputer Science > Machine Learning\narXiv:2001.10396 (cs)\n[Submitted on 28 Jan 2020 (v1), last revised 2 Mar 2020 (this version, v2)]\nTitle: Bandit optimisation of functions in the Mat\u00e9rn kernel RKHS\nAuthors: David Janz, David R. Burt, Javier Gonz\u00e1lez\nDownload PDF\nAbstract: We consider the problem of optimising functions in the reproducing kernel Hilbert space (RKHS) of a Mat\u00e9rn kernel with smoothness parameter $\\nu$ over the domain $[0,1]^d$ under noisy bandit feedback. Our contribution, the $\\pi$-GP-UCB algorithm, is the first practical approach with guaranteed sublinear regret for all $\\nu>1$ and $d \\geq 1$. Empirical validation suggests better performance and drastically improved computational scalablity compared with its predecessor, Improved GP-UCB.\nComments: AISTATS 2020, camera ready\nSubjects: Machine Learning (cs.LG) ; Machine Learning (stat.ML)\nCite as: arXiv:2001.10396 [cs.LG]\n(or arXiv:2001.10396v2 [cs.LG] for this version)\nhttps://doi.org/10.48550/arXiv.2001.10396\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: David Janz [view email]\n[v1] Tue, 28 Jan 2020 15:09:21 UTC (189 KB)\n[v2] Mon, 2 Mar 2020 14:50:32 UTC (383 KB)\nFull-text links:\nDownload:\n* PDF\n* Other formats\n(license)\nCurrent browse context:\ncs.LG\n< prev | next >\nnew | recent | 2001\nChange to browse by:\ncs\nstat\nstat.ML\nReferences & Citations\n* NASA ADS\n* Google Scholar\n* Semantic Scholar\nDBLP - CS Bibliography\nlisting | bibtex\nDavid Janz\nDavid R. Burt\nJavier Gonz\u00e1lez\na export bibtex citation Loading...\nBibtex formatted citation\n\u00d7\nloading...\nData provided by:\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer (What is the Explorer?)\nLitmaps Toggle\nLitmaps (What is Litmaps?)\nscite.ai Toggle\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nCode, Data and Media Associated with this Article\nLinks to Code Toggle\nPapers with Code (What is Papers with Code?)\nScienceCast Toggle\nScienceCast (What is ScienceCast?)\nDemos\nDemos\nReplicate Toggle\nReplicate (What is Replicate?)\nSpaces Toggle\nHugging Face Spaces (What is Spaces?)\nRelated Papers\nRecommenders and Search Tools\nConnected Papers Toggle\nConnected Papers (What is Connected Papers?)\nCore recommender toggle\nCORE Recommender (What is CORE?)\nIArxiv recommender toggle\nIArxiv Recommender (What is IArxiv?)\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community? Learn more about arXivLabs and how to get involved.\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n* About\n* Help\n* Click here to contact arXiv Contact\n* Click here to subscribe Subscribe\n* Copyright\n* Privacy Policy\n* Web Accessibility Assistance\n* arXiv Operational Status\nGet status notifications via email or slack",
          "original_query": "Bandit optimisation of functions in the Mat\u00e9rn kernel RKHS",
          "cleaned_query": "Bandit optimisation of functions in the Mat\u00e9rn kernel RKHS"
        },
        {
          "success": true,
          "title": "Lenient Regret and Good-Action Identification in Gaussian ...",
          "url": "https://arxiv.org/abs/2102.05793",
          "content": "# Statistics > Machine Learning\n\n**arXiv:2102.05793** (stat)\n\n\\[Submitted on 11 Feb 2021 ( [v1](https://arxiv.org/abs/2102.05793v1)), last revised 26 May 2021 (this version, v2)\\]\n\n# Title:Lenient Regret and Good-Action Identification in Gaussian Process Bandits\n\nAuthors: [Xu Cai](https://arxiv.org/search/stat?searchtype=author&query=Cai,+X), [Selwyn Gomes](https://arxiv.org/search/stat?searchtype=author&query=Gomes,+S), [Jonathan Scarlett](https://arxiv.org/search/stat?searchtype=author&query=Scarlett,+J)\n\nView a PDF of the paper titled Lenient Regret and Good-Action Identification in Gaussian Process Bandits, by Xu Cai and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2102.05793)\n\n> Abstract:In this paper, we study the problem of Gaussian process (GP) bandits under relaxed optimization criteria stating that any function value above a certain threshold is \"good enough\". On the theoretical side, we study various {\\\\em lenient regret} notions in which all near-optimal actions incur zero penalty, and provide upper bounds on the lenient regret for GP-UCB and an elimination algorithm, circumventing the usual $O(\\\\sqrt{T})$ term (with time horizon $T$) resulting from zooming extremely close towards the function maximum. In addition, we complement these upper bounds with algorithm-independent lower bounds. On the practical side, we consider the problem of finding a single \"good action\" according to a known pre-specified threshold, and introduce several good-action identification algorithms that exploit knowledge of the threshold. We experimentally find that such algorithms can often find a good action faster than standard optimization-based approaches.\n\n| | |\n| --- | --- |\n| Comments: | ICML 2021 |\n| Subjects: | Machine Learning (stat.ML); Information Theory (cs.IT); Machine Learning (cs.LG); Optimization and Control (math.OC) |\n| Cite as: | [arXiv:2102.05793](https://arxiv.org/abs/2102.05793) \\[stat.ML\\] |\n| | (or [arXiv:2102.05793v2](https://arxiv.org/abs/2102.05793v2) \\[stat.ML\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2102.05793](https://doi.org/10.48550/arXiv.2102.05793) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jonathan Scarlett \\[ [view email](https://arxiv.org/show-email/f3ef04ef/2102.05793)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2102.05793v1)**\nThu, 11 Feb 2021 01:16:58 UTC (275 KB)\n\n**\\[v2\\]**\nWed, 26 May 2021 06:46:03 UTC (443 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Lenient Regret and Good-Action Identification in Gaussian Process Bandits, by Xu Cai and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2102.05793)\n- [TeX Source](https://arxiv.org/src/2102.05793)\n- [Other Formats](https://arxiv.org/format/2102.05793)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2102.05793&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2102.05793&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2021-02](https://arxiv.org/list/stat.ML/2021-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2102.05793?context=cs)\n\n[cs.IT](https://arxiv.org/abs/2102.05793?context=cs.IT)\n\n[cs.LG](https://arxiv.org/abs/2102.05793?context=cs.LG)\n\n[math](https://arxiv.org/abs/2102.05793?context=math)\n\n[math.IT](https://arxiv.org/abs/2102.05793?context=math.IT)\n\n[math.OC](https://arxiv.org/abs/2102.05793?context=math.OC)\n\n[stat](https://arxiv.org/abs/2102.05793?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2102.05793)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2102.05793)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2102.05793)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2102.05793&description=Lenient Regret and Good-Action Identification in Gaussian Process Bandits) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2102.05793&title=Lenient Regret and Good-Action Identification in Gaussian Process Bandits)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2102.05793) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Lenient regret and good-action identification in Gaussian process bandits",
          "cleaned_query": "Lenient regret and good-action identification in Gaussian process bandits"
        },
        {
          "success": true,
          "title": "Posterior consistency of Gaussian process prior for nonparametric ...",
          "url": "https://arxiv.org/abs/math/0702686",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: Consider binary observations whose response probability is an unknown smooth\nfunction of a set of covariates. Suppose that a prior on the response\nprobability function is induced by a Gaussian process mapped to the unit\ninterval through a link function. In this paper we study consistency of the\nresulting posterior distribution. If the covariance kernel has derivatives up\nto a desired order and the bandwidth parameter of the kernel is allowed to take\narbitrarily small values, we show that the posterior distribution is consistent\nin the $L_1$-distance. As an auxiliary result to our proofs, we show that,\nunder certain conditions, a Gaussian process assigns positive probabilities to\nthe uniform neighborhoods of a continuous function. This result may be of\nindependent interest in the literature for small ball probabilities of Gaussian\nprocesses.\n \n \n \n \n Submission history From: Anindya Roy [ view email]\u00a0[via VTEX proxy]\n [v1] \nFri, 23 Feb 2007 10:25:53 UTC (74 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > math > arXiv:math/0702686\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Mathematics > Statistics Theory\n\n arXiv:math/0702686 (math)\n [Submitted on 23 Feb 2007]\n\n Title: Posterior consistency of Gaussian process prior for nonparametric binary regression\n\n Authors: Subhashis Ghosal, Anindya Roy\n Download PDF\n Abstract: Consider binary observations whose response probability is an unknown smooth function of a set of covariates. Suppose that a prior on the response probability function is induced by a Gaussian process mapped to the unit interval through a link function. In this paper we study consistency of the resulting posterior distribution. If the covariance kernel has derivatives up to a desired order and the bandwidth parameter of the kernel is allowed to take arbitrarily small values, we show that the posterior distribution is consistent in the $L_1$-distance. As an auxiliary result to our proofs, we show that, under certain conditions, a Gaussian process assigns positive probabilities to the uniform neighborhoods of a continuous function. This result may be of independent interest in the literature for small ball probabilities of Gaussian processes.\n Comments: Published at this http URL in the Annals of Statistics (this http URL) by the Institute of Mathematical Statistics (this http URL)\n Subjects: Statistics Theory (math.ST) \n MSC classes: 62G08, 62G20 (Primary) \n Report number: IMS-AOS-AOS0197 \n Cite as: arXiv:math/0702686 [math.ST] \n (or arXiv:math/0702686v1 [math.ST] for this version) \n https://doi.org/10.48550/arXiv.math/0702686 \n Focus to learn more \n arXiv-issued DOI via DataCite \n Journal reference: Annals of Statistics 2006, Vol. 34, No. 5, 2413-2429 \n https://doi.org/10.1214/009053606000000795 \n Related DOI: Focus to learn more \n DOI(s) linking to related resources \n \n\n Submission history\n\n From: Anindya Roy [view email] [via VTEX proxy]\n [v1] Fri, 23 Feb 2007 10:25:53 UTC (74 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * PostScript\n * Other formats\n (license)\n Current browse context:\n math\n < prev | next >\n new | recent | 0702\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs and how to get involved.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Posterior consistency of Gaussian process prior for nonparametric binary regression",
          "cleaned_query": "Posterior consistency of Gaussian process prior for nonparametric binary regression"
        }
      ],
      "generated_ideas": [
        "1. **Adaptive-Threshold \u201cGood-Enough\u201d GP Bandits**\n   Develop a GP bandit algorithm that *learns* the satisfaction threshold online (instead of assuming it known), by placing a prior over the threshold and updating it jointly with the GP posterior. Prove lenient-regret and sample-complexity guarantees that interpolate between good-action identification (Paper 5) and classical GP-UCB regret (Paper 1), with bounds expressed via information gain (Papers 1,3).",
        "**Instance-Dependent Lower Bounds for Multi-D Mat\u00e9rn GP Bandits via Local Smoothness**\n   Extend the 1D tight lower-bound machinery (Paper 2) to higher dimensions for Mat\u00e9rn RKHS objectives by constructing \u201cneedle-in-a-haystack\u201d families whose difficulty depends on local H\u00f6lder/Matern smoothness and near-optimal set volume. Provide matching (up to logs) upper/lower bounds for cumulative regret that explicitly depend on \u03bd and d, sharpening the remaining gaps beyond eigenvalue-based global bounds (Paper 3).",
        "**Kernel-Eigenvalue-Aware Exploration Scheduling for GP-UCB/TS**\n   Design a practical GP-UCB/GP-TS variant that estimates the leading kernel spectrum online (e.g., via randomized Nystr\u00f6m features) and sets exploration parameters using empirical eigen-decay rather than worst-case \u03b3\\_T bounds. The contribution is a regret analysis where the bound depends on *observed* spectral decay (Paper 3) and a corresponding implementation that automatically adapts between SE-like and Mat\u00e9rn-like regimes.",
        "**\u03c0-GP-UCB for Heteroscedastic and Heavy-Tailed Noise**\n   Generalize \u03c0-GP-UCB (Paper 4) to settings where noise variance depends on the action and/or noise is sub-exponential (e.g., Student-t). Modify confidence intervals using robust estimators or noise-GP modeling, and prove sublinear regret in Mat\u00e9rn RKHS with explicit dependence on heteroscedasticity, maintaining the scalability advantages emphasized in Paper 4.",
        "**Posterior-Consistent GP Bandits for Binary Feedback Optimization**\n   Combine GP posterior consistency for binary regression under link functions (Paper 6) with bandit optimization: observe Bernoulli outcomes with success probability \u03c3(f(x)) and aim to maximize f (or success probability). Create a GP-UCB-style algorithm using appropriate concentration for the linked model and prove regret/good-action identification guarantees leveraging consistency and small-ball arguments (Paper 6) plus information-gain-style complexity (Papers 1,3).",
        "**Information-Gain-Optimal Experimental Design for Good-Action Identification**\n   Recast good-action identification (Paper 5) as a constrained experimental design problem: choose points to maximize mutual information about the *superlevel set* {x : f(x) \u2265 \u03c4}. Propose an acquisition function approximating this set-information objective and prove sample-complexity bounds in terms of a *thresholded* information gain (a refinement of \u03b3\\_T from Papers 1,3) that can be substantially smaller than standard \u03b3\\_T.",
        "**Near-Optimal Regret for Mat\u00e9rn \u03bd\u2208(1,2] via Multi-Scale Discretization + \u03c0-Updates**\n   Address the difficult smoothness regime where \u03bd is just above 1 (Paper 4 ensures sublinear regret but may be loose) by combining multi-scale discretizations (zooming) with \u03c0-GP-UCB\u2019s computational approach. Provide improved regret rates (or tighter constants) that reflect limited differentiability, and validate on synthetic Mat\u00e9rn bandits where existing GP-UCB variants are overly conservative.",
        "**Two-Phase \u201cSatisficing then Refining\u201d Algorithms with Unified Guarantees**\n   Create an algorithm that first targets lenient regret (quickly finds any \u03c4-good point) and then switches to classical optimization to approach the maximum, with an automatic stopping/switching rule based on posterior mass above \u03c4. Prove a combined guarantee: fast time-to-good-action (Paper 5) plus asymptotically near-optimal cumulative regret (Papers 1,2,3), clarifying the trade-off between satisficing and optimizing.",
        "**Domain-Shape and Boundary-Effect Corrections in \u03b3\\_T for GP Bandits**\n   Extend eigenvalue-based information gain bounds (Paper 3) beyond the standard [0,1]^d cube to irregular domains/manifolds and quantify boundary effects on kernel spectra. Derive regret bounds that depend on geometric quantities (e.g., volume, reach, Laplacian spectrum) and test whether Mat\u00e9rn-bandit performance predictions improve on real spatial domains (sensor placement motivation from Paper 1).",
        "**Action Elimination with Certified RKHS-Norm Estimation**\n   Develop an elimination-style GP bandit algorithm (related to Paper 5) that jointly estimates an unknown RKHS norm bound B (frequentist setting of Paper 1) from data and uses it to tighten confidence sets, reducing over-exploration. Provide regret guarantees with a data-dependent B\u0302 and show empirically that this norm-adaptation closes gaps between theory-driven exploration and \u03c0-GP-UCB scalability (Paper 4)."
      ],
      "generated_ideas_raw": "1. **Adaptive-Threshold \u201cGood-Enough\u201d GP Bandits**\n   Develop a GP bandit algorithm that *learns* the satisfaction threshold online (instead of assuming it known), by placing a prior over the threshold and updating it jointly with the GP posterior. Prove lenient-regret and sample-complexity guarantees that interpolate between good-action identification (Paper 5) and classical GP-UCB regret (Paper 1), with bounds expressed via information gain (Papers 1,3).\n\n2. **Instance-Dependent Lower Bounds for Multi-D Mat\u00e9rn GP Bandits via Local Smoothness**\n   Extend the 1D tight lower-bound machinery (Paper 2) to higher dimensions for Mat\u00e9rn RKHS objectives by constructing \u201cneedle-in-a-haystack\u201d families whose difficulty depends on local H\u00f6lder/Matern smoothness and near-optimal set volume. Provide matching (up to logs) upper/lower bounds for cumulative regret that explicitly depend on \u03bd and d, sharpening the remaining gaps beyond eigenvalue-based global bounds (Paper 3).\n\n3. **Kernel-Eigenvalue-Aware Exploration Scheduling for GP-UCB/TS**\n   Design a practical GP-UCB/GP-TS variant that estimates the leading kernel spectrum online (e.g., via randomized Nystr\u00f6m features) and sets exploration parameters using empirical eigen-decay rather than worst-case \u03b3\\_T bounds. The contribution is a regret analysis where the bound depends on *observed* spectral decay (Paper 3) and a corresponding implementation that automatically adapts between SE-like and Mat\u00e9rn-like regimes.\n\n4. **\u03c0-GP-UCB for Heteroscedastic and Heavy-Tailed Noise**\n   Generalize \u03c0-GP-UCB (Paper 4) to settings where noise variance depends on the action and/or noise is sub-exponential (e.g., Student-t). Modify confidence intervals using robust estimators or noise-GP modeling, and prove sublinear regret in Mat\u00e9rn RKHS with explicit dependence on heteroscedasticity, maintaining the scalability advantages emphasized in Paper 4.\n\n5. **Posterior-Consistent GP Bandits for Binary Feedback Optimization**\n   Combine GP posterior consistency for binary regression under link functions (Paper 6) with bandit optimization: observe Bernoulli outcomes with success probability \u03c3(f(x)) and aim to maximize f (or success probability). Create a GP-UCB-style algorithm using appropriate concentration for the linked model and prove regret/good-action identification guarantees leveraging consistency and small-ball arguments (Paper 6) plus information-gain-style complexity (Papers 1,3).\n\n6. **Information-Gain-Optimal Experimental Design for Good-Action Identification**\n   Recast good-action identification (Paper 5) as a constrained experimental design problem: choose points to maximize mutual information about the *superlevel set* {x : f(x) \u2265 \u03c4}. Propose an acquisition function approximating this set-information objective and prove sample-complexity bounds in terms of a *thresholded* information gain (a refinement of \u03b3\\_T from Papers 1,3) that can be substantially smaller than standard \u03b3\\_T.\n\n7. **Near-Optimal Regret for Mat\u00e9rn \u03bd\u2208(1,2] via Multi-Scale Discretization + \u03c0-Updates**\n   Address the difficult smoothness regime where \u03bd is just above 1 (Paper 4 ensures sublinear regret but may be loose) by combining multi-scale discretizations (zooming) with \u03c0-GP-UCB\u2019s computational approach. Provide improved regret rates (or tighter constants) that reflect limited differentiability, and validate on synthetic Mat\u00e9rn bandits where existing GP-UCB variants are overly conservative.\n\n8. **Two-Phase \u201cSatisficing then Refining\u201d Algorithms with Unified Guarantees**\n   Create an algorithm that first targets lenient regret (quickly finds any \u03c4-good point) and then switches to classical optimization to approach the maximum, with an automatic stopping/switching rule based on posterior mass above \u03c4. Prove a combined guarantee: fast time-to-good-action (Paper 5) plus asymptotically near-optimal cumulative regret (Papers 1,2,3), clarifying the trade-off between satisficing and optimizing.\n\n9. **Domain-Shape and Boundary-Effect Corrections in \u03b3\\_T for GP Bandits**\n   Extend eigenvalue-based information gain bounds (Paper 3) beyond the standard [0,1]^d cube to irregular domains/manifolds and quantify boundary effects on kernel spectra. Derive regret bounds that depend on geometric quantities (e.g., volume, reach, Laplacian spectrum) and test whether Mat\u00e9rn-bandit performance predictions improve on real spatial domains (sensor placement motivation from Paper 1).\n\n10. **Action Elimination with Certified RKHS-Norm Estimation**\n   Develop an elimination-style GP bandit algorithm (related to Paper 5) that jointly estimates an unknown RKHS norm bound B (frequentist setting of Paper 1) from data and uses it to tighten confidence sets, reducing over-exploration. Provide regret guarantees with a data-dependent B\u0302 and show empirically that this norm-adaptation closes gaps between theory-driven exploration and \u03c0-GP-UCB scalability (Paper 4).",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "1. **Adaptive-Threshold \u201cGood-Enough\u201d GP Bandits**\n   Develop a GP bandit algorithm that *learns* the satisfaction threshold online (instead of assuming it known), by placing a prior over the threshol",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "**Instance-Dependent Lower Bounds for Multi-D Mat\u00e9rn GP Bandits via Local Smoothness**\n   Extend the 1D tight lower-bound machinery (Paper 2) to higher dimensions for Mat\u00e9rn RKHS objectives by constru",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "**Kernel-Eigenvalue-Aware Exploration Scheduling for GP-UCB/TS**\n   Design a practical GP-UCB/GP-TS variant that estimates the leading kernel spectrum online (e.g., via randomized Nystr\u00f6m features) an",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "**\u03c0-GP-UCB for Heteroscedastic and Heavy-Tailed Noise**\n   Generalize \u03c0-GP-UCB (Paper 4) to settings where noise variance depends on the action and/or noise is sub-exponential (e.g., Student-t). Modif",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "**Posterior-Consistent GP Bandits for Binary Feedback Optimization**\n   Combine GP posterior consistency for binary regression under link functions (Paper 6) with bandit optimization: observe Bernoull",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "**Information-Gain-Optimal Experimental Design for Good-Action Identification**\n   Recast good-action identification (Paper 5) as a constrained experimental design problem: choose points to maximize m",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "**Near-Optimal Regret for Mat\u00e9rn \u03bd\u2208(1,2] via Multi-Scale Discretization + \u03c0-Updates**\n   Address the difficult smoothness regime where \u03bd is just above 1 (Paper 4 ensures sublinear regret but may be lo",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "**Two-Phase \u201cSatisficing then Refining\u201d Algorithms with Unified Guarantees**\n   Create an algorithm that first targets lenient regret (quickly finds any \u03c4-good point) and then switches to classical op",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "**Domain-Shape and Boundary-Effect Corrections in \u03b3\\_T for GP Bandits**\n   Extend eigenvalue-based information gain bounds (Paper 3) beyond the standard [0,1]^d cube to irregular domains/manifolds and",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "**Action Elimination with Certified RKHS-Norm Estimation**\n   Develop an elimination-style GP bandit algorithm (related to Paper 5) that jointly estimates an unknown RKHS norm bound B (frequentist set",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 29,
      "paper_title": "Auto-Compressing Networks",
      "contribution": "Introduce a novel architecture (ACN) that replaces short residual connections with long additive feedforward connections to the output, inducing an architectural auto-compression dynamic that concentrates useful information into earlier layers during training and yields compact, more robust representations without sacrificing accuracy.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 13033,
      "output_tokens": 963,
      "predecessor_details": [
        {
          "success": true,
          "title": "[1512.03385] Deep Residual Learning for Image Recognition - arXiv",
          "url": "https://arxiv.org/abs/1512.03385",
          "content": "[1512.03385] Deep Residual Learning for Image Recognition\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1512.03385\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:1512.03385**(cs)\n[Submitted on 10 Dec 2015]\n# Title:Deep Residual Learning for Image Recognition\nAuthors:[Kaiming He](https://arxiv.org/search/cs?searchtype=author&amp;query=He,+K),[Xiangyu Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X),[Shaoqing Ren](https://arxiv.org/search/cs?searchtype=author&amp;query=Ren,+S),[Jian Sun](https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+J)\nView a PDF of the paper titled Deep Residual Learning for Image Recognition, by Kaiming He and 3 other authors\n[View PDF](https://arxiv.org/pdf/1512.03385)> > Abstract:\n> Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\n> The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. Comments:|Tech report|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:1512.03385](https://arxiv.org/abs/1512.03385)[cs.CV]|\n|(or[arXiv:1512.03385v1](https://arxiv.org/abs/1512.03385v1)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.1512.03385](https://doi.org/10.48550/arXiv.1512.03385)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Kaiming He [[view email](https://arxiv.org/show-email/aa803c8a/1512.03385)]\n**[v1]**Thu, 10 Dec 2015 19:51:55 UTC (494 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Deep Residual Learning for Image Recognition, by Kaiming He and 3 other authors\n* [View PDF](https://arxiv.org/pdf/1512.03385)\n* [TeX Source](https://arxiv.org/src/1512.03385)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1512.03385&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1512.03385&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2015-12](https://arxiv.org/list/cs.CV/2015-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/1512.03385?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1512.03385)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1512.03385)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1512.03385)\n### [107 blog links](https://arxiv.org/tb/1512.03385)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1512.html#HeZRS15)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/HeZRS15)\n[Kaiming He]()\n[Xiangyu Zhang]()\n[Shaoqing Ren]()\n[Jian Sun]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1512.03385)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Deep residual learning for image recognition",
          "cleaned_query": "Deep residual learning for image recognition"
        },
        {
          "success": true,
          "title": "Highway Networks for Improved Surface Reconstruction: The Role of Residuals and Weight Updates",
          "url": "https://arxiv.org/abs/2407.08134",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2407.08134** (cs)\n\n\\[Submitted on 11 Jul 2024\\]\n\n# Title:Highway Networks for Improved Surface Reconstruction: The Role of Residuals and Weight Updates\n\nAuthors: [A. Noorizadegan](https://arxiv.org/search/cs?searchtype=author&query=Noorizadegan,+A), [Y.C. Hon](https://arxiv.org/search/cs?searchtype=author&query=Hon,+Y), [D.L. Young](https://arxiv.org/search/cs?searchtype=author&query=Young,+D), [C.S. Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+C)\n\nView a PDF of the paper titled Highway Networks for Improved Surface Reconstruction: The Role of Residuals and Weight Updates, by A. Noorizadegan and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2407.08134) [HTML (experimental)](https://arxiv.org/html/2407.08134v1)\n\n> Abstract:Surface reconstruction from point clouds is a fundamental challenge in computer graphics and medical imaging. In this paper, we explore the application of advanced neural network architectures for the accurate and efficient reconstruction of surfaces from data points. We introduce a novel variant of the Highway network (Hw) called Square-Highway (SqrHw) within the context of multilayer perceptrons and investigate its performance alongside plain neural networks and a simplified Hw in various numerical examples. These examples include the reconstruction of simple and complex surfaces, such as spheres, human hands, and intricate models like the Stanford Bunny. We analyze the impact of factors such as the number of hidden layers, interior and exterior points, and data distribution on surface reconstruction quality. Our results show that the proposed SqrHw architecture outperforms other neural network configurations, achieving faster convergence and higher-quality surface reconstructions. Additionally, we demonstrate the SqrHw's ability to predict surfaces over missing data, a valuable feature for challenging applications like medical imaging. Furthermore, our study delves into further details, demonstrating that the proposed method based on highway networks yields more stable weight norms and backpropagation gradients compared to the Plain Network architecture. This research not only advances the field of computer graphics but also holds utility for other purposes such as function interpolation and physics-informed neural networks, which integrate multilayer perceptrons into their algorithms.\n\n| | |\n| --- | --- |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2407.08134](https://arxiv.org/abs/2407.08134) \\[cs.CV\\] |\n| (or [arXiv:2407.08134v1](https://arxiv.org/abs/2407.08134v1) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2407.08134](https://doi.org/10.48550/arXiv.2407.08134) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Amir Noorizadegan Ph.D. \\[ [view email](https://arxiv.org/show-email/3da0f25e/2407.08134)\\] **\\[v1\\]**\nThu, 11 Jul 2024 02:15:21 UTC (6,615 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Highway Networks for Improved Surface Reconstruction: The Role of Residuals and Weight Updates, by A. Noorizadegan and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2407.08134)\n- [HTML (experimental)](https://arxiv.org/html/2407.08134v1)\n- [TeX Source](https://arxiv.org/src/2407.08134)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2407.08134&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2407.08134&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-07](https://arxiv.org/list/cs.CV/2024-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2407.08134?context=cs) [cs.AI](https://arxiv.org/abs/2407.08134?context=cs.AI) [cs.LG](https://arxiv.org/abs/2407.08134?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2407.08134)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2407.08134)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2407.08134)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2407.08134) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Highway Networks",
          "cleaned_query": "Highway Networks"
        },
        {
          "success": true,
          "title": "Residual Networks Behave Like Ensembles of Relatively Shallow ...",
          "url": "https://arxiv.org/abs/1605.06431",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:1605.06431** (cs)\n\n\\[Submitted on 20 May 2016 ( [v1](https://arxiv.org/abs/1605.06431v1)), last revised 27 Oct 2016 (this version, v2)\\]\n\n# Title:Residual Networks Behave Like Ensembles of Relatively Shallow Networks\n\nAuthors: [Andreas Veit](https://arxiv.org/search/cs?searchtype=author&query=Veit,+A), [Michael Wilber](https://arxiv.org/search/cs?searchtype=author&query=Wilber,+M), [Serge Belongie](https://arxiv.org/search/cs?searchtype=author&query=Belongie,+S)\n\nView a PDF of the paper titled Residual Networks Behave Like Ensembles of Relatively Shallow Networks, by Andreas Veit and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/1605.06431)\n\n> Abstract:In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.\n\n| | |\n| --- | --- |\n| Comments: | NIPS 2016 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |\n| Cite as: | [arXiv:1605.06431](https://arxiv.org/abs/1605.06431) \\[cs.CV\\] |\n| | (or [arXiv:1605.06431v2](https://arxiv.org/abs/1605.06431v2) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1605.06431](https://doi.org/10.48550/arXiv.1605.06431) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Andreas Veit \\[ [view email](https://arxiv.org/show-email/91b4ec0e/1605.06431)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1605.06431v1)**\nFri, 20 May 2016 16:44:03 UTC (139 KB)\n\n**\\[v2\\]**\nThu, 27 Oct 2016 00:43:58 UTC (348 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Residual Networks Behave Like Ensembles of Relatively Shallow Networks, by Andreas Veit and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/1605.06431)\n- [TeX Source](https://arxiv.org/src/1605.06431)\n- [Other Formats](https://arxiv.org/format/1605.06431)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1605.06431&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1605.06431&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2016-05](https://arxiv.org/list/cs.CV/2016-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1605.06431?context=cs)\n\n[cs.AI](https://arxiv.org/abs/1605.06431?context=cs.AI)\n\n[cs.LG](https://arxiv.org/abs/1605.06431?context=cs.LG)\n\n[cs.NE](https://arxiv.org/abs/1605.06431?context=cs.NE)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1605.06431)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1605.06431)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1605.06431)\n\n### [5 blog links](https://arxiv.org/tb/1605.06431)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1605.html#VeitWB16) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/VeitWB16)\n\n[Andreas Veit](https://dblp.uni-trier.de/search/author?author=Andreas%20Veit)\n\n[Michael J. Wilber](https://dblp.uni-trier.de/search/author?author=Michael%20J.%20Wilber)\n\n[Serge J. Belongie](https://dblp.uni-trier.de/search/author?author=Serge%20J.%20Belongie)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1605.06431&description=Residual Networks Behave Like Ensembles of Relatively Shallow Networks) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1605.06431&title=Residual Networks Behave Like Ensembles of Relatively Shallow Networks)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1605.06431) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Residual networks behave like ensembles of relatively shallow networks",
          "cleaned_query": "Residual networks behave like ensembles of relatively shallow networks"
        },
        {
          "success": true,
          "title": "Understanding intermediate layers using linear classifier probes",
          "url": "https://arxiv.org/abs/1610.01644",
          "content": "[1610.01644] Understanding intermediate layers using linear classifier probes\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nIn just 5 minutes help us improve arXiv:\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:1610.01644\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:1610.01644**(stat)\n[Submitted on 5 Oct 2016 ([v1](https://arxiv.org/abs/1610.01644v1)), last revised 22 Nov 2018 (this version, v4)]\n# Title:Understanding intermediate layers using linear classifier probes\nAuthors:[Guillaume Alain](https://arxiv.org/search/stat?searchtype=author&amp;query=Alain,+G),[Yoshua Bengio](https://arxiv.org/search/stat?searchtype=author&amp;query=Bengio,+Y)\nView a PDF of the paper titled Understanding intermediate layers using linear classifier probes, by Guillaume Alain and Yoshua Bengio\n[View PDF](https://arxiv.org/pdf/1610.01644)> > Abstract:\n> Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as &#34;probes&#34;, trained entirely independently of the model itself.\n> This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems.\n> We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model. Subjects:|Machine Learning (stat.ML); Machine Learning (cs.LG)|\nCite as:|[arXiv:1610.01644](https://arxiv.org/abs/1610.01644)[stat.ML]|\n|(or[arXiv:1610.01644v4](https://arxiv.org/abs/1610.01644v4)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.1610.01644](https://doi.org/10.48550/arXiv.1610.01644)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Guillaume Alain [[view email](https://arxiv.org/show-email/c4f40cfa/1610.01644)]\n**[[v1]](https://arxiv.org/abs/1610.01644v1)**Wed, 5 Oct 2016 20:59:01 UTC (4,314 KB)\n**[[v2]](https://arxiv.org/abs/1610.01644v2)**Mon, 10 Oct 2016 02:33:57 UTC (10,356 KB)\n**[[v3]](https://arxiv.org/abs/1610.01644v3)**Fri, 14 Oct 2016 18:47:19 UTC (4,453 KB)\n**[v4]**Thu, 22 Nov 2018 23:40:00 UTC (4,238 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Understanding intermediate layers using linear classifier probes, by Guillaume Alain and Yoshua Bengio\n* [View PDF](https://arxiv.org/pdf/1610.01644)\n* [TeX Source](https://arxiv.org/src/1610.01644)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1610.01644&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1610.01644&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2016-10](https://arxiv.org/list/stat.ML/2016-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/1610.01644?context=cs)\n[cs.LG](https://arxiv.org/abs/1610.01644?context=cs.LG)\n[stat](https://arxiv.org/abs/1610.01644?context=stat)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1610.01644)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1610.01644)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1610.01644)\n### [1 blog link](https://arxiv.org/tb/1610.01644)\n([what is this?](https://info.arxiv.org/help/trackback.html))\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1610.01644)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Understanding intermediate layers using linear classifier probes",
          "cleaned_query": "Understanding intermediate layers using linear classifier probes"
        },
        {
          "success": true,
          "title": "(PDF) Deep Networks with Stochastic Depth - ResearchGate",
          "url": "https://www.researchgate.net/publication/308278012_Deep_Networks_with_Stochastic_Depth",
          "content": "(PDF) Deep Networks with Stochastic Depth\n* [Home](directory/publications)\n* [Probability Theory](topic/Probability-Theory/publications)\n* [Stochastic](topic/Stochastic/publications)\nConference PaperPDF Available\n# Deep Networks with Stochastic Depth\n* October 2016\n* [Lecture Notes in Computer Science](journal/Lecture-Notes-in-Computer-Science-0302-9743)9908:646-661\nDOI:[10.1007/978-3-319-46493-0\\_39](https://doi.org/10.1007/978-3-319-46493-0_39)\n* Conference: European Conference on Computer Vision\nAuthors:\n[](profile/Gao-Huang)\n[Gao Huang](profile/Gao-Huang)\n* [Tsinghua University](https://www.researchgate.net/institution/Tsinghua-University)\n[](scientific-contributions/Yu-Sun-2108180850)\n[Yu Sun](scientific-contributions/Yu-Sun-2108180850)\n[Yu Sun](scientific-contributions/Yu-Sun-2108180850)\n* This person is not on ResearchGate, or hasn't claimed this research yet.\n[](profile/Zhuang-Liu)\n[Zhuang Liu](profile/Zhuang-Liu)\n* [Tsinghua University](https://www.researchgate.net/institution/Tsinghua-University)\n[](scientific-contributions/Daniel-Sedra-2108159175)\n[Daniel Sedra](scientific-contributions/Daniel-Sedra-2108159175)\n[Daniel Sedra](scientific-contributions/Daniel-Sedra-2108159175)\n* This person is not on ResearchGate, or hasn't claimed this research yet.\nShow all 5 authorsHide\n[Download full-text PDF](profile/Gao-Huang/publication/308278012_Deep_Networks_with_Stochastic_Depth/links/5a7470d8aca2720bc0ddd3df/Deep-Networks-with-Stochastic-Depth.pdf)[Read full-text](publication/308278012_Deep_Networks_with_Stochastic_Depth#read)\n[Download full-text PDF](https://www.researchgate.net/profile/Gao-Huang/publication/308278012_Deep_Networks_with_Stochastic_Depth/links/5a7470d8aca2720bc0ddd3df/Deep-Networks-with-Stochastic-Depth.pdf)\n[Read full-text](publication/308278012_Deep_Networks_with_Stochastic_Depth#read)\n[Download citation](https://www.researchgate.net/publication/308278012_Deep_Networks_with_Stochastic_Depth/citation/download)\nCopy linkLink copied\n[\nRead full-text\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#read)[\nDownload citation\n](https://www.researchgate.net/publication/308278012_Deep_Networks_with_Stochastic_Depth/citation/download)\nCopy linkLink copied\n## Abstract and Figures\nVery deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91 % on CIFAR-10).\n[\n](https://www.researchgate.net/figure/The-first-convolutional-layers-mean-gradient-magnitude-for-each-epoch-during-training_fig7_308278012)\n[\nThe first convolutional layer\u2019s mean gradient magnitude for each epoch during training. The vertical dotted lines indicate scheduled reductions in learning rate by a factor of 10, which cause gradients to shrink.\n\u2026](https://www.researchgate.net/figure/The-first-convolutional-layers-mean-gradient-magnitude-for-each-epoch-during-training_fig7_308278012)\nFigures - uploaded by[Gao Huang](profile/Gao-Huang)\nAuthor content\nAll figure content in this area was uploaded by Gao Huang\nContent may be subject to copyright.\n**Discover the world's research**\n* 25+ million members\n* 160+ million publication pages\n* 2.3+ billion citations[Join for free](signup.SignUp.html)\n[](publication/308278012_Deep_Networks_with_Stochastic_Depth#read-preview)\nContent uploaded by[Gao Huang](profile/Gao-Huang)\nAuthor content\nAll content in this area was uploaded by Gao Huang on Feb 02, 2018\nContent may be subject to copyright.\nDeep Networks with Stochastic Depth\nGao Huang1(B\n), Yu Sun1,ZhuangLiu\n2, Daniel Sedra1,\nand Kilian Q. Weinberger1\n1Cornell University, Ithaca, USA\n{gh349,ys646,dms422,kqw4}@cornell.edu\n2Tsinghua University, Beijing, China\nliuzhuang13@mails.tsinghua.edu.cn\nAbstract.Verydeep convolutional networks with hundreds of layers\nhave led to signi\ufb01cant reductions in error on competitive benchmarks.\nAlthough the unmatched expressiveness of the many layers can be highly\ndesirable at test time, training very deep networks comes with its own\nset of challenges. The gradients can vanish, the forward \ufb02ow often dimin-\nishes, and the training time can be painfully slow. To addressthese prob-\nlems, we proposestochastic depth, a training procedure that enables the\nseemingly contradictory setup totrain shortnetworks anduse deepnet-\nworks at test time. We start with very deep networks but during train-\ning, for each mini-batch, randomly drop a subset of layers and bypass\nthem with the identity function. This simple approach complements the\nrecent success of residual networks. It reduces training time substantially\nand improves the test error signi\ufb01cantly on almost all data sets that we\nused for evaluation. With stochastic depth we can increase the depth\nof residual networks even beyond 1200 layers and still yield meaningful\nimprovements in test error (4.91% on CIFAR-10).\n1 Introduction\nConvolutional Neural Networks (CNNs) were arguably popularized within the\nvision community in 2009 through AlexNet [1] and its celebrated victory at the\nImageNet competition [2]. Since then there has been a notable shift towards\nCNNs in many areas of computer vision [3\u20138]. As this shift unfolds, a second\ntrend emerges; deeper and deeper CNN architectures are being developed and\ntrained. Whereas AlexNet had 5 convolutional layers [1], the VGG network and\nGoogLeNet in 2014 had 19 and 22 layers respectively [5,7], and most recently\nthe ResNet architecture featured 152 layers [8].\nNetwork depth is a major determinant of model expressiveness, both in the-\nory [9,10] and in practice [5,7,8]. However, very deep models also introduce new\nchallenges: vanishing gradients in backward propagation, diminishing feature\nreuse in forward propagation, and long training time.\nG. Huang and Y. Sun are contributed equally.\nc\n\ue002Springer International Publishing AG 2016\nB. Leibe et al. (Eds.): ECCV 2016, Part IV, LNCS 9908, pp. 646\u2013661, 2016.\nDOI: 10.1007/978-3-319-46493-039\n[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)[\n](publication/308278012_Deep_Networks_with_Stochastic_Depth#pfe)\nDeep Networks with Stochastic Depth647\nVanishing Gradientsis a well known nuisance in neural networks with many\nlayers [11]. As the gradient information is back-propagated, repeated multipli-\ncation or convolution with small weights renders the gradient information inef-\nfectively smal",
          "original_query": "Deep networks with stochastic depth",
          "cleaned_query": "Deep networks with stochastic depth"
        },
        {
          "success": true,
          "title": "[1609.01596] Direct Feedback Alignment Provides Learning in Deep ...",
          "url": "https://arxiv.org/abs/1609.01596",
          "content": "# Statistics > Machine Learning\n\n**arXiv:1609.01596** (stat)\n\n\\[Submitted on 6 Sep 2016 ( [v1](https://arxiv.org/abs/1609.01596v1)), last revised 21 Dec 2016 (this version, v5)\\]\n\n# Title:Direct Feedback Alignment Provides Learning in Deep Neural Networks\n\nAuthors: [Arild N\u00f8kland](https://arxiv.org/search/stat?searchtype=author&query=N%C3%B8kland,+A)\n\nView a PDF of the paper titled Direct Feedback Alignment Provides Learning in Deep Neural Networks, by Arild N{\\\\o}kland\n\n[View PDF](https://arxiv.org/pdf/1609.01596)\n\n> Abstract:Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task.\n\n| | |\n| --- | --- |\n| Comments: | Accepted for publication at NIPS 2016. \\[v2\\] Corrected convolutional results for feedback-alignment. \\[v3,v4,v5\\] Corrected theorem and proof |\n| Subjects: | Machine Learning (stat.ML); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:1609.01596](https://arxiv.org/abs/1609.01596) \\[stat.ML\\] |\n| | (or [arXiv:1609.01596v5](https://arxiv.org/abs/1609.01596v5) \\[stat.ML\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1609.01596](https://doi.org/10.48550/arXiv.1609.01596) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Arild N\u00f8kland \\[ [view email](https://arxiv.org/show-email/dd9ff0c1/1609.01596)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1609.01596v1)**\nTue, 6 Sep 2016 15:07:32 UTC (1,691 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1609.01596v2)**\nSat, 24 Sep 2016 10:12:35 UTC (1,692 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/1609.01596v3)**\nThu, 6 Oct 2016 16:19:16 UTC (1,692 KB)\n\n**[\\[v4\\]](https://arxiv.org/abs/1609.01596v4)**\nSun, 23 Oct 2016 18:14:54 UTC (1,692 KB)\n\n**\\[v5\\]**\nWed, 21 Dec 2016 16:36:40 UTC (1,692 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Direct Feedback Alignment Provides Learning in Deep Neural Networks, by Arild N{\\\\o}kland\n\n- [View PDF](https://arxiv.org/pdf/1609.01596)\n- [TeX Source](https://arxiv.org/src/1609.01596)\n- [Other Formats](https://arxiv.org/format/1609.01596)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1609.01596&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1609.01596&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2016-09](https://arxiv.org/list/stat.ML/2016-09)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1609.01596?context=cs)\n\n[cs.LG](https://arxiv.org/abs/1609.01596?context=cs.LG)\n\n[stat](https://arxiv.org/abs/1609.01596?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1609.01596)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1609.01596)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1609.01596)\n\n### [1 blog link](https://arxiv.org/tb/1609.01596)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1609.01596&description=Direct Feedback Alignment Provides Learning in Deep Neural Networks) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1609.01596&title=Direct Feedback Alignment Provides Learning in Deep Neural Networks)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1609.01596) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Direct feedback alignment provides learning in deep neural networks",
          "cleaned_query": "Direct feedback alignment provides learning in deep neural networks"
        }
      ],
      "generated_ideas": [
        "Probe-Guided Adaptive Stochastic Depth for Residual Networks\nTrain ResNets with stochastic depth where each block\u2019s drop probability is updated online using linear probe accuracy gains between adjacent stages. The key contribution is a closed-loop training rule that prunes \u201cnon-contributing\u201d depth early while preserving blocks that measurably increase linear separability, improving training speed and accuracy/compute trade-offs.",
        "Path-Length Regularization to Promote Useful Long-Path Learning in ResNets\nBuilding on the \u201cResNets as ensembles of paths\u201d view, introduce a regularizer that explicitly increases gradient contribution from longer paths (e.g., by penalizing vanishing path-wise Jacobians or reweighting residual branches by path length). This would test whether long paths can be made consistently useful (not just short ones), potentially improving robustness and out-of-distribution generalization.",
        "Square-Highway Implicit Surface Networks for Signed Distance Function (SDF) Learning\nExtend Square-Highway (SqrHw) MLPs from point-set surface reconstruction to learning SDFs with Eikonal and normal-consistency losses. The contribution is a more stable, faster-converging implicit representation for complex geometry (hands/organs) that better tolerates missing regions, leveraging SqrHw\u2019s reported gradient/weight-norm stability.",
        "Probe-Based Diagnostics for Implicit Geometry Networks (Where Does Geometry Emerge?)\nAdapt linear classifier probes to implicit surface reconstruction pipelines by probing intermediate MLP activations for linear predictability of geometric attributes (inside/outside, curvature class, normal direction bins). This yields an actionable interpretability tool to pinpoint which layers encode global shape vs. fine detail, guiding architecture depth, skip placement, and sampling strategies.",
        "Hybrid Learning: Direct Feedback Alignment for Deep Residual CNNs with Skip-Local Updates\nImplement Direct Feedback Alignment (DFA) in modern residual architectures by attaching fixed random feedback matrices to residual stages and using skip connections to stabilize credit assignment. The key contribution is a biologically-motivated, backprop-free training recipe evaluated at scale (e.g., ImageNet subsets), analyzing when residual structure closes the performance gap vs. backprop.",
        "DFA for Implicit Surface Reconstruction with Highway Gating\nDevelop a DFA-trained SqrHw/Highway MLP for surface reconstruction where each hidden layer receives direct random feedback from reconstruction losses (point-to-surface distance, occupancy, SDF). This explores whether highway-style gating plus residual pathways make DFA viable for continuous coordinate networks, enabling more local learning and potentially better stability under sparse/noisy point clouds.",
        "Lesion-Driven Block Pruning and Distillation Using Path Independence in ResNets\nUse structured lesion studies (block removals) during training to estimate block/path independence, then prune blocks with minimal impact and distill the remaining subnetwork into a smaller model. The contribution is a principled compression pipeline grounded in the ensemble-of-paths interpretation, producing compact models without relying solely on magnitude pruning heuristics.",
        "Stability-Constrained Residual/Highway Design via Weight-Norm and Gradient-Norm Controllers\nGeneralize the SqrHw observation of stable weight norms by adding explicit controllers (constraints or adaptive penalties) that keep per-block weight norms and gradient norms within target ranges in ResNets and Highway/SqrHw MLPs. The contribution is an optimization-centric architecture/training method that aims to prevent exploding/vanishing updates in extremely deep networks (CNNs or coordinate MLPs).",
        "Data-Distribution-Aware Sampling Policies for Surface Reconstruction Using Probe Signals\nFor point-cloud-to-surface learning, dynamically choose interior/exterior samples and missing-region augmentation based on probe-measured separability of occupancy/SDF labels at intermediate layers. This produces an actionable sampling algorithm that focuses training on \u201ccurrently confusing\u201d spatial regions, improving reconstruction quality under non-uniform sampling and occlusions (noted as key factors in the SqrHw study).",
        "Multi-Objective Routing Across Residual Paths for Joint Classification + Reconstruction\nCreate a shared backbone with residual blocks whose gates route examples along different effective path lengths depending on task (classification vs. implicit reconstruction head), with routing decisions trained using probe-based auxiliary losses. The contribution is a concrete multitask architecture that exploits path diversity: short paths for semantics, longer paths for fine geometric detail, aiming for better shared representations and compute-efficient inference."
      ],
      "generated_ideas_raw": "1. **Probe-Guided Adaptive Stochastic Depth for Residual Networks**  \nTrain ResNets with stochastic depth where each block\u2019s drop probability is updated online using linear probe accuracy gains between adjacent stages. The key contribution is a closed-loop training rule that prunes \u201cnon-contributing\u201d depth early while preserving blocks that measurably increase linear separability, improving training speed and accuracy/compute trade-offs.\n\n2. **Path-Length Regularization to Promote Useful Long-Path Learning in ResNets**  \nBuilding on the \u201cResNets as ensembles of paths\u201d view, introduce a regularizer that explicitly increases gradient contribution from longer paths (e.g., by penalizing vanishing path-wise Jacobians or reweighting residual branches by path length). This would test whether long paths can be made consistently useful (not just short ones), potentially improving robustness and out-of-distribution generalization.\n\n3. **Square-Highway Implicit Surface Networks for Signed Distance Function (SDF) Learning**  \nExtend Square-Highway (SqrHw) MLPs from point-set surface reconstruction to learning SDFs with Eikonal and normal-consistency losses. The contribution is a more stable, faster-converging implicit representation for complex geometry (hands/organs) that better tolerates missing regions, leveraging SqrHw\u2019s reported gradient/weight-norm stability.\n\n4. **Probe-Based Diagnostics for Implicit Geometry Networks (Where Does Geometry Emerge?)**  \nAdapt linear classifier probes to implicit surface reconstruction pipelines by probing intermediate MLP activations for linear predictability of geometric attributes (inside/outside, curvature class, normal direction bins). This yields an actionable interpretability tool to pinpoint which layers encode global shape vs. fine detail, guiding architecture depth, skip placement, and sampling strategies.\n\n5. **Hybrid Learning: Direct Feedback Alignment for Deep Residual CNNs with Skip-Local Updates**  \nImplement Direct Feedback Alignment (DFA) in modern residual architectures by attaching fixed random feedback matrices to residual stages and using skip connections to stabilize credit assignment. The key contribution is a biologically-motivated, backprop-free training recipe evaluated at scale (e.g., ImageNet subsets), analyzing when residual structure closes the performance gap vs. backprop.\n\n6. **DFA for Implicit Surface Reconstruction with Highway Gating**  \nDevelop a DFA-trained SqrHw/Highway MLP for surface reconstruction where each hidden layer receives direct random feedback from reconstruction losses (point-to-surface distance, occupancy, SDF). This explores whether highway-style gating plus residual pathways make DFA viable for continuous coordinate networks, enabling more local learning and potentially better stability under sparse/noisy point clouds.\n\n7. **Lesion-Driven Block Pruning and Distillation Using Path Independence in ResNets**  \nUse structured lesion studies (block removals) during training to estimate block/path independence, then prune blocks with minimal impact and distill the remaining subnetwork into a smaller model. The contribution is a principled compression pipeline grounded in the ensemble-of-paths interpretation, producing compact models without relying solely on magnitude pruning heuristics.\n\n8. **Stability-Constrained Residual/Highway Design via Weight-Norm and Gradient-Norm Controllers**  \nGeneralize the SqrHw observation of stable weight norms by adding explicit controllers (constraints or adaptive penalties) that keep per-block weight norms and gradient norms within target ranges in ResNets and Highway/SqrHw MLPs. The contribution is an optimization-centric architecture/training method that aims to prevent exploding/vanishing updates in extremely deep networks (CNNs or coordinate MLPs).\n\n9. **Data-Distribution-Aware Sampling Policies for Surface Reconstruction Using Probe Signals**  \nFor point-cloud-to-surface learning, dynamically choose interior/exterior samples and missing-region augmentation based on probe-measured separability of occupancy/SDF labels at intermediate layers. This produces an actionable sampling algorithm that focuses training on \u201ccurrently confusing\u201d spatial regions, improving reconstruction quality under non-uniform sampling and occlusions (noted as key factors in the SqrHw study).\n\n10. **Multi-Objective Routing Across Residual Paths for Joint Classification + Reconstruction**  \nCreate a shared backbone with residual blocks whose gates route examples along different effective path lengths depending on task (classification vs. implicit reconstruction head), with routing decisions trained using probe-based auxiliary losses. The contribution is a concrete multitask architecture that exploits path diversity: short paths for semantics, longer paths for fine geometric detail, aiming for better shared representations and compute-efficient inference.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Probe-Guided Adaptive Stochastic Depth for Residual Networks\nTrain ResNets with stochastic depth where each block\u2019s drop probability is updated online using linear probe accuracy gains between adjacen",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Path-Length Regularization to Promote Useful Long-Path Learning in ResNets\nBuilding on the \u201cResNets as ensembles of paths\u201d view, introduce a regularizer that explicitly increases gradient contribution",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Square-Highway Implicit Surface Networks for Signed Distance Function (SDF) Learning\nExtend Square-Highway (SqrHw) MLPs from point-set surface reconstruction to learning SDFs with Eikonal and normal-c",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Probe-Based Diagnostics for Implicit Geometry Networks (Where Does Geometry Emerge?)\nAdapt linear classifier probes to implicit surface reconstruction pipelines by probing intermediate MLP activations",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Hybrid Learning: Direct Feedback Alignment for Deep Residual CNNs with Skip-Local Updates\nImplement Direct Feedback Alignment (DFA) in modern residual architectures by attaching fixed random feedback ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "DFA for Implicit Surface Reconstruction with Highway Gating\nDevelop a DFA-trained SqrHw/Highway MLP for surface reconstruction where each hidden layer receives direct random feedback from reconstructi",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Lesion-Driven Block Pruning and Distillation Using Path Independence in ResNets\nUse structured lesion studies (block removals) during training to estimate block/path independence, then prune blocks wi",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Stability-Constrained Residual/Highway Design via Weight-Norm and Gradient-Norm Controllers\nGeneralize the SqrHw observation of stable weight norms by adding explicit controllers (constraints or adapt",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Data-Distribution-Aware Sampling Policies for Surface Reconstruction Using Probe Signals\nFor point-cloud-to-surface learning, dynamically choose interior/exterior samples and missing-region augmentati",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Multi-Objective Routing Across Residual Paths for Joint Classification + Reconstruction\nCreate a shared backbone with residual blocks whose gates route examples along different effective path lengths ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 30,
      "paper_title": "MokA: Multimodal Low-Rank Adaptation for MLLMs",
      "contribution": "Introduce a multimodal-aware low-rank adaptation method (MokA) that decomposes adaptation into modality-specific unimodal compression and explicit cross-modal interaction, yielding efficient and effective fine-tuning for MLLMs.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 6,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12446,
      "output_tokens": 1046,
      "predecessor_details": [
        {
          "success": true,
          "title": "LoRA: Low-Rank Adaptation of Large Language Models - arXiv",
          "url": "https://arxiv.org/abs/2106.09685",
          "content": "[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2106.09685\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2106.09685**(cs)\n[Submitted on 17 Jun 2021 ([v1](https://arxiv.org/abs/2106.09685v1)), last revised 16 Oct 2021 (this version, v2)]\n# Title:LoRA: Low-Rank Adaptation of Large Language Models\nAuthors:[Edward J. Hu](https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+E+J),[Yelong Shen](https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+Y),[Phillip Wallis](https://arxiv.org/search/cs?searchtype=author&amp;query=Wallis,+P),[Zeyuan Allen-Zhu](https://arxiv.org/search/cs?searchtype=author&amp;query=Allen-Zhu,+Z),[Yuanzhi Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y),[Shean Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+S),[Lu Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+L),[Weizhu Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+W)\nView a PDF of the paper titled LoRA: Low-Rank Adaptation of Large Language Models, by Edward J. Hu and 7 other authors\n[View PDF](https://arxiv.org/pdf/2106.09685)> > Abstract:\n> An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at [> this https URL\n](https://github.com/microsoft/LoRA)> . Comments:|Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency|\nSubjects:|Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)|\nCite as:|[arXiv:2106.09685](https://arxiv.org/abs/2106.09685)[cs.CL]|\n|(or[arXiv:2106.09685v2](https://arxiv.org/abs/2106.09685v2)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2106.09685](https://doi.org/10.48550/arXiv.2106.09685)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Edward J. Hu [[view email](https://arxiv.org/show-email/e4479443/2106.09685)]\n**[[v1]](https://arxiv.org/abs/2106.09685v1)**Thu, 17 Jun 2021 17:37:18 UTC (1,791 KB)\n**[v2]**Sat, 16 Oct 2021 18:40:34 UTC (896 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled LoRA: Low-Rank Adaptation of Large Language Models, by Edward J. Hu and 7 other authors\n* [View PDF](https://arxiv.org/pdf/2106.09685)\n* [TeX Source](https://arxiv.org/src/2106.09685)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2106.09685&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2106.09685&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2021-06](https://arxiv.org/list/cs.CL/2021-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/2106.09685?context=cs)\n[cs.AI](https://arxiv.org/abs/2106.09685?context=cs.AI)\n[cs.LG](https://arxiv.org/abs/2106.09685?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2106.09685)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2106.09685)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2106.09685)\n### [12 blog links](https://arxiv.org/tb/2106.09685)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2106.html#abs-2106-09685)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2106-09685)\n[Yelong Shen]()\n[Phillip Wallis]()\n[Zeyuan Allen-Zhu]()\n[Yuanzhi Li]()\n[Weizhu Chen]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpac",
          "original_query": "Lora: Low-rank adaptation of large language models",
          "cleaned_query": "Lora: Low-rank adaptation of large language models"
        },
        {
          "success": true,
          "title": "MiniGPT-4: Enhancing Vision-Language Understanding ...",
          "url": "https://arxiv.org/abs/2304.10592",
          "content": "[2304.10592] MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2304.10592\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2304.10592**(cs)\n[Submitted on 20 Apr 2023 ([v1](https://arxiv.org/abs/2304.10592v1)), last revised 2 Oct 2023 (this version, v2)]\n# Title:MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models\nAuthors:[Deyao Zhu](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+D),[Jun Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+J),[Xiaoqian Shen](https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+X),[Xiang Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+X),[Mohamed Elhoseiny](https://arxiv.org/search/cs?searchtype=author&amp;query=Elhoseiny,+M)\nView a PDF of the paper titled MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models, by Deyao Zhu and 4 other authors\n[View PDF](https://arxiv.org/pdf/2304.10592)> > Abstract:\n> The recent GPT-4 has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind GPT-4 continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of GPT-4 stem from the utilization of sophisticated large language models (LLM). To examine this phenomenon, we present MiniGPT-4, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer. Our work, for the first time, uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by GPT-4, such as detailed image description generation and website creation from hand-drawn drafts. Furthermore, we also observe other emerging capabilities in MiniGPT-4, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model&#39;s generation reliability and overall usability. Our code, pre-trained model, and collected dataset are available at [> this https URL\n](https://minigpt-4.github.io/)> . Comments:|Project Website:[this https URL](https://minigpt-4.github.io/;)Code, Pretrained Model, and Dataset:[this https URL](https://github.com/Vision-CAIR/MiniGPT-4;)Deyao Zhu and Jun Chen contributed equally to this work|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:2304.10592](https://arxiv.org/abs/2304.10592)[cs.CV]|\n|(or[arXiv:2304.10592v2](https://arxiv.org/abs/2304.10592v2)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2304.10592](https://doi.org/10.48550/arXiv.2304.10592)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Deyao Zhu [[view email](https://arxiv.org/show-email/7f3b0628/2304.10592)]\n**[[v1]](https://arxiv.org/abs/2304.10592v1)**Thu, 20 Apr 2023 18:25:35 UTC (6,248 KB)\n**[v2]**Mon, 2 Oct 2023 16:38:35 UTC (4,567 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models, by Deyao Zhu and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2304.10592)\n* [TeX Source](https://arxiv.org/src/2304.10592)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2304.10592&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2304.10592&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2023-04](https://arxiv.org/list/cs.CV/2023-04)\nChange to browse by:\n[cs](https://arxiv.org/abs/2304.10592?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2304.10592)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2304.10592)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2304.10592)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these valu",
          "original_query": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
          "cleaned_query": "Minigpt-4: Enhancing vision-language understanding with advanced large language models"
        },
        {
          "success": true,
          "title": "mPLUG-Owl: Modularization Empowers LLMs with Multimodality",
          "url": "https://arxiv.org/abs/2304.14178",
          "content": "# Computer Science > Computation and Language\n\n**arXiv:2304.14178** (cs)\n\n\\[Submitted on 27 Apr 2023 ( [v1](https://arxiv.org/abs/2304.14178v1)), last revised 29 Mar 2024 (this version, v3)\\]\n\n# Title:mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality\n\nAuthors: [Qinghao Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye,+Q), [Haiyang Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+H), [Guohai Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+G), [Jiabo Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye,+J), [Ming Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan,+M), [Yiyang Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou,+Y), [Junyang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+J), [Anwen Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu,+A), [Pengcheng Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi,+P), [Yaya Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi,+Y), [Chenliang Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+C), [Yuanhong Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+Y), [Hehong Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+H), [Junfeng Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian,+J), [Qi Qian](https://arxiv.org/search/cs?searchtype=author&query=Qian,+Q), [Ji Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+J), [Fei Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+F), [Jingren Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou,+J)\n\nView a PDF of the paper titled mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality, by Qinghao Ye and 17 other authors\n\n[View PDF](https://arxiv.org/pdf/2304.14178) [HTML (experimental)](https://arxiv.org/html/2304.14178v3)\n\n> Abstract:Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at [this https URL](https://github.com/X-PLUG/mPLUG-Owl). The online demo is available at [this https URL](https://www.modelscope.cn/studios/damo/mPLUG-Owl).\n\n| | |\n| --- | --- |\n| Comments: | Working in Process |\n| Subjects: | Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2304.14178](https://arxiv.org/abs/2304.14178) \\[cs.CL\\] |\n| (or [arXiv:2304.14178v3](https://arxiv.org/abs/2304.14178v3) \\[cs.CL\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2304.14178](https://doi.org/10.48550/arXiv.2304.14178) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Qinghao Ye \\[ [view email](https://arxiv.org/show-email/e71da28e/2304.14178)\\] **[\\[v1\\]](https://arxiv.org/abs/2304.14178v1)**\nThu, 27 Apr 2023 13:27:01 UTC (32,001 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2304.14178v2)**\nFri, 22 Mar 2024 07:23:22 UTC (32,001 KB)\n**\\[v3\\]**\nFri, 29 Mar 2024 08:13:38 UTC (32,001 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality, by Qinghao Ye and 17 other authors\n\n- [View PDF](https://arxiv.org/pdf/2304.14178)\n- [HTML (experimental)](https://arxiv.org/html/2304.14178v3)\n- [TeX Source](https://arxiv.org/src/2304.14178)\n\n[view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2304.14178&function=prev&context=cs.CL)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2304.14178&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2023-04](https://arxiv.org/list/cs.CL/2023-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2304.14178?context=cs) [cs.CV](https://arxiv.org/abs/2304.14178?context=cs.CV) [cs.LG](https://arxiv.org/abs/2304.14178?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2304.14178)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2304.14178)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2304.14178)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data",
          "original_query": "mplug-owl: Modularization empowers large language models with multimodality",
          "cleaned_query": "mplug-owl: Modularization empowers large language models with multimodality"
        },
        {
          "success": true,
          "title": "[2304.08485] Visual Instruction Tuning",
          "url": "https://arxiv.org/abs/2304.08485",
          "content": "[2304.08485] Visual Instruction Tuning\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2304.08485\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2304.08485**(cs)\n[Submitted on 17 Apr 2023 ([v1](https://arxiv.org/abs/2304.08485v1)), last revised 11 Dec 2023 (this version, v2)]\n# Title:Visual Instruction Tuning\nAuthors:[Haotian Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+H),[Chunyuan Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+C),[Qingyang Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+Q),[Yong Jae Lee](https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+Y+J)\nView a PDF of the paper titled Visual Instruction Tuning, by Haotian Liu and 3 other authors\n[View PDF](https://arxiv.org/pdf/2304.08485)[HTML (experimental)](https://arxiv.org/html/2304.08485v2)> > Abstract:\n> Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language [> this http URL\n](http://understanding.Our)> early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available. Comments:|NeurIPS 2023 Oral; project page:[this https URL](https://llava-vl.github.io/)|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)|\nCite as:|[arXiv:2304.08485](https://arxiv.org/abs/2304.08485)[cs.CV]|\n|(or[arXiv:2304.08485v2](https://arxiv.org/abs/2304.08485v2)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2304.08485](https://doi.org/10.48550/arXiv.2304.08485)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Haotian Liu [[view email](https://arxiv.org/show-email/368f8739/2304.08485)]\n**[[v1]](https://arxiv.org/abs/2304.08485v1)**Mon, 17 Apr 2023 17:59:25 UTC (4,360 KB)\n**[v2]**Mon, 11 Dec 2023 17:46:14 UTC (4,985 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Visual Instruction Tuning, by Haotian Liu and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2304.08485)\n* [HTML (experimental)](https://arxiv.org/html/2304.08485v2)\n* [TeX Source](https://arxiv.org/src/2304.08485)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2304.08485&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2304.08485&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2023-04](https://arxiv.org/list/cs.CV/2023-04)\nChange to browse by:\n[cs](https://arxiv.org/abs/2304.08485?context=cs)\n[cs.AI](https://arxiv.org/abs/2304.08485?context=cs.AI)\n[cs.CL](https://arxiv.org/abs/2304.08485?context=cs.CL)\n[cs.LG](https://arxiv.org/abs/2304.08485?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2304.08485)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2304.08485)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2304.08485)\n### [3 blog links](https://arxiv.org/tb/2304.08485)\n([what is this?](https://info.arxiv.org/help/trackback.html))\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2304.08485)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Visual instruction tuning",
          "cleaned_query": "Visual instruction tuning"
        },
        {
          "success": true,
          "title": "Balanced Multimodal Learning via On-the-fly Gradient Modulation",
          "url": "https://arxiv.org/abs/2203.15332",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2203.15332** (cs)\n\n\\[Submitted on 29 Mar 2022\\]\n\n# Title:Balanced Multimodal Learning via On-the-fly Gradient Modulation\n\nAuthors: [Xiaokang Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng,+X), [Yake Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei,+Y), [Andong Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng,+A), [Dong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+D), [Di Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu,+D)\n\nView a PDF of the paper titled Balanced Multimodal Learning via On-the-fly Gradient Modulation, by Xiaokang Peng and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2203.15332)\n\n> Abstract:Multimodal learning helps to comprehensively understand the world, by integrating different senses. Accordingly, multiple input modalities are expected to boost model performance, but we actually find that they are not fully exploited even when the multimodal model outperforms its uni-modal counterpart. Specifically, in this paper we point out that existing multimodal discriminative models, in which uniform objective is designed for all modalities, could remain under-optimized uni-modal representations, caused by another dominated modality in some scenarios, e.g., sound in blowing wind event, vision in drawing picture event, etc. To alleviate this optimization imbalance, we propose on-the-fly gradient modulation to adaptively control the optimization of each modality, via monitoring the discrepancy of their contribution towards the learning objective. Further, an extra Gaussian noise that changes dynamically is introduced to avoid possible generalization drop caused by gradient modulation. As a result, we achieve considerable improvement over common fusion methods on different multimodal tasks, and this simple strategy can also boost existing multimodal methods, which illustrates its efficacy and versatility. The source code is available at \\\\url{ [this https URL](https://github.com/GeWu-Lab/OGM-GE_CVPR2022)}.\n\n| | |\n| --- | --- |\n| Comments: | Accepted by CVPR 2022 (ORAL) |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2203.15332](https://arxiv.org/abs/2203.15332) \\[cs.CV\\] |\n| (or [arXiv:2203.15332v1](https://arxiv.org/abs/2203.15332v1) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2203.15332](https://doi.org/10.48550/arXiv.2203.15332) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Yake Wei \\[ [view email](https://arxiv.org/show-email/c269f548/2203.15332)\\] **\\[v1\\]**\nTue, 29 Mar 2022 08:26:38 UTC (505 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Balanced Multimodal Learning via On-the-fly Gradient Modulation, by Xiaokang Peng and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2203.15332)\n- [TeX Source](https://arxiv.org/src/2203.15332)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2203.15332&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2203.15332&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2022-03](https://arxiv.org/list/cs.CV/2022-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2203.15332?context=cs) [cs.AI](https://arxiv.org/abs/2203.15332?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2203.15332)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2203.15332)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2203.15332)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2203.15332) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Balanced multimodal learning via on-the-fly gradient modulation",
          "cleaned_query": "Balanced multimodal learning via on-the-fly gradient modulation"
        },
        {
          "success": true,
          "title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
          "url": "https://arxiv.org/abs/2402.09353",
          "content": "[2402.09353] DoRA: Weight-Decomposed Low-Rank Adaptation\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2402.09353\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2402.09353**(cs)\n[Submitted on 14 Feb 2024 ([v1](https://arxiv.org/abs/2402.09353v1)), last revised 9 Jul 2024 (this version, v6)]\n# Title:DoRA: Weight-Decomposed Low-Rank Adaptation\nAuthors:[Shih-Yang Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+S),[Chien-Yi Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+C),[Hongxu Yin](https://arxiv.org/search/cs?searchtype=author&amp;query=Yin,+H),[Pavlo Molchanov](https://arxiv.org/search/cs?searchtype=author&amp;query=Molchanov,+P),[Yu-Chiang Frank Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y+F),[Kwang-Ting Cheng](https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng,+K),[Min-Hung Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+M)\nView a PDF of the paper titled DoRA: Weight-Decomposed Low-Rank Adaptation, by Shih-Yang Liu and 6 other authors\n[View PDF](https://arxiv.org/pdf/2402.09353)[HTML (experimental)](https://arxiv.org/html/2402.09353v6)> > Abstract:\n> Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing \\ours, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. \\ours~consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding. Code is available at [> this https URL\n](https://github.com/NVlabs/DoRA)> . Comments:|ICML2024(Oral)|\nSubjects:|Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:2402.09353](https://arxiv.org/abs/2402.09353)[cs.CL]|\n|(or[arXiv:2402.09353v6](https://arxiv.org/abs/2402.09353v6)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2402.09353](https://doi.org/10.48550/arXiv.2402.09353)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Shih-Yang Liu [[view email](https://arxiv.org/show-email/0f954ad7/2402.09353)]\n**[[v1]](https://arxiv.org/abs/2402.09353v1)**Wed, 14 Feb 2024 17:59:34 UTC (495 KB)\n**[[v2]](https://arxiv.org/abs/2402.09353v2)**Fri, 1 Mar 2024 16:26:41 UTC (495 KB)\n**[[v3]](https://arxiv.org/abs/2402.09353v3)**Tue, 5 Mar 2024 07:31:21 UTC (495 KB)\n**[[v4]](https://arxiv.org/abs/2402.09353v4)**Sun, 28 Apr 2024 09:06:50 UTC (521 KB)\n**[[v5]](https://arxiv.org/abs/2402.09353v5)**Mon, 3 Jun 2024 07:27:15 UTC (5,473 KB)\n**[v6]**Tue, 9 Jul 2024 05:59:16 UTC (12,309 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled DoRA: Weight-Decomposed Low-Rank Adaptation, by Shih-Yang Liu and 6 other authors\n* [View PDF](https://arxiv.org/pdf/2402.09353)\n* [HTML (experimental)](https://arxiv.org/html/2402.09353v6)\n* [TeX Source](https://arxiv.org/src/2402.09353)\n[![license icon](https://arxiv.org/icons/licenses/by-nc-sa-4.0.png)view license](http://creativecommons.org/licenses/by-nc-sa/4.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2402.09353&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2402.09353&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2024-02](https://arxiv.org/list/cs.CL/2024-02)\nChange to browse by:\n[cs](https://arxiv.org/abs/2402.09353?context=cs)\n[cs.CV](https://arxiv.org/abs/2402.09353?context=cs.CV)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2402.09353)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2402.09353)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2402.09353)\n### [1 blog link](https://arxiv.org/tb/2402.09353)\n([what is this?](https://info.arxiv.org/help/trackback.html))\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, ",
          "original_query": "Dora: Weight-decomposed low-rank adaptation",
          "cleaned_query": "Dora: Weight-decomposed low-rank adaptation"
        }
      ],
      "generated_ideas": [
        "Rank-Adaptive DoRA: Layerwise Budgeting of Direction vs. Magnitude Updates\n- Extend DoRA by dynamically allocating low-rank capacity per layer/token based on observed rank-deficiency signals (as studied in LoRA) and training instability metrics. Concretely, learn a small controller that increases/decreases rank (or activates more DoRA blocks) for layers where multimodal alignment loss is underfitting, while freezing others to keep PEFT budgets fixed.",
        "Mixture-of-Adapters for Multimodal Specialization Without Extra Inference Latency\n- Build a modular multimodal system (mPLUG-Owl-style) where multiple LoRA/DoRA experts are trained for distinct visual skills (OCR, spatial reasoning, chart understanding, multi-image correlation). Use lightweight routing computed from the frozen vision encoder\u2019s global features to select/compose a small subset of adapters, and fuse them via low-rank linear combination so inference cost matches a single adapter.",
        "Continual Multimodal Learning With Adapter Versioning and Anti-Forgetting Constraints\n- Develop a continual instruction-tuning pipeline where new tasks/domains add new LoRA/DoRA \u201cversions\u201d while preserving prior capabilities through constraints on adapter subspace overlap (e.g., orthogonality penalties between low-rank directions). Evaluate on sequentially introduced multimodal skills (captioning \u2192 VQA \u2192 OCR \u2192 docs) and quantify forgetting using held-out OwlEval-style sets.",
        "Token-Level Modality Gradient Modulation for Multimodal LLM Decoding\n- Extend OGM beyond modality-wide scaling by modulating gradients conditioned on token types during instruction tuning (e.g., spatial words, color terms, OCR strings). Implement token-category detectors (from the LLM tokenizer + simple lexicons) to scale adapter updates only when the supervision signal truly depends on vision, aiming to reduce \u201clanguage-only shortcuts\u201d in LLaVA-like training.",
        "DoRA for Visual Hallucination Reduction Through Directional Alignment Regularization\n- Use DoRA\u2019s magnitude/direction decomposition to explicitly regularize the *directional* updates that correlate with hallucination (e.g., confident but unsupported object mentions). Add a contrastive loss: for minimally edited images (remove an object), enforce that the directional component responsible for that object\u2019s token likelihood changes significantly, while unrelated directions remain stable.",
        "Multi-Image Reasoning With Shared Low-Rank Memory Across Turns\n- Build on mPLUG-Owl\u2019s multi-image correlation by introducing a shared, persistent low-rank memory adapter that stores cross-image entities across a dialogue. Concretely, maintain a small set of trainable low-rank \u201cmemory keys/values\u201d updated via backprop through multi-turn conversations, and test improvements on tasks requiring consistent references across multiple images and turns."
      ],
      "generated_ideas_raw": "1. **Modality-Balanced LoRA for Vision-Language Instruction Tuning**\n   - Combine on-the-fly gradient modulation (OGM) with LoRA/DoRA adapters to prevent the LLM from dominating training when aligning vision features (as in LLaVA/MiniGPT-4). Implement OGM at the adapter-gradient level per modality pathway (vision projection/abstractor vs. LLM LoRA) and evaluate gains in visual grounding, hallucination rate, and instruction-following on OwlEval and LLaVA benchmarks.\n\n2. **Rank-Adaptive DoRA: Layerwise Budgeting of Direction vs. Magnitude Updates**\n   - Extend DoRA by dynamically allocating low-rank capacity per layer/token based on observed rank-deficiency signals (as studied in LoRA) and training instability metrics. Concretely, learn a small controller that increases/decreases rank (or activates more DoRA blocks) for layers where multimodal alignment loss is underfitting, while freezing others to keep PEFT budgets fixed.\n\n3. **Mixture-of-Adapters for Multimodal Specialization Without Extra Inference Latency**\n   - Build a modular multimodal system (mPLUG-Owl-style) where multiple LoRA/DoRA experts are trained for distinct visual skills (OCR, spatial reasoning, chart understanding, multi-image correlation). Use lightweight routing computed from the frozen vision encoder\u2019s global features to select/compose a small subset of adapters, and fuse them via low-rank linear combination so inference cost matches a single adapter.\n\n4. **Low-Rank Cross-Modal Abstractor: Replacing Dense Projection With Structured Low-Rank Maps**\n   - In MiniGPT-4/LLaVA, the vision-to-LLM connector is typically a projection layer; replace it with a structured low-rank \u201cabstractor\u201d that factorizes mappings into semantic subspaces (objects, text, layout). Train these factors in stage-1 alignment and freeze them in stage-2 instruction tuning, testing whether structured low-rank improves compositional generalization (e.g., novel object-attribute bindings).\n\n5. **Self-Refining Visual Instruction Data via Adapter Disagreement**\n   - Create an automatic data curation loop for visual instruction tuning (\u00e0 la GPT-4-generated LLaVA data) using disagreement between multiple PEFT variants (LoRA vs. DoRA; different ranks; different adapter placements). Images/instructions that maximize disagreement trigger targeted re-captioning or re-instruction generation, yielding a higher-quality second-stage dataset similar in spirit to MiniGPT-4\u2019s detailed descriptions but produced automatically.\n\n6. **Continual Multimodal Learning With Adapter Versioning and Anti-Forgetting Constraints**\n   - Develop a continual instruction-tuning pipeline where new tasks/domains add new LoRA/DoRA \u201cversions\u201d while preserving prior capabilities through constraints on adapter subspace overlap (e.g., orthogonality penalties between low-rank directions). Evaluate on sequentially introduced multimodal skills (captioning \u2192 VQA \u2192 OCR \u2192 docs) and quantify forgetting using held-out OwlEval-style sets.\n\n7. **Token-Level Modality Gradient Modulation for Multimodal LLM Decoding**\n   - Extend OGM beyond modality-wide scaling by modulating gradients conditioned on token types during instruction tuning (e.g., spatial words, color terms, OCR strings). Implement token-category detectors (from the LLM tokenizer + simple lexicons) to scale adapter updates only when the supervision signal truly depends on vision, aiming to reduce \u201clanguage-only shortcuts\u201d in LLaVA-like training.\n\n8. **DoRA for Visual Hallucination Reduction Through Directional Alignment Regularization**\n   - Use DoRA\u2019s magnitude/direction decomposition to explicitly regularize the *directional* updates that correlate with hallucination (e.g., confident but unsupported object mentions). Add a contrastive loss: for minimally edited images (remove an object), enforce that the directional component responsible for that object\u2019s token likelihood changes significantly, while unrelated directions remain stable.\n\n9. **Multi-Image Reasoning With Shared Low-Rank Memory Across Turns**\n   - Build on mPLUG-Owl\u2019s multi-image correlation by introducing a shared, persistent low-rank memory adapter that stores cross-image entities across a dialogue. Concretely, maintain a small set of trainable low-rank \u201cmemory keys/values\u201d updated via backprop through multi-turn conversations, and test improvements on tasks requiring consistent references across multiple images and turns.\n\n10. **Benchmarking Rank-Deficiency in Multimodal Alignment: Where Do Low-Rank Updates Fail?**\n   - Create a diagnostic suite that measures when low-rank adaptation is insufficient for multimodal models by probing layerwise intrinsic rank needs during (1) vision-text alignment, (2) instruction tuning, and (3) domain shifts (documents, charts, scene text). Provide actionable guidelines: optimal adapter placement (attention vs. MLP), recommended ranks, and when DoRA-style magnitude tuning closes the gap to full fine-tuning.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Rank-Adaptive DoRA: Layerwise Budgeting of Direction vs. Magnitude Updates\n- Extend DoRA by dynamically allocating low-rank capacity per layer/token based on observed rank-deficiency signals (as studi",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Mixture-of-Adapters for Multimodal Specialization Without Extra Inference Latency\n- Build a modular multimodal system (mPLUG-Owl-style) where multiple LoRA/DoRA experts are trained for distinct visual",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Continual Multimodal Learning With Adapter Versioning and Anti-Forgetting Constraints\n- Develop a continual instruction-tuning pipeline where new tasks/domains add new LoRA/DoRA \u201cversions\u201d while prese",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Token-Level Modality Gradient Modulation for Multimodal LLM Decoding\n- Extend OGM beyond modality-wide scaling by modulating gradients conditioned on token types during instruction tuning (e.g., spati",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "DoRA for Visual Hallucination Reduction Through Directional Alignment Regularization\n- Use DoRA\u2019s magnitude/direction decomposition to explicitly regularize the *directional* updates that correlate wi",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Multi-Image Reasoning With Shared Low-Rank Memory Across Turns\n- Build on mPLUG-Owl\u2019s multi-image correlation by introducing a shared, persistent low-rank memory adapter that stores cross-image entiti",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 31,
      "paper_title": "Advancing Expert Specialization for Better MoE",
      "contribution": "Introduces complementary orthogonality and variance regularizers that, when added to standard MoE auxiliary balancing losses, reduce expert overlap and produce more discriminative routing and specialist experts\u2014improving downstream performance without architectural changes.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 6,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 9561,
      "output_tokens": 912,
      "predecessor_details": [
        {
          "success": true,
          "title": "Adaptive Mixtures of Local Experts | MIT Press Journals & Magazine",
          "url": "https://ieeexplore.ieee.org/document/6797059",
          "content": "Adaptive Mixtures of Local Experts \\| MIT Press Journals & Magazine \\| IEEE Xplore\n\n### IEEE Account\n\n- [Change Username/Password](https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Update Address](https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n\n### Purchase Details\n\n- [Payment Options](https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Order History](https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [View Purchased Documents](https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp)\n\n### Profile Information\n\n- [Communications Preferences](https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Profession and Education](https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Technical Interests](https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n\n### Need Help?\n\n- **US & Canada:** +1 800 678 4333\n- **Worldwide:** +1 732 981 0060\n\n- [Contact & Support](https://ieeexplore.ieee.org/xpl/contact)\n\n- [About IEEE _Xplore_](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-ieee-xplore)\n- [Contact Us](https://ieeexplore.ieee.org/xpl/contact)\n- [Help](https://ieeexplore.ieee.org/Xplorehelp)\n- [Accessibility](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/accessibility-statement)\n- [Terms of Use](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/terms-of-use)\n- [Nondiscrimination Policy](http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html)\n- [Sitemap](https://ieeexplore.ieee.org/xpl/sitemap.jsp)\n- [Privacy & Opting Out of Cookies](http://www.ieee.org/about/help/security_privacy.html)\n\nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.\n\n\u00a9 Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.",
          "original_query": "Adaptive Mixtures of Local Experts",
          "cleaned_query": "Adaptive Mixtures of Local Experts"
        },
        {
          "success": true,
          "title": "The Sparsely-Gated Mixture-of-Experts Layer",
          "url": "https://arxiv.org/abs/1701.06538",
          "content": "[1701.06538] Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1701.06538\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1701.06538**(cs)\n[Submitted on 23 Jan 2017]\n# Title:Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\nAuthors:[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&amp;query=Shazeer,+N),[Azalia Mirhoseini](https://arxiv.org/search/cs?searchtype=author&amp;query=Mirhoseini,+A),[Krzysztof Maziarz](https://arxiv.org/search/cs?searchtype=author&amp;query=Maziarz,+K),[Andy Davis](https://arxiv.org/search/cs?searchtype=author&amp;query=Davis,+A),[Quoc Le](https://arxiv.org/search/cs?searchtype=author&amp;query=Le,+Q),[Geoffrey Hinton](https://arxiv.org/search/cs?searchtype=author&amp;query=Hinton,+G),[Jeff Dean](https://arxiv.org/search/cs?searchtype=author&amp;query=Dean,+J)\nView a PDF of the paper titled Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer, by Noam Shazeer and 6 other authors\n[View PDF](https://arxiv.org/pdf/1701.06538)> > Abstract:\n> The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost. Subjects:|Machine Learning (cs.LG); Computation and Language (cs.CL); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)|\nCite as:|[arXiv:1701.06538](https://arxiv.org/abs/1701.06538)[cs.LG]|\n|(or[arXiv:1701.06538v1](https://arxiv.org/abs/1701.06538v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1701.06538](https://doi.org/10.48550/arXiv.1701.06538)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Noam Shazeer [[view email](https://arxiv.org/show-email/9288b55a/1701.06538)]\n**[v1]**Mon, 23 Jan 2017 18:10:00 UTC (294 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer, by Noam Shazeer and 6 other authors\n* [View PDF](https://arxiv.org/pdf/1701.06538)\n* [TeX Source](https://arxiv.org/src/1701.06538)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1701.06538&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1701.06538&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2017-01](https://arxiv.org/list/cs.LG/2017-01)\nChange to browse by:\n[cs](https://arxiv.org/abs/1701.06538?context=cs)\n[cs.CL](https://arxiv.org/abs/1701.06538?context=cs.CL)\n[cs.NE](https://arxiv.org/abs/1701.06538?context=cs.NE)\n[stat](https://arxiv.org/abs/1701.06538?context=stat)\n[stat.ML](https://arxiv.org/abs/1701.06538?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1701.06538)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1701.06538)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1701.06538)\n### [4 blog links](https://arxiv.org/tb/1701.06538)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1701.html#ShazeerMMDLHD17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/ShazeerMMDLHD17)\n[Noam Shazeer]()\n[Azalia Mirhoseini]()\n[Krzysztof Maziarz]()\n[Andy Davis]()\n[Quoc V. Le]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that all",
          "original_query": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (Noisy Top-k Gating)",
          "cleaned_query": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer (Noisy Top-k Gating)"
        },
        {
          "success": true,
          "title": "GShard: Scaling Giant Models with Conditional Computation ... - arXiv",
          "url": "https://arxiv.org/abs/2006.16668",
          "content": "[2006.16668] GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2006.16668\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2006.16668**(cs)\n[Submitted on 30 Jun 2020]\n# Title:GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\nAuthors:[Dmitry Lepikhin](https://arxiv.org/search/cs?searchtype=author&amp;query=Lepikhin,+D),[HyoukJoong Lee](https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+H),[Yuanzhong Xu](https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Y),[Dehao Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+D),[Orhan Firat](https://arxiv.org/search/cs?searchtype=author&amp;query=Firat,+O),[Yanping Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+Y),[Maxim Krikun](https://arxiv.org/search/cs?searchtype=author&amp;query=Krikun,+M),[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&amp;query=Shazeer,+N),[Zhifeng Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Z)\nView a PDF of the paper titled GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding, by Dmitry Lepikhin and 8 other authors\n[View PDF](https://arxiv.org/pdf/2006.16668)> > Abstract:\n> Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art. Subjects:|Computation and Language (cs.CL); Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2006.16668](https://arxiv.org/abs/2006.16668)[cs.CL]|\n|(or[arXiv:2006.16668v1](https://arxiv.org/abs/2006.16668v1)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2006.16668](https://doi.org/10.48550/arXiv.2006.16668)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Orhan Firat [[view email](https://arxiv.org/show-email/b27e9780/2006.16668)]\n**[v1]**Tue, 30 Jun 2020 10:42:02 UTC (3,323 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding, by Dmitry Lepikhin and 8 other authors\n* [View PDF](https://arxiv.org/pdf/2006.16668)\n* [TeX Source](https://arxiv.org/src/2006.16668)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2006.16668&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2006.16668&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2020-06](https://arxiv.org/list/cs.CL/2020-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/2006.16668?context=cs)\n[cs.LG](https://arxiv.org/abs/2006.16668?context=cs.LG)\n[stat](https://arxiv.org/abs/2006.16668?context=stat)\n[stat.ML](https://arxiv.org/abs/2006.16668?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2006.16668)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2006.16668)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2006.16668)\n### [1 blog link](https://arxiv.org/tb/2006.16668)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2006.html#abs-2006-16668)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2006-16668)\n[Dmitry Lepikhin]()\n[HyoukJoong Lee]()\n[Dehao Chen]()\n[Orhan Firat]()\n[Yanping Huang]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an ide",
          "original_query": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
          "cleaned_query": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
        },
        {
          "success": true,
          "title": "Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
          "url": "https://arxiv.org/abs/2101.03961",
          "content": "[2101.03961] Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2101.03961\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2101.03961**(cs)\n[Submitted on 11 Jan 2021 ([v1](https://arxiv.org/abs/2101.03961v1)), last revised 16 Jun 2022 (this version, v3)]\n# Title:Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\nAuthors:[William Fedus](https://arxiv.org/search/cs?searchtype=author&amp;query=Fedus,+W),[Barret Zoph](https://arxiv.org/search/cs?searchtype=author&amp;query=Zoph,+B),[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&amp;query=Shazeer,+N)\nView a PDF of the paper titled Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity, by William Fedus and 2 other authors\n[View PDF](https://arxiv.org/pdf/2101.03961)> > Abstract:\n> In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the &#34;Colossal Clean Crawled Corpus&#34; and achieve a 4x speedup over the T5-XXL model. Comments:|JMLR|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2101.03961](https://arxiv.org/abs/2101.03961)[cs.LG]|\n|(or[arXiv:2101.03961v3](https://arxiv.org/abs/2101.03961v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2101.03961](https://doi.org/10.48550/arXiv.2101.03961)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: William Fedus [[view email](https://arxiv.org/show-email/2e7b79e7/2101.03961)]\n**[[v1]](https://arxiv.org/abs/2101.03961v1)**Mon, 11 Jan 2021 16:11:52 UTC (1,509 KB)\n**[[v2]](https://arxiv.org/abs/2101.03961v2)**Sat, 30 Apr 2022 00:02:01 UTC (1,148 KB)\n**[v3]**Thu, 16 Jun 2022 20:36:07 UTC (1,146 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity, by William Fedus and 2 other authors\n* [View PDF](https://arxiv.org/pdf/2101.03961)\n* [TeX Source](https://arxiv.org/src/2101.03961)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2101.03961&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2101.03961&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2021-01](https://arxiv.org/list/cs.LG/2021-01)\nChange to browse by:\n[cs](https://arxiv.org/abs/2101.03961?context=cs)\n[cs.AI](https://arxiv.org/abs/2101.03961?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2101.03961)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2101.03961)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2101.03961)\n### [5 blog links](https://arxiv.org/tb/2101.03961)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2101.html#abs-2101-03961)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2101-03961)\n[William Fedus]()\n[Barret Zoph]()\n[Noam Shazeer]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add va",
          "original_query": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
          "cleaned_query": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"
        },
        {
          "success": true,
          "title": "Reducing Overfitting in Deep Networks by Decorrelating ... - arXiv",
          "url": "https://arxiv.org/abs/1511.06068",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1511.06068** (cs)\n\n\\[Submitted on 19 Nov 2015 ( [v1](https://arxiv.org/abs/1511.06068v1)), last revised 10 Jun 2016 (this version, v4)\\]\n\n# Title:Reducing Overfitting in Deep Networks by Decorrelating Representations\n\nAuthors: [Michael Cogswell](https://arxiv.org/search/cs?searchtype=author&query=Cogswell,+M), [Faruk Ahmed](https://arxiv.org/search/cs?searchtype=author&query=Ahmed,+F), [Ross Girshick](https://arxiv.org/search/cs?searchtype=author&query=Girshick,+R), [Larry Zitnick](https://arxiv.org/search/cs?searchtype=author&query=Zitnick,+L), [Dhruv Batra](https://arxiv.org/search/cs?searchtype=author&query=Batra,+D)\n\nView a PDF of the paper titled Reducing Overfitting in Deep Networks by Decorrelating Representations, by Michael Cogswell and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/1511.06068)\n\n> Abstract:One major challenge in training Deep Neural Networks is preventing overfitting. Many techniques such as data augmentation and novel regularizers such as Dropout have been proposed to prevent overfitting without requiring a massive amount of training data. In this work, we propose a new regularizer called DeCov which leads to significantly reduced overfitting (as indicated by the difference between train and val performance), and better generalization. Our regularizer encourages diverse or non-redundant representations in Deep Neural Networks by minimizing the cross-covariance of hidden activations. This simple intuition has been explored in a number of past works but surprisingly has never been applied as a regularizer in supervised learning. Experiments across a range of datasets and network architectures show that this loss always reduces overfitting while almost always maintaining or increasing generalization performance and often improving performance over Dropout.\n\n| | |\n| --- | --- |\n| Comments: | 12 pages, 5 figures, 5 tables, Accepted to ICLR 2016, (v4 adds acknowledgements) |\n| Subjects: | Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1511.06068](https://arxiv.org/abs/1511.06068) \\[cs.LG\\] |\n| | (or [arXiv:1511.06068v4](https://arxiv.org/abs/1511.06068v4) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1511.06068](https://doi.org/10.48550/arXiv.1511.06068) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Michael Cogswell \\[ [view email](https://arxiv.org/show-email/712c70e7/1511.06068)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1511.06068v1)**\nThu, 19 Nov 2015 06:23:09 UTC (2,394 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1511.06068v2)**\nThu, 7 Jan 2016 21:12:29 UTC (2,513 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/1511.06068v3)**\nMon, 29 Feb 2016 21:23:05 UTC (2,509 KB)\n\n**\\[v4\\]**\nFri, 10 Jun 2016 10:59:37 UTC (2,521 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Reducing Overfitting in Deep Networks by Decorrelating Representations, by Michael Cogswell and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/1511.06068)\n- [TeX Source](https://arxiv.org/src/1511.06068)\n- [Other Formats](https://arxiv.org/format/1511.06068)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1511.06068&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1511.06068&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2015-11](https://arxiv.org/list/cs.LG/2015-11)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1511.06068?context=cs)\n\n[stat](https://arxiv.org/abs/1511.06068?context=stat)\n\n[stat.ML](https://arxiv.org/abs/1511.06068?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1511.06068)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1511.06068)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1511.06068)\n\n### [1 blog link](https://arxiv.org/tb/1511.06068)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/conf/iclr/iclr2016.html#CogswellAGZB15) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/conf/iclr/CogswellAGZB15)\n\n[Michael Cogswell](https://dblp.uni-trier.de/search/author?author=Michael%20Cogswell)\n\n[Faruk Ahmed](https://dblp.uni-trier.de/search/author?author=Faruk%20Ahmed)\n\n[Ross B. Girshick](https://dblp.uni-trier.de/search/author?author=Ross%20B.%20Girshick)\n\n[Larry Zitnick](https://dblp.uni-trier.de/search/author?author=Larry%20Zitnick)\n\n[Dhruv Batra](https://dblp.uni-trier.de/search/author?author=Dhruv%20Batra)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1511.06068&description=Reducing Overfitting in Deep Networks by Decorrelating Representations) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1511.06068&title=Reducing Overfitting in Deep Networks by Decorrelating Representations)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1511.06068) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](h",
          "original_query": "Reducing Overfitting in Deep Networks by Decorrelating Representations (DeCov)",
          "cleaned_query": "Reducing Overfitting in Deep Networks by Decorrelating Representations (DeCov)"
        },
        {
          "success": true,
          "title": "Can We Gain More from Orthogonality Regularizations in Training ...",
          "url": "https://arxiv.org/abs/1810.09102",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "Can We Gain More from Orthogonality Regularizations in Training Deep Networks?",
          "cleaned_query": "Can We Gain More from Orthogonality Regularizations in Training Deep Networks?"
        }
      ],
      "generated_ideas": [
        "DeCov-Routed Switch Transformers for Balanced Expert Specialization\nAdd a DeCov-style cross-covariance penalty not on token representations, but on *expert outputs per layer* (and optionally on router logits) to force experts to learn non-redundant functions. Evaluate whether this reduces expert collapse and improves quality-per-FLOP on multilingual MT and LM, while keeping Switch routing unchanged.",
        "Communication-Aware Routing via Learned Sharding Constraints (GShard + MoE)\nExtend the router loss to include a differentiable proxy for cross-device traffic (e.g., penalize assignments that send tokens to non-local expert shards), aligning routing with GShard\u2019s automatic sharding patterns. The contribution is a joint optimization of quality and communication cost, with experiments measuring step time, all-to-all volume, and model quality at constant hardware.",
        "Continual Expert Growth with Decorrelated Initialization for New Domains\nDevelop a method to add new experts over time (new domains/languages) while freezing most existing experts, initializing added experts to be decorrelated/orthogonal to existing expert subspaces (using DeCov/orthogonality objectives). Evaluate on sequential domain adaptation to quantify reduced catastrophic forgetting and improved parameter efficiency compared to full fine-tuning.",
        "Token-Type Conditional Computation: Separate Expert Pools for Syntax, Semantics, and Entities\nIntroduce lightweight token classifiers (or use self-supervised tags) that route tokens into specialized expert pools (e.g., entity-heavy tokens vs function words), rather than a single shared expert set. The key test is whether structured conditional computation improves perplexity/translation quality and increases expert interpretability without increasing FLOPs.",
        "Diversity-Driven Load Balancing: Replace Auxiliary Load Loss with Mutual-Information Objectives\nReplace or augment the standard MoE load-balancing loss with an objective that maximizes mutual information between routing decisions and input features while constraining entropy to avoid collapse. This directly targets the \u201cexperts become redundant\u201d failure mode by encouraging informative partitions of the input space, and can be compared against Switch\u2019s standard auxiliary losses.",
        "Expert Redundancy Auditing and Pruning with DeCov/Orthogonality Metrics\nPropose a post-training analysis toolkit that computes cross-expert covariance/orthogonality scores (on activations and gradients) to identify redundant experts, then prunes or merges them with minimal fine-tuning. This yields an actionable compression pathway for trillion-parameter sparse models, reporting retained quality vs reduced memory/communication cost."
      ],
      "generated_ideas_raw": "1. **DeCov-Routed Switch Transformers for Balanced Expert Specialization**  \n   Add a DeCov-style cross-covariance penalty not on token representations, but on *expert outputs per layer* (and optionally on router logits) to force experts to learn non-redundant functions. Evaluate whether this reduces expert collapse and improves quality-per-FLOP on multilingual MT and LM, while keeping Switch routing unchanged.\n\n2. **Orthogonal Router Projections to Stabilize Sparse Gating at Scale**  \n   Modify the gating network to use an orthogonally-regularized (or parametrically orthogonal) projection before the top-k selection, building on orthogonality regularization findings and MoE routing instability issues. Test whether orthogonal router features reduce load imbalance and gradient spikes in large-batch bfloat16 training (as in Switch/GShard settings).\n\n3. **Hierarchical Mixtures of Local Experts Inside Transformer Blocks**  \n   Combine the \u201clocal experts\u201d idea (partitioning input space) with modern sparse MoE by introducing a two-level hierarchy: a coarse router selects a *region* (locality bucket), then a fine router selects experts within that region. This is actionable as a drop-in replacement for Switch top-1 routing, and can be benchmarked for improved routing interpretability and reduced cross-device communication.\n\n4. **Communication-Aware Routing via Learned Sharding Constraints (GShard + MoE)**  \n   Extend the router loss to include a differentiable proxy for cross-device traffic (e.g., penalize assignments that send tokens to non-local expert shards), aligning routing with GShard\u2019s automatic sharding patterns. The contribution is a joint optimization of quality and communication cost, with experiments measuring step time, all-to-all volume, and model quality at constant hardware.\n\n5. **Continual Expert Growth with Decorrelated Initialization for New Domains**  \n   Develop a method to add new experts over time (new domains/languages) while freezing most existing experts, initializing added experts to be decorrelated/orthogonal to existing expert subspaces (using DeCov/orthogonality objectives). Evaluate on sequential domain adaptation to quantify reduced catastrophic forgetting and improved parameter efficiency compared to full fine-tuning.\n\n6. **Token-Type Conditional Computation: Separate Expert Pools for Syntax, Semantics, and Entities**  \n   Introduce lightweight token classifiers (or use self-supervised tags) that route tokens into specialized expert pools (e.g., entity-heavy tokens vs function words), rather than a single shared expert set. The key test is whether structured conditional computation improves perplexity/translation quality and increases expert interpretability without increasing FLOPs.\n\n7. **Diversity-Driven Load Balancing: Replace Auxiliary Load Loss with Mutual-Information Objectives**  \n   Replace or augment the standard MoE load-balancing loss with an objective that maximizes mutual information between routing decisions and input features while constraining entropy to avoid collapse. This directly targets the \u201cexperts become redundant\u201d failure mode by encouraging informative partitions of the input space, and can be compared against Switch\u2019s standard auxiliary losses.\n\n8. **Low-Precision Robust MoE Training via Expert Output Whitening**  \n   Build a numerically stable \u201cexpert output whitening\u201d module (running-statistics, blockwise) to normalize and decorrelate expert activations, specifically targeting bfloat16/low-precision instabilities highlighted in Switch Transformers. Measure whether whitening reduces NaNs/loss spikes and allows higher learning rates or larger expert counts at fixed compute.\n\n9. **Routing Uncertainty Estimation for Selective Computation and Calibration**  \n   Add an uncertainty head to the router (e.g., predictive entropy or ensemble of routers) and use it to adapt k (top-1 vs top-2) per token: confident tokens use cheaper computation; uncertain tokens consult more experts. Evaluate compute-quality tradeoffs and calibration (ECE) improvements, especially on out-of-distribution or low-resource language subsets.\n\n10. **Expert Redundancy Auditing and Pruning with DeCov/Orthogonality Metrics**  \n   Propose a post-training analysis toolkit that computes cross-expert covariance/orthogonality scores (on activations and gradients) to identify redundant experts, then prunes or merges them with minimal fine-tuning. This yields an actionable compression pathway for trillion-parameter sparse models, reporting retained quality vs reduced memory/communication cost.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "DeCov-Routed Switch Transformers for Balanced Expert Specialization\nAdd a DeCov-style cross-covariance penalty not on token representations, but on *expert outputs per layer* (and optionally on router",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Communication-Aware Routing via Learned Sharding Constraints (GShard + MoE)\nExtend the router loss to include a differentiable proxy for cross-device traffic (e.g., penalize assignments that send toke",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Continual Expert Growth with Decorrelated Initialization for New Domains\nDevelop a method to add new experts over time (new domains/languages) while freezing most existing experts, initializing added ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Token-Type Conditional Computation: Separate Expert Pools for Syntax, Semantics, and Entities\nIntroduce lightweight token classifiers (or use self-supervised tags) that route tokens into specialized e",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Diversity-Driven Load Balancing: Replace Auxiliary Load Loss with Mutual-Information Objectives\nReplace or augment the standard MoE load-balancing loss with an objective that maximizes mutual informat",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Expert Redundancy Auditing and Pruning with DeCov/Orthogonality Metrics\nPropose a post-training analysis toolkit that computes cross-expert covariance/orthogonality scores (on activations and gradient",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 32,
      "paper_title": "From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics",
      "contribution": "Provides a two-stage gradient-flow analysis of linearized Transformer attention training under small initialization, proving an initial escape-and-rowwise-condensation phase for value/output parameters followed by an active key/query-driven phase that produces asymptotic normalized rank collapse.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12841,
      "output_tokens": 1103,
      "predecessor_details": [
        {
          "success": true,
          "title": "[1706.03762] Attention Is All You Need - arXiv",
          "url": "https://arxiv.org/abs/1706.03762",
          "content": "[1706.03762] Attention Is All You Need[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1706.03762\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:1706.03762**(cs)\n[Submitted on 12 Jun 2017 ([v1](https://arxiv.org/abs/1706.03762v1)), last revised 2 Aug 2023 (this version, v7)]\n# Title:Attention Is All You Need\nAuthors:[Ashish Vaswani](https://arxiv.org/search/cs?searchtype=author&amp;query=Vaswani,+A),[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&amp;query=Shazeer,+N),[Niki Parmar](https://arxiv.org/search/cs?searchtype=author&amp;query=Parmar,+N),[Jakob Uszkoreit](https://arxiv.org/search/cs?searchtype=author&amp;query=Uszkoreit,+J),[Llion Jones](https://arxiv.org/search/cs?searchtype=author&amp;query=Jones,+L),[Aidan N. Gomez](https://arxiv.org/search/cs?searchtype=author&amp;query=Gomez,+A+N),[Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaiser,+L),[Illia Polosukhin](https://arxiv.org/search/cs?searchtype=author&amp;query=Polosukhin,+I)\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n[View PDF](https://arxiv.org/pdf/1706.03762)[HTML (experimental)](https://arxiv.org/html/1706.03762v7)> > Abstract:\n> The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Comments:|15 pages, 5 figures|\nSubjects:|Computation and Language (cs.CL); Machine Learning (cs.LG)|\nCite as:|[arXiv:1706.03762](https://arxiv.org/abs/1706.03762)[cs.CL]|\n|(or[arXiv:1706.03762v7](https://arxiv.org/abs/1706.03762v7)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Llion Jones [[view email](https://arxiv.org/show-email/f53b7360/1706.03762)]\n**[[v1]](https://arxiv.org/abs/1706.03762v1)**Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB)\n**[[v2]](https://arxiv.org/abs/1706.03762v2)**Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB)\n**[[v3]](https://arxiv.org/abs/1706.03762v3)**Tue, 20 Jun 2017 05:20:02 UTC (1,125 KB)\n**[[v4]](https://arxiv.org/abs/1706.03762v4)**Fri, 30 Jun 2017 17:29:30 UTC (1,124 KB)\n**[[v5]](https://arxiv.org/abs/1706.03762v5)**Wed, 6 Dec 2017 03:30:32 UTC (1,124 KB)\n**[[v6]](https://arxiv.org/abs/1706.03762v6)**Mon, 24 Jul 2023 00:48:54 UTC (1,124 KB)\n**[v7]**Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n* [View PDF](https://arxiv.org/pdf/1706.03762)\n* [HTML (experimental)](https://arxiv.org/html/1706.03762v7)\n* [TeX Source](https://arxiv.org/src/1706.03762)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1706.03762&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1706.03762&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2017-06](https://arxiv.org/list/cs.CL/2017-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1706.03762?context=cs)\n[cs.LG](https://arxiv.org/abs/1706.03762?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1706.03762)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1706.03762)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1706.03762)\n### [123 blog links](https://arxiv.org/tb/1706.03762)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1706.html#VaswaniSPUJGKP17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/VaswaniSPUJGKP17)\n[Ashish Vaswani]()\n[Noam Shazeer]()\n[Niki Parmar]()\n[Jakob Uszkoreit]()\n[Llion Jones]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces",
          "original_query": "Attention Is All You Need (Vaswani et al., 2017)",
          "cleaned_query": "Attention Is All You Need"
        },
        {
          "success": true,
          "title": "Neural Tangent Kernel: Convergence and Generalization in ... - arXiv",
          "url": "https://arxiv.org/abs/1806.07572",
          "content": "[1806.07572] Neural Tangent Kernel: Convergence and Generalization in Neural Networks[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1806.07572\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1806.07572**(cs)\n[Submitted on 20 Jun 2018 ([v1](https://arxiv.org/abs/1806.07572v1)), last revised 10 Feb 2020 (this version, v4)]\n# Title:Neural Tangent Kernel: Convergence and Generalization in Neural Networks\nAuthors:[Arthur Jacot](https://arxiv.org/search/cs?searchtype=author&amp;query=Jacot,+A),[Franck Gabriel](https://arxiv.org/search/cs?searchtype=author&amp;query=Gabriel,+F),[Cl\u00e9ment Hongler](https://arxiv.org/search/cs?searchtype=author&amp;query=Hongler,+C)\nView a PDF of the paper titled Neural Tangent Kernel: Convergence and Generalization in Neural Networks, by Arthur Jacot and 2 other authors\n[View PDF](https://arxiv.org/pdf/1806.07572)> > Abstract:\n> At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function $f_\\theta$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function $f_\\theta$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit. Subjects:|Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Probability (math.PR); Machine Learning (stat.ML)|\nCite as:|[arXiv:1806.07572](https://arxiv.org/abs/1806.07572)[cs.LG]|\n|(or[arXiv:1806.07572v4](https://arxiv.org/abs/1806.07572v4)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1806.07572](https://doi.org/10.48550/arXiv.1806.07572)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\nJournalreference:|In Advances in neural information processing systems (pp. 8571-8580) 2018|\n## Submission history\nFrom: Arthur Jacot [[view email](https://arxiv.org/show-email/cfefbf84/1806.07572)]\n**[[v1]](https://arxiv.org/abs/1806.07572v1)**Wed, 20 Jun 2018 06:35:46 UTC (211 KB)\n**[[v2]](https://arxiv.org/abs/1806.07572v2)**Mon, 12 Nov 2018 10:31:42 UTC (125 KB)\n**[[v3]](https://arxiv.org/abs/1806.07572v3)**Mon, 26 Nov 2018 15:42:05 UTC (127 KB)\n**[v4]**Mon, 10 Feb 2020 08:39:09 UTC (128 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Neural Tangent Kernel: Convergence and Generalization in Neural Networks, by Arthur Jacot and 2 other authors\n* [View PDF](https://arxiv.org/pdf/1806.07572)\n* [TeX Source](https://arxiv.org/src/1806.07572)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1806.07572&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1806.07572&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2018-06](https://arxiv.org/list/cs.LG/2018-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1806.07572?context=cs)\n[cs.NE](https://arxiv.org/abs/1806.07572?context=cs.NE)\n[math](https://arxiv.org/abs/1806.07572?context=math)\n[math.PR](https://arxiv.org/abs/1806.07572?context=math.PR)\n[stat](https://arxiv.org/abs/1806.07572?context=stat)\n[stat.ML](https://arxiv.org/abs/1806.07572?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1806.07572)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1806.07572)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1806.07572)\n### [2 blog links](https://arxiv.org/tb/1806.07572)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1806.html#abs-1806-07572)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1806-07572)\n[Arthur Jacot]()\n[Franck Gabriel]()\n[Cl\u00e9ment Hongler]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/wel",
          "original_query": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks (Jacot et al., 2018)",
          "cleaned_query": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks"
        },
        {
          "success": true,
          "title": "Gradient Flow Matching for Learning Update Dynamics in Neural ...",
          "url": "https://arxiv.org/html/2505.20221v1",
          "content": "\n \n \n Xiao Shou \n Department of Computer Science\n Baylor University\n Waco, TX 76706 \n xiao_shou@baylor.edu \n &amp;Yanna Ding \n Department of Computer Science\n RPI \n Troy, NY 12180 \n dingy6@rpi.edu \nJianxi Gao \n Department of Computer Science\n RPI \n Troy, NY 12180 \n gaoj8@rpi.edu \n \n \n \n \n Abstract \n Training deep neural networks remains computationally intensive due to the iterative nature of gradient-based optimization. We propose Gradient Flow Matching (GFM), a continuous-time modeling framework that treats neural network training as a dynamical system governed by learned optimizer-aware vector fields. By leveraging conditional flow matching, GFM captures the underlying update rules of optimizers such as SGD, Adam, and RMSprop, enabling smooth extrapolation of weight trajectories toward convergence. Unlike black-box sequence models, GFM incorporates structural knowledge of gradient-based updates into the learning objective, facilitating accurate forecasting of final weights from partial training sequences. Empirically, GFM achieves forecasting accuracy that is competitive with Transformer-based models and significantly outperforms LSTM and other classical baselines. Furthermore, GFM generalizes across neural architectures and initializations, providing a unified framework for studying optimization dynamics and accelerating convergence prediction. \n \n \n \n 1 Introduction \n Deep neural networks (DNNs) have achieved remarkable success in tasks such as image classification\u00a0 (LeCun et\u00a0al., 1998) and natural language processing\u00a0 (Vaswani et\u00a0al., 2017), driven by increasingly deep and complex architectures. As models scale to hundreds of layers and billions of parameters, training them requires vast computational resources and prolonged optimization cycles. The resulting workloads scale non-linearly with model size, creating serious bottlenecks in time and resource efficiency. \n While inference has benefited from acceleration techniques like model compression\u00a0 (Tan and Le, 2019) and knowledge distillation\u00a0 (Sanh et\u00a0al., 2019), improving training efficiency remains a more fundamental challenge. This is largely because gradient-based optimization is inherently iterative: algorithms like SGD and Adam\u00a0 (Kingma and Ba, 2014) update weights step-by-step, requiring thousands of iterations to reach convergence. Increasing hardware capacity helps, but cannot fully eliminate the cost of these sequential updates. Addressing this bottleneck calls for methods that rethink how optimization trajectories are modeled and predicted. \n Recent studies have revealed that weight evolution during training exhibits predictable patterns, opening opportunities for forecasting-based acceleration. For instance, the Weight Nowcaster Network (WNN)\u00a0 (Jang et\u00a0al., 2023) predicts near-future weights to skip small optimization steps, while LFD-2\u00a0 (Shou et\u00a0al., 2024) performs long-horizon \"farcasting\" from partial weight sequences. However, these methods treat optimization as a generic sequence modeling problem, without explicitly modeling the underlying dynamics of gradient-based updates. Consequently, they overlook the structured relationship between gradients, learning rates, and weight trajectories that governs the optimization process itself. \n \n In this work, we propose Gradient Flow Matching (GFM), a continuous-time framework that models neural network training as a dynamical system governed by optimizer-aware vector fields. Leveraging conditional flow matching\u00a0 (Tong et\u00a0al., 2023), GFM reinterprets weight trajectories not as arbitrary sequences, but as realizations of structured flows conditioned on partial observations and optimization algorithms. This perspective enables more accurate and generalizable forecasting of final weights by explicitly incorporating the dynamics of gradient-based updates. Our main contributions are as follows: \n \n \n \u2022 \n A flow-matching-compatible formulation of first-order dynamics. While continuous-time views of gradient descent are well-studied, we tailor them specifically for conditional flow matching, enabling the learning of vector fields that are both optimizer-aware and trajectory-faithful. \n \n \n \u2022 \n A conditional flow matching loss for weight evolution. We introduce a loss function that aligns learned vector fields with observed parameter updates, supporting both short-term extrapolation and long-horizon forecasting within a unified framework. \n \n \n \u2022 \n Extension to momentum and adaptive optimizers. GFM accommodates common optimizers such as Adam\u00a0 (Kingma and Ba, 2014) and RMSProp\u00a0 (Shi and Li, 2021), capturing their dynamics within a continuous-time approximation through the learned flow field. \n \n \n \n By marrying weight trajectory forecasting with continuous-time modeling, GFM offers a unified and extensible lens for analyzing and predicting the evolution of neural network weights. \n \n \n \n 2 Related Work \n \n Weight Forecasting and Acceleration. \n The idea of forecasting weights to accelerate training dates back to Introspection\u00a0 (Sinha et\u00a0al., 2017), which used past training history to predict future parameters. The Weight Nowcaster Network (WNN)\u00a0 (Jang et\u00a0al., 2023) extended this idea to short-horizon prediction, enabling periodic skipping of optimization steps. While effective on small-scale models, WNN and its variants typically forecast only a few steps ahead and treat each weight independently. To support long-range forecasting, Shou et al.\u00a0 (Shou et\u00a0al., 2024) proposed \"farcasting\" with LFD-2, showing that even simple models can extrapolate full training trajectories from just the initial and final weights. Knyazev et al.\u00a0 (Knyazev et\u00a0al., 2025) concurrently introduced a graph-based approach (NiNo) that incorporates architectural structure into the forecasting process. Other acceleration strategies include meta-learned optimizers such as L2O\u00a0 (Chen et\u00a0al., 2022), which train neural networks to generate optimizer updates, albeit with significant meta-training cost. Simpler alternatives like Lookahead\u00a0 (Zhang et\u00a0al., 2019) reduce overhead by interleaving fast inner updates with slow outer updates, leveraging future trends without explicitly predicting them. \n \n \n Flow-Based and Continuous-Time Models. \n Continuous gradient flows provide an alternative perspective on optimization by interpreting training as a trajectory governed by an ordinary differential equation (ODE). Neural ODEs\u00a0 (Chen et\u00a0al., 2018) enable continuous-time modeling of hidden states in deep networks, offering flexibility for irregularly spaced observations. However, they can be computationally expensive due to the need for numerical integration at each time step, particularly in stiff or high-dimensional systems. Moreover, standard Neural ODEs are not specifically designed to capture the structure of gradient-based optimization dynamics. In practice, many forecasting methods discretize these trajectories using fixed steps, making them analogous to sequence models like LSTMs. Flow Matching (FM)\u00a0 ( Lipman et\u00a0al.,) and its conditional variant\u00a0 (Tong et\u00a0al., 2023) offer a scalable, simulation-free way to learn continuous-time dynamics. \n \n \n Time Series Forecasting. \n Our work is related to long-term time series forecasting, where both classical and modern models have been extensively explored. Recent studies have demonstrated that simple linear models can rival deep sequence models for long-horizon prediction, as shown in DLinear and its variants\u00a0 (Zeng et\u00a0al., 2023). Yet applying standard time-series models such as \u00a0 (Nie et\u00a0al., 2022) or RNNs directly to neural weight sequences is often impractical due to the high dimensionality and nonstationary nature of the optimization trajectory. While recent efforts explore foundation models for time series\u00a0 (Ye et\u00a0al., 2024), their computational demands remain a challenge in our setting. \n \n \n Learning Curve Extrapolation. \n An orthogonal line of research focuses on predicting the evolution of training loss or accuracy over time, r",
          "original_query": "A gradient-flow analytical framework for the dynamics of learning (Zhou et al., 2022)",
          "cleaned_query": "A gradient-flow analytical framework for the dynamics of learning"
        },
        {
          "success": true,
          "title": "Implicit Bias: What Is It? (Chapter 2) - The Cambridge Handbook of Implicit Bias and Racism",
          "url": "https://www.cambridge.org/core/books/abs/cambridge-handbook-of-implicit-bias-and-racism/implicit-bias-what-is-it/B08CC53F541F6A9F3AAF87516D5ED7CA",
          "content": "Implicit Bias: What Is It? (Chapter 2) - The Cambridge Handbook of Implicit Bias and Racism\n[Skip to main content](#maincontent)[Accessibility help](https://www.cambridge.org/core/accessibility)\n## Login Alert\n[Cancel](#)\n[Log in](https://www.cambridge.org/core/login?ref&#x3D;/core/books/abs/cambridge-handbook-of-implicit-bias-and-racism/implicit-bias-what-is-it/B08CC53F541F6A9F3AAF87516D5ED7CA)\n[\u00d7](#)\n[](#)\n[](#)\n&#215;\n[](https://www.cambridge.org/academic)\n[\nLogo for Cambridge Core from Cambridge University Press. Click to return to homepage.\n](https://www.cambridge.org/core/)\nSearch\n[\nLogo for Cambridge Core from Cambridge University Press. Click to return to homepage.\n](https://www.cambridge.org/core/)\nInstitution Login\nSearch\nHostname: page-component-68c7f8b79f-kpv4p\nTotal loading time: 0\nRender date: 2025-12-16T09:26:35.211Z\nHas data issue: false\nhasContentIssue false\n* [Home](https://www.cambridge.org/core)\n* &gt;[Books](https://www.cambridge.org/core/publications/books)\n* &gt;[The Cambridge Handbook of Implicit Bias and Racism](https://www.cambridge.org/core/books/the-cambridge-handbook-of-implicit-bias-and-racism/DE430B0B0E51DEA222DC78896656A982)\n* &gt;Implicit Bias: What Is It?\n![](https://assets.cambridge.org/97811088/40309/cover/9781108840309.jpg)[The Cambridge Handbook of Implicit Bias and Racism](https://www.cambridge.org/core/books/the-cambridge-handbook-of-implicit-bias-and-racism/DE430B0B0E51DEA222DC78896656A982)\n[Buy print or eBook[Opens in a new window]](https://www.cambridge.org/core_title/gb/557381)\n## Book contents\n* [\nThe Cambridge Handbook of Implicit Bias and Racism\n](https://www.cambridge.org/core/product/identifier/9781108885492#HT-FNMP-1/type/BOOK_PART)\n* [\nCambridge Handbooks in Psychology\n](https://www.cambridge.org/core/product/identifier/9781108885492#ST-FNMP-1/type/BOOK_PART)\n* [\nThe Cambridge Handbook of Implicit Bias and Racism\n](https://www.cambridge.org/core/product/identifier/9781108885492#TPT-FNMP-1/type/BOOK_PART)\n* [\nCopyright page\n](https://www.cambridge.org/core/product/identifier/9781108885492#IMP-FNMP-1/type/BOOK_PART)\n* [\nContents\n](https://www.cambridge.org/core/product/identifier/9781108885492#FMT-TOC-1/type/BOOK_PART)\n* [\nFigures\n](https://www.cambridge.org/core/product/identifier/9781108885492#FMT-TOC-2/type/BOOK_PART)\n* [\nTables\n](https://www.cambridge.org/core/product/identifier/9781108885492#FMT-TOC-3/type/BOOK_PART)\n* [\nContributors\n](https://www.cambridge.org/core/product/identifier/9781108885492#FMT-FNMP-1/type/BOOK_PART)\n* [\nForeword\n](https://www.cambridge.org/core/product/identifier/9781108885492#FMT-FWD-1/type/BOOK_PART)\n* [\nTaking Stock of Explicit and Implicit Prejudice\n](https://www.cambridge.org/core/product/identifier/9781108885492#CT-BP-1/type/BOOK_PART)\n* [1\nReport from the NSF Conference on Implicit Bias\n](https://www.cambridge.org/core/product/identifier/9781108885492#CN-BP-1/type/BOOK_PART)\n* [Section I\nWhat is Implicit Bias and (How) Can We Measure It?\n](https://www.cambridge.org/core/product/identifier/9781108885492#PTN-BP-1/type/BOOK_PART)\n* [\nIntroduction\n](https://www.cambridge.org/core/product/identifier/9781108885492#CT-BP-2/type/BOOK_PART)\n* [2\nImplicit Bias: What Is It?\n](https://www.cambridge.org/core/product/identifier/9781108885492#CN-BP-2/type/BOOK_PART)\n* [3\nLessons from Two Decades of Project Implicit\n](https://www.cambridge.org/core/product/identifier/9781108885492#CN-BP-3/type/BOOK_PART)\n* [4\nAversive Racism and Implicit Bias\n](https://www.cambridge.org/core/product/identifier/9781108885492#CN-BP-4/type/BOOK_PART)\n* [5\nStretching the Limits of Science: Was the Implicit-Racism Debate a \u201cBridge Too Far\u201d for Social Psychology?\n](https://www.cambridge.org/core/product/identifier/9781108885492#CN-BP-5/type/BOOK_PART)\n* [Section II\nDo Measures of Implicit Bias Predict Cognition and Behavior?\n](https://www.cambridge.org/core/product/identifier/9781108885492#PTN-BP-2/type/BOOK_PART)\n* [Section III\nChallenges of Research on Implicit Bias\n](https://www.cambridge.org/core/product/identifier/9781108885492#PTN-BP-3/type/BOOK_PART)\n* [Section IV\nImproving Measurement and Theorizing About Implicit Bias\n](https://www.cambridge.org/core/product/identifier/9781108885492#PTN-BP-4/type/BOOK_PART)\n* [Section V\nHow to Change Implicit Bias?\n](https://www.cambridge.org/core/product/identifier/9781108885492#PTN-BP-5/type/BOOK_PART)\n* [Section VI\nExplicit Prejudice; Alive and Well?\n](https://www.cambridge.org/core/product/identifier/9781108885492#PTN-BP-6/type/BOOK_PART)\n* [Section VII\nThe Public\u2019s (Mis)understanding of Implicit Bias\n](https://www.cambridge.org/core/product/identifier/9781108885492#PTN-BP-7/type/BOOK_PART)\n* [\nIndex\n](https://www.cambridge.org/core/product/identifier/9781108885492#EMT-IDX-1/type/BOOK_PART)\n* [References](#references-list)\n# 2 - Implicit Bias: What Is It?\nfromSection I - What is Implicit Bias and (How) Can We Measure It?\nPublished online by Cambridge University Press:**21 December 2024**\nBy\n[Russell H. Fazio](https://www.cambridge.org/core/search?filters[authorTerms]=Russell%20H.%20Fazio&amp;eventCode=SE-AU),\n[Javier A. Granados Samayoa](https://www.cambridge.org/core/search?filters[authorTerms]=Javier%20A.%20Granados%20Samayoa&amp;eventCode=SE-AU),\n[Shelby T. Boggs](https://www.cambridge.org/core/search?filters[authorTerms]=Shelby%20T.%20Boggs&amp;eventCode=SE-AU)and\n[Jesse Ladanyi](https://www.cambridge.org/core/search?filters[authorTerms]=Jesse%20Ladanyi&amp;eventCode=SE-AU)\nEdited by\n[Jon A. Krosnick](https://www.cambridge.org/core/search?filters[authorTerms]=Jon%20A.%20Krosnick&amp;eventCode=SE-AU),\n[Tobias H. Stark](https://www.cambridge.org/core/search?filters[authorTerms]=Tobias%20H.%20Stark&amp;eventCode=SE-AU)and\n[Amanda L. Scott](https://www.cambridge.org/core/search?filters[authorTerms]=Amanda%20L.%20Scott&amp;eventCode=SE-AU)\n[Show author details](#authors-details)\nJon A. KrosnickAffiliation:\nStanford University, California\nTobias H. StarkAffiliation:\nUtrecht University, The Netherlands\nAmanda L. ScottAffiliation:\nThe Strategy Team, Columbus, Ohio\n[](#appTabs)\n* [Chapter](#chapter-tab)\n* [Accessibility](#accessibility-tab)\n[Book contents](#toc-list-mobile)\n* [The Cambridge Handbook of Implicit Bias and Racism](https://www.cambridge.org/core/product/identifier/9781108885492#HT-FNMP-1/type/BOOK_PART)\n* [Cambridge Handbooks in Psychology](https://www.cambridge.org/core/product/identifier/9781108885492#ST-FNMP-1/type/BOOK_PART)\n* [The Cambridge Handbook of Implicit Bias and Racism](https://www.cambridge.org/core/product/identifier/9781108885492#TPT-FNMP-1/type/BOOK_PART)\n* [Copyright page](https://www.cambridge.org/core/product/identifier/9781108885492#IMP-FNMP-1/type/BOOK_PART)\n* [Contents](https://www.cambridge.org/core/product/identifier/9781108885492#FMT-TOC-1/type/BOOK_PART)\n* [Figures](https://www.cambridge.org/core/product/identifier/9781108885492#FMT-TOC-2/type/BOOK_PART)\n* [Tables](https://www.cambridge.org/core/product/identifier/9781108885492#FMT-TOC-3/type/BOOK_PART)\n* [Contributors](https://www.cambridge.org/core/product/identifier/9781108885492#FMT-FNMP-1/type/BOOK_PART)\n* [Foreword](https://www.cambridge.org/core/product/identifier/9781108885492#FMT-FWD-1/type/BOOK_PART)\n* [Taking Stock of Explicit and Implicit Prejudice](https://www.cambridge.org/core/product/identifier/9781108885492#CT-BP-1/type/BOOK_PART)\n* [1Report from the NSF Conference on Implicit Bias](https://www.cambridge.org/core/product/identifier/9781108885492#CN-BP-1/type/BOOK_PART)\n* [Section IWhat is Implicit Bias and (How) Can We Measure It?](https://www.cambridge.org/core/product/identifier/9781108885492#PTN-BP-1/type/BOOK_PART)\n* [Introduction](https://www.cambridge.org/core/product/identifier/9781108885492#CT-BP-2/type/BOOK_PART)\n* [2Implicit Bias: What Is It?](https://www.cambridge.org/core/product/identifier/9781108885492#CN-BP-2/type/BOOK_PART)\n* [3Lessons from Two Decades of Project Implicit](https://www.cambridge.org/core/product/identifier/9781108885492#CN",
          "original_query": "Implicit bias / directional convergence results for linear networks and matrix factorization (e.g., Gunasekar et al., Saxe et al.)",
          "cleaned_query": "Implicit bias",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "[2011.10036] On the Dynamics of Training Attention Models - arXiv",
          "url": "https://arxiv.org/abs/2011.10036",
          "content": "[2011.10036] On the Dynamics of Training Attention Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2011.10036\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2011.10036**(cs)\n[Submitted on 19 Nov 2020 ([v1](https://arxiv.org/abs/2011.10036v1)), last revised 19 Mar 2021 (this version, v2)]\n# Title:On the Dynamics of Training Attention Models\nAuthors:[Haoye Lu](https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+H),[Yongyi Mao](https://arxiv.org/search/cs?searchtype=author&amp;query=Mao,+Y),[Amiya Nayak](https://arxiv.org/search/cs?searchtype=author&amp;query=Nayak,+A)\nView a PDF of the paper titled On the Dynamics of Training Attention Models, by Haoye Lu and 2 other authors\n[View PDF](https://arxiv.org/pdf/2011.10036)> > Abstract:\n> The attention mechanism has been widely used in deep neural networks as a model component. By now, it has become a critical building block in many state-of-the-art natural language models. Despite its great success established empirically, the working mechanism of attention has not been investigated at a sufficient theoretical depth to date. In this paper, we set up a simple text classification task and study the dynamics of training a simple attention-based classification model using gradient descent. In this setting, we show that, for the discriminative words that the model should attend to, a persisting identity exists relating its embedding and the inner product of its key and the query. This allows us to prove that training must converge to attending to the discriminative words when the attention output is classified by a linear classifier. Experiments are performed, which validate our theoretical analysis and provide further insights. Subjects:|Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)|\nCite as:|[arXiv:2011.10036](https://arxiv.org/abs/2011.10036)[cs.LG]|\n|(or[arXiv:2011.10036v2](https://arxiv.org/abs/2011.10036v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2011.10036](https://doi.org/10.48550/arXiv.2011.10036)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Haoye Lu Mr. [[view email](https://arxiv.org/show-email/c1c6f9f9/2011.10036)]\n**[[v1]](https://arxiv.org/abs/2011.10036v1)**Thu, 19 Nov 2020 18:55:30 UTC (1,017 KB)\n**[v2]**Fri, 19 Mar 2021 03:51:05 UTC (1,064 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled On the Dynamics of Training Attention Models, by Haoye Lu and 2 other authors\n* [View PDF](https://arxiv.org/pdf/2011.10036)\n* [TeX Source](https://arxiv.org/src/2011.10036)\n[![license icon](https://arxiv.org/icons/licenses/by-nc-sa-4.0.png)view license](http://creativecommons.org/licenses/by-nc-sa/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2011.10036&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2011.10036&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2020-11](https://arxiv.org/list/cs.LG/2020-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/2011.10036?context=cs)\n[cs.NE](https://arxiv.org/abs/2011.10036?context=cs.NE)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2011.10036)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2011.10036)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2011.10036)\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2011.html#abs-2011-10036)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2011-10036)\n[Haoye Lu]()\n[Yongyi Mao]()\n[Amiya Nayak]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2011.10036)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Training dynamics analyses of attention / single-block Transformer variants (e.g., Tian et al., 2023)",
          "cleaned_query": "Training dynamics analyses of attention"
        },
        {
          "success": true,
          "title": "An analysis for reasoning bias of language models with small ... - arXiv",
          "url": "https://arxiv.org/html/2502.04375v1",
          "content": "An analysis for reasoning bias of language models with small initialization\n# An analysis for reasoning bias of language models with small initialization\nJunjie YaoInstitute of Natural Sciences, MOE-LSC, Shanghai Jiao Tong UniversitySchool of Mathematical Sciences, Shanghai Jiao Tong UniversityZhongwang ZhangInstitute of Natural Sciences, MOE-LSC, Shanghai Jiao Tong UniversitySchool of Mathematical Sciences, Shanghai Jiao Tong UniversityCorresponding author:[0123zzw666@sjtu.edu.cn](mailto:0123zzw666@sjtu.edu.cn),[xuzhiqin@sjtu.edu.cn](mailto:xuzhiqin@sjtu.edu.cn)Zhi-Qin John XuInstitute of Natural Sciences, MOE-LSC, Shanghai Jiao Tong UniversitySchool of Mathematical Sciences, Shanghai Jiao Tong UniversitySchool of Artificial Intelligence, Shanghai Jiao Tong UniversityKey Laboratory of Marine Intelligent Equipment and System, Ministry of Education, P.R. ChinaCenter for LLM, Institute for Advanced Algorithms Research, ShanghaiCorresponding author:[0123zzw666@sjtu.edu.cn](mailto:0123zzw666@sjtu.edu.cn),[xuzhiqin@sjtu.edu.cn](mailto:xuzhiqin@sjtu.edu.cn)\n###### Abstract\nTransformer-based Large Language Models (LLMs) have revolutionized Natural Language Processing by demonstrating exceptional performance across diverse tasks. This study investigates the impact of the parameter initialization scale on the training behavior and task preferences of LLMs. We discover that smaller initialization scales encourage models to favor reasoning tasks, whereas larger initialization scales lead to a preference for memorization tasks. We validate this reasoning bias via real datasets and meticulously designed anchor functions. Further analysis of initial training dynamics suggests that specific model components, particularly the embedding space and self-attention mechanisms, play pivotal roles in shaping these learning biases. We provide a theoretical framework from the perspective of model training dynamics to explain these phenomena. Additionally, experiments on real-world language tasks corroborate our theoretical insights. This work enhances our understanding of how initialization strategies influence LLM performance on reasoning tasks and offers valuable guidelines for training models.\n## 1Introduction\nWith the rapid advancement of deep learning technologies, Large Language Models have achieved remarkable success in the field of Natural Language Processing (NLP). These models have demonstrated exceptional capabilities across a wide range of tasks, from text generation to complex reasoning. Reasoning, in particular, is a critical ability for LLMs. A number of studies have focused on improving the reasoning ability of these models through data-driven approaches, such as RHO-1> [\n[> 20\n](https://arxiv.org/html/2502.04375v1#bib.bib20)> ]\nand Phi-4> [\n[> 1\n](https://arxiv.org/html/2502.04375v1#bib.bib1)> ]\n. However, there remains an ongoing debate as to whether LLMs genuinely learn the underlying logical rules or merely mimic patterns observed in the data> [\n[> 24\n](https://arxiv.org/html/2502.04375v1#bib.bib24)> , [> 32\n](https://arxiv.org/html/2502.04375v1#bib.bib32)> ]\n.\n![Refer to caption](extracted/6180987/figure/PrOntoQA/qa_tinystory.png)Figure 1:Comparison of training loss between PrOntoQA and TinyStories in one next-token prediction training for this mix dataset. The red line represents the training loss on the PrOntoQA dataset, while the blue line depicts the training loss on the TinyStories dataset.\nAn alternative approach to enhancing the reasoning ability of LLMs focuses on the model architecture and its training process. In one such study examining the use of Transformers to model compositional functions, it was observed that the scale of model parameter initialization significantly impacts the model\u2019s reasoning behavior> [\n[> 48\n](https://arxiv.org/html/2502.04375v1#bib.bib48)> , [> 49\n](https://arxiv.org/html/2502.04375v1#bib.bib49)> ]\n. Specifically, smaller initialization scales bias the model toward fitting the data by learning primitive-level functions and compositional rules, whereas larger initialization scales tend to encourage memorization of input-output mappings. A qualitative rationale for this phenomenon has been proposed: with a small initialization, a well-documented effect known as neuron condensation emerges during training> [\n[> 23\n](https://arxiv.org/html/2502.04375v1#bib.bib23)> , [> 53\n](https://arxiv.org/html/2502.04375v1#bib.bib53)> , [> 46\n](https://arxiv.org/html/2502.04375v1#bib.bib46)> , [> 51\n](https://arxiv.org/html/2502.04375v1#bib.bib51)> , [> 47\n](https://arxiv.org/html/2502.04375v1#bib.bib47)> , [> 52\n](https://arxiv.org/html/2502.04375v1#bib.bib52)> ]\n. This phenomenon suggests that neurons within the same layer tend to behave similarly, promoting data fitting with the least possible complexity. To achieve a low-complexity result, the model must learn a minimal set of rules leading to capture the intrinsic primitive functions and compositional rules. However, this rationale does not reveal a critical question: how the optimization process, together with the Transformer structure, can achieve reasoning solutions with small initialization.\nIn this work, we identify a reasoning bias during the training of neural networks that learn natural language when initialized with small parameter scales. To illustrate this phenomenon, we employ a GPT-2 model> [\n[> 27\n](https://arxiv.org/html/2502.04375v1#bib.bib27)> ]\nto train on a mixed dataset comprising two types of language data with distinct levels of reasoning complexity, within a single next-token prediction training framework. The first dataset, PrOntoQA> [\n[> 30\n](https://arxiv.org/html/2502.04375v1#bib.bib30)> ]\n, consists of question-answering examples that include chains of thought, which explicitly describe the reasoning necessary to answer the questions correctly. The second dataset, TinyStories> [\n[> 12\n](https://arxiv.org/html/2502.04375v1#bib.bib12)> ]\n, is a synthetic corpus of short stories containing only words typically understood by children aged 3 to 4 years. As shown in Figure[1](https://arxiv.org/html/2502.04375v1#S1.F1), the training loss for PrOntoQA decreases significantly faster than for TinyStories, suggesting that the model encounters and learns the reasoning patterns more readily.\nWe uncover a key mechanism whereby reasoning tasks are learned earlier during training because the tokens associated with these tasks become more differentiated in the embedding space at an early stage of the training process. We validate this mechanism using both synthetic data and real-world datasets. Furthermore, we provide a theoretical explanation for the evolution of token embeddings, which depends on the distribution of sample labels. Since each token is encoded as a one-hot vector, its embedding is adjusted based on the loss associated with the labels of that token. Consequently, different label distributions can lead to distinct learning behaviors for token embeddings. For memory tasks, the labels associated with each token are typically random and lack explicit structure, which results in similar distributions for different memory token labels. As a result, the embeddings for memory tokens are difficult to differentiate in the early stages of training. In contrast, reasoning tokens often exhibit distinct label distributions, leading to more differentiated embedding vectors for these tokens. These insights are elaborated through a simplified model using a multi-layer perceptron (MLP) and embedding structure, followed by an analysis of a Transformer model.\nThe primary contribution of this research lies in uncovering the profound impact of the parameter initialization scale on the training behavior and task preferences of LLMs. By combining theoretical analysis with empirical evidence, we enhance the understanding of LLM training dynamics and provide new insights for optimizing model initialization strategies.\n## 2Related Works\nRecent advancements in large language mo",
          "original_query": "Empirical and theoretical work on small initialization / implicit regularization in large models (e.g., Soudry et al., later empirical studies)",
          "cleaned_query": "Empirical and theoretical work on small initialization"
        }
      ],
      "generated_ideas": [
        "1. **NTK-Guided Initialization for Reasoning-Favoring Transformers**  \nDerive and empirically validate a criterion that chooses per-module initialization scales (embeddings, Q/K/V, MLP) to maximize \u201creasoning-mode\u201d learning signals predicted by the Transformer\u2019s Neural Tangent Kernel (NTK) spectrum. Concretely, estimate early-training NTK eigenvalue alignment on mixed reasoning vs memorization corpora (as in Paper 6) and tune init scales to favor components that speed up learning on reasoning-labeled subsets.",
        "**Flow-Matching Forecasting of Attention Maps (Not Just Weights)**  \nExtend Gradient Flow Matching (GFM) to jointly model the continuous-time dynamics of weights and induced attention matrices, adding a consistency loss between predicted weights and predicted attention patterns. This yields a practical tool to forecast when/where attention will concentrate (Paper 5) from partial training, enabling early detection of \u201cspurious\u201d vs \u201cdiscriminative\u201d attention behaviors.",
        "**Optimizer-Aware NTK Drift Modeling for Transformers**  \nCombine NTK theory (Paper 2) with GFM\u2019s learned vector fields (Paper 3) by explicitly predicting the time-evolution (\u201cdrift\u201d) of the empirical NTK under different optimizers (SGD/Adam/RMSProp). The contribution is a measurable bridge between finite-width Transformer training and kernel predictions: a model that forecasts not only final weights but also how kernel geometry changes, and how that correlates with generalization on language tasks.",
        "**Provable Convergence-to-Discriminative Tokens in Multi-Head Self-Attention**  \nGeneralize the convergence result from a simple attention classifier (Paper 5) to multi-head self-attention blocks with residual connections, isolating conditions under which at least one head provably locks onto discriminative tokens. The actionable outcome is a set of architectural/regularization knobs (e.g., head-wise temperature schedules, norm constraints on keys/queries) that guarantee discriminative attention emergence in controlled text classification setups.",
        "**Early-Stopping Rules from Attention-NTK Principal Components**  \nUse the NTK principal component analysis perspective (Paper 2) to build an early-stopping criterion specialized for attention models: stop when the loss projection onto top attention-NTK eigenfunctions saturates while lower components begin fitting (overfitting onset). Implement this by approximating leading eigencomponents of the empirical NTK for a Transformer layer subset and validating reduced memorization with preserved reasoning performance on mixed datasets (Paper 6).",
        "**Selective \u201cReasoning Bias\u201d via Layerwise Small-Init Schedules**  \nInstead of globally small initialization (Paper 6), design a layerwise schedule where early layers (embeddings + first attention blocks) are initialized small to promote compositional rule learning, while later layers use standard scales to preserve capacity. Test the hypothesis that this produces a controllable tradeoff curve between reasoning accuracy and memorization, and identify which submodules dominate the bias using ablations guided by Paper 6\u2019s component analysis.",
        "**Fairness-Aware Training Dynamics: Linking Implicit Bias to Attention Formation**  \nOperationalize constructs from implicit bias measurement (Paper 4) into a dataset and evaluation protocol where \u201cimplicit associations\u201d are reflected as spurious correlates in text (e.g., demographic terms co-occurring with sentiment/competence labels). Then analyze whether small-init \u201creasoning bias\u201d (Paper 6) and attention convergence properties (Paper 5) reduce reliance on spurious demographic tokens, using attention attribution and counterfactual token swaps as mechanistic probes.",
        "**Flow-Matching Accelerated Training with Corrective Kernel Constraints**  \nTurn GFM from a forecaster into an accelerator: periodically jump along the learned continuous-time flow to skip optimizer steps (Paper 3), but add a constraint that keeps the model within a trust region defined by preserving key NTK statistics (Paper 2). This aims to prevent \u201cforecast drift\u201d that harms generalization, producing an actionable algorithm with measurable wall-clock savings and bounded deviation in function space.",
        "**Transformer Design via Target NTK Engineering for Long-Range Dependency Tasks**  \nUse the NTK framework (Paper 2) to define desired kernel properties for long-context sequence transduction (Paper 1), then modify attention parameterization (e.g., scaling of Q/K, positional encoding choices, head dimension allocation) to match a target kernel profile. The contribution is a principled, measurable design loop: predict which architectural changes increase kernel sensitivity to long-range token interactions, then validate on synthetic and real long-context benchmarks.",
        "**Mechanistic Prediction of Memorization vs Reasoning from Early Attention Statistics**  \nBuild a diagnostic that predicts\u2014within the first N training steps\u2014whether a Transformer will end up memorization-dominant or reasoning-dominant (Paper 6), using features from attention dynamics (entropy, head specialization, query-key inner product growth) motivated by Paper 5. Pair this with a lightweight intervention (e.g., adaptive rescaling of embeddings or attention temperature) to flip trajectories toward reasoning without changing data, providing a concrete control method grounded in training dynamics."
      ],
      "generated_ideas_raw": "1. **NTK-Guided Initialization for Reasoning-Favoring Transformers**  \nDerive and empirically validate a criterion that chooses per-module initialization scales (embeddings, Q/K/V, MLP) to maximize \u201creasoning-mode\u201d learning signals predicted by the Transformer\u2019s Neural Tangent Kernel (NTK) spectrum. Concretely, estimate early-training NTK eigenvalue alignment on mixed reasoning vs memorization corpora (as in Paper 6) and tune init scales to favor components that speed up learning on reasoning-labeled subsets.\n\n2. **Flow-Matching Forecasting of Attention Maps (Not Just Weights)**  \nExtend Gradient Flow Matching (GFM) to jointly model the continuous-time dynamics of weights and induced attention matrices, adding a consistency loss between predicted weights and predicted attention patterns. This yields a practical tool to forecast when/where attention will concentrate (Paper 5) from partial training, enabling early detection of \u201cspurious\u201d vs \u201cdiscriminative\u201d attention behaviors.\n\n3. **Optimizer-Aware NTK Drift Modeling for Transformers**  \nCombine NTK theory (Paper 2) with GFM\u2019s learned vector fields (Paper 3) by explicitly predicting the time-evolution (\u201cdrift\u201d) of the empirical NTK under different optimizers (SGD/Adam/RMSProp). The contribution is a measurable bridge between finite-width Transformer training and kernel predictions: a model that forecasts not only final weights but also how kernel geometry changes, and how that correlates with generalization on language tasks.\n\n4. **Provable Convergence-to-Discriminative Tokens in Multi-Head Self-Attention**  \nGeneralize the convergence result from a simple attention classifier (Paper 5) to multi-head self-attention blocks with residual connections, isolating conditions under which at least one head provably locks onto discriminative tokens. The actionable outcome is a set of architectural/regularization knobs (e.g., head-wise temperature schedules, norm constraints on keys/queries) that guarantee discriminative attention emergence in controlled text classification setups.\n\n5. **Early-Stopping Rules from Attention-NTK Principal Components**  \nUse the NTK principal component analysis perspective (Paper 2) to build an early-stopping criterion specialized for attention models: stop when the loss projection onto top attention-NTK eigenfunctions saturates while lower components begin fitting (overfitting onset). Implement this by approximating leading eigencomponents of the empirical NTK for a Transformer layer subset and validating reduced memorization with preserved reasoning performance on mixed datasets (Paper 6).\n\n6. **Selective \u201cReasoning Bias\u201d via Layerwise Small-Init Schedules**  \nInstead of globally small initialization (Paper 6), design a layerwise schedule where early layers (embeddings + first attention blocks) are initialized small to promote compositional rule learning, while later layers use standard scales to preserve capacity. Test the hypothesis that this produces a controllable tradeoff curve between reasoning accuracy and memorization, and identify which submodules dominate the bias using ablations guided by Paper 6\u2019s component analysis.\n\n7. **Fairness-Aware Training Dynamics: Linking Implicit Bias to Attention Formation**  \nOperationalize constructs from implicit bias measurement (Paper 4) into a dataset and evaluation protocol where \u201cimplicit associations\u201d are reflected as spurious correlates in text (e.g., demographic terms co-occurring with sentiment/competence labels). Then analyze whether small-init \u201creasoning bias\u201d (Paper 6) and attention convergence properties (Paper 5) reduce reliance on spurious demographic tokens, using attention attribution and counterfactual token swaps as mechanistic probes.\n\n8. **Flow-Matching Accelerated Training with Corrective Kernel Constraints**  \nTurn GFM from a forecaster into an accelerator: periodically jump along the learned continuous-time flow to skip optimizer steps (Paper 3), but add a constraint that keeps the model within a trust region defined by preserving key NTK statistics (Paper 2). This aims to prevent \u201cforecast drift\u201d that harms generalization, producing an actionable algorithm with measurable wall-clock savings and bounded deviation in function space.\n\n9. **Transformer Design via Target NTK Engineering for Long-Range Dependency Tasks**  \nUse the NTK framework (Paper 2) to define desired kernel properties for long-context sequence transduction (Paper 1), then modify attention parameterization (e.g., scaling of Q/K, positional encoding choices, head dimension allocation) to match a target kernel profile. The contribution is a principled, measurable design loop: predict which architectural changes increase kernel sensitivity to long-range token interactions, then validate on synthetic and real long-context benchmarks.\n\n10. **Mechanistic Prediction of Memorization vs Reasoning from Early Attention Statistics**  \nBuild a diagnostic that predicts\u2014within the first N training steps\u2014whether a Transformer will end up memorization-dominant or reasoning-dominant (Paper 6), using features from attention dynamics (entropy, head specialization, query-key inner product growth) motivated by Paper 5. Pair this with a lightweight intervention (e.g., adaptive rescaling of embeddings or attention temperature) to flip trajectories toward reasoning without changing data, providing a concrete control method grounded in training dynamics.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "1. **NTK-Guided Initialization for Reasoning-Favoring Transformers**  \nDerive and empirically validate a criterion that chooses per-module initialization scales (embeddings, Q/K/V, MLP) to maximize \u201cr",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "**Flow-Matching Forecasting of Attention Maps (Not Just Weights)**  \nExtend Gradient Flow Matching (GFM) to jointly model the continuous-time dynamics of weights and induced attention matrices, adding",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "**Optimizer-Aware NTK Drift Modeling for Transformers**  \nCombine NTK theory (Paper 2) with GFM\u2019s learned vector fields (Paper 3) by explicitly predicting the time-evolution (\u201cdrift\u201d) of the empirical",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "**Provable Convergence-to-Discriminative Tokens in Multi-Head Self-Attention**  \nGeneralize the convergence result from a simple attention classifier (Paper 5) to multi-head self-attention blocks with",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "**Early-Stopping Rules from Attention-NTK Principal Components**  \nUse the NTK principal component analysis perspective (Paper 2) to build an early-stopping criterion specialized for attention models:",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "**Selective \u201cReasoning Bias\u201d via Layerwise Small-Init Schedules**  \nInstead of globally small initialization (Paper 6), design a layerwise schedule where early layers (embeddings + first attention blo",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "**Fairness-Aware Training Dynamics: Linking Implicit Bias to Attention Formation**  \nOperationalize constructs from implicit bias measurement (Paper 4) into a dataset and evaluation protocol where \u201cim",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "**Flow-Matching Accelerated Training with Corrective Kernel Constraints**  \nTurn GFM from a forecaster into an accelerator: periodically jump along the learned continuous-time flow to skip optimizer s",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "**Transformer Design via Target NTK Engineering for Long-Range Dependency Tasks**  \nUse the NTK framework (Paper 2) to define desired kernel properties for long-context sequence transduction (Paper 1)",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "**Mechanistic Prediction of Memorization vs Reasoning from Early Attention Statistics**  \nBuild a diagnostic that predicts\u2014within the first N training steps\u2014whether a Transformer will end up memorizat",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 33,
      "paper_title": "Large Language Diffusion Models",
      "contribution": "Introduces LLaDA, a large-scale discrete diffusion language model trained under the same pretraining and SFT paradigms as modern LLMs, showing that diffusion-based approaches can match autoregressive LLMs on in-context learning, instruction following, and other tasks.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 13860,
      "output_tokens": 1112,
      "predecessor_details": [
        {
          "success": true,
          "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
          "url": "https://arxiv.org/abs/1503.03585",
          "content": "[1503.03585] Deep Unsupervised Learning using Nonequilibrium Thermodynamics\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1503.03585\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1503.03585**(cs)\n[Submitted on 12 Mar 2015 ([v1](https://arxiv.org/abs/1503.03585v1)), last revised 18 Nov 2015 (this version, v8)]\n# Title:Deep Unsupervised Learning using Nonequilibrium Thermodynamics\nAuthors:[Jascha Sohl-Dickstein](https://arxiv.org/search/cs?searchtype=author&amp;query=Sohl-Dickstein,+J),[Eric A. Weiss](https://arxiv.org/search/cs?searchtype=author&amp;query=Weiss,+E+A),[Niru Maheswaranathan](https://arxiv.org/search/cs?searchtype=author&amp;query=Maheswaranathan,+N),[Surya Ganguli](https://arxiv.org/search/cs?searchtype=author&amp;query=Ganguli,+S)\nView a PDF of the paper titled Deep Unsupervised Learning using Nonequilibrium Thermodynamics, by Jascha Sohl-Dickstein and 3 other authors\n[View PDF](https://arxiv.org/pdf/1503.03585)> > Abstract:\n> A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm. Subjects:|Machine Learning (cs.LG); Disordered Systems and Neural Networks (cond-mat.dis-nn); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML)|\nCite as:|[arXiv:1503.03585](https://arxiv.org/abs/1503.03585)[cs.LG]|\n|(or[arXiv:1503.03585v8](https://arxiv.org/abs/1503.03585v8)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1503.03585](https://doi.org/10.48550/arXiv.1503.03585)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jascha Sohl-Dickstein [[view email](https://arxiv.org/show-email/33b8482c/1503.03585)]\n**[[v1]](https://arxiv.org/abs/1503.03585v1)**Thu, 12 Mar 2015 04:51:37 UTC (5,395 KB)\n**[[v2]](https://arxiv.org/abs/1503.03585v2)**Thu, 2 Apr 2015 06:48:02 UTC (5,397 KB)\n**[[v3]](https://arxiv.org/abs/1503.03585v3)**Wed, 29 Apr 2015 06:00:20 UTC (5,403 KB)\n**[[v4]](https://arxiv.org/abs/1503.03585v4)**Wed, 13 May 2015 01:57:49 UTC (5,409 KB)\n**[[v5]](https://arxiv.org/abs/1503.03585v5)**Wed, 20 May 2015 03:19:10 UTC (4,586 KB)\n**[[v6]](https://arxiv.org/abs/1503.03585v6)**Thu, 9 Jul 2015 16:16:33 UTC (6,085 KB)\n**[[v7]](https://arxiv.org/abs/1503.03585v7)**Tue, 21 Jul 2015 19:44:20 UTC (6,092 KB)\n**[v8]**Wed, 18 Nov 2015 21:50:51 UTC (6,095 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Deep Unsupervised Learning using Nonequilibrium Thermodynamics, by Jascha Sohl-Dickstein and 3 other authors\n* [View PDF](https://arxiv.org/pdf/1503.03585)\n* [TeX Source](https://arxiv.org/src/1503.03585)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1503.03585&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1503.03585&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2015-03](https://arxiv.org/list/cs.LG/2015-03)\nChange to browse by:\n[cond-mat](https://arxiv.org/abs/1503.03585?context=cond-mat)\n[cond-mat.dis-nn](https://arxiv.org/abs/1503.03585?context=cond-mat.dis-nn)\n[cs](https://arxiv.org/abs/1503.03585?context=cs)\n[q-bio](https://arxiv.org/abs/1503.03585?context=q-bio)\n[q-bio.NC](https://arxiv.org/abs/1503.03585?context=q-bio.NC)\n[stat](https://arxiv.org/abs/1503.03585?context=stat)\n[stat.ML](https://arxiv.org/abs/1503.03585?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1503.03585)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1503.03585)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1503.03585)\n### [2 blog links](https://arxiv.org/tb/1503.03585)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1503.html#Sohl-DicksteinW15)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/Sohl-DicksteinW15)\n[Jascha Sohl-Dickstein]()\n[Eric A. Weiss]()\n[Niru Maheswaranathan]()\n[Surya Ganguli]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArx",
          "original_query": "Deep unsupervised learning using nonequilibrium thermodynamics",
          "cleaned_query": "Deep unsupervised learning using nonequilibrium thermodynamics"
        },
        {
          "success": true,
          "title": "[1706.03762] Attention Is All You Need - arXiv",
          "url": "https://arxiv.org/abs/1706.03762",
          "content": "[1706.03762] Attention Is All You Need[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1706.03762\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:1706.03762**(cs)\n[Submitted on 12 Jun 2017 ([v1](https://arxiv.org/abs/1706.03762v1)), last revised 2 Aug 2023 (this version, v7)]\n# Title:Attention Is All You Need\nAuthors:[Ashish Vaswani](https://arxiv.org/search/cs?searchtype=author&amp;query=Vaswani,+A),[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&amp;query=Shazeer,+N),[Niki Parmar](https://arxiv.org/search/cs?searchtype=author&amp;query=Parmar,+N),[Jakob Uszkoreit](https://arxiv.org/search/cs?searchtype=author&amp;query=Uszkoreit,+J),[Llion Jones](https://arxiv.org/search/cs?searchtype=author&amp;query=Jones,+L),[Aidan N. Gomez](https://arxiv.org/search/cs?searchtype=author&amp;query=Gomez,+A+N),[Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaiser,+L),[Illia Polosukhin](https://arxiv.org/search/cs?searchtype=author&amp;query=Polosukhin,+I)\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n[View PDF](https://arxiv.org/pdf/1706.03762)[HTML (experimental)](https://arxiv.org/html/1706.03762v7)> > Abstract:\n> The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Comments:|15 pages, 5 figures|\nSubjects:|Computation and Language (cs.CL); Machine Learning (cs.LG)|\nCite as:|[arXiv:1706.03762](https://arxiv.org/abs/1706.03762)[cs.CL]|\n|(or[arXiv:1706.03762v7](https://arxiv.org/abs/1706.03762v7)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Llion Jones [[view email](https://arxiv.org/show-email/f53b7360/1706.03762)]\n**[[v1]](https://arxiv.org/abs/1706.03762v1)**Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB)\n**[[v2]](https://arxiv.org/abs/1706.03762v2)**Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB)\n**[[v3]](https://arxiv.org/abs/1706.03762v3)**Tue, 20 Jun 2017 05:20:02 UTC (1,125 KB)\n**[[v4]](https://arxiv.org/abs/1706.03762v4)**Fri, 30 Jun 2017 17:29:30 UTC (1,124 KB)\n**[[v5]](https://arxiv.org/abs/1706.03762v5)**Wed, 6 Dec 2017 03:30:32 UTC (1,124 KB)\n**[[v6]](https://arxiv.org/abs/1706.03762v6)**Mon, 24 Jul 2023 00:48:54 UTC (1,124 KB)\n**[v7]**Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n* [View PDF](https://arxiv.org/pdf/1706.03762)\n* [HTML (experimental)](https://arxiv.org/html/1706.03762v7)\n* [TeX Source](https://arxiv.org/src/1706.03762)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1706.03762&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1706.03762&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2017-06](https://arxiv.org/list/cs.CL/2017-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1706.03762?context=cs)\n[cs.LG](https://arxiv.org/abs/1706.03762?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1706.03762)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1706.03762)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1706.03762)\n### [123 blog links](https://arxiv.org/tb/1706.03762)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1706.html#VaswaniSPUJGKP17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/VaswaniSPUJGKP17)\n[Ashish Vaswani]()\n[Noam Shazeer]()\n[Niki Parmar]()\n[Jakob Uszkoreit]()\n[Llion Jones]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces",
          "original_query": "Attention Is All You Need",
          "cleaned_query": "Attention Is All You Need"
        },
        {
          "success": true,
          "title": "Structured Denoising Diffusion Models in Discrete State-Spaces",
          "url": "https://arxiv.org/abs/2107.03006",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2107.03006** (cs)\n\n\\[Submitted on 7 Jul 2021 ( [v1](https://arxiv.org/abs/2107.03006v1)), last revised 22 Feb 2023 (this version, v3)\\]\n\n# Title:Structured Denoising Diffusion Models in Discrete State-Spaces\n\nAuthors: [Jacob Austin](https://arxiv.org/search/cs?searchtype=author&query=Austin,+J), [Daniel D. Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson,+D+D), [Jonathan Ho](https://arxiv.org/search/cs?searchtype=author&query=Ho,+J), [Daniel Tarlow](https://arxiv.org/search/cs?searchtype=author&query=Tarlow,+D), [Rianne van den Berg](https://arxiv.org/search/cs?searchtype=author&query=van+den+Berg,+R)\n\nView a PDF of the paper titled Structured Denoising Diffusion Models in Discrete State-Spaces, by Jacob Austin and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2107.03006)\n\n> Abstract:Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.\n\n| | |\n| --- | --- |\n| Comments: | 10 pages plus references and appendices. First two authors contributed equally |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2107.03006](https://arxiv.org/abs/2107.03006) \\[cs.LG\\] |\n| (or [arXiv:2107.03006v3](https://arxiv.org/abs/2107.03006v3) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2107.03006](https://doi.org/10.48550/arXiv.2107.03006) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jacob Austin \\[ [view email](https://arxiv.org/show-email/4a0743b9/2107.03006)\\] **[\\[v1\\]](https://arxiv.org/abs/2107.03006v1)**\nWed, 7 Jul 2021 04:11:00 UTC (4,063 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2107.03006v2)**\nTue, 13 Jul 2021 17:09:20 UTC (4,062 KB)\n**\\[v3\\]**\nWed, 22 Feb 2023 16:05:48 UTC (4,062 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Structured Denoising Diffusion Models in Discrete State-Spaces, by Jacob Austin and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2107.03006)\n- [TeX Source](https://arxiv.org/src/2107.03006)\n- [Other Formats](https://arxiv.org/format/2107.03006)\n\n[view license](http://creativecommons.org/licenses/by-nc-sa/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2107.03006&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2107.03006&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-07](https://arxiv.org/list/cs.LG/2021-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2107.03006?context=cs) [cs.AI](https://arxiv.org/abs/2107.03006?context=cs.AI) [cs.CL](https://arxiv.org/abs/2107.03006?context=cs.CL) [cs.CV](https://arxiv.org/abs/2107.03006?context=cs.CV)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2107.03006)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2107.03006)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2107.03006)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2107.html#abs-2107-03006) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2107-03006)\n\n[Jonathan Ho](https://dblp.uni-trier.de/search/author?author=Jonathan%20Ho) [Daniel Tarlow](https://dblp.uni-trier.de/search/author?author=Daniel%20Tarlow) [Rianne van den Berg](https://dblp.uni-trier.de/search/author?author=Rianne%20van%20den%20Berg)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2107.03006) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Structured denoising diffusion models in discrete state-spaces",
          "cleaned_query": "Structured denoising diffusion models in discrete state-spaces"
        },
        {
          "success": true,
          "title": "Discrete Diffusion Modeling by Estimating the Ratios of the Data ...",
          "url": "https://arxiv.org/abs/2310.16834",
          "content": "[2310.16834] Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:2310.16834\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:2310.16834**(stat)\n[Submitted on 25 Oct 2023 ([v1](https://arxiv.org/abs/2310.16834v1)), last revised 6 Jun 2024 (this version, v3)]\n# Title:Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution\nAuthors:[Aaron Lou](https://arxiv.org/search/stat?searchtype=author&amp;query=Lou,+A),[Chenlin Meng](https://arxiv.org/search/stat?searchtype=author&amp;query=Meng,+C),[Stefano Ermon](https://arxiv.org/search/stat?searchtype=author&amp;query=Ermon,+S)\nView a PDF of the paper titled Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution, by Aaron Lou and 2 other authors\n[View PDF](https://arxiv.org/pdf/2310.16834)[HTML (experimental)](https://arxiv.org/html/2310.16834v3)> > Abstract:\n> Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel loss that naturally extends score matching to discrete spaces, integrates seamlessly to build discrete diffusion models, and significantly boosts performance. Experimentally, we test our Score Entropy Discrete Diffusion models (SEDD) on standard language modeling tasks. For comparable model sizes, SEDD beats existing language diffusion paradigms (reducing perplexity by $25$-$75$\\%) and is competitive with autoregressive models, in particular outperforming GPT-2. Furthermore, compared to autoregressive mdoels, SEDD generates faithful text without requiring distribution annealing techniques like temperature scaling (around $6$-$8\\times$ better generative perplexity than un-annealed GPT-2), can trade compute and quality (similar quality with $32\\times$ fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting). Comments:|ICML 2024 Oral. Code at[this https URL](https://github.com/louaaron/Score-Entropy-Discrete-Diffusion)|\nSubjects:|Machine Learning (stat.ML); Computation and Language (cs.CL); Machine Learning (cs.LG)|\nCite as:|[arXiv:2310.16834](https://arxiv.org/abs/2310.16834)[stat.ML]|\n|(or[arXiv:2310.16834v3](https://arxiv.org/abs/2310.16834v3)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.2310.16834](https://doi.org/10.48550/arXiv.2310.16834)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Aaron Lou [[view email](https://arxiv.org/show-email/67665857/2310.16834)]\n**[[v1]](https://arxiv.org/abs/2310.16834v1)**Wed, 25 Oct 2023 17:59:12 UTC (1,210 KB)\n**[[v2]](https://arxiv.org/abs/2310.16834v2)**Wed, 21 Feb 2024 01:00:33 UTC (2,170 KB)\n**[v3]**Thu, 6 Jun 2024 21:06:44 UTC (2,173 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution, by Aaron Lou and 2 other authors\n* [View PDF](https://arxiv.org/pdf/2310.16834)\n* [HTML (experimental)](https://arxiv.org/html/2310.16834v3)\n* [TeX Source](https://arxiv.org/src/2310.16834)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2310.16834&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2310.16834&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2023-10](https://arxiv.org/list/stat.ML/2023-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/2310.16834?context=cs)\n[cs.CL](https://arxiv.org/abs/2310.16834?context=cs.CL)\n[cs.LG](https://arxiv.org/abs/2310.16834?context=cs.LG)\n[stat](https://arxiv.org/abs/2310.16834?context=stat)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2310.16834)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2310.16834)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2310.16834)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework th",
          "original_query": "Discrete diffusion language modeling by estimating the ratios of the data distribution",
          "cleaned_query": "Discrete diffusion language modeling by estimating the ratios of the data distribution"
        },
        {
          "success": true,
          "title": "Your Absorbing Discrete Diffusion Secretly Models the Conditional ...",
          "url": "https://arxiv.org/abs/2406.03736",
          "content": "[2406.03736] Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nIn just 5 minutes help us improve arXiv:\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2406.03736\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2406.03736**(cs)\n[Submitted on 6 Jun 2024 ([v1](https://arxiv.org/abs/2406.03736v1)), last revised 11 Feb 2025 (this version, v3)]\n# Title:Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data\nAuthors:[Jingyang Ou](https://arxiv.org/search/cs?searchtype=author&amp;query=Ou,+J),[Shen Nie](https://arxiv.org/search/cs?searchtype=author&amp;query=Nie,+S),[Kaiwen Xue](https://arxiv.org/search/cs?searchtype=author&amp;query=Xue,+K),[Fengqi Zhu](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+F),[Jiacheng Sun](https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+J),[Zhenguo Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Z),[Chongxuan Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+C)\nView a PDF of the paper titled Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data, by Jingyang Ou and 6 other authors\n[View PDF](https://arxiv.org/pdf/2406.03736)[HTML (experimental)](https://arxiv.org/html/2406.03736v3)> > Abstract:\n> Discrete diffusion models with absorbing processes have shown promise in language modeling. The key quantities to be estimated are the ratios between the marginal probabilities of two transitive states at all timesteps, called the concrete score. In this paper, we reveal that the concrete score in absorbing diffusion can be expressed as conditional probabilities of clean data, multiplied by a time-dependent scalar in an analytic form. Motivated by this finding, we propose reparameterized absorbing discrete diffusion (RADD), a dedicated diffusion model without time-condition that characterizes the time-independent conditional probabilities. Besides its simplicity, RADD can reduce the number of function evaluations (NFEs) by caching the output of the time-independent network when the noisy sample remains unchanged in a sampling interval, which enables sampling acceleration. Built upon the new perspective of conditional distributions, we further unify absorbing discrete diffusion and any-order autoregressive models (AO-ARMs), showing that the upper bound on the negative log-likelihood for the diffusion model can be interpreted as an expected negative log-likelihood for AO-ARMs. Further, our RADD models achieve SOTA performance among diffusion models on 5 zero-shot language modeling benchmarks (measured by perplexity) at the GPT-2 scale. Our code is available at [> this https URL\n](https://github.com/ML-GSAI/RADD)> . Subjects:|Machine Learning (cs.LG); Computation and Language (cs.CL)|\nCite as:|[arXiv:2406.03736](https://arxiv.org/abs/2406.03736)[cs.LG]|\n|(or[arXiv:2406.03736v3](https://arxiv.org/abs/2406.03736v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2406.03736](https://doi.org/10.48550/arXiv.2406.03736)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jingyang Ou [[view email](https://arxiv.org/show-email/6cbdc5b4/2406.03736)]\n**[[v1]](https://arxiv.org/abs/2406.03736v1)**Thu, 6 Jun 2024 04:22:11 UTC (296 KB)\n**[[v2]](https://arxiv.org/abs/2406.03736v2)**Sat, 6 Jul 2024 14:40:08 UTC (307 KB)\n**[v3]**Tue, 11 Feb 2025 15:42:19 UTC (299 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data, by Jingyang Ou and 6 other authors\n* [View PDF](https://arxiv.org/pdf/2406.03736)\n* [HTML (experimental)](https://arxiv.org/html/2406.03736v3)\n* [TeX Source](https://arxiv.org/src/2406.03736)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2406.03736&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2406.03736&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2024-06](https://arxiv.org/list/cs.LG/2024-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/2406.03736?context=cs)\n[cs.CL](https://arxiv.org/abs/2406.03736?context=cs.CL)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2406.03736)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2406.03736)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2406.03736)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features dir",
          "original_query": "Your absorbing discrete diffusion secretly models the conditional distributions of clean data",
          "cleaned_query": "Your absorbing discrete diffusion secretly models the conditional distributions of clean data"
        },
        {
          "success": true,
          "title": "[2202.04200] MaskGIT: Masked Generative Image Transformer - arXiv",
          "url": "https://arxiv.org/abs/2202.04200",
          "content": "[2202.04200] MaskGIT: Masked Generative Image Transformer\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2202.04200\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2202.04200**(cs)\n[Submitted on 8 Feb 2022]\n# Title:MaskGIT: Masked Generative Image Transformer\nAuthors:[Huiwen Chang](https://arxiv.org/search/cs?searchtype=author&amp;query=Chang,+H),[Han Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+H),[Lu Jiang](https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+L),[Ce Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+C),[William T. Freeman](https://arxiv.org/search/cs?searchtype=author&amp;query=Freeman,+W+T)\nView a PDF of the paper titled MaskGIT: Masked Generative Image Transformer, by Huiwen Chang and 4 other authors\n[View PDF](https://arxiv.org/pdf/2202.04200)> > Abstract:\n> Generative transformers have experienced rapid popularity growth in the computer vision community in synthesizing high-fidelity and high-resolution images. The best generative transformer models so far, however, still treat an image naively as a sequence of tokens, and decode an image sequentially following the raster scan ordering (i.e. line-by-line). We find this strategy neither optimal nor efficient. This paper proposes a novel image synthesis paradigm using a bidirectional transformer decoder, which we term MaskGIT. During training, MaskGIT learns to predict randomly masked tokens by attending to tokens in all directions. At inference time, the model begins with generating all tokens of an image simultaneously, and then refines the image iteratively conditioned on the previous generation. Our experiments demonstrate that MaskGIT significantly outperforms the state-of-the-art transformer model on the ImageNet dataset, and accelerates autoregressive decoding by up to 64x. Besides, we illustrate that MaskGIT can be easily extended to various image editing tasks, such as inpainting, extrapolation, and image manipulation. Subjects:|Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:2202.04200](https://arxiv.org/abs/2202.04200)[cs.CV]|\n|(or[arXiv:2202.04200v1](https://arxiv.org/abs/2202.04200v1)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2202.04200](https://doi.org/10.48550/arXiv.2202.04200)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Huiwen Chang [[view email](https://arxiv.org/show-email/48dbc634/2202.04200)]\n**[v1]**Tue, 8 Feb 2022 23:54:06 UTC (37,390 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled MaskGIT: Masked Generative Image Transformer, by Huiwen Chang and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2202.04200)\n* [TeX Source](https://arxiv.org/src/2202.04200)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2202.04200&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2202.04200&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2022-02](https://arxiv.org/list/cs.CV/2022-02)\nChange to browse by:\n[cs](https://arxiv.org/abs/2202.04200?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2202.04200)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2202.04200)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2202.04200)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2202.04200)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "MaskGIT: Masked generative image transformer",
          "cleaned_query": "MaskGIT: Masked generative image transformer"
        }
      ],
      "generated_ideas": [
        "1. **Transition-Matrix Search for D3PMs via Thermodynamic Work Regularization**  \n   Learn the discrete corruption transition matrices in D3PMs (Paper 3) with a bilevel objective that adds a nonequilibrium \u201cwork/entropy production\u201d penalty inspired by diffusion as gradual structure destruction (Paper 1). Concretely, parameterize the transition matrix family (e.g., nearest-neighbor in embedding space + absorbing states) and optimize it jointly with the reverse model to maximize likelihood while keeping the forward process minimally dissipative, yielding better sample quality/perplexity at fixed NFEs.",
        "**Transformer-Based Concrete-Score Parameterization for Absorbing Diffusion**  \n   Replace time-conditioned networks in absorbing discrete diffusion with a Transformer that predicts time-independent conditional probabilities as in RADD (Paper 5), but with explicit multi-head attention over unmasked tokens to better capture long-range dependencies (Paper 2). Evaluate whether attention-based conditional estimators improve caching hit-rate and reduce NFEs further by producing more stable token posteriors across sampling intervals.",
        "**Score-Entropy Mask Schedules: Unifying SEDD and MaskGIT Inference**  \n   Design a principled iterative masking/unmasking schedule for MaskGIT (Paper 6) by using score entropy (Paper 4) as a token-wise \u201cuncertainty potential\u201d to decide which tokens to resample each step. Implement an inference algorithm that alternates (i) score-entropy-based token selection and (ii) bidirectional Transformer denoising, and compare against heuristic confidence-based masking for image tokens and text tokens.",
        "**Hybrid Autoregressive\u2013Diffusion Language Models with AO-ARM Bounds**  \n   Use the AO-ARM interpretation of absorbing diffusion objectives (Paper 5) to build a model that mixes left-to-right autoregressive decoding for \u201chigh-confidence\u201d spans and discrete diffusion infilling for \u201cuncertain\u201d spans, all trained under a single variational/upper-bound objective. The actionable contribution is a gating policy (learned or entropy-thresholded) that chooses AR vs diffusion updates per span to reduce compute while preserving controllable infilling quality.",
        "**Discrete Diffusion for Structured Editing with Transformer Cross-Attention Constraints**  \n   Extend discrete diffusion (Papers 3\u20135) to constrained generation/editing by injecting hard/soft constraints through cross-attention keys/values (Paper 2) (e.g., must-include phrases, syntax templates, or protected tokens). Concretely, add a constraint encoder and train the reverse process to satisfy constraints at every denoising step, enabling controllable rewriting and infilling without post-hoc decoding tricks.",
        "**Nearest-Neighbor Embedding Corruptions for Text: From D3PM Kernels to SEDD Losses**  \n   Create a corruption process for language where transitions are restricted to nearest neighbors in a learned token-embedding graph (Paper 3), then train with score entropy (Paper 4) to estimate density ratios over this non-uniform kernel. This yields an actionable method to reduce \u201csemantic drift\u201d during corruption (compared to uniform masking) and test whether it improves faithfulness and perplexity on LM benchmarks.",
        "**Nonequilibrium Diagnostics for Discrete Diffusion: Entropy Production as a Training Signal**  \n   Introduce measurable nonequilibrium quantities (Paper 1)\u2014e.g., discrete-time entropy production along forward/reverse trajectories\u2014as diagnostics and auxiliary losses for discrete diffusion models (Papers 3\u20135). Implement estimators of per-step dissipation and test whether penalizing high-dissipation steps improves stability, calibration of token probabilities, and sample efficiency.",
        "**Cache-Aware Samplers for Discrete Diffusion with Attention Reuse**  \n   Generalize RADD\u2019s caching idea (Paper 5) by reusing intermediate attention computations (key/value caches) across denoising steps in Transformer-based discrete diffusion (Paper 2). Concretely, design a sampler that detects unchanged token subsets and recomputes attention only for affected blocks, reporting end-to-end latency gains and the quality\u2013compute tradeoff relative to naive full recomputation.",
        "**Multimodal Discrete Diffusion over Image Tokens + Text Tokens with Joint Corruption**  \n   Combine MaskGIT-style image tokenization and iterative refinement (Paper 6) with discrete diffusion objectives (Papers 3\u20134) to model joint distributions over (image tokens, caption tokens). Define a coupled forward process that corrupts both modalities (with modality-specific transition matrices) and learn a reverse Transformer that performs cross-modal denoising, enabling stronger text-conditioned image generation and bidirectional editing.",
        "**Adaptive-Time Discrete Diffusion: Learn to Stop Early per Sample**  \n   Build an adaptive halting criterion for discrete diffusion sampling using score entropy (Paper 4) or conditional-probability sharpness (Paper 5) to decide when further denoising steps yield diminishing returns. Implement per-sample early stopping (and optional step-skipping) and evaluate compute savings vs quality on language generation and image-token generation, aiming to formalize the compute\u2013quality \u201canytime\u201d behavior hinted by diffusion models."
      ],
      "generated_ideas_raw": "1. **Transition-Matrix Search for D3PMs via Thermodynamic Work Regularization**  \n   Learn the discrete corruption transition matrices in D3PMs (Paper 3) with a bilevel objective that adds a nonequilibrium \u201cwork/entropy production\u201d penalty inspired by diffusion as gradual structure destruction (Paper 1). Concretely, parameterize the transition matrix family (e.g., nearest-neighbor in embedding space + absorbing states) and optimize it jointly with the reverse model to maximize likelihood while keeping the forward process minimally dissipative, yielding better sample quality/perplexity at fixed NFEs.\n\n2. **Transformer-Based Concrete-Score Parameterization for Absorbing Diffusion**  \n   Replace time-conditioned networks in absorbing discrete diffusion with a Transformer that predicts time-independent conditional probabilities as in RADD (Paper 5), but with explicit multi-head attention over unmasked tokens to better capture long-range dependencies (Paper 2). Evaluate whether attention-based conditional estimators improve caching hit-rate and reduce NFEs further by producing more stable token posteriors across sampling intervals.\n\n3. **Score-Entropy Mask Schedules: Unifying SEDD and MaskGIT Inference**  \n   Design a principled iterative masking/unmasking schedule for MaskGIT (Paper 6) by using score entropy (Paper 4) as a token-wise \u201cuncertainty potential\u201d to decide which tokens to resample each step. Implement an inference algorithm that alternates (i) score-entropy-based token selection and (ii) bidirectional Transformer denoising, and compare against heuristic confidence-based masking for image tokens and text tokens.\n\n4. **Hybrid Autoregressive\u2013Diffusion Language Models with AO-ARM Bounds**  \n   Use the AO-ARM interpretation of absorbing diffusion objectives (Paper 5) to build a model that mixes left-to-right autoregressive decoding for \u201chigh-confidence\u201d spans and discrete diffusion infilling for \u201cuncertain\u201d spans, all trained under a single variational/upper-bound objective. The actionable contribution is a gating policy (learned or entropy-thresholded) that chooses AR vs diffusion updates per span to reduce compute while preserving controllable infilling quality.\n\n5. **Discrete Diffusion for Structured Editing with Transformer Cross-Attention Constraints**  \n   Extend discrete diffusion (Papers 3\u20135) to constrained generation/editing by injecting hard/soft constraints through cross-attention keys/values (Paper 2) (e.g., must-include phrases, syntax templates, or protected tokens). Concretely, add a constraint encoder and train the reverse process to satisfy constraints at every denoising step, enabling controllable rewriting and infilling without post-hoc decoding tricks.\n\n6. **Nearest-Neighbor Embedding Corruptions for Text: From D3PM Kernels to SEDD Losses**  \n   Create a corruption process for language where transitions are restricted to nearest neighbors in a learned token-embedding graph (Paper 3), then train with score entropy (Paper 4) to estimate density ratios over this non-uniform kernel. This yields an actionable method to reduce \u201csemantic drift\u201d during corruption (compared to uniform masking) and test whether it improves faithfulness and perplexity on LM benchmarks.\n\n7. **Nonequilibrium Diagnostics for Discrete Diffusion: Entropy Production as a Training Signal**  \n   Introduce measurable nonequilibrium quantities (Paper 1)\u2014e.g., discrete-time entropy production along forward/reverse trajectories\u2014as diagnostics and auxiliary losses for discrete diffusion models (Papers 3\u20135). Implement estimators of per-step dissipation and test whether penalizing high-dissipation steps improves stability, calibration of token probabilities, and sample efficiency.\n\n8. **Cache-Aware Samplers for Discrete Diffusion with Attention Reuse**  \n   Generalize RADD\u2019s caching idea (Paper 5) by reusing intermediate attention computations (key/value caches) across denoising steps in Transformer-based discrete diffusion (Paper 2). Concretely, design a sampler that detects unchanged token subsets and recomputes attention only for affected blocks, reporting end-to-end latency gains and the quality\u2013compute tradeoff relative to naive full recomputation.\n\n9. **Multimodal Discrete Diffusion over Image Tokens + Text Tokens with Joint Corruption**  \n   Combine MaskGIT-style image tokenization and iterative refinement (Paper 6) with discrete diffusion objectives (Papers 3\u20134) to model joint distributions over (image tokens, caption tokens). Define a coupled forward process that corrupts both modalities (with modality-specific transition matrices) and learn a reverse Transformer that performs cross-modal denoising, enabling stronger text-conditioned image generation and bidirectional editing.\n\n10. **Adaptive-Time Discrete Diffusion: Learn to Stop Early per Sample**  \n   Build an adaptive halting criterion for discrete diffusion sampling using score entropy (Paper 4) or conditional-probability sharpness (Paper 5) to decide when further denoising steps yield diminishing returns. Implement per-sample early stopping (and optional step-skipping) and evaluate compute savings vs quality on language generation and image-token generation, aiming to formalize the compute\u2013quality \u201canytime\u201d behavior hinted by diffusion models.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "1. **Transition-Matrix Search for D3PMs via Thermodynamic Work Regularization**  \n   Learn the discrete corruption transition matrices in D3PMs (Paper 3) with a bilevel objective that adds a nonequili",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "**Transformer-Based Concrete-Score Parameterization for Absorbing Diffusion**  \n   Replace time-conditioned networks in absorbing discrete diffusion with a Transformer that predicts time-independent c",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "**Score-Entropy Mask Schedules: Unifying SEDD and MaskGIT Inference**  \n   Design a principled iterative masking/unmasking schedule for MaskGIT (Paper 6) by using score entropy (Paper 4) as a token-wi",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "**Hybrid Autoregressive\u2013Diffusion Language Models with AO-ARM Bounds**  \n   Use the AO-ARM interpretation of absorbing diffusion objectives (Paper 5) to build a model that mixes left-to-right autoregr",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "**Discrete Diffusion for Structured Editing with Transformer Cross-Attention Constraints**  \n   Extend discrete diffusion (Papers 3\u20135) to constrained generation/editing by injecting hard/soft constrai",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "**Nearest-Neighbor Embedding Corruptions for Text: From D3PM Kernels to SEDD Losses**  \n   Create a corruption process for language where transitions are restricted to nearest neighbors in a learned t",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "**Nonequilibrium Diagnostics for Discrete Diffusion: Entropy Production as a Training Signal**  \n   Introduce measurable nonequilibrium quantities (Paper 1)\u2014e.g., discrete-time entropy production alon",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "**Cache-Aware Samplers for Discrete Diffusion with Attention Reuse**  \n   Generalize RADD\u2019s caching idea (Paper 5) by reusing intermediate attention computations (key/value caches) across denoising st",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "**Multimodal Discrete Diffusion over Image Tokens + Text Tokens with Joint Corruption**  \n   Combine MaskGIT-style image tokenization and iterative refinement (Paper 6) with discrete diffusion objecti",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "**Adaptive-Time Discrete Diffusion: Learn to Stop Early per Sample**  \n   Build an adaptive halting criterion for discrete diffusion sampling using score entropy (Paper 4) or conditional-probability s",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 34,
      "paper_title": "Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation",
      "contribution": "A training-free, plug-and-play method (ALFAR) that maximizes the utility of retrieved contextual knowledge for MLLMs by (1) adaptively reallocating attention from visual to relevant context tokens (guided by query-context relevance) and (2) decoupling and adaptively weighting parametric and contextual signals at the output logits to resolve knowledge conflicts.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": true,
      "matching_idea_idx": 2,
      "input_tokens": 11584,
      "output_tokens": 1009,
      "predecessor_details": [
        {
          "success": true,
          "title": "REALM: Retrieval-Augmented Language Model Pre-Training - arXiv",
          "url": "https://arxiv.org/abs/2002.08909",
          "content": "[2002.08909] REALM: Retrieval-Augmented Language Model Pre-Training\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2002.08909\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2002.08909**(cs)\n[Submitted on 10 Feb 2020]\n# Title:REALM: Retrieval-Augmented Language Model Pre-Training\nAuthors:[Kelvin Guu](https://arxiv.org/search/cs?searchtype=author&amp;query=Guu,+K),[Kenton Lee](https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+K),[Zora Tung](https://arxiv.org/search/cs?searchtype=author&amp;query=Tung,+Z),[Panupong Pasupat](https://arxiv.org/search/cs?searchtype=author&amp;query=Pasupat,+P),[Ming-Wei Chang](https://arxiv.org/search/cs?searchtype=author&amp;query=Chang,+M)\nView a PDF of the paper titled REALM: Retrieval-Augmented Language Model Pre-Training, by Kelvin Guu and 4 other authors\n[View PDF](https://arxiv.org/pdf/2002.08909)> > Abstract:\n> Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts.\n> To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.\n> We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity. Subjects:|Computation and Language (cs.CL); Machine Learning (cs.LG)|\nCite as:|[arXiv:2002.08909](https://arxiv.org/abs/2002.08909)[cs.CL]|\n|(or[arXiv:2002.08909v1](https://arxiv.org/abs/2002.08909v1)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2002.08909](https://doi.org/10.48550/arXiv.2002.08909)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Kelvin Guu [[view email](https://arxiv.org/show-email/bfd1a70d/2002.08909)]\n**[v1]**Mon, 10 Feb 2020 18:40:59 UTC (270 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled REALM: Retrieval-Augmented Language Model Pre-Training, by Kelvin Guu and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2002.08909)\n* [TeX Source](https://arxiv.org/src/2002.08909)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2002.08909&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2002.08909&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2020-02](https://arxiv.org/list/cs.CL/2020-02)\nChange to browse by:\n[cs](https://arxiv.org/abs/2002.08909?context=cs)\n[cs.LG](https://arxiv.org/abs/2002.08909?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2002.08909)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2002.08909)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2002.08909)\n### [3 blog links](https://arxiv.org/tb/2002.08909)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2002.html#abs-2002-08909)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2002-08909)\n[Kelvin Guu]()\n[Kenton Lee]()\n[Zora Tung]()\n[Panupong Pasupat]()\n[Ming-Wei Chang]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2002.08909)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html",
          "original_query": "Retrieval augmented language model pre-training",
          "cleaned_query": "Retrieval augmented language model pre-training"
        },
        {
          "success": true,
          "title": "Hierarchical Retrieval-Augmented Generation for Multimodal LLMs",
          "url": "https://arxiv.org/abs/2404.15406",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2404.15406** (cs)\n\n\\[Submitted on 23 Apr 2024 ( [v1](https://arxiv.org/abs/2404.15406v1)), last revised 22 May 2024 (this version, v2)\\]\n\n# Title:Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs\n\nAuthors: [Davide Caffagni](https://arxiv.org/search/cs?searchtype=author&query=Caffagni,+D), [Federico Cocchi](https://arxiv.org/search/cs?searchtype=author&query=Cocchi,+F), [Nicholas Moratelli](https://arxiv.org/search/cs?searchtype=author&query=Moratelli,+N), [Sara Sarto](https://arxiv.org/search/cs?searchtype=author&query=Sarto,+S), [Marcella Cornia](https://arxiv.org/search/cs?searchtype=author&query=Cornia,+M), [Lorenzo Baraldi](https://arxiv.org/search/cs?searchtype=author&query=Baraldi,+L), [Rita Cucchiara](https://arxiv.org/search/cs?searchtype=author&query=Cucchiara,+R)\n\nView a PDF of the paper titled Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs, by Davide Caffagni and 6 other authors\n\n[View PDF](https://arxiv.org/pdf/2404.15406) [HTML (experimental)](https://arxiv.org/html/2404.15406v2)\n\n> Abstract:Multimodal LLMs are the natural evolution of LLMs, and enlarge their capabilities so as to work beyond the pure textual modality. As research is being carried out to design novel architectures and vision-and-language adapters, in this paper we concentrate on endowing such models with the capability of answering questions that require external knowledge. Our approach, termed Wiki-LLaVA, aims at integrating an external knowledge source of multimodal documents, which is accessed through a hierarchical retrieval pipeline. Relevant passages, using this approach, are retrieved from the external knowledge source and employed as additional context for the LLM, augmenting the effectiveness and precision of generated dialogues. We conduct extensive experiments on datasets tailored for visual question answering with external data and demonstrate the appropriateness of our approach.\n\n| | |\n| --- | --- |\n| Comments: | CVPR 2024 Workshop on What is Next in Multimodal Foundation Models |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multimedia (cs.MM) |\n| Cite as: | [arXiv:2404.15406](https://arxiv.org/abs/2404.15406) \\[cs.CV\\] |\n| (or [arXiv:2404.15406v2](https://arxiv.org/abs/2404.15406v2) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2404.15406](https://doi.org/10.48550/arXiv.2404.15406) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Marcella Cornia \\[ [view email](https://arxiv.org/show-email/1756e98b/2404.15406)\\] **[\\[v1\\]](https://arxiv.org/abs/2404.15406v1)**\nTue, 23 Apr 2024 18:00:09 UTC (905 KB)\n**\\[v2\\]**\nWed, 22 May 2024 07:15:18 UTC (905 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs, by Davide Caffagni and 6 other authors\n\n- [View PDF](https://arxiv.org/pdf/2404.15406)\n- [HTML (experimental)](https://arxiv.org/html/2404.15406v2)\n- [TeX Source](https://arxiv.org/src/2404.15406)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2404.15406&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2404.15406&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-04](https://arxiv.org/list/cs.CV/2024-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2404.15406?context=cs) [cs.AI](https://arxiv.org/abs/2404.15406?context=cs.AI) [cs.CL](https://arxiv.org/abs/2404.15406?context=cs.CL) [cs.MM](https://arxiv.org/abs/2404.15406?context=cs.MM)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2404.15406)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2404.15406)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2404.15406)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2404.15406) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Wiki-llava: Hierarchical retrieval-augmented generation for multimodal llms",
          "cleaned_query": "Wiki-llava: Hierarchical retrieval-augmented generation for multimodal llms"
        },
        {
          "success": true,
          "title": "On the Importance of Attention in Different LLM Layers",
          "url": "https://arxiv.org/abs/2409.03621",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "Attend first, consolidate later: On the importance of attention in different llm layers",
          "cleaned_query": "Attend first, consolidate later: On the importance of attention in different llm layers"
        },
        {
          "success": true,
          "title": "Unraveling Cross-Modality Knowledge Conflicts in Large Vision ...",
          "url": "https://arxiv.org/abs/2410.03659",
          "content": "[2410.03659] Unraveling Cross-Modality Knowledge Conflicts in Large Vision-Language Models[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2410.03659\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2410.03659**(cs)\n[Submitted on 4 Oct 2024 ([v1](https://arxiv.org/abs/2410.03659v1)), last revised 11 Oct 2024 (this version, v2)]\n# Title:Unraveling Cross-Modality Knowledge Conflicts in Large Vision-Language Models\nAuthors:[Tinghui Zhu](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+T),[Qin Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Q),[Fei Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+F),[Zhengzhong Tu](https://arxiv.org/search/cs?searchtype=author&amp;query=Tu,+Z),[Muhao Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+M)\nView a PDF of the paper titled Unraveling Cross-Modality Knowledge Conflicts in Large Vision-Language Models, by Tinghui Zhu and 4 other authors\n[View PDF](https://arxiv.org/pdf/2410.03659)[HTML (experimental)](https://arxiv.org/html/2410.03659v2)> > Abstract:\n> Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities for capturing and reasoning over multimodal inputs. However, these models are prone to parametric knowledge conflicts, which arise from inconsistencies of represented knowledge between their vision and language components. In this paper, we formally define the problem of $\\textbf{cross-modality parametric knowledge conflict}$ and present a systematic approach to detect, interpret, and mitigate them. We introduce a pipeline that identifies conflicts between visual and textual answers, showing a persistently high conflict rate across modalities in recent LVLMs regardless of the model size. We further investigate how these conflicts interfere with the inference process and propose a contrastive metric to discern the conflicting samples from the others. Building on these insights, we develop a novel dynamic contrastive decoding method that removes undesirable logits inferred from the less confident modality components based on answer confidence. For models that do not provide logits, we also introduce two prompt-based strategies to mitigate the conflicts. Our methods achieve promising improvements in accuracy on both the ViQuAE and InfoSeek datasets. Specifically, using LLaVA-34B, our proposed dynamic contrastive decoding improves an average accuracy of 2.24%. Comments:|Website:[this https URL](https://darthzhu.github.io/cross-modality-knowledge-conflict/)|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL)|\nCite as:|[arXiv:2410.03659](https://arxiv.org/abs/2410.03659)[cs.CV]|\n|(or[arXiv:2410.03659v2](https://arxiv.org/abs/2410.03659v2)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2410.03659](https://doi.org/10.48550/arXiv.2410.03659)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Tinghui Zhu [[view email](https://arxiv.org/show-email/d20349e0/2410.03659)]\n**[[v1]](https://arxiv.org/abs/2410.03659v1)**Fri, 4 Oct 2024 17:59:28 UTC (1,933 KB)\n**[v2]**Fri, 11 Oct 2024 15:07:31 UTC (1,933 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Unraveling Cross-Modality Knowledge Conflicts in Large Vision-Language Models, by Tinghui Zhu and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2410.03659)\n* [HTML (experimental)](https://arxiv.org/html/2410.03659v2)\n* [TeX Source](https://arxiv.org/src/2410.03659)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2410.03659&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2410.03659&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2024-10](https://arxiv.org/list/cs.CV/2024-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/2410.03659?context=cs)\n[cs.CL](https://arxiv.org/abs/2410.03659?context=cs.CL)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2410.03659)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2410.03659)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2410.03659)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individua",
          "original_query": "Unraveling cross-modality knowledge conflicts in large vision-language models",
          "cleaned_query": "Unraveling cross-modality knowledge conflicts in large vision-language models"
        },
        {
          "success": true,
          "title": "[2410.05162] Deciphering the Interplay of Parametric and Non ...",
          "url": "https://arxiv.org/abs/2410.05162",
          "content": "[2410.05162] Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2410.05162\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2410.05162**(cs)\n[Submitted on 7 Oct 2024]\n# Title:Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models\nAuthors:[Mehrdad Farahani](https://arxiv.org/search/cs?searchtype=author&amp;query=Farahani,+M),[Richard Johansson](https://arxiv.org/search/cs?searchtype=author&amp;query=Johansson,+R)\nView a PDF of the paper titled Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models, by Mehrdad Farahani and 1 other authors\n[View PDF](https://arxiv.org/pdf/2410.05162)[HTML (experimental)](https://arxiv.org/html/2410.05162v1)> > Abstract:\n> Generative language models often struggle with specialized or less-discussed knowledge. A potential solution is found in Retrieval-Augmented Generation (RAG) models which act like retrieving information before generating responses. In this study, we explore how the \\textsc{Atlas} approach, a RAG model, decides between what it already knows (parametric) and what it retrieves (non-parametric). We use causal mediation analysis and controlled experiments to examine how internal representations influence information processing. Our findings disentangle the effects of parametric knowledge and the retrieved context. They indicate that in cases where the model can choose between both types of information (parametric and non-parametric), it relies more on the context than the parametric knowledge. Furthermore, the analysis investigates the computations involved in \\emph{how} the model uses the information from the context. We find that multiple mechanisms are active within the model and can be detected with mediation analysis: first, the decision of \\emph{whether the context is relevant}, and second, how the encoder computes output representations to support copying when relevant. Comments:|Accepted at EMNLP 2024|\nSubjects:|Computation and Language (cs.CL)|\nCite as:|[arXiv:2410.05162](https://arxiv.org/abs/2410.05162)[cs.CL]|\n|(or[arXiv:2410.05162v1](https://arxiv.org/abs/2410.05162v1)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2410.05162](https://doi.org/10.48550/arXiv.2410.05162)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Mehrdad Farahani [[view email](https://arxiv.org/show-email/00037065/2410.05162)]\n**[v1]**Mon, 7 Oct 2024 16:14:47 UTC (332 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models, by Mehrdad Farahani and 1 other authors\n* [View PDF](https://arxiv.org/pdf/2410.05162)\n* [HTML (experimental)](https://arxiv.org/html/2410.05162v1)\n* [TeX Source](https://arxiv.org/src/2410.05162)\n[![license icon](https://arxiv.org/icons/licenses/by-sa-4.0.png)view license](http://creativecommons.org/licenses/by-sa/4.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2410.05162&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2410.05162&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2024-10](https://arxiv.org/list/cs.CL/2024-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/2410.05162?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2410.05162)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2410.05162)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2410.05162)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2410.05162)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Deciphering the interplay of parametric and non-parametric memory in retrieval-augmented language models",
          "cleaned_query": "Deciphering the interplay of parametric and non-parametric memory in retrieval-augmented language models"
        },
        {
          "success": true,
          "title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and ...",
          "url": "https://arxiv.org/abs/2410.07176",
          "content": "# Computer Science > Computation and Language\n\n**arXiv:2410.07176** (cs)\n\n\\[Submitted on 9 Oct 2024\\]\n\n# Title:Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models\n\nAuthors: [Fei Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+F), [Xingchen Wan](https://arxiv.org/search/cs?searchtype=author&query=Wan,+X), [Ruoxi Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun,+R), [Jiefeng Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+J), [Sercan \u00d6. Ar\u0131k](https://arxiv.org/search/cs?searchtype=author&query=Ar%C4%B1k,+S+%C3%96)\n\nView a PDF of the paper titled Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models, by Fei Wang and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2410.07176) [HTML (experimental)](https://arxiv.org/html/2410.07176v1)\n\n> Abstract:Retrieval-Augmented Generation (RAG), while effective in integrating external knowledge to address the limitations of large language models (LLMs), can be undermined by imperfect retrieval, which may introduce irrelevant, misleading, or even malicious information. Despite its importance, previous studies have rarely explored the behavior of RAG through joint analysis on how errors from imperfect retrieval attribute and propagate, and how potential conflicts arise between the LLMs' internal knowledge and external sources. We find that imperfect retrieval augmentation might be inevitable and quite harmful, through controlled analysis under realistic conditions. We identify the knowledge conflicts between LLM-internal and external knowledge from retrieval as a bottleneck to overcome in the post-retrieval stage of RAG. To render LLMs resilient to imperfect retrieval, we propose Astute RAG, a novel RAG approach that adaptively elicits essential information from LLMs' internal knowledge, iteratively consolidates internal and external knowledge with source-awareness, and finalizes the answer according to information reliability. Our experiments using Gemini and Claude demonstrate that Astute RAG significantly outperforms previous robustness-enhanced RAG methods. Notably, Astute RAG is the only approach that matches or exceeds the performance of LLMs without RAG under worst-case scenarios. Further analysis reveals that Astute RAG effectively resolves knowledge conflicts, improving the reliability and trustworthiness of RAG systems.\n\n| | |\n| --- | --- |\n| Comments: | Preprint |\n| Subjects: | Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2410.07176](https://arxiv.org/abs/2410.07176) \\[cs.CL\\] |\n| | (or [arXiv:2410.07176v1](https://arxiv.org/abs/2410.07176v1) \\[cs.CL\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2410.07176](https://doi.org/10.48550/arXiv.2410.07176) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Fei Wang \\[ [view email](https://arxiv.org/show-email/edbc8d6f/2410.07176)\\]\n\n**\\[v1\\]**\nWed, 9 Oct 2024 17:59:58 UTC (754 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models, by Fei Wang and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2410.07176)\n- [HTML (experimental)](https://arxiv.org/html/2410.07176v1)\n- [TeX Source](https://arxiv.org/src/2410.07176)\n- [Other Formats](https://arxiv.org/format/2410.07176)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CL\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2410.07176&function=prev&context=cs.CL)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2410.07176&function=next&context=cs.CL)\n\n[new](https://arxiv.org/list/cs.CL/new) \\| [recent](https://arxiv.org/list/cs.CL/recent) \\| [2024-10](https://arxiv.org/list/cs.CL/2024-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2410.07176?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2410.07176?context=cs.AI)\n\n[cs.LG](https://arxiv.org/abs/2410.07176?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2410.07176)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2410.07176)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2410.07176)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2410.07176&description=Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2410.07176&title=Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2410.07176) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Astute RAG: Overcoming imperfect retrieval augmentation and knowledge conflicts for large language models",
          "cleaned_query": "Astute RAG: Overcoming imperfect retrieval augmentation and knowledge conflicts for large language models"
        }
      ],
      "generated_ideas": [
        "Layer-Adaptive Retrieval Injection for RAG Models\nBuild a RAG architecture that decides *which transformer layers* receive retrieved context (early vs mid vs late) based on an attention-importance profile of the current query. Use a lightweight controller trained with downstream QA loss to gate retrieval-attention blocks per layer, aiming to reduce distraction from irrelevant passages while preserving copy behavior when evidence is strong.",
        "Causal-Mediation-Guided Retriever Training\nExtend REALM-style unsupervised retriever pretraining by adding a causal mediation objective: reward retrieved documents whose tokens causally mediate the final answer (as in mediation analysis of parametric vs non-parametric memory). Concretely, approximate token-level mediation via representation interventions (e.g., masking/patching encoder states) and backpropagate to the retriever to learn \u201ccausally useful\u201d retrieval rather than merely topically similar retrieval.",
        "Cross-Modality Conflict-Aware Multimodal RAG (Vision\u2013Text Arbitration)\nCombine hierarchical retrieval (Wiki-LLaVA) with dynamic contrastive decoding (cross-modality conflict paper) by explicitly modeling three sources: vision head, language model prior, and retrieved evidence. Implement an arbitration module that estimates per-source confidence and suppresses logits from the least reliable source *conditioned on retrieved grounding*, improving robustness when the image and text priors disagree.",
        "Adversarial Multimodal Retrieval Benchmark for Knowledge Conflict Stress-Testing\nCreate a dataset generator that introduces realistic \u201cimperfect retrieval\u201d in multimodal settings: near-duplicate passages, outdated captions, visually plausible but false entities, and prompt-injected malicious snippets. Evaluate Wiki-LLaVA-style hierarchical retrieval + Astute RAG consolidation to quantify which failure modes come from retrieval vs parametric conflicts, and release standardized metrics for conflict rate, evidence faithfulness, and worst-case degradation.",
        "Source-Aware Evidence Consolidation with Explicit Reliability Calibration\nExtend Astute RAG by attaching calibrated reliability scores to each retrieved passage (e.g., recency, provenance, agreement across documents) and to parametric memory (self-consistency across sampled decodes). Train a consolidation model to produce a structured \u201cevidence table\u201d (claim \u2192 supporting spans \u2192 source reliabilities) that conditions final generation, enabling controllable abstention when neither parametric nor retrieved evidence is trustworthy.",
        "Token-Level Copy vs Reason Tradeoff Controller in Retrieval-Augmented Generation\nBuilding on findings that models often rely on context when available, design a controller that predicts whether the answer should be copied verbatim from retrieved spans or synthesized via reasoning. Supervise the controller using heuristics from string overlap/attribution and optimize end-to-end so the model reduces over-copying from misleading retrieval while still copying when spans are clearly answer-bearing.",
        "Joint Pretraining of Multimodal REALM with Image-Conditioned Retrieval\nGeneralize REALM\u2019s unsupervised retriever pretraining to multimodal inputs by using masked language modeling on image-caption pairs (or image + question) where retrieval is conditioned on visual embeddings plus text. Pretrain on Wikipedia-like multimodal corpora (image, caption, article sections) so the retriever learns to fetch passages aligned with visual entities, improving open-domain VQA requiring external knowledge.",
        "Conflict Localization Maps for LVLMs via Attention and Logit Attribution\nDevelop an interpretability tool that localizes *where* cross-modality knowledge conflicts arise: which image regions, which retrieved passages, and which transformer layers/heads drive the conflicting answer logits. Use attention-importance analysis across layers plus logit attribution under modality ablations to output \u201cconflict maps,\u201d enabling targeted mitigation (e.g., suppressing only specific heads or retrieved spans rather than whole modalities).",
        "Learning to Abstain Under Retrieval Uncertainty with Worst-Case Guarantees\nCreate a training recipe where a RAG model learns a calibrated abstention decision when retrieval quality is low or conflicts are detected (parametric vs retrieved, or vision vs language). Implement a worst-case-oriented objective (inspired by Astute RAG\u2019s analysis of inevitable imperfect retrieval) that optimizes performance under adversarially perturbed retrieval sets, producing models that degrade gracefully and explicitly refuse when evidence is insufficient."
      ],
      "generated_ideas_raw": "1. **Layer-Adaptive Retrieval Injection for RAG Models**\n   Build a RAG architecture that decides *which transformer layers* receive retrieved context (early vs mid vs late) based on an attention-importance profile of the current query. Use a lightweight controller trained with downstream QA loss to gate retrieval-attention blocks per layer, aiming to reduce distraction from irrelevant passages while preserving copy behavior when evidence is strong.\n\n2. **Causal-Mediation-Guided Retriever Training**\n   Extend REALM-style unsupervised retriever pretraining by adding a causal mediation objective: reward retrieved documents whose tokens causally mediate the final answer (as in mediation analysis of parametric vs non-parametric memory). Concretely, approximate token-level mediation via representation interventions (e.g., masking/patching encoder states) and backpropagate to the retriever to learn \u201ccausally useful\u201d retrieval rather than merely topically similar retrieval.\n\n3. **Cross-Modality Conflict-Aware Multimodal RAG (Vision\u2013Text Arbitration)**\n   Combine hierarchical retrieval (Wiki-LLaVA) with dynamic contrastive decoding (cross-modality conflict paper) by explicitly modeling three sources: vision head, language model prior, and retrieved evidence. Implement an arbitration module that estimates per-source confidence and suppresses logits from the least reliable source *conditioned on retrieved grounding*, improving robustness when the image and text priors disagree.\n\n4. **Adversarial Multimodal Retrieval Benchmark for Knowledge Conflict Stress-Testing**\n   Create a dataset generator that introduces realistic \u201cimperfect retrieval\u201d in multimodal settings: near-duplicate passages, outdated captions, visually plausible but false entities, and prompt-injected malicious snippets. Evaluate Wiki-LLaVA-style hierarchical retrieval + Astute RAG consolidation to quantify which failure modes come from retrieval vs parametric conflicts, and release standardized metrics for conflict rate, evidence faithfulness, and worst-case degradation.\n\n5. **Source-Aware Evidence Consolidation with Explicit Reliability Calibration**\n   Extend Astute RAG by attaching calibrated reliability scores to each retrieved passage (e.g., recency, provenance, agreement across documents) and to parametric memory (self-consistency across sampled decodes). Train a consolidation model to produce a structured \u201cevidence table\u201d (claim \u2192 supporting spans \u2192 source reliabilities) that conditions final generation, enabling controllable abstention when neither parametric nor retrieved evidence is trustworthy.\n\n6. **Hierarchical Retriever with Conflict-Seeking Second Stage**\n   Modify Wiki-LLaVA\u2019s hierarchical pipeline so the second-stage retriever is *trained to find contradicting evidence* to the top-1 hypothesis from parametric memory (or the first retrieved passage). Use this \u201cconflict-seeking retrieval\u201d to surface disagreements early, then apply contrastive decoding to downweight unsupported hypotheses, targeting reduced hallucinations and better uncertainty handling on ambiguous questions.\n\n7. **Token-Level Copy vs Reason Tradeoff Controller in Retrieval-Augmented Generation**\n   Building on findings that models often rely on context when available, design a controller that predicts whether the answer should be copied verbatim from retrieved spans or synthesized via reasoning. Supervise the controller using heuristics from string overlap/attribution and optimize end-to-end so the model reduces over-copying from misleading retrieval while still copying when spans are clearly answer-bearing.\n\n8. **Joint Pretraining of Multimodal REALM with Image-Conditioned Retrieval**\n   Generalize REALM\u2019s unsupervised retriever pretraining to multimodal inputs by using masked language modeling on image-caption pairs (or image + question) where retrieval is conditioned on visual embeddings plus text. Pretrain on Wikipedia-like multimodal corpora (image, caption, article sections) so the retriever learns to fetch passages aligned with visual entities, improving open-domain VQA requiring external knowledge.\n\n9. **Conflict Localization Maps for LVLMs via Attention and Logit Attribution**\n   Develop an interpretability tool that localizes *where* cross-modality knowledge conflicts arise: which image regions, which retrieved passages, and which transformer layers/heads drive the conflicting answer logits. Use attention-importance analysis across layers plus logit attribution under modality ablations to output \u201cconflict maps,\u201d enabling targeted mitigation (e.g., suppressing only specific heads or retrieved spans rather than whole modalities).\n\n10. **Learning to Abstain Under Retrieval Uncertainty with Worst-Case Guarantees**\n   Create a training recipe where a RAG model learns a calibrated abstention decision when retrieval quality is low or conflicts are detected (parametric vs retrieved, or vision vs language). Implement a worst-case-oriented objective (inspired by Astute RAG\u2019s analysis of inevitable imperfect retrieval) that optimizes performance under adversarially perturbed retrieval sets, producing models that degrade gracefully and explicitly refuse when evidence is insufficient.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Layer-Adaptive Retrieval Injection for RAG Models\nBuild a RAG architecture that decides *which transformer layers* receive retrieved context (early vs mid vs late) based on an attention-importance pro",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Causal-Mediation-Guided Retriever Training\nExtend REALM-style unsupervised retriever pretraining by adding a causal mediation objective: reward retrieved documents whose tokens causally mediate the fi",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Cross-Modality Conflict-Aware Multimodal RAG (Vision\u2013Text Arbitration)\nCombine hierarchical retrieval (Wiki-LLaVA) with dynamic contrastive decoding (cross-modality conflict paper) by explicitly model",
          "is_match": true
        },
        {
          "idea_idx": 3,
          "idea_text": "Adversarial Multimodal Retrieval Benchmark for Knowledge Conflict Stress-Testing\nCreate a dataset generator that introduces realistic \u201cimperfect retrieval\u201d in multimodal settings: near-duplicate passa",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Source-Aware Evidence Consolidation with Explicit Reliability Calibration\nExtend Astute RAG by attaching calibrated reliability scores to each retrieved passage (e.g., recency, provenance, agreement a",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Token-Level Copy vs Reason Tradeoff Controller in Retrieval-Augmented Generation\nBuilding on findings that models often rely on context when available, design a controller that predicts whether the an",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Joint Pretraining of Multimodal REALM with Image-Conditioned Retrieval\nGeneralize REALM\u2019s unsupervised retriever pretraining to multimodal inputs by using masked language modeling on image-caption pai",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Conflict Localization Maps for LVLMs via Attention and Logit Attribution\nDevelop an interpretability tool that localizes *where* cross-modality knowledge conflicts arise: which image regions, which re",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Learning to Abstain Under Retrieval Uncertainty with Worst-Case Guarantees\nCreate a training recipe where a RAG model learns a calibrated abstention decision when retrieval quality is low or conflicts",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 35,
      "paper_title": "Interactive Cross-modal Learning for Text-3D Scene Retrieval",
      "contribution": "Introduce IDeal, an interactive Text-3D Scene Retrieval method that iteratively refines text\u20133D alignment with a questioner/answerer loop (IRR) and an Interaction Adaptation Tuning (IAT) strategy to fuse feature- and semantic-level signals and bridge domain gaps for improved re-ranking.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 6,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 9036,
      "output_tokens": 903,
      "predecessor_details": [
        {
          "success": true,
          "title": "[2305.20062] Chatting Makes Perfect: Chat-based Image Retrieval",
          "url": "https://arxiv.org/abs/2305.20062",
          "content": "[View PDF](https://arxiv.org/pdf/2305.20062)\n\n> Abstract:Chats emerge as an effective user-friendly approach for information retrieval, and are successfully employed in many domains, such as customer service, healthcare, and finance. However, existing image retrieval approaches typically address the case of a single query-to-image round, and the use of chats for image retrieval has been mostly overlooked. In this work, we introduce ChatIR: a chat-based image retrieval system that engages in a conversation with the user to elicit information, in addition to an initial query, in order to clarify the user's search intent. Motivated by the capabilities of today's foundation models, we leverage Large Language Models to generate follow-up questions to an initial image description. These questions form a dialog with the user in order to retrieve the desired image from a large corpus. In this study, we explore the capabilities of such a system tested on a large dataset and reveal that engaging in a dialog yields significant gains in image retrieval. We start by building an evaluation pipeline from an existing manually generated dataset and explore different modules and training strategies for ChatIR. Our comparison includes strong baselines derived from related applications trained with Reinforcement Learning. Our system is capable of retrieving the target image from a pool of 50K images with over 78% success rate after 5 dialogue rounds, compared to 75% when questions are asked by humans, and 64% for a single shot text-to-image retrieval. Extensive evaluations reveal the strong capabilities and examine the limitations of CharIR under different settings. Project repository is available at [this https URL](https://github.com/levymsn/ChatIR).\n\n## Submission history\n\nFrom: Matan Levy \\[ [view email](https://arxiv.org/show-email/cba7e593/2305.20062)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2305.20062v1)**\nWed, 31 May 2023 17:38:08 UTC (30,299 KB)\n\n**\\[v2\\]**\nThu, 5 Oct 2023 16:40:02 UTC (23,091 KB)",
          "original_query": "Chatting makes perfect: Chat-based image retrieval",
          "cleaned_query": "Chatting makes perfect: Chat-based image retrieval"
        },
        {
          "success": true,
          "title": "Simple Baselines for Interactive Video Retrieval with Questions and ...",
          "url": "https://arxiv.org/abs/2308.10402",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2308.10402** (cs)\n\n\\[Submitted on 21 Aug 2023\\]\n\n# Title:Simple Baselines for Interactive Video Retrieval with Questions and Answers\n\nAuthors: [Kaiqu Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang,+K), [Samuel Albanie](https://arxiv.org/search/cs?searchtype=author&query=Albanie,+S)\n\nView a PDF of the paper titled Simple Baselines for Interactive Video Retrieval with Questions and Answers, by Kaiqu Liang and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2308.10402)\n\n> Abstract:To date, the majority of video retrieval systems have been optimized for a \"single-shot\" scenario in which the user submits a query in isolation, ignoring previous interactions with the system. Recently, there has been renewed interest in interactive systems to enhance retrieval, but existing approaches are complex and deliver limited gains in performance. In this work, we revisit this topic and propose several simple yet effective baselines for interactive video retrieval via question-answering. We employ a VideoQA model to simulate user interactions and show that this enables the productive study of the interactive retrieval task without access to ground truth dialogue data. Experiments on MSR-VTT, MSVD, and AVSD show that our framework using question-based interaction significantly improves the performance of text-based video retrieval systems.\n\n| | |\n| --- | --- |\n| Comments: | ICCV 2023, project page: [this https URL](https://github.com/kevinliang888/IVR-QA-baselines) |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC) |\n| Cite as: | [arXiv:2308.10402](https://arxiv.org/abs/2308.10402) \\[cs.CV\\] |\n| | (or [arXiv:2308.10402v1](https://arxiv.org/abs/2308.10402v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2308.10402](https://doi.org/10.48550/arXiv.2308.10402) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Kaiqu Liang \\[ [view email](https://arxiv.org/show-email/edfc74dc/2308.10402)\\]\n\n**\\[v1\\]**\nMon, 21 Aug 2023 00:32:19 UTC (908 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Simple Baselines for Interactive Video Retrieval with Questions and Answers, by Kaiqu Liang and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2308.10402)\n- [TeX Source](https://arxiv.org/src/2308.10402)\n- [Other Formats](https://arxiv.org/format/2308.10402)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2308.10402&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2308.10402&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2023-08](https://arxiv.org/list/cs.CV/2023-08)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2308.10402?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2308.10402?context=cs.AI)\n\n[cs.CL](https://arxiv.org/abs/2308.10402?context=cs.CL)\n\n[cs.HC](https://arxiv.org/abs/2308.10402?context=cs.HC)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2308.10402)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2308.10402)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2308.10402)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2308.10402&description=Simple Baselines for Interactive Video Retrieval with Questions and Answers) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2308.10402&title=Simple Baselines for Interactive Video Retrieval with Questions and Answers)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2308.10402) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Simple baselines for interactive video retrieval with questions and answers",
          "cleaned_query": "Simple baselines for interactive video retrieval with questions and answers"
        },
        {
          "success": true,
          "title": "MERLIN: Multimodal Embedding Refinement via LLM- ...",
          "url": "https://arxiv.org/abs/2407.12508",
          "content": "[2407.12508] MERLIN: Multimodal Embedding Refinement via LLM-based Iterative Navigation for Text-Video Retrieval-Rerank Pipeline\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nIn just 5 minutes help us improve arXiv:\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2407.12508\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2407.12508**(cs)\n[Submitted on 17 Jul 2024 ([v1](https://arxiv.org/abs/2407.12508v1)), last revised 16 Oct 2024 (this version, v2)]\n# Title:MERLIN: Multimodal Embedding Refinement via LLM-based Iterative Navigation for Text-Video Retrieval-Rerank Pipeline\nAuthors:[Donghoon Han](https://arxiv.org/search/cs?searchtype=author&amp;query=Han,+D),[Eunhwan Park](https://arxiv.org/search/cs?searchtype=author&amp;query=Park,+E),[Gisang Lee](https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+G),[Adam Lee](https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+A),[Nojun Kwak](https://arxiv.org/search/cs?searchtype=author&amp;query=Kwak,+N)\nView a PDF of the paper titled MERLIN: Multimodal Embedding Refinement via LLM-based Iterative Navigation for Text-Video Retrieval-Rerank Pipeline, by Donghoon Han and 3 other authors\n[View PDF](https://arxiv.org/pdf/2407.12508)[HTML (experimental)](https://arxiv.org/html/2407.12508v2)> > Abstract:\n> The rapid expansion of multimedia content has made accurately retrieving relevant videos from large collections increasingly challenging. Recent advancements in text-video retrieval have focused on cross-modal interactions, large-scale foundation model training, and probabilistic modeling, yet often neglect the crucial user perspective, leading to discrepancies between user queries and the content retrieved. To address this, we introduce MERLIN (Multimodal Embedding Refinement via LLM-based Iterative Navigation), a novel, training-free pipeline that leverages Large Language Models (LLMs) for iterative feedback learning. MERLIN refines query embeddings from a user perspective, enhancing alignment between queries and video content through a dynamic question answering process. Experimental results on datasets like MSR-VTT, MSVD, and ActivityNet demonstrate that MERLIN substantially improves Recall@1, outperforming existing systems and confirming the benefits of integrating LLMs into multimodal retrieval systems for more responsive and context-aware multimedia retrieval. Comments:|EMNLP 2024 Industry Track Accepted (Camera-Ready Version)|\nSubjects:|Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:2407.12508](https://arxiv.org/abs/2407.12508)[cs.CL]|\n|(or[arXiv:2407.12508v2](https://arxiv.org/abs/2407.12508v2)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2407.12508](https://doi.org/10.48550/arXiv.2407.12508)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Eunhwan Park [[view email](https://arxiv.org/show-email/89ff6070/2407.12508)]\n**[[v1]](https://arxiv.org/abs/2407.12508v1)**Wed, 17 Jul 2024 11:45:02 UTC (6,580 KB)\n**[v2]**Wed, 16 Oct 2024 06:25:50 UTC (6,581 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled MERLIN: Multimodal Embedding Refinement via LLM-based Iterative Navigation for Text-Video Retrieval-Rerank Pipeline, by Donghoon Han and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2407.12508)\n* [HTML (experimental)](https://arxiv.org/html/2407.12508v2)\n* [TeX Source](https://arxiv.org/src/2407.12508)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2407.12508&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2407.12508&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2024-07](https://arxiv.org/list/cs.CL/2024-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/2407.12508?context=cs)\n[cs.AI](https://arxiv.org/abs/2407.12508?context=cs.AI)\n[cs.CV](https://arxiv.org/abs/2407.12508?context=cs.CV)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2407.12508)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2407.12508)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2407.12508)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn",
          "original_query": "Merlin: Multimodal embedding refinement via llm-based iterative navigation for text-video retrieval-rerank pipeline",
          "cleaned_query": "Merlin: Multimodal embedding refinement via llm-based iterative navigation for text-video retrieval-rerank pipeline"
        },
        {
          "success": true,
          "title": "Learning Transferable Visual Models From Natural Language ...",
          "url": "https://arxiv.org/abs/2103.00020",
          "content": "[2103.00020] Learning Transferable Visual Models From Natural Language Supervision\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2103.00020\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2103.00020**(cs)\n[Submitted on 26 Feb 2021]\n# Title:Learning Transferable Visual Models From Natural Language Supervision\nAuthors:[Alec Radford](https://arxiv.org/search/cs?searchtype=author&amp;query=Radford,+A),[Jong Wook Kim](https://arxiv.org/search/cs?searchtype=author&amp;query=Kim,+J+W),[Chris Hallacy](https://arxiv.org/search/cs?searchtype=author&amp;query=Hallacy,+C),[Aditya Ramesh](https://arxiv.org/search/cs?searchtype=author&amp;query=Ramesh,+A),[Gabriel Goh](https://arxiv.org/search/cs?searchtype=author&amp;query=Goh,+G),[Sandhini Agarwal](https://arxiv.org/search/cs?searchtype=author&amp;query=Agarwal,+S),[Girish Sastry](https://arxiv.org/search/cs?searchtype=author&amp;query=Sastry,+G),[Amanda Askell](https://arxiv.org/search/cs?searchtype=author&amp;query=Askell,+A),[Pamela Mishkin](https://arxiv.org/search/cs?searchtype=author&amp;query=Mishkin,+P),[Jack Clark](https://arxiv.org/search/cs?searchtype=author&amp;query=Clark,+J),[Gretchen Krueger](https://arxiv.org/search/cs?searchtype=author&amp;query=Krueger,+G),[Ilya Sutskever](https://arxiv.org/search/cs?searchtype=author&amp;query=Sutskever,+I)\nView a PDF of the paper titled Learning Transferable Visual Models From Natural Language Supervision, by Alec Radford and 11 other authors\n[View PDF](https://arxiv.org/pdf/2103.00020)> > Abstract:\n> State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at [> this https URL\n](https://github.com/OpenAI/CLIP)> . Subjects:|Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)|\nCite as:|[arXiv:2103.00020](https://arxiv.org/abs/2103.00020)[cs.CV]|\n|(or[arXiv:2103.00020v1](https://arxiv.org/abs/2103.00020v1)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2103.00020](https://doi.org/10.48550/arXiv.2103.00020)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jong Wook Kim [[view email](https://arxiv.org/show-email/6c157f7d/2103.00020)]\n**[v1]**Fri, 26 Feb 2021 19:04:58 UTC (6,174 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Learning Transferable Visual Models From Natural Language Supervision, by Alec Radford and 11 other authors\n* [View PDF](https://arxiv.org/pdf/2103.00020)\n* [TeX Source](https://arxiv.org/src/2103.00020)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2103.00020&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2103.00020&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2021-03](https://arxiv.org/list/cs.CV/2021-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2103.00020?context=cs)\n[cs.LG](https://arxiv.org/abs/2103.00020?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2103.00020)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2103.00020)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2103.00020)\n### [16 blog links](https://arxiv.org/tb/2103.00020)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2103.html#abs-2103-00020)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2103-00020)\n[Alec Radford]()\n[Jong Wook Kim]()\n[Aditya Ramesh]()\n[Gabriel Goh]()\n[Girish Sastry]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommende",
          "original_query": "Learning transferable visual models from natural language supervision (CLIP)",
          "cleaned_query": "Learning transferable visual models from natural language supervision (CLIP)"
        },
        {
          "success": true,
          "title": "PointCloud-Text Matching: Benchmark Datasets and a Baseline - arXiv",
          "url": "https://arxiv.org/abs/2403.19386",
          "content": "\n \n No PDF available, click to view other formats \nIn this paper, we present and study a new instance-level retrieval task: PointCloud-Text Matching~(PTM), which aims to find the exact cross-modal instance that matches a given point-cloud query or text query. PTM could be applied to various scenarios, such as indoor/urban-canyon localization and scene retrieval. However, there exists no suitable and targeted dataset for PTM in practice. Therefore, we construct three new PTM benchmark datasets, namely 3D2T-SR, 3D2T-NR, and 3D2T-QA. We observe that the data is challenging and with noisy correspondence due to the sparsity, noise, or disorder of point clouds and the ambiguity, vagueness, or incompleteness of texts, which make existing cross-modal matching methods ineffective for PTM. To tackle these challenges, we propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa). RoMa consists of two modules: a Dual Attention Perception module (DAP) and a Robust Negative Contrastive Learning module (RNCL). Specifically, DAP leverages token-level and feature-level attention to adaptively focus on useful local and global features, and aggregate them into common representations, thereby reducing the adverse impact of noise and ambiguity. To handle noisy correspondence, RNCL divides negative pairs, which are much less error-prone than positive pairs, into clean and noisy subsets, and assigns them forward and reverse optimization directions respectively, thus enhancing robustness against noisy correspondence. We conduct extensive experiments on our benchmarks and demonstrate the superiority of our RoMa.\n \n \n Submission history From: Yanglin Feng [ view email] [v1] \nThu, 28 Mar 2024 12:51:15 UTC (1,680 KB) \n [v2] \nThu, 5 Sep 2024 03:18:11 UTC (1 KB) (withdrawn) \n",
          "original_query": "Pointcloud-text matching: Benchmark dataset and baseline",
          "cleaned_query": "Pointcloud-text matching: Benchmark dataset and baseline"
        },
        {
          "success": true,
          "title": "Text2Loc: 3D Point Cloud Localization from Natural Language",
          "url": "https://www.researchgate.net/publication/384222286_Text2Loc_3D_Point_Cloud_Localization_from_Natural_Language",
          "content": "To read the full-text of this research, you can request a copy directly from the authors.... For this, we propose to use the diffusion-based conditional distribution learning technique, where the pose samples of the target distribution are obtained by iteratively updating the noisy poses, while conditioning upon the text features. This differentiates our method from those in [22, 62], which focus on finding the most likely pose and require highly detailed text descriptions. ...... Our method significantly differs from the closest related works: PoseDiffusion [58] and Text2Pose [22], Text2Loc [62], and RET [57]. PoseDiffusion localizes the central object, whose poses in the general settings are deterministic and less ambagious, unlike that of the text description. ...... Hloc [48] improves large-scale localization by combining structure-from-motion with global features, while iNeRF [65] and iGS [53] invert NeRFs and 3DGS for 6DoF pose estimation. Text-based methods like Text2Pos [57] and Text2Loc [62] generate poses from descriptions, but can suffer from ambiguity. In contrast, our method learns a distribution of poses from text and refines them using a 3DGS map, marking the first work to bridge text descriptions with pose distributions and use Gaussian representations for large-scale pose refinement. ... Qi Ma Runyi Yang Bin Ren Danda Pani Paudel Localizing text descriptions in large-scale 3D scenes is inherently an ambiguous task. This nonetheless arises while describing general concepts, e.g. all traffic lights in a city. To facilitate reasoning based on such concepts, text localization in the form of distribution is required. In this paper, we generate the distribution of the camera poses conditioned upon the textual description. To facilitate such generation, we propose a diffusion-based architecture that conditionally diffuses the noisy 6DoF camera poses to their plausible locations. The conditional signals are derived from the text descriptions, using the pre-trained text encoders. The connection between text descriptions and pose distribution is established through pretrained Vision-Language-Model, i.e. CLIP. Furthermore, we demonstrate that the candidate poses for the distribution can be further refined by rendering potential poses using 3D Gaussian splatting, guiding incorrectly posed samples towards locations that better align with the textual description, through visual reasoning. We demonstrate the effectiveness of our method by comparing it with both standard retrieval methods and learning-based approaches. Our proposed method consistently outperforms these baselines across all five large-scale datasets. Our source code and dataset will be made publicly available.... However, most research focuses on matching image-to-3D data or image-to-image data. Recent studies have begun to explore a novel localization approach based on natural language text, which holds significant value for many practical applications [36,39, 40, 43]. As shown in Figure 1, taxi drivers rely on verbal instructions from passengers to determine their location [19], or pedestrians describe their position during emergency calls [7]. ...... Recent natural language localization methods, such as Text2Pose [19] and Text2Loc [40], have been limited to using natural language to identify individual locations within point clouds. Constructing 3D maps using LiDAR or photogrammetry is expensive on a global scale [1,13,27], and the storage costs for 3D maps are also high, often requiring costly cloud infrastructure, which hinders localization on mobile devices. ...... For instance, Loc4Plan [36] emphasizes the necessity of visual spatial localization prior to navigation. Text2Pose [19] and Text2Loc [40] explore the use of natural language to identify individual positions in outdoor point cloud maps. However, point cloud retrieval tasks tend to incur high storage and computational costs when handling large-scale areas. ... Junyan Ye Honglin Lin Leyan Ou Weijia Li Cross-view geo-localization identifies the locations of street-view images by matching them with geo-tagged satellite images or OSM. However, most studies focus on image-to-image retrieval, with fewer addressing text-guided retrieval, a task vital for applications like pedestrian navigation and emergency response. In this work, we introduce a novel task for cross-view geo-localization with natural language descriptions, which aims to retrieve corresponding satellite images or OSM database based on scene text. To support this task, we construct the CVG-Text dataset by collecting cross-view data from multiple cities and employing a scene text generation approach that leverages the annotation capabilities of Large Multimodal Models to produce high-quality scene text descriptions with localization details.Additionally, we propose a novel text-based retrieval localization method, CrossText2Loc, which improves recall by 10% and demonstrates excellent long-text retrieval capabilities. In terms of explainability, it not only provides similarity scores but also offers retrieval reasons. More information can be found at https://yejy53.github.io/CVG-Text/.... To address these limitations, cross-modal methods [8,10,16,20, 39, 41] have emerged, offering flexibility by enabling place recognition across different query and map data sources. For example, camera-to-LiDAR approaches [8,20] support querying a LiDAR database with 2D images, which is beneficial in situations where processing large point clouds is impractical or where LiDAR data is unavailable or compromised. ...... This cross-modal capability is also highly relevant to real-world navigation tasks, such as delivery services, where couriers may rely on verbal directions from recipients to find precise drop-off points. More broadly, the \"last mile problem\"-navigating to an unfamiliar final destination-highlights the need for flexible place recognition methods that can incorporate natural language input [10,16, 39, 41]. Despite this demand, there remains no universal place recognition solution that seamlessly integrates text, images, and point clouds. ...... Building on this, Wang et al. [34] introduced a Transformer-based approach to improve representation discriminability for both point clouds and text queries. Text2Loc [39] further enhances localization accuracy and efficiency by adopting contrastive learning and a lighter, faster strategy that eliminates the need for a textinstance matcher. In this work, we aim to address the place recognition problem in a unified solution that leverages any single modality independently. ... To date, most place recognition methods focus on single-modality retrieval. While they perform well in specific environments, cross-modal methods offer greater flexibility by allowing seamless switching between map and query sources. It also promises to reduce computation requirements by having a unified model, and achieving greater sample efficiency by sharing parameters. In this work, we develop a universal solution to place recognition, UniLoc, that works with any single query modality (natural language, image, or point cloud). UniLoc leverages recent advances in large-scale contrastive learning, and learns by matching hierarchically at two levels: instance-level matching and scene-level matching. Specifically, we propose a novel Self-Attention based Pooling (SAP) module to evaluate the importance of instance descriptors when aggregated into a place-level descriptor. Experiments on the KITTI-360 dataset demonstrate the benefits of cross-modality for place recognition, achieving superior performance in cross-modal settings and competitive results also for uni-modal scenarios. Our project page is publicly available at https://yan-xia.github.io/projects/UniLoc/.... To address this limitation, Wang et al. introduced the Relation-Enhanced Transformer (RET) to establish relationships between point clouds and text [1], utilizing crossattention in the fine p",
          "original_query": "\u201cWhere am I?\u201d scene retrieval with language",
          "cleaned_query": "\u201cWhere am I?\u201d scene retrieval with language"
        }
      ],
      "generated_ideas": [
        "Uncertainty-Aware Question Asking for Chat-Based Image Retrieval\n- Extend ChatIR by maintaining a probabilistic belief over candidate images (e.g., entropy over top\u2011K CLIP scores) and selecting follow-up questions via expected information gain. Implement and compare entropy-reduction policies against LLM-only question generation, measuring success rate vs. number of turns and \u201cquestion cost.\u201d",
        "Multimodal Clarification Questions Grounded in Retrieved Evidence\n- Augment ChatIR/MERLIN with \u201cevidence-conditioned\u201d question generation that explicitly references attributes observed in the current top retrieved images/videos (e.g., \u201cIs the object more like the *left* or *right* example?\u201d). This makes dialogs more discriminative and testable; evaluate gains in retrieval accuracy and reduction of ambiguous user answers.",
        "Robust Interactive Retrieval Under Noisy/Unreliable User Answers\n- Combine RoMa\u2019s robustness ideas (clean/noisy partitioning) with interactive retrieval by treating user answers as potentially noisy labels. Train a dialog policy and embedding updater that down-weights inconsistent answers and actively asks verification questions when contradictions are detected; evaluate against simulated and real users with controlled noise rates.",
        "Training-Free Iterative Embedding Refinement for ChatIR (MERLIN-for-Images)\n- Adapt MERLIN\u2019s \u201cLLM-based iterative navigation\u201d to image retrieval by iteratively rewriting/steering the query embedding using short QA turns, without fine-tuning the backbone retriever. Compare against standard query rewriting and RL baselines on large-scale image pools, focusing on generalization to unseen domains.",
        "Video Retrieval with Temporal Disambiguation Questions\n- Extend the IVR-QA baselines by generating questions that target temporal structure (before/after, duration, repeated actions) and refine retrieval using time-aware embedding updates. Implement a temporal \u201cslot filling\u201d state (e.g., action order constraints) and measure improvements on datasets where many clips share similar objects but differ in event sequence.",
        "Dialog-Conditioned Reranking with Cross-Modal Rationale Supervision\n- Build a reranker that consumes the full dialog history and produces not only a ranking but also a faithful rationale (\u201cbecause you said: red backpack, crowded station, night\u201d). Use LLM-generated rationales as weak supervision, then validate faithfulness by perturbing dialog turns and checking ranking/rationale consistency."
      ],
      "generated_ideas_raw": "1. **Uncertainty-Aware Question Asking for Chat-Based Image Retrieval**\n   - Extend ChatIR by maintaining a probabilistic belief over candidate images (e.g., entropy over top\u2011K CLIP scores) and selecting follow-up questions via expected information gain. Implement and compare entropy-reduction policies against LLM-only question generation, measuring success rate vs. number of turns and \u201cquestion cost.\u201d\n\n2. **Multimodal Clarification Questions Grounded in Retrieved Evidence**\n   - Augment ChatIR/MERLIN with \u201cevidence-conditioned\u201d question generation that explicitly references attributes observed in the current top retrieved images/videos (e.g., \u201cIs the object more like the *left* or *right* example?\u201d). This makes dialogs more discriminative and testable; evaluate gains in retrieval accuracy and reduction of ambiguous user answers.\n\n3. **Joint Interactive Retrieval Across Images, Videos, and Point Clouds**\n   - Build a unified interactive retrieval system where a single dialog can retrieve from heterogeneous corpora (2D images, videos, and 3D point clouds) using a shared CLIP-like latent space plus modality adapters (inspired by RoMa\u2019s dual attention). Design cross-modal questions (\u201cIs it an indoor corridor or an outdoor plaza?\u201d) and benchmark on mixed-modality retrieval tasks with shared scene semantics.\n\n4. **Robust Interactive Retrieval Under Noisy/Unreliable User Answers**\n   - Combine RoMa\u2019s robustness ideas (clean/noisy partitioning) with interactive retrieval by treating user answers as potentially noisy labels. Train a dialog policy and embedding updater that down-weights inconsistent answers and actively asks verification questions when contradictions are detected; evaluate against simulated and real users with controlled noise rates.\n\n5. **Training-Free Iterative Embedding Refinement for ChatIR (MERLIN-for-Images)**\n   - Adapt MERLIN\u2019s \u201cLLM-based iterative navigation\u201d to image retrieval by iteratively rewriting/steering the query embedding using short QA turns, without fine-tuning the backbone retriever. Compare against standard query rewriting and RL baselines on large-scale image pools, focusing on generalization to unseen domains.\n\n6. **Video Retrieval with Temporal Disambiguation Questions**\n   - Extend the IVR-QA baselines by generating questions that target temporal structure (before/after, duration, repeated actions) and refine retrieval using time-aware embedding updates. Implement a temporal \u201cslot filling\u201d state (e.g., action order constraints) and measure improvements on datasets where many clips share similar objects but differ in event sequence.\n\n7. **Dialog-Conditioned Reranking with Cross-Modal Rationale Supervision**\n   - Build a reranker that consumes the full dialog history and produces not only a ranking but also a faithful rationale (\u201cbecause you said: red backpack, crowded station, night\u201d). Use LLM-generated rationales as weak supervision, then validate faithfulness by perturbing dialog turns and checking ranking/rationale consistency.\n\n8. **Interactive PointCloud\u2013Text Matching with Attribute Probing**\n   - Create an interactive extension of PTM where the system asks targeted questions about geometric/structural attributes (e.g., \u201cAre there two parallel walls?\u201d \u201cIs the space open or narrow?\u201d) and refines the matching embedding via RoMa-like dual attention. Release a small benchmark built from 3D2T-* by converting captions into multi-turn attribute dialogs and evaluate turn-wise retrieval gains.\n\n9. **Text-to-3D Localization via Conversational Pose Distribution Refinement**\n   - Extend Text2Loc-style pose-distribution diffusion by adding a dialog loop: the system queries the user about uncertain landmarks (\u201cDo you see a bridge or an underpass?\u201d), then reconditions the diffusion sampler to tighten the pose distribution. Quantify calibration (coverage vs. sharpness) and final localization error after each turn, using 3DGS refinement as the final step.\n\n10. **Cost-Bounded Interactive Retrieval: Optimizing Turn Count vs. Compute**\n   - Formalize an objective that trades off retrieval success, number of dialog turns, and compute (LLM tokens, rerank cost, 3D rendering cost for Text2Loc/3DGS). Develop a stopping/asking policy that decides when to stop querying and return results, and benchmark across image/video/3D retrieval to produce practical operating curves for deployment.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Uncertainty-Aware Question Asking for Chat-Based Image Retrieval\n- Extend ChatIR by maintaining a probabilistic belief over candidate images (e.g., entropy over top\u2011K CLIP scores) and selecting follow",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Multimodal Clarification Questions Grounded in Retrieved Evidence\n- Augment ChatIR/MERLIN with \u201cevidence-conditioned\u201d question generation that explicitly references attributes observed in the current ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Robust Interactive Retrieval Under Noisy/Unreliable User Answers\n- Combine RoMa\u2019s robustness ideas (clean/noisy partitioning) with interactive retrieval by treating user answers as potentially noisy l",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Training-Free Iterative Embedding Refinement for ChatIR (MERLIN-for-Images)\n- Adapt MERLIN\u2019s \u201cLLM-based iterative navigation\u201d to image retrieval by iteratively rewriting/steering the query embedding u",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Video Retrieval with Temporal Disambiguation Questions\n- Extend the IVR-QA baselines by generating questions that target temporal structure (before/after, duration, repeated actions) and refine retrie",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Dialog-Conditioned Reranking with Cross-Modal Rationale Supervision\n- Build a reranker that consumes the full dialog history and produces not only a ranking but also a faithful rationale (\u201cbecause you",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 36,
      "paper_title": "Rethinking Joint Maximum Mean Discrepancy for Visual Domain Adaptation",
      "contribution": "They derive a concise, representer-theorem based form of JMMD that (1) unifies marginal/conditional/weighted distances as special cases via label kernels, (2) explains why JMMD can hurt feature discrimination through a graph-embedding view, and (3) repairs this by jointly optimizing JMMD with HSIC (JMMD-HSIC) to produce a tractable, discrimination-preserving adaptation loss.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 2,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 9617,
      "output_tokens": 957,
      "predecessor_details": [
        {
          "success": true,
          "title": "[0805.2368] A Kernel Method for the Two-Sample Problem - arXiv",
          "url": "https://arxiv.org/abs/0805.2368",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:0805.2368** (cs)\n\n\\[Submitted on 15 May 2008\\]\n\n# Title:A Kernel Method for the Two-Sample Problem\n\nAuthors: [Arthur Gretton](https://arxiv.org/search/cs?searchtype=author&query=Gretton,+A), [Karsten Borgwardt](https://arxiv.org/search/cs?searchtype=author&query=Borgwardt,+K), [Malte J. Rasch](https://arxiv.org/search/cs?searchtype=author&query=Rasch,+M+J), [Bernhard Scholkopf](https://arxiv.org/search/cs?searchtype=author&query=Scholkopf,+B), [Alexander J. Smola](https://arxiv.org/search/cs?searchtype=author&query=Smola,+A+J)\n\nView a PDF of the paper titled A Kernel Method for the Two-Sample Problem, by Arthur Gretton and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/0805.2368)\n\n> Abstract: We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI) |\n| ACM\u00a0classes: | G.3; I.2.6 |\n| Cite as: | [arXiv:0805.2368](https://arxiv.org/abs/0805.2368) \\[cs.LG\\] |\n| (or [arXiv:0805.2368v1](https://arxiv.org/abs/0805.2368v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.0805.2368](https://doi.org/10.48550/arXiv.0805.2368) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Alex Smola J \\[ [view email](https://arxiv.org/show-email/6cd3ab59/0805.2368)\\] **\\[v1\\]**\nThu, 15 May 2008 17:46:53 UTC (102 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled A Kernel Method for the Two-Sample Problem, by Arthur Gretton and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/0805.2368)\n- [TeX Source](https://arxiv.org/src/0805.2368)\n- [Other Formats](https://arxiv.org/format/0805.2368)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=0805.2368&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=0805.2368&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2008-05](https://arxiv.org/list/cs.LG/2008-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/0805.2368?context=cs) [cs.AI](https://arxiv.org/abs/0805.2368?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:0805.2368)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=0805.2368)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:0805.2368)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr0805.html#abs-0805-2368) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-0805-2368)\n\n[Arthur Gretton](https://dblp.uni-trier.de/search/author?author=Arthur%20Gretton) [Karsten M. Borgwardt](https://dblp.uni-trier.de/search/author?author=Karsten%20M.%20Borgwardt) [Malte J. Rasch](https://dblp.uni-trier.de/search/author?author=Malte%20J.%20Rasch) [Bernhard Sch\u00f6lkopf](https://dblp.uni-trier.de/search/author?author=Bernhard%20Sch%C3%B6lkopf) [Alexander J. Smola](https://dblp.uni-trier.de/search/author?author=Alexander%20J.%20Smola)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/0805.2368) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "A kernel method for the two-sample-problem",
          "cleaned_query": "A kernel method for the two-sample-problem"
        },
        {
          "success": true,
          "title": "Measuring Statistical Dependence with Hilbert-Schmidt Norms",
          "url": "https://link.springer.com/chapter/10.1007/11564089_7",
          "content": "# Measuring Statistical Dependence with Hilbert-Schmidt Norms\n\n- Conference paper\n\n- pp 63\u201377\n- [Cite this conference paper](https://link.springer.com/link.springer.com#citeas)\n\n[Algorithmic Learning Theory](https://link.springer.com/book/10.1007/11564089)(ALT 2005)\n\n## Abstract\n\nWe propose an independence criterion based on the eigenspectrum of covariance operators in reproducing kernel Hilbert spaces (RKHSs), consisting of an empirical estimate of the Hilbert-Schmidt norm of the cross-covariance operator (we term this a Hilbert-Schmidt Independence Criterion, or HSIC). This approach has several advantages, compared with previous kernel-based independence criteria. First, the empirical estimate is simpler than any other kernel dependence test, and requires no user-defined regularisation. Second, there is a clearly defined population quantity which the empirical estimate approaches in the large sample limit, with exponential convergence guaranteed between the two: this ensures that independence tests based on HSIC do not suffer from slow learning rates. Finally, we show in the context of independent component analysis (ICA) that the performance of HSIC is competitive with that of previously published kernel-based criteria, and of other recently published ICA methods.\n\nThis is a preview of subscription content, [log in via an institution](https://wayf.springernature.com?redirect_uri=https%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F11564089_7%3Ferror%3Dcookies_not_supported%26code%3Db2747270-a4e8-44fe-bec4-c25d2bdaf9f4)\nto check access.\n\n## Access this chapter\n\n[Log in via an institution](https://wayf.springernature.com?redirect_uri=https%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F11564089_7%3Ferror%3Dcookies_not_supported%26code%3Db2747270-a4e8-44fe-bec4-c25d2bdaf9f4)\n\n[Institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/ebooks)\n\n## Preview\n\nUnable to display preview.\u00a0[Download preview\\\nPDF.](https://page-one.springer.com/pdf/preview/10.1007/11564089_7)\n\nUnable to display preview.\u00a0[Download preview\\\nPDF.](https://page-one.springer.com/pdf/preview/10.1007/11564089_7)\n\n### Similar content being viewed by others\n\n### [Kernel learning and optimization with Hilbert\u2013Schmidt independence criterion](https://link.springer.com/10.1007/s13042-017-0675-7?fromPaywallRec=true)\n\nArticle11 April 2017\n\n### [Generalization of the HSIC and distance covariance using PDI kernels](https://link.springer.com/10.1007/s43037-023-00296-9?fromPaywallRec=true)\n\nArticle29 August 2023\n\n### [Nonuniform Sampling, Reproducing Kernels, and the Associated Hilbert Spaces](https://link.springer.com/10.1007/BF03549597?fromPaywallRec=true)\n\nArticle01 January 2016\n\n### Explore related subjects\n\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n\n- [Linear Algebra](https://link.springer.com/subjects/linear-algebra)\n- [Multivariate Analysis](https://link.springer.com/subjects/multivariate-analysis)\n- [Psychometrics](https://link.springer.com/subjects/psychometrics)\n- [Functional Analysis](https://link.springer.com/subjects/functional-analysis)\n- [Statistical Theory and Methods](https://link.springer.com/subjects/statistical-theory-and-methods)\n- [Statistics](https://link.springer.com/subjects/statistics)\n\n## References\n\n01. Achard, S., Pham, D.-T., Jutten, C.: Quadratic dependence measure for nonlinear blind source separation. In: 4th International Conference on ICA and BSS (2003)\n\n [Google Scholar](https://scholar.google.com/scholar?&q=Achard%2C%20S.%2C%20Pham%2C%20D.-T.%2C%20Jutten%2C%20C.%3A%20Quadratic%20dependence%20measure%20for%20nonlinear%20blind%20source%20separation.%20In%3A%204th%20International%20Conference%20on%20ICA%20and%20BSS%20%282003%29)\n\n02. Amari, S.-I., Cichoki, A., Yang, H.: A new learning algorithm for blind signal separation. Advances in Neural Information Processing Systems\u00a08, 757\u2013763 (1996)\n\n [Google Scholar](https://scholar.google.com/scholar_lookup?&title=A%20new%20learning%20algorithm%20for%20blind%20signal%20separation&journal=Advances%20in%20Neural%20Information%20Processing%20Systems&volume=8&pages=757-763&publication_year=1996&author=Amari%2CS.-I.&author=Cichoki%2CA.&author=Yang%2CH.)\n\n03. Bach, F., Jordan, M.: Kernel independent component analysis. Journal of Machine Learning Research\u00a03, 1\u201348 (2002)\n\n [Article](https://doi.org/10.1162%2F153244303768966085) [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=1966051) [Google Scholar](https://scholar.google.com/scholar_lookup?&title=Kernel%20independent%20component%20analysis&journal=Journal%20of%20Machine%20Learning%20Research&volume=3&pages=1-48&publication_year=2002&author=Bach%2CF.&author=Jordan%2CM.)\n\n04. Baker, C.R.: Joint measures and cross-covariance operators. Transactions of the American Mathematical Society\u00a0186, 273\u2013289 (1973)\n\n [Article](https://doi.org/10.1090%2FS0002-9947-1973-0336795-3) [Google Scholar](https://scholar.google.com/scholar_lookup?&title=Joint%20measures%20and%20cross-covariance%20operators&journal=Transactions%20of%20the%20American%20Mathematical%20Society&volume=186&pages=273-289&publication_year=1973&author=Baker%2CC.R.)\n\n05. Bell, A., Sejnowski, T.: An information-maximization approach to blind separation and blind deconvolution. Neural Computation\u00a07(6), 1129\u20131159 (1995)\n\n [Article](https://doi.org/10.1162%2Fneco.1995.7.6.1129) [Google Scholar](https://scholar.google.com/scholar_lookup?&title=An%20information-maximization%20approach%20to%20blind%20separation%20and%20blind%20deconvolution&journal=Neural%20Computation&volume=7&issue=6&pages=1129-1159&publication_year=1995&author=Bell%2CA.&author=Sejnowski%2CT.)\n\n06. Cardoso, J.-F.: Blind signal separation: statistical principles. Proceedings of the IEEE\u00a090(8), 2009\u20132026 (1998)\n\n [Article](https://doi.org/10.1109%2F5.720250) [Google Scholar](https://scholar.google.com/scholar_lookup?&title=Blind%20signal%20separation%3A%20statistical%20principles&journal=Proceedings%20of%20the%20IEEE&volume=90&issue=8&pages=2009-2026&publication_year=1998&author=Cardoso%2CJ.-F.)\n\n07. Chen, A., Bickel, P.: Consistent independent component analysis and prewhitening, Tech. report, Berkeley (2004)\n\n [Google Scholar](https://scholar.google.com/scholar?&q=Chen%2C%20A.%2C%20Bickel%2C%20P.%3A%20Consistent%20independent%20component%20analysis%20and%20prewhitening%2C%20Tech.%20report%2C%20Berkeley%20%282004%29)\n\n08. Devroye, L., Gy\u00f6rfi, L., Lugosi, G.: A probabilistic theory of pattern recognition. In: Applications of mathematics, vol.\u00a031. Springer, New York (1996)\n\n [Google Scholar](https://scholar.google.com/scholar?&q=Devroye%2C%20L.%2C%20Gy%C3%B6rfi%2C%20L.%2C%20Lugosi%2C%20G.%3A%20A%20probabilistic%20theory%20of%20pattern%20recognition.%20In%3A%20Applications%20of%20mathematics%2C%20vol.%C2%A031.%20Springer%2C%20New%20York%20%281996%29)\n\n09. Fukumizu, K., Bach, F.R., Jordan, M.I.: Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces. Journal of Machine Learning Research\u00a05, 73\u201399 (2004)\n\n [MathSciNet](http://www.ams.org/mathscinet-getitem?mr=2247974) [Google Scholar](https://scholar.google.com/scholar_lookup?&title=Dimensionality%20reduction%20for%20supervised%20learning%20with%20reproducing%20kernel%20hilbert%20spaces&journal=Journal%20of%20Machine%20Learning%20Research&volume=5&pages=73-99&publication_year=2004&author=Fukumizu%2CK.&author=Bach%2CF.R.&author=Jordan%2CM.I.)\n\n10. Gretton, A., Herbrich, R., Smola, A.: The kernel mutual information, Tech. report, Cambridge University Engineering Department and Max Planck Institute for Biological Cybernetics (2003)\n\n [Google Scholar](https://scholar.google.com/scholar?&q=Gretton%2C%20A.%2C%20Herbrich%2C%20R.%2C%20Smola%2C%20A.%3A%20The%20kernel%20mutual%20information%2C%20Tech.%20report%2C%20Cambridge%20University%20Engineering%20Department%20and%20Max%20Planck%20Institute%20for%20Biological%20Cybernetics%20%282003%29)\n\n11. Gretton, A., Smola, A., Bousquet, O., Herbrich, R., Be",
          "original_query": "Measuring statistical dependence with Hilbert\u2013Schmidt norms",
          "cleaned_query": "Measuring statistical dependence with Hilbert\u2013Schmidt norms"
        },
        {
          "success": true,
          "title": "Learning with Kernels: Support Vector Machines ...",
          "url": "https://dl.acm.org/doi/10.5555/559923",
          "content": "![logo](data:,)\n\n## This website uses cookies\n\nWe occasionally run membership recruitment campaigns on social media channels and use cookies to track post-clicks. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you\u2019ve provided to them or that they\u2019ve collected from your use of their services. Use the check boxes below to choose the types of cookies you consent to have stored on your device.\n\nDo not sell or share my personal information\n\n[Use necessary cookies only](https://dl.acm.org/doi/10.5555/559923) [Allow all cookies](https://dl.acm.org/doi/10.5555/559923) [Show details](https://dl.acm.org/doi/10.5555/559923)\n\n[OK](https://dl.acm.org/doi/10.5555/559923)\n\n[Use necessary cookies only](https://dl.acm.org/doi/10.5555/559923) [Allow selected cookies](https://dl.acm.org/doi/10.5555/559923) [Allow all cookies](https://dl.acm.org/doi/10.5555/559923)\n\nNecessary\n\nPreferences\n\nStatistics\n\nMarketing\n\n[Show details](https://dl.acm.org/doi/10.5555/559923)\n\n[Cookie declaration](https://dl.acm.org/doi/10.5555/559923) [\\[#IABV2SETTINGS#\\]](https://dl.acm.org/doi/10.5555/559923) [About](https://dl.acm.org/doi/10.5555/559923)\n\n[Necessary (9)](https://dl.acm.org/doi/10.5555/559923) [Preferences (5)](https://dl.acm.org/doi/10.5555/559923) [Statistics (14)](https://dl.acm.org/doi/10.5555/559923) [Marketing (25)](https://dl.acm.org/doi/10.5555/559923) [Unclassified (21)](https://dl.acm.org/doi/10.5555/559923)\n\nNecessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies. These cookies do not gather information about you that could be used for marketing purposes and do not remember where you have been on the internet.\n\n| Name | Provider | Purpose | Maximum Storage Duration | Type |\n| --- | --- | --- | --- | --- |\n| \\_\\_cf\\_bm\u00a0\\[x2\\] | [ACM](https://www.acm.org/privacy-policy) | This cookie is used to distinguish between humans and bots. This is beneficial for the website, in order to make valid reports on the use of their website. | 1 day | HTTP Cookie |\n| \\_\\_jid | c.disquscdn.com | Used to add comments to the website and remember the user's Disqus login credentials across websites that use said service. | Session | HTTP Cookie |\n| disqusauth | c.disquscdn.com | Registers whether the user is logged in. This allows the website owner to make parts of the website inaccessible, based on the user's log-in status. | Session | HTTP Cookie |\n| \\_cfuvid | [ACM](https://www.acm.org/privacy-policy) | This cookie is a part of the services provided by Cloudflare - Including load-balancing, deliverance of website content and serving DNS connection for website operators. | Session | HTTP Cookie |\n| CookieConsent | [Cookiebot](https://www.cookiebot.com/goto/privacy-policy/) | Stores the user's cookie consent state for the current domain | 1 year | HTTP Cookie |\n| JSESSIONID | [ACM](https://www.acm.org/privacy-policy) | Preserves users states across page requests. | Session | HTTP Cookie |\n| \\_gh\\_sess | [Github](https://help.github.com/en/articles/github-privacy-statement) | Preserves users states across page requests. | Session | HTTP Cookie |\n| logged\\_in | [Github](https://help.github.com/en/articles/github-privacy-statement) | Registers whether the user is logged in. This allows the website owner to make parts of the website inaccessible, based on the user's log-in status. | 1 year | HTTP Cookie |\n\nPreference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\n\n| Name | Provider | Purpose | Maximum Storage Duration | Type |\n| --- | --- | --- | --- | --- |\n| aet-dismiss | c.disquscdn.com | Necessary for the functionality of the website's comment-system. | Persistent | HTML Local Storage |\n| drafts.queue | c.disquscdn.com | Necessary for the functionality of the website's comment-system. | Persistent | HTML Local Storage |\n| submitted\\_posts\\_cache | c.disquscdn.com | Necessary for the functionality of the website's comment-system. | Persistent | HTML Local Storage |\n| mopDeploy | [Mopinion](https://mopinion.com/privacy/) | Pending | Session | HTML Local Storage |\n| MACHINE\\_LAST\\_SEEN | [ACM](https://www.acm.org/privacy-policy) | Pending | 300 days | HTTP Cookie |\n\nStatistic cookies help website owners understand how visitors interact with websites by collecting and reporting information anonymously.\n\n| Name | Provider | Purpose | Maximum Storage Duration | Type |\n| --- | --- | --- | --- | --- |\n| \\_ga | [Google](https://business.safety.google/privacy/) | Registers a unique ID that is used to generate statistical data on how the visitor uses the website. | 2 years | HTTP Cookie |\n| \\_ga\\_# | [Google](https://business.safety.google/privacy/) | Used by Google Analytics to collect data on the number of times a user has visited the website as well as dates for the first and most recent visit. | 2 years | HTTP Cookie |\n| \\_gat | [Google](https://business.safety.google/privacy/) | Used by Google Analytics to throttle request rate | 1 day | HTTP Cookie |\n| \\_gid | [Google](https://business.safety.google/privacy/) | Registers a unique ID that is used to generate statistical data on how the visitor uses the website. | 1 day | HTTP Cookie |\n| \\_hjSession\\_# | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Collects statistics on the visitor's visits to the website, such as the number of visits, average time spent on the website and what pages have been read. | 1 day | HTTP Cookie |\n| \\_hjSessionUser\\_# | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Collects statistics on the visitor's visits to the website, such as the number of visits, average time spent on the website and what pages have been read. | 1 year | HTTP Cookie |\n| \\_hjTLDTest | [Hotjar](https://www.hotjar.com/legal/policies/privacy/) | Registers statistical data on users' behaviour on the website. Used for internal analytics by the website operator. | Session | HTTP Cookie |\n| \\_hp2\\_# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 1 day | HTTP Cookie |\n| \\_hp2\\_hld#.# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 1 day | HTTP Cookie |\n| \\_hp2\\_id.# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 13 months | HTTP Cookie |\n| \\_hp2\\_ses\\_props.# | [Heap Analytics](https://heap.io/privacy) | Collects data on the user\u2019s navigation and behavior on the website. This is used to compile statistical reports and heatmaps for the website owner. | 1 day | HTTP Cookie |\n| disqus\\_unique | c.disquscdn.com | Collects statistics related to the user's visits to the website, such as number of visits, average time spent on the website and loaded pages. | Session | HTTP Cookie |\n| \\_octo | [Github](https://help.github.com/en/articles/github-privacy-statement) | Pending | 1 year | HTTP Cookie |\n| collect | [Google](https://business.safety.google/privacy/) | Used to send data to Google Analytics about the visitor's device and behavior. Tracks the visitor across devices and marketing channels. | Session | Pixel Tracker |\n\nMarketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers.\n\n| Name | Provider | Purpose | Maximum Storage Duration | Type |\n| --- | --- | --- | --- | --- |\n| badge",
          "original_query": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond",
          "cleaned_query": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond"
        },
        {
          "success": true,
          "title": "Deep Transfer Learning with Joint Adaptation Networks - arXiv",
          "url": "https://arxiv.org/abs/1605.06636",
          "content": "[1605.06636] Deep Transfer Learning with Joint Adaptation Networks\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1605.06636\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1605.06636**(cs)\n[Submitted on 21 May 2016 ([v1](https://arxiv.org/abs/1605.06636v1)), last revised 17 Aug 2017 (this version, v2)]\n# Title:Deep Transfer Learning with Joint Adaptation Networks\nAuthors:[Mingsheng Long](https://arxiv.org/search/cs?searchtype=author&amp;query=Long,+M),[Han Zhu](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+H),[Jianmin Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+J),[Michael I. Jordan](https://arxiv.org/search/cs?searchtype=author&amp;query=Jordan,+M+I)\nView a PDF of the paper titled Deep Transfer Learning with Joint Adaptation Networks, by Mingsheng Long and 3 other authors\n[View PDF](https://arxiv.org/pdf/1605.06636)> > Abstract:\n> Deep networks have been successfully applied to learn transferable features for adapting models from a source domain to a different target domain. In this paper, we present joint adaptation networks (JAN), which learn a transfer network by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion. Adversarial training strategy is adopted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be performed by stochastic gradient descent with the gradients computed by back-propagation in linear-time. Experiments testify that our model yields state of the art results on standard datasets. Comments:|34th International Conference on Machine Learning|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:1605.06636](https://arxiv.org/abs/1605.06636)[cs.LG]|\n|(or[arXiv:1605.06636v2](https://arxiv.org/abs/1605.06636v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1605.06636](https://doi.org/10.48550/arXiv.1605.06636)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Mingsheng Long [[view email](https://arxiv.org/show-email/691da6d9/1605.06636)]\n**[[v1]](https://arxiv.org/abs/1605.06636v1)**Sat, 21 May 2016 12:56:14 UTC (112 KB)\n**[v2]**Thu, 17 Aug 2017 07:35:59 UTC (428 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Deep Transfer Learning with Joint Adaptation Networks, by Mingsheng Long and 3 other authors\n* [View PDF](https://arxiv.org/pdf/1605.06636)\n* [TeX Source](https://arxiv.org/src/1605.06636)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1605.06636&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1605.06636&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2016-05](https://arxiv.org/list/cs.LG/2016-05)\nChange to browse by:\n[cs](https://arxiv.org/abs/1605.06636?context=cs)\n[stat](https://arxiv.org/abs/1605.06636?context=stat)\n[stat.ML](https://arxiv.org/abs/1605.06636?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1605.06636)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1605.06636)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1605.06636)\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1605.html#Long0J16a)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/Long0J16a)\n[Mingsheng Long]()\n[Jianmin Wang]()\n[Michael I. Jordan]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1605.06636)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Deep transfer learning with joint adaptation networks",
          "cleaned_query": "Deep transfer learning with joint adaptation networks"
        },
        {
          "success": true,
          "title": "[PDF] Transfer Feature Learning with Joint Distribution Adaptation",
          "url": "https://www.semanticscholar.org/paper/Transfer-Feature-Learning-with-Joint-Distribution-Long-Wang/9c495a9b835f7803746fce1f711dad0eeb411112",
          "content": "[PDF] Transfer Feature Learning with Joint Distribution Adaptation | Semantic Scholar\n[Skip to search form](#search-form)[Skip to main content](#main-content)[Skip to account menu](#account-menu)\n[Semantic ScholarSemantic Scholar's Logo](https://www.semanticscholar.org/)\nSearch 231,121,630 papers from all fields of science\nSearch\n* DOI:[10.1109/ICCV.2013.274](https://doi.org/10.1109/ICCV.2013.274)\n* Corpus ID: 13798326# Transfer Feature Learning with Joint Distribution Adaptation\n```\n@article{Long2013TransferFL,\ntitle={Transfer Feature Learning with Joint Distribution Adaptation},\nauthor={Mingsheng Long and Jianmin Wang and Guiguang Ding and Jiaguang Sun and Philip S. Yu},\njournal={2013 IEEE International Conference on Computer Vision},\nyear={2013},\npages={2200-2207},\nurl={https://api.semanticscholar.org/CorpusID:13798326}\n}\n```\n* [Mingsheng Long](https://www.semanticscholar.org/author/Mingsheng-Long/35776445),[Jianmin Wang](https://www.semanticscholar.org/author/Jianmin-Wang/2144499343),+2 authors[Philip S. Yu](https://www.semanticscholar.org/author/Philip-S.-Yu/144019071)\n* Publishedin[IEEE International Conference\u2026](https://www.semanticscholar.org/venue?name=IEEE%20International%20Conference%20on%20Computer%20Vision)1 December 2013\n* Computer Science\nTLDR\nJDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robust for substantial distribution difference.Expand\n[View on IEEE](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6751384)\n[cv-foundation.org](http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Long_Transfer_Feature_Learning_2013_ICCV_paper.pdf)\nSave to LibrarySave\nCreate AlertAlert\nCite\nShare\n1,766 Citations\n[\nHighly Influential Citations\n](#citing-papers)[](https://www.semanticscholar.org/faq#influential-citations)\n321\n[\nBackground Citations\n](#citing-papers)\n596\n[\nMethods Citations\n](#citing-papers)\n748\n[\nResults Citations\n](#citing-papers)\n21\n[View All](#citing-papers)\n## Figures and Tables from this paper\n* [\n![figure 1](https://figures.semanticscholar.org/9c495a9b835f7803746fce1f711dad0eeb411112/1-Figure1-1.png)\nfigure 1](https://www.semanticscholar.org/paper/Transfer-Feature-Learning-with-Joint-Distribution-Long-Wang/9c495a9b835f7803746fce1f711dad0eeb411112/figure/0)\n* [\n![table 1](https://figures.semanticscholar.org/9c495a9b835f7803746fce1f711dad0eeb411112/2-Table1-1.png)\ntable 1](https://www.semanticscholar.org/paper/Transfer-Feature-Learning-with-Joint-Distribution-Long-Wang/9c495a9b835f7803746fce1f711dad0eeb411112/figure/1)\n* [\n![figure 2](https://figures.semanticscholar.org/9c495a9b835f7803746fce1f711dad0eeb411112/5-Figure2-1.png)\nfigure 2](https://www.semanticscholar.org/paper/Transfer-Feature-Learning-with-Joint-Distribution-Long-Wang/9c495a9b835f7803746fce1f711dad0eeb411112/figure/2)\n* [\n![table 2](https://figures.semanticscholar.org/9c495a9b835f7803746fce1f711dad0eeb411112/5-Table2-1.png)\ntable 2](https://www.semanticscholar.org/paper/Transfer-Feature-Learning-with-Joint-Distribution-Long-Wang/9c495a9b835f7803746fce1f711dad0eeb411112/figure/3)\n* [\n![figure 3](https://figures.semanticscholar.org/9c495a9b835f7803746fce1f711dad0eeb411112/6-Figure3-1.png)\nfigure 3](https://www.semanticscholar.org/paper/Transfer-Feature-Learning-with-Joint-Distribution-Long-Wang/9c495a9b835f7803746fce1f711dad0eeb411112/figure/4)\n* [\n![table 3](https://figures.semanticscholar.org/9c495a9b835f7803746fce1f711dad0eeb411112/6-Table3-1.png)\ntable 3](https://www.semanticscholar.org/paper/Transfer-Feature-Learning-with-Joint-Distribution-Long-Wang/9c495a9b835f7803746fce1f711dad0eeb411112/figure/5)\n* [\n![figure 4](https://figures.semanticscholar.org/9c495a9b835f7803746fce1f711dad0eeb411112/7-Figure4-1.png)\nfigure 4](https://www.semanticscholar.org/paper/Transfer-Feature-Learning-with-Joint-Distribution-Long-Wang/9c495a9b835f7803746fce1f711dad0eeb411112/figure/6)\n* [\n![figure 5](https://figures.semanticscholar.org/9c495a9b835f7803746fce1f711dad0eeb411112/8-Figure5-1.png)\nfigure 5](https://www.semanticscholar.org/paper/Transfer-Feature-Learning-with-Joint-Distribution-Long-Wang/9c495a9b835f7803746fce1f711dad0eeb411112/figure/7)\n## Topics\nAI-Generated\n[Joint Distribution Adaptation(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/5355022174?corpusId=13798326)[Dimensionality Reduction Procedure(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/40084263692?corpusId=13798326)[Transfer Learning(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/32206922016?corpusId=13798326)[Conditional Distribution(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/19852288951?corpusId=13798326)[Marginal Distribution(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/9288738157?corpusId=13798326)[Feature Representation(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/49485199629?corpusId=13798326)[Computer Vision(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/44382671367?corpusId=13798326)[Source Domain(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/30892276199?corpusId=13798326)[Classifier(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/2379596606?corpusId=13798326)\n## 1,766 Citations\nCitation Type\nHas PDF\nAuthor\nMore Filters\nMore Filters\nFilters\nSort by RelevanceSort by Most Influenced PapersSort by Citation CountSort by Recency\n[### Transfer Learning with Joint Distribution Adaptation and Maximum Margin Criterion\n](https://www.semanticscholar.org/paper/Transfer-Learning-with-Joint-Distribution-and-Wang-Lu/80ddf4539dd8990c5498485f3eeacc0b9b640414)[Pengju Wang](https://www.semanticscholar.org/author/Pengju-Wang/2296393852)[Luxi Lu](https://www.semanticscholar.org/author/Luxi-Lu/1731348)[Jian Li](https://www.semanticscholar.org/author/Jian-Li/2151965742)[Wenyao Gan](https://www.semanticscholar.org/author/Wenyao-Gan/1490933203)\nComputer Science, Mathematics\nJournal of Physics: Conference Series\n* 2019\nTLDR\nA modified transfer learning algorithm with Joint Distribution Adaptation and Maximum Margin Criterion is put forward in this paper, which is aimed at minimizing the distribution discrepancy between two domains and maximizing the separability of each class at the same time.Expand\n* [\n4\n](https://www.semanticscholar.org/paper/80ddf4539dd8990c5498485f3eeacc0b9b640414#citing-papers)\n* [\nPDF\n](https://www.semanticscholar.org/paper/80ddf4539dd8990c5498485f3eeacc0b9b640414)\nSave\n[### Visual domain adaptation via transfer feature learning\n](https://www.semanticscholar.org/paper/Visual-domain-adaptation-via-transfer-feature-Tahmoresnezhad-Hashemi/7b690e253ed72feb191ce76afeacc0a6d1dc36d3)[J. Tahmoresnezhad](https://www.semanticscholar.org/author/J.-Tahmoresnezhad/3370217)[S. Hashemi](https://www.semanticscholar.org/author/S.-Hashemi/143957007)\nComputer Science\n[Knowledge and Information Systems](https://www.semanticscholar.org/venue?name=Knowledge%20and%20Information%20Systems)\n* 2016\nTLDR\nA novel transfer learning and domain adaptation approach, referred to as visual domain adaptation (VDA), which reduces the joint marginal and conditional distributions across domains in an unsupervised manner where no label is available in test set.Expand\n* [\n138\n](https://www.semanticscholar.org/paper/7b690e253ed72feb191ce76afeacc0a6d1dc36d3#citing-papers)\n* [Highly Influenced](https://www.semanticscholar.org/paper/7b690e253ed72feb191ce76afeacc0a6d1dc36d3?sort=is-influential#citing-papers)\n* 9 Excerpts\nSave\n[### Iterative joint classifier and domain adaptation for visual transfer learning\n](https://www.semanticscholar.org/paper/Iterative-joint-classifier-and-domain-adaptation-Saray-Tahmoresnezhad/c83add11068df1cbb2eba49b4b00138e94bc637c)[Shiva Noori Saray](https://www.semanticscholar.org/author/Shiva-Noori-Saray/1768835798)[J.",
          "original_query": "Transfer feature learning with joint distribution adaptation",
          "cleaned_query": "Transfer feature learning with joint distribution adaptation"
        }
      ],
      "generated_ideas": [
        "Linear-Time Streaming MMD/HSIC with Change-Point Guarantees for Online Transfer\nDevelop an online, linear-time estimator for MMD and HSIC using random features or block-based U-statistics, enabling continual monitoring of drift and dependence in streaming target data. Couple this with a change-point detector that triggers re-adaptation in JAN/JDA-style models, and provide finite-sample false-alarm control via sequential testing corrections.",
        "Localized MMD: Subpopulation-Specific Domain Alignment\nCreate a \u201clocalized JMMD\u201d that aligns distributions within automatically discovered clusters (e.g., via mixture models in feature space) instead of globally aligning all data. This directly addresses cases where only some modes shift; the method is actionable by adding cluster assignments and computing per-cluster MMD terms, then weighting them by cluster mass to stabilize training."
      ],
      "generated_ideas_raw": "1. **Calibration-Aware MMD for Conditional Shift Detection**\n   Build an MMD-based test that targets *conditional* distribution shift by weighting discrepancies across predicted-class strata using calibrated confidence scores from a classifier. The method would extend two-sample MMD (Paper 1) to better separate marginal shift from label/conditional shift (Paper 5), with a permutation or wild-bootstrap procedure for valid p-values under estimated labels.\n\n2. **HSIC-Regularized Joint Adaptation Networks for Spurious Correlation Removal**\n   Augment JAN (Paper 4) with an HSIC penalty (Paper 2) that explicitly minimizes dependence between learned features and known nuisance variables (e.g., background, site ID, patient scanner) while still minimizing JMMD between domains. This yields a concrete training objective and evaluation protocol showing improved robustness under domain shift where spurious correlations differ across domains.\n\n3. **Graph-Domain Adaptation via Joint MMD on Multi-Resolution Graph Kernels**\n   Design a domain adaptation pipeline for graph-structured data by combining graph kernels (Paper 1\u2019s graph distribution comparisons) with JMMD across multiple GNN layers (Paper 4). Concretely, compute MMD/JMMD using kernels over node/edge embeddings at several receptive-field radii, aligning joint distributions across layers to transfer between graph domains (e.g., molecules from different assay sources).\n\n4. **Linear-Time Streaming MMD/HSIC with Change-Point Guarantees for Online Transfer**\n   Develop an online, linear-time estimator for MMD and HSIC using random features or block-based U-statistics, enabling continual monitoring of drift and dependence in streaming target data. Couple this with a change-point detector that triggers re-adaptation in JAN/JDA-style models, and provide finite-sample false-alarm control via sequential testing corrections.\n\n5. **Kernel Selection for Maximum Test Power in JMMD-Based Domain Adaptation**\n   Propose an end-to-end kernel learning strategy that chooses (or mixes) kernels to maximize two-sample test power (Paper 1) for the specific joint distributions used in JMMD (Paper 4). Implement as a bilevel optimization: inner loop trains the network, outer loop updates kernel parameters to maximize an estimate of test power or minimize Type-II error under a held-out shifted split.\n\n6. **Localized MMD: Subpopulation-Specific Domain Alignment**\n   Create a \u201clocalized JMMD\u201d that aligns distributions within automatically discovered clusters (e.g., via mixture models in feature space) instead of globally aligning all data. This directly addresses cases where only some modes shift; the method is actionable by adding cluster assignments and computing per-cluster MMD terms, then weighting them by cluster mass to stabilize training.\n\n7. **Identifiable Feature Disentanglement Using HSIC + JMMD in Deep Transfer**\n   Build a representation that splits into (a) domain-invariant, label-predictive components and (b) domain-specific components by combining HSIC independence constraints (Paper 2) with JMMD alignment (Paper 4). Train with (i) HSIC to enforce independence between invariant and specific subspaces and (ii) JMMD to align only the invariant subspace, then validate via controlled shifts where domain-specific factors are manipulated.\n\n8. **Confidence-Weighted Hungarian Matching for Cross-Database Entity Alignment with MMD**\n   Extend the Paper 1 database attribute matching setup by integrating a learned feature extractor and using MMD as a matching cost that is *confidence-weighted* by attribute reliability. Concretely, learn attribute embeddings with a JAN/JDA-style objective, compute pairwise distributional discrepancies for candidate matches, and solve global assignment via Hungarian matching with uncertainty-aware costs.\n\n9. **Multi-Source Joint Distribution Adaptation with HSIC-Based Source Weighting**\n   For multi-source domain adaptation, use HSIC (Paper 2) to estimate dependence between each source domain\u2019s features and target pseudo-labels, then weight sources accordingly during JDA/JMMD alignment (Papers 5 and 4). This yields an implementable algorithm that downweights sources that are weakly related to the target task, improving robustness when some sources are misleading.\n\n10. **Test-Driven Adaptation: Stop-and-Select Criteria Using Two-Sample p-Values**\n   Introduce a \u201ctest-driven\u201d training loop where adaptation proceeds until a statistically principled stopping criterion is met: the MMD/JMMD two-sample test (Papers 1 and 4) fails to reject alignment for selected layers while validation accuracy does not degrade. This contributes a reproducible model-selection mechanism for unsupervised domain adaptation that reduces over-alignment and negative transfer.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Linear-Time Streaming MMD/HSIC with Change-Point Guarantees for Online Transfer\nDevelop an online, linear-time estimator for MMD and HSIC using random features or block-based U-statistics, enabling co",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Localized MMD: Subpopulation-Specific Domain Alignment\nCreate a \u201clocalized JMMD\u201d that aligns distributions within automatically discovered clusters (e.g., via mixture models in feature space) instead ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 37,
      "paper_title": "Pan-LUT: Efficient Pan-sharpening via Learnable Look-Up Tables",
      "contribution": "Introduce a lightweight, learnable look-up-table (LUT) framework (PGLUT, SDLUT, AOLUT) that replaces heavy CNN components to perform high-quality, extremely fast pan-sharpening capable of processing very large remote-sensing images on commodity GPUs/CPUs.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 7897,
      "output_tokens": 981,
      "predecessor_details": [
        {
          "success": true,
          "title": "Adaptive Rectangular Convolution for Remote Sensing Pansharpening",
          "url": "https://arxiv.org/abs/2503.00467",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "Pansharpening by convolutional neural networks",
          "cleaned_query": "Pansharpening by convolutional neural networks"
        },
        {
          "success": true,
          "title": "PanNet: A Deep Network Architecture for Pan-Sharpening",
          "url": "https://www.researchgate.net/publication/322058155_PanNet_A_Deep_Network_Architecture_for_Pan-Sharpening",
          "content": "PanNet: A Deep Network Architecture for Pan-Sharpening | Request PDF\nConference Paper\n# PanNet: A Deep Network Architecture for Pan-Sharpening\n* October 2017\nDOI:[10.1109/ICCV.2017.193](https://doi.org/10.1109/ICCV.2017.193)\n* Conference: 2017 IEEE International Conference on Computer Vision (ICCV)\nAuthors:\n[](scientific-contributions/Junfeng-Yang-2136887012)\n[Junfeng Yang](scientific-contributions/Junfeng-Yang-2136887012)\n[Junfeng Yang](scientific-contributions/Junfeng-Yang-2136887012)\n* This person is not on ResearchGate, or hasn't claimed this research yet.\n[](profile/Xueyang-Fu)\n[Xueyang Fu](profile/Xueyang-Fu)\n* [University of Science and Technology of China](https://www.researchgate.net/institution/University-of-Science-and-Technology-of-China)\n[](scientific-contributions/Yuwen-Hu-2114816326)\n[Yuwen Hu](scientific-contributions/Yuwen-Hu-2114816326)\n[Yuwen Hu](scientific-contributions/Yuwen-Hu-2114816326)\n* This person is not on ResearchGate, or hasn't claimed this research yet.\n[](profile/Yue-Huang-35)\n[Yue Huang](profile/Yue-Huang-35)\n* [Beihang University](https://www.researchgate.net/institution/Beihang-University)\nShow all 6 authorsHide\nRequest full-text PDF\nTo read the full-text of this research, you can request a copy directly from the authors.\nRequest full-text\n[Download citation](https://www.researchgate.net/publication/322058155_PanNet_A_Deep_Network_Architecture_for_Pan-Sharpening/citation/download)\nCopy linkLink copied\n[\nRequest full-text\n](https://www.researchgate.net/lite.research.ResearchResourcesSummary.requestFulltext.html?publicationUid=322058155&amp;ev=su_requestFulltext)[\nDownload citation\n](https://www.researchgate.net/publication/322058155_PanNet_A_Deep_Network_Architecture_for_Pan-Sharpening/citation/download)\nCopy linkLink copied\nTo read the full-text of this research, you can request a copy directly from the authors.\n**Discover the world's research**\n* 25+ million members\n* 160+ million publication pages\n* 2.3+ billion citations[Join for free](signup.SignUp.html)\n## No full-text available\nTo read the full-text of this research,\nyou can request a copy directly from the authors.\nRequest full-text PDF\n... Nevertheless, the former three types of methods usually struggle to balance spectral and spatial fidelity or rely on hand-crafted priors. In contrast, DL-based methods not only avoid these limitations but also dominate this topic recently for their superior ability to learn complex mappings(Yang et al. 2017). However, as shown in Figure 1(a), most DL-based methods rely on ground truth (GT) for supervision of the models, such as the classical PNN (Giuseppe et al. 2016), Pan-Net (Yang et al. 2017), and SDRCNN (Fang, Cai, and Fan 2023). ...\n... In contrast, DL-based methods not only avoid these limitations but also dominate this topic recently for their superior ability to learn complex mappings (Yang et al. 2017). However, as shown in Figure 1(a), most DL-based methods rely on ground truth (GT) for supervision of the models, such as the classical PNN (Giuseppe et al. 2016), Pan-Net(Yang et al. 2017), and SDRCNN (Fang, Cai, and Fan 2023). Despite the continuous proposal of various methods, most of them focus on architecture designing (Chen et al. 2025) or novel regularization terms (Zeng et al. 2025a) instead of eliminating the dependency on GT. ...\n... Optimized by pixel-level losses (e.g., \u21131 or \u21132 ) between the fused result and the ground truth, supervised methods are trained at reduced resolution using paired triplets of LRMS, PAN, and HRMS images. Representative works include PNN (Giuseppe et al. 2016), PanNet(Yang et al. 2017), and SDRCNN (Fang, Cai, and Fan 2023) as well as more recent architectures like ADKNet (Peng et al. 2022), PSCINN , SSFMamba (Ma et al. 2025), and Fu-sionMamba (Peng et al. 2024). Despite strong performance on synthetic datasets, these methods suffer from poor generalization to real full-resolution images due to the domain gap between the simulated reduced-resolution and real fullresolution data. ...\n[CLIPPan: Adapting CLIP as A Supervisor for Unsupervised Pansharpening](publication/397663330_CLIPPan_Adapting_CLIP_as_A_Supervisor_for_Unsupervised_Pansharpening)\nPreprint\n* Nov 2025\n* [Lihua Jian](https://www.researchgate.net/profile/Lihua-Jian)\n* [Jiabo Liu](https://www.researchgate.net/scientific-contributions/Jiabo-Liu-2309391744)\n* [Shaowu Wu](https://www.researchgate.net/profile/Shaowu-Wu)\n* [Lihui Chen](https://www.researchgate.net/profile/Lihui-Chen)\nDespite remarkable advancements in supervised pansharpening neural networks, these methods face domain adaptation challenges of resolution due to the intrinsic disparity between simulated reduced-resolution training data and real-world full-resolution scenarios.To bridge this gap, we propose an unsupervised pansharpening framework, CLIPPan, that enables model training at full resolution directly by taking CLIP, a visual-language model, as a supervisor. However, directly applying CLIP to supervise pansharpening remains challenging due to its inherent bias toward natural images and limited understanding of pansharpening tasks. Therefore, we first introduce a lightweight fine-tuning pipeline that adapts CLIP to recognize low-resolution multispectral, panchromatic, and high-resolution multispectral images, as well as to understand the pansharpening process. Then, building on the adapted CLIP, we formulate a novel \\\\textit{loss integrating semantic language constraints}, which aligns image-level fusion transitions with protocol-aligned textual prompts (e.g., Wald's or Khan's descriptions), thus enabling CLIPPan to use language as a powerful supervisory signal and guide fusion learning without ground truth. Extensive experiments demonstrate that CLIPPan consistently improves spectral and spatial fidelity across various pansharpening backbones on real-world datasets, setting a new state of the art for unsupervised full-resolution pansharpening.\n[View](publication/397663330_CLIPPan_Adapting_CLIP_as_A_Supervisor_for_Unsupervised_Pansharpening)\nShow abstract\n... SAM \u2193ERGAS \u2193Q4 \u2191EXP [51] 8.435 \u00b11.925 11.819 \u00b11.905 0.584 \u00b10.075 TV [52] 7.565 \u00b11.535 7.781 \u00b10.699 0.820 \u00b10.090 MTF-GLP-FS [10] 7.093 \u00b11.816 7.374 \u00b10.724 0.835 \u00b10.080 BSD-PC [9] 8.898 \u00b11.980 7.515 \u00b10.800 0.831 \u00b10.098 CVPR2019 [12] 7.988 \u00b11.820 6.959 \u00b11.268 0.737 \u00b10.087 LRTCFPan [53] 7.197 \u00b11.711 9.328 \u00b10.812 0.855 \u00b10.087 PNN [54] 5.205 \u00b10.963 4.472 \u00b10.373 0.918 \u00b10.094 PanNet[55]5.791 \u00b11.184 5.863 \u00b10.888 0.885 \u00b10.092 DiCNN [56] 5.380 \u00b11.027 5.135 \u00b10.481 0.904 \u00b10.090 FusionNet [57] 4.923 \u00b10.908 4.159 \u00b10.328 0.925 \u00b10.094 DCFNet [58] 4.512 \u00b10.773 3.809 \u00b10.336 0.934 \u00b10.087 LAGConv [59] 4.547 \u00b10.830 3.826 \u00b10.420 0.934 \u00b10.088 HMPNet [60] 4.617 \u00b10.404 3.404 \u00b10.478 0.936 \u00b10.102 CMT [61] 4.535 \u00b10.822 3.744 \u00b10.321 0.935 \u00b10.086 CANNet [62] 4.507 \u00b10.835 3.652 \u00b10.327 0.937 \u00b10.083 ARConv [63] 4.430 \u00b10.811 3.633 \u00b10.327 0.939 \u00b10.081 Proposed 4.411 \u00b10.511 3.395 \u00b10.127 0.936 \u00b10.041 ...\n... From the error maps in Figure 5, it can be observed that there are still significant performance differences between traditional fusion methods and deep learning-based fusion methods. [52] 1.918 \u00b10.398 1.745 \u00b10.405 0.905 \u00b10.027 MTF-GLP-FS [10] 1.655 \u00b10.385 1.589 \u00b10.395 0.897 \u00b10.035 BSD-PC [9] 1.681 \u00b10.360 1.667 \u00b10.445 0.892 \u00b10.035 CVPR2019 [12] 1.598 \u00b10.353 1.877 \u00b10.448 0.886 \u00b10.028 LRTCFPan [53] 1.315 \u00b10.283 1.301 \u00b10.313 0.932 \u00b10.033 PNN [54] 1.048 \u00b10.226 1.057 \u00b10.236 0.960 \u00b10.010 PanNet[55]0.997 \u00b10.212 0.919 \u00b10.191 0.967 \u00b10.010 DiCNN [56] 1.053 \u00b10.231 1.081 \u00b10.254 0.959 \u00b10.010 FusionNet [57] 0.974 \u00b10.212 0.988 \u00b10.222 0.964 \u00b10.009 DCFNet [58] 0.872 \u00b10.169 0.784 \u00b10.146 0.974 \u00b10.009 LAGConv [59] 0.786 \u00b10.148 0.687 \u00b10.113 0.981 \u00b10.008 HMPNet [60] 0.803 \u00b10.141 0.564 \u00b10.099 0.981 \u00b10.020 CMT [61] 0.753 \u00b10.138 0.648 \u00b10.109 0.982 \u00b10.007 CANNet [62] 0.707 \u00b10.148 0.630 \u00b10.128 0.983 \u00b10.006 ARConv [63] 0 To assess the generalization capability of the FFIMamba model, we carried out additional experimental validation on the publicly available",
          "original_query": "Pannet: A deep network architecture for pan-sharpening",
          "cleaned_query": "Pannet: A deep network architecture for pan-sharpening"
        },
        {
          "success": true,
          "title": "Learning Image-Adaptive 3D Lookup Tables for High Performance Photo Enhancement in Real-Time",
          "url": "https://ieeexplore.ieee.org/document/9206076/",
          "content": "\n \n \n \n \n \n IEEE Account \n \n Change Username/Password \n Update Address \n \n \n \n Purchase Details \n \n Payment Options \n Order History \n View Purchased Documents \n \n \n \n Profile Information \n \n Communications Preferences \n Profession and Education \n Technical Interests \n \n \n \n Need Help? \n \n US &amp; Canada: +1 800 678 4333 \n Worldwide: +1 732 981 0060 \n \n Contact &amp; Support \n \n \n \n \n \n About IEEE Xplore \n Contact Us \n Help \n Accessibility \n Terms of Use \n Nondiscrimination Policy \n Sitemap \n Privacy &amp; Opting Out of Cookies \n \n \nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity. \u00a9 Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.\n \n \n \n",
          "original_query": "Learning image-adaptive 3d lookup tables for high performance photo enhancement in real-time",
          "cleaned_query": "Learning image-adaptive 3d lookup tables for high performance photo enhancement in real-time"
        },
        {
          "success": true,
          "title": "Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup ...",
          "url": "https://arxiv.org/abs/2108.08697",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2108.08697** (cs)\n\n\\[Submitted on 19 Aug 2021\\]\n\n# Title:Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables\n\nAuthors: [Tao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+T), [Yong Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+Y), [Jingyang Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng,+J), [Yipeng Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+Y), [Xian Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+X), [Fenglong Song](https://arxiv.org/search/cs?searchtype=author&query=Song,+F), [Youliang Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan,+Y)\n\nView a PDF of the paper titled Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables, by Tao Wang and 6 other authors\n\n[View PDF](https://arxiv.org/pdf/2108.08697)\n\n> Abstract:Recently, deep learning-based image enhancement algorithms achieved state-of-the-art (SOTA) performance on several publicly available datasets. However, most existing methods fail to meet practical requirements either for visual perception or for computation efficiency, especially for high-resolution images. In this paper, we propose a novel real-time image enhancer via learnable spatial-aware 3-dimentional lookup tables(3D LUTs), which well considers global scenario and local spatial information. Specifically, we introduce a light weight two-head weight predictor that has two outputs. One is a 1D weight vector used for image-level scenario adaptation, the other is a 3D weight map aimed for pixel-wise category fusion. We learn the spatial-aware 3D LUTs and fuse them according to the aforementioned weights in an end-to-end manner. The fused LUT is then used to transform the source image into the target tone in an efficient way. Extensive results show that our model outperforms SOTA image enhancement methods on public datasets both subjectively and objectively, and that our model only takes about 4ms to process a 4K resolution image on one NVIDIA V100 GPU.\n\n| | |\n| --- | --- |\n| Comments: | Accepted to ICCV2021 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV) |\n| Cite as: | [arXiv:2108.08697](https://arxiv.org/abs/2108.08697) \\[cs.CV\\] |\n| | (or [arXiv:2108.08697v1](https://arxiv.org/abs/2108.08697v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2108.08697](https://doi.org/10.48550/arXiv.2108.08697) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Tao Wang \\[ [view email](https://arxiv.org/show-email/e9720b1c/2108.08697)\\]\n\n**\\[v1\\]**\nThu, 19 Aug 2021 14:04:59 UTC (7,777 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables, by Tao Wang and 6 other authors\n\n- [View PDF](https://arxiv.org/pdf/2108.08697)\n- [TeX Source](https://arxiv.org/src/2108.08697)\n- [Other Formats](https://arxiv.org/format/2108.08697)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2108.08697&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2108.08697&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2021-08](https://arxiv.org/list/cs.CV/2021-08)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2108.08697?context=cs)\n\n[eess](https://arxiv.org/abs/2108.08697?context=eess)\n\n[eess.IV](https://arxiv.org/abs/2108.08697?context=eess.IV)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2108.08697)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2108.08697)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2108.08697)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2108.html#abs-2108-08697) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2108-08697)\n\n[Tao Wang](https://dblp.uni-trier.de/search/author?author=Tao%20Wang)\n\n[Yong Li](https://dblp.uni-trier.de/search/author?author=Yong%20Li)\n\n[Yipeng Ma](https://dblp.uni-trier.de/search/author?author=Yipeng%20Ma)\n\n[Xian Wang](https://dblp.uni-trier.de/search/author?author=Xian%20Wang)\n\n[Youliang Yan](https://dblp.uni-trier.de/search/author?author=Youliang%20Yan)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2108.08697&description=Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2108.08697&title=Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2108.08697) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Real-time image enhancer via learnable spatial-aware 3d lookup tables",
          "cleaned_query": "Real-time image enhancer via learnable spatial-aware 3d lookup tables"
        },
        {
          "success": true,
          "title": "IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution",
          "url": "https://arxiv.org/abs/2507.09923",
          "content": "[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)\n\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\n# Electrical Engineering and Systems Science > Image and Video Processing\n\n**arXiv:2507.09923** (eess)\n\n\\[Submitted on 14 Jul 2025 ( [v1](https://arxiv.org/abs/2507.09923v1)), last revised 28 Jul 2025 (this version, v3)\\]\n\n# Title:IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution\n\nAuthors: [Sejin Park](https://arxiv.org/search/eess?searchtype=author&query=Park,+S), [Sangmin Lee](https://arxiv.org/search/eess?searchtype=author&query=Lee,+S), [Kyong Hwan Jin](https://arxiv.org/search/eess?searchtype=author&query=Jin,+K+H), [Seung-Won Jung](https://arxiv.org/search/eess?searchtype=author&query=Jung,+S)\n\nView a PDF of the paper titled IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution, by Sejin Park and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2507.09923)\n\n> Abstract:Super-resolution (SR) has been a pivotal task in image processing, aimed at enhancing image resolution across various applications. Recently, look-up table (LUT)-based approaches have attracted interest due to their efficiency and performance. However, these methods are typically designed for fixed scale factors, making them unsuitable for arbitrary-scale image SR (ASISR). Existing ASISR techniques often employ implicit neural representations, which come with considerable computational cost and memory demands. To address these limitations, we propose Interpolation Mixing LUT (IM-LUT), a novel framework that operates ASISR by learning to blend multiple interpolation functions to maximize their representational capacity. Specifically, we introduce IM-Net, a network trained to predict mixing weights for interpolation functions based on local image patterns and the target scale factor. To enhance efficiency of interpolation-based methods, IM-Net is transformed into IM-LUT, where LUTs are employed to replace computationally expensive operations, enabling lightweight and fast inference on CPUs while preserving reconstruction quality. Experimental results on several benchmark datasets demonstrate that IM-LUT consistently achieves a superior balance between image quality and efficiency compared to existing methods, highlighting its potential as a promising solution for resource-constrained applications.\n\n| | |\n| --- | --- |\n| Comments: | ICCV 2025 |\n| Subjects: | Image and Video Processing (eess.IV); Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2507.09923](https://arxiv.org/abs/2507.09923) \\[eess.IV\\] |\n| (or [arXiv:2507.09923v3](https://arxiv.org/abs/2507.09923v3) \\[eess.IV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2507.09923](https://doi.org/10.48550/arXiv.2507.09923) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Sejin Park \\[ [view email](https://arxiv.org/show-email/9a1ae4bd/2507.09923)\\] **[\\[v1\\]](https://arxiv.org/abs/2507.09923v1)**\nMon, 14 Jul 2025 05:02:57 UTC (24,652 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2507.09923v2)**\nTue, 15 Jul 2025 10:04:02 UTC (24,652 KB)\n**\\[v3\\]**\nMon, 28 Jul 2025 06:48:28 UTC (24,652 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution, by Sejin Park and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2507.09923)\n- [TeX Source](https://arxiv.org/src/2507.09923)\n- [Other Formats](https://arxiv.org/format/2507.09923)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\neess.IV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2507.09923&function=prev&context=eess.IV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2507.09923&function=next&context=eess.IV)\n\n[new](https://arxiv.org/list/eess.IV/new) \\| [recent](https://arxiv.org/list/eess.IV/recent) \\| [2025-07](https://arxiv.org/list/eess.IV/2025-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2507.09923?context=cs) [cs.CV](https://arxiv.org/abs/2507.09923?context=cs.CV) [eess](https://arxiv.org/abs/2507.09923?context=eess)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2507.09923)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2507.09923)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2507.09923)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2507.09923&description=IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2507.09923&title=IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2507.09923) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Practical single-image super-resolution using look-up table",
          "cleaned_query": "Practical single-image super-resolution using look-up table"
        }
      ],
      "generated_ideas": [
        "IM-LUT Pansharpening for Arbitrary Resolution Ratios\nDevelop a LUT-based pansharpening model that, like IM-LUT, supports arbitrary MS\u2194PAN scale ratios by predicting mixing weights conditioned on local patterns and the desired ratio. Replace most convolutional fusion blocks (e.g., in PanNet) with interpolation-mixing LUT inference to enable CPU/edge deployment for large scenes.",
        "CLIPPan-Style Language Supervision for Efficient LUT-Based Full-Resolution Training\nExtend CLIPPan\u2019s language-supervised unsupervised setting by supervising a LUT-based pansharpening model (rather than a heavy CNN) using protocol-aligned prompts (Wald/Khan) plus CLIP embedding alignment. The key contribution is a full-resolution, no-GT training pipeline that outputs a compact LUT representation suitable for real-time onboard processing.",
        "Orientation-Conditioned Rectangular Convolution via LUT-Generated Kernels\nCombine Adaptive Rectangular Convolution with LUT ideas by learning a discrete set of rectangular kernel \u201ctemplates\u201d stored as a LUT and selecting/mixing them per-pixel based on predicted orientation/structure cues from PAN. This makes kernel adaptation cheaper and more interpretable while targeting anisotropic structures (roads, building edges) common in remote sensing.",
        "Uncertainty-Gated Fusion: Confidence Maps to Avoid Hallucinations\nIntroduce a per-pixel uncertainty estimator that predicts where PAN detail injection is reliable (e.g., not causing spectral artifacts) and gates the fusion strength accordingly in PanNet/ARC. Train using self-consistency constraints (downsample/upsample cycles) and disagreement between multiple augmentations to penalize confident but inconsistent detail transfer.",
        "Cross-Sensor Domain Generalization with Prompt-Tuned CLIP Supervisors\nAddress the reduced-resolution vs full-resolution domain gap by prompt-tuning (or lightweight adapting) CLIP separately for multiple sensors (WorldView, Gaofen, etc.) and learning a sensor-conditioned fusion model. The model uses sensor metadata (band response, GSD) as conditioning so it generalizes without per-sensor retraining, while CLIP-based semantic losses keep outputs perceptually and structurally plausible.",
        "Bandwise LUTs for Spectral Response Mismatch Compensation\nDesign a bandwise (or groupwise) LUT module that explicitly models sensor spectral response differences and injects PAN details into each MS band with band-specific transformations. Train the LUTs jointly with a fusion backbone using physically motivated constraints (e.g., PAN as weighted sum of bands) so the method reduces color shifts when sensors have non-ideal or unknown spectral overlaps.",
        "Edge-Consistent Arbitrary-Scale PanSharpening with Scale-Conditioned Losses\nCreate an arbitrary-scale pansharpening framework (inspired by IM-LUT) where training samples span many scale ratios, and the loss terms are explicitly scale-conditioned (edge consistency at the PAN scale, spectral consistency at the MS scale). This enables a single model to handle heterogeneous archives where MS and PAN resolutions vary by scene or preprocessing pipeline.",
        "Multi-Objective Pansharpening via Learned LUT Pareto Controls\nBuild a controllable pansharpening model that exposes a user-set parameter trading spectral fidelity vs spatial sharpness, implemented by mixing multiple learned LUTs (e.g., \u201cspectral-preserving\u201d vs \u201cdetail-boosting\u201d) with a continuous control. Train using a Pareto-front objective so practitioners can tune outputs for downstream tasks (mapping vs visualization) without retraining.",
        "Task-Driven Pansharpening: Joint Optimization for Detection/Segmentation with Efficient Heads\nCouple a pansharpening backbone (PanNet/ARC) with a downstream remote-sensing task network (e.g., building/road segmentation) and train end-to-end so the fused image is optimized for task performance, not only reconstruction metrics. Add a lightweight LUT refinement head to keep inference fast, and include constraints to preserve radiometry so improvements are not achieved by task-specific distortions."
      ],
      "generated_ideas_raw": "1. **IM-LUT Pansharpening for Arbitrary Resolution Ratios**\n   Develop a LUT-based pansharpening model that, like IM-LUT, supports arbitrary MS\u2194PAN scale ratios by predicting mixing weights conditioned on local patterns and the desired ratio. Replace most convolutional fusion blocks (e.g., in PanNet) with interpolation-mixing LUT inference to enable CPU/edge deployment for large scenes.\n\n2. **Spatial-Aware 3D LUT Fusion as a Post-Podule for PanNet/ARC**\n   Add a learnable spatial-aware 3D LUT \u201crefinement head\u201d after an existing pansharpening backbone (PanNet or Adaptive Rectangular Convolution) to correct residual spectral distortions and local contrast errors at negligible runtime. Train the LUT head with a combination of spectral consistency losses and edge-guided spatial losses so it learns content-adaptive tone/reflectance correction specific to remote-sensing imagery.\n\n3. **CLIPPan-Style Language Supervision for Efficient LUT-Based Full-Resolution Training**\n   Extend CLIPPan\u2019s language-supervised unsupervised setting by supervising a LUT-based pansharpening model (rather than a heavy CNN) using protocol-aligned prompts (Wald/Khan) plus CLIP embedding alignment. The key contribution is a full-resolution, no-GT training pipeline that outputs a compact LUT representation suitable for real-time onboard processing.\n\n4. **Orientation-Conditioned Rectangular Convolution via LUT-Generated Kernels**\n   Combine Adaptive Rectangular Convolution with LUT ideas by learning a discrete set of rectangular kernel \u201ctemplates\u201d stored as a LUT and selecting/mixing them per-pixel based on predicted orientation/structure cues from PAN. This makes kernel adaptation cheaper and more interpretable while targeting anisotropic structures (roads, building edges) common in remote sensing.\n\n5. **Uncertainty-Gated Fusion: Confidence Maps to Avoid Hallucinations**\n   Introduce a per-pixel uncertainty estimator that predicts where PAN detail injection is reliable (e.g., not causing spectral artifacts) and gates the fusion strength accordingly in PanNet/ARC. Train using self-consistency constraints (downsample/upsample cycles) and disagreement between multiple augmentations to penalize confident but inconsistent detail transfer.\n\n6. **Cross-Sensor Domain Generalization with Prompt-Tuned CLIP Supervisors**\n   Address the reduced-resolution vs full-resolution domain gap by prompt-tuning (or lightweight adapting) CLIP separately for multiple sensors (WorldView, Gaofen, etc.) and learning a sensor-conditioned fusion model. The model uses sensor metadata (band response, GSD) as conditioning so it generalizes without per-sensor retraining, while CLIP-based semantic losses keep outputs perceptually and structurally plausible.\n\n7. **Bandwise LUTs for Spectral Response Mismatch Compensation**\n   Design a bandwise (or groupwise) LUT module that explicitly models sensor spectral response differences and injects PAN details into each MS band with band-specific transformations. Train the LUTs jointly with a fusion backbone using physically motivated constraints (e.g., PAN as weighted sum of bands) so the method reduces color shifts when sensors have non-ideal or unknown spectral overlaps.\n\n8. **Edge-Consistent Arbitrary-Scale PanSharpening with Scale-Conditioned Losses**\n   Create an arbitrary-scale pansharpening framework (inspired by IM-LUT) where training samples span many scale ratios, and the loss terms are explicitly scale-conditioned (edge consistency at the PAN scale, spectral consistency at the MS scale). This enables a single model to handle heterogeneous archives where MS and PAN resolutions vary by scene or preprocessing pipeline.\n\n9. **Multi-Objective Pansharpening via Learned LUT Pareto Controls**\n   Build a controllable pansharpening model that exposes a user-set parameter trading spectral fidelity vs spatial sharpness, implemented by mixing multiple learned LUTs (e.g., \u201cspectral-preserving\u201d vs \u201cdetail-boosting\u201d) with a continuous control. Train using a Pareto-front objective so practitioners can tune outputs for downstream tasks (mapping vs visualization) without retraining.\n\n10. **Task-Driven Pansharpening: Joint Optimization for Detection/Segmentation with Efficient Heads**\n   Couple a pansharpening backbone (PanNet/ARC) with a downstream remote-sensing task network (e.g., building/road segmentation) and train end-to-end so the fused image is optimized for task performance, not only reconstruction metrics. Add a lightweight LUT refinement head to keep inference fast, and include constraints to preserve radiometry so improvements are not achieved by task-specific distortions.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "IM-LUT Pansharpening for Arbitrary Resolution Ratios\nDevelop a LUT-based pansharpening model that, like IM-LUT, supports arbitrary MS\u2194PAN scale ratios by predicting mixing weights conditioned on local",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "CLIPPan-Style Language Supervision for Efficient LUT-Based Full-Resolution Training\nExtend CLIPPan\u2019s language-supervised unsupervised setting by supervising a LUT-based pansharpening model (rather tha",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Orientation-Conditioned Rectangular Convolution via LUT-Generated Kernels\nCombine Adaptive Rectangular Convolution with LUT ideas by learning a discrete set of rectangular kernel \u201ctemplates\u201d stored as",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Uncertainty-Gated Fusion: Confidence Maps to Avoid Hallucinations\nIntroduce a per-pixel uncertainty estimator that predicts where PAN detail injection is reliable (e.g., not causing spectral artifacts",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Cross-Sensor Domain Generalization with Prompt-Tuned CLIP Supervisors\nAddress the reduced-resolution vs full-resolution domain gap by prompt-tuning (or lightweight adapting) CLIP separately for multip",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Bandwise LUTs for Spectral Response Mismatch Compensation\nDesign a bandwise (or groupwise) LUT module that explicitly models sensor spectral response differences and injects PAN details into each MS b",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Edge-Consistent Arbitrary-Scale PanSharpening with Scale-Conditioned Losses\nCreate an arbitrary-scale pansharpening framework (inspired by IM-LUT) where training samples span many scale ratios, and th",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Multi-Objective Pansharpening via Learned LUT Pareto Controls\nBuild a controllable pansharpening model that exposes a user-set parameter trading spectral fidelity vs spatial sharpness, implemented by ",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Task-Driven Pansharpening: Joint Optimization for Detection/Segmentation with Efficient Heads\nCouple a pansharpening backbone (PanNet/ARC) with a downstream remote-sensing task network (e.g., building",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 38,
      "paper_title": "Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks",
      "contribution": "Using dynamical mean field theory the authors show that, in the joint large-width and large-sample regime, training dynamics exhibits a separation of timescales that (i) produces slow growth of function complexity, (ii) yields an inductive bias toward low-complexity solutions determined by initialization, and (iii) dynamically decouples feature learning from overfitting \u2014 predicting nonmonotone test error and a late-time 'feature unlearning' regime.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 14181,
      "output_tokens": 1037,
      "predecessor_details": [
        {
          "success": true,
          "title": "A Mean Field View of the Landscape of Two-Layers Neural Networks",
          "url": "https://arxiv.org/abs/1804.06561",
          "content": "# Statistics > Machine Learning\n\n**arXiv:1804.06561** (stat)\n\n\\[Submitted on 18 Apr 2018 ( [v1](https://arxiv.org/abs/1804.06561v1)), last revised 28 Aug 2018 (this version, v2)\\]\n\n# Title:A Mean Field View of the Landscape of Two-Layers Neural Networks\n\nAuthors: [Song Mei](https://arxiv.org/search/stat?searchtype=author&query=Mei,+S), [Andrea Montanari](https://arxiv.org/search/stat?searchtype=author&query=Montanari,+A), [Phan-Minh Nguyen](https://arxiv.org/search/stat?searchtype=author&query=Nguyen,+P)\n\nView a PDF of the paper titled A Mean Field View of the Landscape of Two-Layers Neural Networks, by Song Mei and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/1804.06561)\n\n> Abstract:Multi-layer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires to optimize a non-convex high-dimensional objective (risk function), a problem which is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the first case, does this happen because local minima are absent, or because SGD somehow avoids them? In the second, why do local minima reached by SGD have good generalization properties?\n>\n> In this paper we consider a simple case, namely two-layers neural networks, and prove that -in a suitable scaling limit- SGD dynamics is captured by a certain non-linear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples, and show how DD can be used to prove convergence of SGD to networks with nearly ideal generalization error. This description allows to 'average-out' some of the complexities of the landscape of neural networks, and can be used to prove a general convergence result for noisy SGD.\n\n| | |\n| --- | --- |\n| Comments: | 103 pages |\n| Subjects: | Machine Learning (stat.ML); Statistical Mechanics (cond-mat.stat-mech); Machine Learning (cs.LG); Statistics Theory (math.ST) |\n| Cite as: | [arXiv:1804.06561](https://arxiv.org/abs/1804.06561) \\[stat.ML\\] |\n| | (or [arXiv:1804.06561v2](https://arxiv.org/abs/1804.06561v2) \\[stat.ML\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1804.06561](https://doi.org/10.48550/arXiv.1804.06561) Focus to learn more arXiv-issued DOI via DataCite |\n| Related DOI: | [https://doi.org/10.1073/pnas.1806579115](https://doi.org/10.1073/pnas.1806579115) Focus to learn more DOI(s) linking to related resources |\n\n## Submission history\n\nFrom: Song Mei \\[ [view email](https://arxiv.org/show-email/6fcaee66/1804.06561)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1804.06561v1)**\nWed, 18 Apr 2018 05:31:45 UTC (1,533 KB)\n\n**\\[v2\\]**\nTue, 28 Aug 2018 06:21:23 UTC (1,535 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled A Mean Field View of the Landscape of Two-Layers Neural Networks, by Song Mei and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/1804.06561)\n- [TeX Source](https://arxiv.org/src/1804.06561)\n- [Other Formats](https://arxiv.org/format/1804.06561)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1804.06561&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1804.06561&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2018-04](https://arxiv.org/list/stat.ML/2018-04)\n\nChange to browse by:\n\n[cond-mat](https://arxiv.org/abs/1804.06561?context=cond-mat)\n\n[cond-mat.stat-mech](https://arxiv.org/abs/1804.06561?context=cond-mat.stat-mech)\n\n[cs](https://arxiv.org/abs/1804.06561?context=cs)\n\n[cs.LG](https://arxiv.org/abs/1804.06561?context=cs.LG)\n\n[math](https://arxiv.org/abs/1804.06561?context=math)\n\n[math.ST](https://arxiv.org/abs/1804.06561?context=math.ST)\n\n[stat](https://arxiv.org/abs/1804.06561?context=stat)\n\n[stat.TH](https://arxiv.org/abs/1804.06561?context=stat.TH)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1804.06561)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1804.06561)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1804.06561)\n\n### [1 blog link](https://arxiv.org/tb/1804.06561)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1804.06561&description=A Mean Field View of the Landscape of Two-Layers Neural Networks) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1804.06561&title=A Mean Field View of the Landscape of Two-Layers Neural Networks)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1804.06561) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "A mean field view of the landscape of two-layer neural networks",
          "cleaned_query": "A mean field view of the landscape of two-layer neural networks"
        },
        {
          "success": true,
          "title": "Neural Tangent Kernel: Convergence and Generalization in ... - arXiv",
          "url": "https://arxiv.org/abs/1806.07572",
          "content": "[1806.07572] Neural Tangent Kernel: Convergence and Generalization in Neural Networks[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1806.07572\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1806.07572**(cs)\n[Submitted on 20 Jun 2018 ([v1](https://arxiv.org/abs/1806.07572v1)), last revised 10 Feb 2020 (this version, v4)]\n# Title:Neural Tangent Kernel: Convergence and Generalization in Neural Networks\nAuthors:[Arthur Jacot](https://arxiv.org/search/cs?searchtype=author&amp;query=Jacot,+A),[Franck Gabriel](https://arxiv.org/search/cs?searchtype=author&amp;query=Gabriel,+F),[Cl\u00e9ment Hongler](https://arxiv.org/search/cs?searchtype=author&amp;query=Hongler,+C)\nView a PDF of the paper titled Neural Tangent Kernel: Convergence and Generalization in Neural Networks, by Arthur Jacot and 2 other authors\n[View PDF](https://arxiv.org/pdf/1806.07572)> > Abstract:\n> At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function $f_\\theta$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function $f_\\theta$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit. Subjects:|Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Probability (math.PR); Machine Learning (stat.ML)|\nCite as:|[arXiv:1806.07572](https://arxiv.org/abs/1806.07572)[cs.LG]|\n|(or[arXiv:1806.07572v4](https://arxiv.org/abs/1806.07572v4)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1806.07572](https://doi.org/10.48550/arXiv.1806.07572)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\nJournalreference:|In Advances in neural information processing systems (pp. 8571-8580) 2018|\n## Submission history\nFrom: Arthur Jacot [[view email](https://arxiv.org/show-email/cfefbf84/1806.07572)]\n**[[v1]](https://arxiv.org/abs/1806.07572v1)**Wed, 20 Jun 2018 06:35:46 UTC (211 KB)\n**[[v2]](https://arxiv.org/abs/1806.07572v2)**Mon, 12 Nov 2018 10:31:42 UTC (125 KB)\n**[[v3]](https://arxiv.org/abs/1806.07572v3)**Mon, 26 Nov 2018 15:42:05 UTC (127 KB)\n**[v4]**Mon, 10 Feb 2020 08:39:09 UTC (128 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Neural Tangent Kernel: Convergence and Generalization in Neural Networks, by Arthur Jacot and 2 other authors\n* [View PDF](https://arxiv.org/pdf/1806.07572)\n* [TeX Source](https://arxiv.org/src/1806.07572)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1806.07572&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1806.07572&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2018-06](https://arxiv.org/list/cs.LG/2018-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1806.07572?context=cs)\n[cs.NE](https://arxiv.org/abs/1806.07572?context=cs.NE)\n[math](https://arxiv.org/abs/1806.07572?context=math)\n[math.PR](https://arxiv.org/abs/1806.07572?context=math.PR)\n[stat](https://arxiv.org/abs/1806.07572?context=stat)\n[stat.ML](https://arxiv.org/abs/1806.07572?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1806.07572)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1806.07572)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1806.07572)\n### [2 blog links](https://arxiv.org/tb/1806.07572)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1806.html#abs-1806-07572)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1806-07572)\n[Arthur Jacot]()\n[Franck Gabriel]()\n[Cl\u00e9ment Hongler]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/wel",
          "original_query": "Neural tangent kernel: Convergence and generalization in neural networks",
          "cleaned_query": "Neural tangent kernel: Convergence and generalization in neural networks"
        },
        {
          "success": true,
          "title": "[1812.07956] On Lazy Training in Differentiable Programming - arXiv",
          "url": "https://arxiv.org/abs/1812.07956",
          "content": "# Mathematics > Optimization and Control\n\n**arXiv:1812.07956** (math)\n\n\\[Submitted on 19 Dec 2018 ( [v1](https://arxiv.org/abs/1812.07956v1)), last revised 7 Jan 2020 (this version, v5)\\]\n\n# Title:On Lazy Training in Differentiable Programming\n\nAuthors: [Lenaic Chizat](https://arxiv.org/search/math?searchtype=author&query=Chizat,+L) (CNRS, UP11), [Edouard Oyallon](https://arxiv.org/search/math?searchtype=author&query=Oyallon,+E), [Francis Bach](https://arxiv.org/search/math?searchtype=author&query=Bach,+F) (LIENS, SIERRA)\n\nView a PDF of the paper titled On Lazy Training in Differentiable Programming, by Lenaic Chizat (CNRS and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/1812.07956)\n\n> Abstract:In a series of recent theoretical works, it was shown that strongly over-parameterized neural networks trained with gradient-based methods could converge exponentially fast to zero training loss, with their parameters hardly varying. In this work, we show that this \"lazy training\" phenomenon is not specific to over-parameterized neural networks, and is due to a choice of scaling, often implicit, that makes the model behave as its linearization around the initialization, thus yielding a model equivalent to learning with positive-definite kernels. Through a theoretical analysis, we exhibit various situations where this phenomenon arises in non-convex optimization and we provide bounds on the distance between the lazy and linearized optimization paths. Our numerical experiments bring a critical note, as we observe that the performance of commonly used non-linear deep convolutional neural networks in computer vision degrades when trained in the lazy regime. This makes it unlikely that \"lazy training\" is behind the many successes of neural networks in difficult high dimensional tasks.\n\n| | |\n| --- | --- |\n| Subjects: | Optimization and Control (math.OC); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:1812.07956](https://arxiv.org/abs/1812.07956) \\[math.OC\\] |\n| | (or [arXiv:1812.07956v5](https://arxiv.org/abs/1812.07956v5) \\[math.OC\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1812.07956](https://doi.org/10.48550/arXiv.1812.07956) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | Advances in Neural Information Processing Systems (NeurIPS), Dec 2019, Vancouver, Canada |\n\n## Submission history\n\nFrom: Lenaic Chizat \\[ [view email](https://arxiv.org/show-email/88471c15/1812.07956)\\]\u00a0\\[via CCSD proxy\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1812.07956v1)**\nWed, 19 Dec 2018 14:11:20 UTC (312 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1812.07956v2)**\nThu, 21 Feb 2019 10:33:25 UTC (312 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/1812.07956v3)**\nMon, 17 Jun 2019 14:39:39 UTC (360 KB)\n\n**[\\[v4\\]](https://arxiv.org/abs/1812.07956v4)**\nTue, 18 Jun 2019 14:36:18 UTC (355 KB)\n\n**\\[v5\\]**\nTue, 7 Jan 2020 16:11:56 UTC (357 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled On Lazy Training in Differentiable Programming, by Lenaic Chizat (CNRS and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/1812.07956)\n- [TeX Source](https://arxiv.org/src/1812.07956)\n- [Other Formats](https://arxiv.org/format/1812.07956)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nmath.OC\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1812.07956&function=prev&context=math.OC)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1812.07956&function=next&context=math.OC)\n\n[new](https://arxiv.org/list/math.OC/new) \\| [recent](https://arxiv.org/list/math.OC/recent) \\| [2018-12](https://arxiv.org/list/math.OC/2018-12)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1812.07956?context=cs)\n\n[cs.LG](https://arxiv.org/abs/1812.07956?context=cs.LG)\n\n[math](https://arxiv.org/abs/1812.07956?context=math)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1812.07956)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1812.07956)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1812.07956)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1812.07956&description=On Lazy Training in Differentiable Programming) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1812.07956&title=On Lazy Training in Differentiable Programming)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1812.07956) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "On lazy training in differentiable programming",
          "cleaned_query": "On lazy training in differentiable programming"
        },
        {
          "success": true,
          "title": "The high-dimensional asymptotics of first order methods with ... - arXiv",
          "url": "https://arxiv.org/abs/2112.07572",
          "content": "# Mathematics > Probability\n\n**arXiv:2112.07572** (math)\n\n\\[Submitted on 14 Dec 2021\\]\n\n# Title:The high-dimensional asymptotics of first order methods with random data\n\nAuthors: [Michael Celentano](https://arxiv.org/search/math?searchtype=author&query=Celentano,+M), [Chen Cheng](https://arxiv.org/search/math?searchtype=author&query=Cheng,+C), [Andrea Montanari](https://arxiv.org/search/math?searchtype=author&query=Montanari,+A)\n\nView a PDF of the paper titled The high-dimensional asymptotics of first order methods with random data, by Michael Celentano and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2112.07572)\n\n> Abstract:We study a class of deterministic flows in ${\\\\mathbb R}^{d\\\\times k}$, parametrized by a random matrix ${\\\\boldsymbol X}\\\\in {\\\\mathbb R}^{n\\\\times d}$ with i.i.d. centered subgaussian entries. We characterize the asymptotic behavior of these flows over bounded time horizons, in the high-dimensional limit in which $n,d\\\\to\\\\infty$ with $k$ fixed and converging aspect ratios $n/d\\\\to\\\\delta$. The asymptotic characterization we prove is in terms of a system of a nonlinear stochastic process in $k$ dimensions, whose parameters are determined by a fixed point condition. This type of characterization is known in physics as dynamical mean field theory. Rigorous results of this type have been obtained in the past for a few spin glass models. Our proof is based on time discretization and a reduction to certain iterative schemes known as approximate message passing (AMP) algorithms, as opposed to earlier work that was based on large deviations theory and stochastic processes theory. The new approach allows for a more elementary proof and implies that the high-dimensional behavior of the flow is universal with respect to the distribution of the entries of ${\\\\boldsymbol X}$. As specific applications, we obtain high-dimensional characterizations of gradient flow in some classical models from statistics and machine learning, under a random design assumption.\n\n| | |\n| --- | --- |\n| Comments: | 83 pages |\n| Subjects: | Probability (math.PR); Statistics Theory (math.ST) |\n| Cite as: | [arXiv:2112.07572](https://arxiv.org/abs/2112.07572) \\[math.PR\\] |\n| (or [arXiv:2112.07572v1](https://arxiv.org/abs/2112.07572v1) \\[math.PR\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2112.07572](https://doi.org/10.48550/arXiv.2112.07572) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Andrea Montanari \\[ [view email](https://arxiv.org/show-email/e0f68b39/2112.07572)\\] **\\[v1\\]**\nTue, 14 Dec 2021 17:29:32 UTC (73 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled The high-dimensional asymptotics of first order methods with random data, by Michael Celentano and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2112.07572)\n- [TeX Source](https://arxiv.org/src/2112.07572)\n- [Other Formats](https://arxiv.org/format/2112.07572)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nmath.PR\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2112.07572&function=prev&context=math.PR)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2112.07572&function=next&context=math.PR)\n\n[new](https://arxiv.org/list/math.PR/new) \\| [recent](https://arxiv.org/list/math.PR/recent) \\| [2021-12](https://arxiv.org/list/math.PR/2021-12)\n\nChange to browse by:\n\n[math](https://arxiv.org/abs/2112.07572?context=math) [math.ST](https://arxiv.org/abs/2112.07572?context=math.ST) [stat](https://arxiv.org/abs/2112.07572?context=stat) [stat.TH](https://arxiv.org/abs/2112.07572?context=stat.TH)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2112.07572)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2112.07572)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2112.07572)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2112.07572) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "The high-dimensional asymptotics of first order methods with random data",
          "cleaned_query": "The high-dimensional asymptotics of first order methods with random data"
        },
        {
          "success": true,
          "title": "[PDF] Learning time-scales in two-layers neural networks - arXiv",
          "url": "https://arxiv.org/pdf/2303.00055",
          "content": "Learning time-scales in two-layers neural networks\nRapha\u00ebl Berthier\u2217, Andrea Montanari\u2020, Kangjie Zhou\u2021\nMarch 24, 2025\nAbstract\nGradient-based learning in multi-layer neural networks displays a number of striking\nfeatures. In particular, the decrease rate of empirical risk is non-monotone even after\naveraging over large batches. Long plateaus in which one observes barely any progress\nalternate with intervals of rapid decrease. These successive phases of learning often take\nplace on very different time scales. Finally, models learnt in an early phase are typically\n\u2018simpler\u2019 or \u2018easier to learn\u2019 although in a way that is difficult to formalize.\nAlthough theoretical explanations of these phenomena have been put forward, each of them\ncaptures at best certain specific regimes. In this paper, we study the gradient flow dynamics\nof a wide two-layer neural network in high-dimension, when data are distributed according\nto a single-index model (i.e., the target function depends on a one-dimensional projection\nof the covariates). Based on a mixture of new rigorous results, non-rigorous mathematical\nderivations, and numerical simulations, we propose a scenario for the learning dynamics\nin this setting. In particular, the proposed evolution exhibits separation of timescales and\nintermittency. These behaviors arise naturally because the population gradient flow can be\nrecast as a singularly perturbed dynamical system.\nKeywords: Deep learning, Neural network, Gradient flow, Dynamical system, Non-convex\noptimization, Incremental learning\nMathematics Subject Classification: 34E15, 37N40, 68T07\n\u2217EPFL, email address: raphael.berthier1@gmail.com\n\u2020Department of Electrical Engineering and Department of Statistics, Stanford University, email address:\nmontanar@stanford.edu\n\u2021Department of Statistics, Stanford University, email address: kangjie@stanford.edu\n1\narXiv:2303.00055v4 [cs.LG] 9 Mar 2025\nContents\n1 Introduction 3\n2 Setting and canonical learning order 4\n3 Further related work 8\n4 The large-network, high-dimensional limit 9\n4.1 Reduction to d-independent flow . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n4.2 Elimination of the products \u27e8ui, uj \u27e9 . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n4.3 Connection with mean field theory . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n4.4 A general formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n5 Numerical solution 14\n6 Timescales hierarchy in the gradient flow dynamics 17\n6.1 Matched asymptotic expansions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n6.2 First time scale: constant component . . . . . . . . . . . . . . . . . . . . . . . . . 19\n6.3 Second time scale: linear component I . . . . . . . . . . . . . . . . . . . . . . . . 20\n6.4 Third time scale: linear component II . . . . . . . . . . . . . . . . . . . . . . . . 23\n6.5 Conjectured behavior for larger time scales . . . . . . . . . . . . . . . . . . . . . 25\n7 Stochastic gradient descent and finite sample size 27\n8 Discussion 29\nA Proof of Proposition 1 36\nB Appendix to Section 4 37\nB.1 Proof of Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\nB.2 Proof of Corollary 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\nB.3 Proof of Proposition 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\nB.4 Derivation of the mean field dynamics (29) . . . . . . . . . . . . . . . . . . . . . 43\nB.5 Details of the alternative mean field approach . . . . . . . . . . . . . . . . . . . . 43\nB.6 Proof of Proposition 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\nC Calculations for the analysis of mean-field gradient flow 46\nC.1 Solution of Eq. (89) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\nC.2 Induced approximation of the risk . . . . . . . . . . . . . . . . . . . . . . . . . . 48\nC.3 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\nD Proofs of Theorem 2 and 3: learning with projected SGD 53\nD.1 Difference between GF and GD . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\nD.2 Difference between GD and SGD . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\nD.3 Difference between SGD and projected SGD . . . . . . . . . . . . . . . . . . . . . 59\nD.4 Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\nE Counterexamples to the canonical learning order 63\nE.1 Case 1: \u03c3k = 0 for some k \u2208 N . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\nE.2 Case 2: \u03c60 = \u00b7 \u00b7 \u00b7 = \u03c6k = 0 for some k \u2265 1 . . . . . . . . . . . . . . . . . . . . . . 63\nE.3 Case 3: \u03c6k = 0 for some k \u2265 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n2\n1 Introduction\nIt is a recurring empirical observation that the training dynamics of neural networks exhibits a\nwhole range of surprising behaviors:\n1. Plateaus. Plotting the training and test error as a function of SGD steps, using either\nsmall stepsize or large batches to average out stochasticity, reveals striking patterns. These\nerror curves display long plateaus where barely anything seems to be happening, which\nare followed by rapid drops [41, 48, 39].\n2. Time-scales separation. The time window for this rapid descent is much shorter than the\ntime spent in the plateaus. Additionally, subsequent phases of learning take increasingly\nlonger times [18, 8].\n3. Incremental learning. Models learnt in the first phases of learning appear to be simpler\nthan in later phases. Among others, [5] demonstrated that easier examples in a dataset\nare learned earlier; [28] showed that models learnt in the first phase of training correlate\nwell with linear models; [22] showed that, in many simplified models, the dynamics of\ngradient descent explores the solution space in an incremental order of complexity; [39]\ndemonstrated that, in certain settings, a function that approximates well the target is only\nlearnt past the point of overfitting.\nUnderstanding these phenomena is not a matter of intellectual curiosity. In particular, incremental\nlearning plays a key role in our understanding of generalization in deep learning. Indeed, in this\nscenario, stopping the learning at a certain time t amounts to controlling the complexity of the\nmodel learnt. The notion of complexity corresponds to the order in which the space of models is\nexplored.\nWhile a number of groups have developed models to explain these phenomena, it is fair to\nsay that a complete picture is still lacking. An exhaustive overview of these works is out of place\nhere. We will outline three possible explanations that have been developed in the past, and\nprovide more pointers in Section 3.\nTheory #1: Dynamics near singular points. Several early works [41, 17, 44] pointed out\nthat the parametrization of multi-layer neural networks presents symmetries and degeneracies.\nFor instance, the function represented by a multi-layer perceptron is invariant under permutations\nof the neurons in the same layer. As a consequence, the population risk has multiple local minima\nconnected through saddles or other singular sub-manifolds. Dynamics near these sub-manifolds\nnaturally exhibits plateaus. Further, random or agnostic initializations typically place the\nnetwork close to such submanifolds.\nTheory #2: Linear networks. Following the pioneering work of [7], a number of authors,\nmost notably [43, 30], studied the behavior of deep neural networks with linear activations. While\nsuch networks can only represent linear functions, the training dynamics is highly non-linear.\nAs demonstrated in [43], learning happens through stages that correspond to the singular value\ndecomposition of the input-output covariance. Time scales are determined by the singular values.\nTheory #3: Kernel regime. Following an initial insight of [26], a number of groups proved\nthat, for certain initializations, the training dynamics and model learnt by overparametrized\nneural",
          "original_query": "Learning time-scales in two-layers neural networks",
          "cleaned_query": "Learning time-scales in two-layers neural networks"
        },
        {
          "success": true,
          "title": "[1710.10345] The Implicit Bias of Gradient Descent on Separable Data",
          "url": "https://arxiv.org/abs/1710.10345",
          "content": "[1710.10345] The Implicit Bias of Gradient Descent on Separable Data\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nIn just 5 minutes help us improve arXiv:\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:1710.10345\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:1710.10345**(stat)\n[Submitted on 27 Oct 2017 ([v1](https://arxiv.org/abs/1710.10345v1)), last revised 26 Oct 2024 (this version, v7)]\n# Title:The Implicit Bias of Gradient Descent on Separable Data\nAuthors:[Daniel Soudry](https://arxiv.org/search/stat?searchtype=author&amp;query=Soudry,+D),[Elad Hoffer](https://arxiv.org/search/stat?searchtype=author&amp;query=Hoffer,+E),[Mor Shpigel Nacson](https://arxiv.org/search/stat?searchtype=author&amp;query=Nacson,+M+S),[Suriya Gunasekar](https://arxiv.org/search/stat?searchtype=author&amp;query=Gunasekar,+S),[Nathan Srebro](https://arxiv.org/search/stat?searchtype=author&amp;query=Nathan)\nView a PDF of the paper titled The Implicit Bias of Gradient Descent on Separable Data, by Daniel Soudry and 4 other authors\n[View PDF](https://arxiv.org/pdf/1710.10345)[HTML (experimental)](https://arxiv.org/html/1710.10345v7)> > Abstract:\n> We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization n more complex models and with other optimization methods. Comments:|Added a missing assumption to Theorem 7 (multi-class case) and a discussion of this assumption after the Theorem|\nSubjects:|Machine Learning (stat.ML); Machine Learning (cs.LG)|\nCite as:|[arXiv:1710.10345](https://arxiv.org/abs/1710.10345)[stat.ML]|\n|(or[arXiv:1710.10345v7](https://arxiv.org/abs/1710.10345v7)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.1710.10345](https://doi.org/10.48550/arXiv.1710.10345)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Daniel Soudry [[view email](https://arxiv.org/show-email/45f47387/1710.10345)]\n**[[v1]](https://arxiv.org/abs/1710.10345v1)**Fri, 27 Oct 2017 21:47:58 UTC (1,075 KB)\n**[[v2]](https://arxiv.org/abs/1710.10345v2)**Tue, 27 Feb 2018 21:12:01 UTC (309 KB)\n**[[v3]](https://arxiv.org/abs/1710.10345v3)**Wed, 21 Mar 2018 17:53:26 UTC (337 KB)\n**[[v4]](https://arxiv.org/abs/1710.10345v4)**Fri, 28 Dec 2018 10:51:36 UTC (338 KB)\n**[[v5]](https://arxiv.org/abs/1710.10345v5)**Tue, 19 Jul 2022 14:58:19 UTC (338 KB)\n**[[v6]](https://arxiv.org/abs/1710.10345v6)**Tue, 16 Apr 2024 08:30:01 UTC (338 KB)\n**[v7]**Sat, 26 Oct 2024 08:55:44 UTC (338 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled The Implicit Bias of Gradient Descent on Separable Data, by Daniel Soudry and 4 other authors\n* [View PDF](https://arxiv.org/pdf/1710.10345)\n* [HTML (experimental)](https://arxiv.org/html/1710.10345v7)\n* [TeX Source](https://arxiv.org/src/1710.10345)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1710.10345&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1710.10345&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2017-10](https://arxiv.org/list/stat.ML/2017-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/1710.10345?context=cs)\n[cs.LG](https://arxiv.org/abs/1710.10345?context=cs.LG)\n[stat](https://arxiv.org/abs/1710.10345?context=stat)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1710.10345)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1710.10345)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1710.10345)\n### [1 blog link](https://arxiv.org/tb/1710.10345)\n([what is this?](https://info.arxiv.org/help/trackback.html))\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and ",
          "original_query": "The implicit bias of gradient descent on separable data",
          "cleaned_query": "The implicit bias of gradient descent on separable data"
        }
      ],
      "generated_ideas": [
        "Bridging Mean-Field PDE and NTK via a Controlled \u201cFeature-Learning\u201d Expansion\nDevelop a perturbative framework that starts from the NTK (lazy) limit and adds the first nontrivial mean-field corrections from distributional dynamics (DD) for two-layer networks. Derive explicit error terms predicting when NTK training deviates from true SGD/gradient flow, and validate by matching the predicted deviation time with observed plateau-to-drop transitions.",
        "A Timescale-Resolved Theory of Early Stopping Beyond the Lazy Regime\nUse the singularly perturbed dynamical-system viewpoint (learning time-scales) to formalize \u201coptimal stopping\u201d rules that differ across phases (e.g., constant/linear/higher-order components). Propose a measurable, training-time diagnostic (e.g., based on evolving alignment statistics or empirical NTK spectrum drift) that triggers stopping at phase boundaries and compare it to classical NTK-based early stopping.",
        "Universal Dynamical Mean Field Theory for Two-Layer Networks with Random Design and Nonlinear Targets\nCombine the AMP-based high-dimensional flow characterization (random data universality) with two-layer mean-field dynamics to obtain a rigorous DMFT description for gradient flow under single-index and multi-index teacher models. The key contribution is a universality theorem showing that macroscopic learning curves and phase transitions depend only on low-order moments (aspect ratio, activation statistics), not on the full covariate distribution.",
        "Implicit Bias in Two-Layer Classification: Margin Dynamics Across Lazy-to-Mean-Field Regimes\nExtend max-margin implicit bias results (separable logistic) to two-layer networks by tracking how the classifier margin evolves under (i) kernel-like lazy training and (ii) feature-learning mean-field training. Prove conditions under which the learned decision boundary converges to an RKHS max-margin separator (NTK regime) versus a different margin notion induced by evolving features (DD regime), and quantify the transition in terms of scaling.",
        "Intermittency Mechanisms: Predicting Plateau Lengths from Spectral Gaps and Teacher Harmonics\nIn the single-index setting, relate plateau durations to gaps in the relevant kernel/feature operator spectrum (e.g., NTK principal components vs. mean-field effective operator). Produce explicit asymptotic formulas for plateau lengths and drop times as functions of (a) activation, (b) data aspect ratio, and (c) teacher expansion coefficients, then test predictions by controlled synthetic experiments.",
        "Scaling Laws that Optimize Feature Learning While Avoiding Lazy-Training Degradation in ConvNets/MLPs\nBuild on the \u201clazy training is a scaling choice\u201d insight by designing and analyzing intermediate scalings that maximize representation movement subject to stability constraints. Provide an actionable recipe for selecting width-dependent learning rates/initialization variances that provably yield nontrivial DD feature learning while maintaining exponential loss decrease, and evaluate on vision benchmarks where lazy training underperforms.",
        "A Coupled NTK\u2013DD Hybrid Model for Finite-Width Training Dynamics\nPropose a hybrid dynamical model where early training is governed by nearly-constant NTK dynamics, but later training transitions to mean-field DD with a slowly evolving kernel. Fit the model\u2019s coupling parameters using finite-width statistics (e.g., empirical kernel drift and parameter displacement norms) and show it predicts training curves and generalization better than pure NTK or pure mean-field approximations.",
        "Stochastic Gradient Noise as an Effective Temperature in Distributional Dynamics\nExtend noisy SGD convergence results in the mean-field PDE framework by explicitly incorporating minibatch-induced diffusion and deriving an effective \u201ctemperature\u201d that depends on batch size and data distribution. Quantify how this temperature changes barrier crossing and phase transitions between learning plateaus, and propose a principled batch-size schedule that accelerates transitions without sacrificing final generalization.",
        "Projected/Constrained Training to Selectively Accelerate Late-Phase Components\nMotivated by projected SGD analyses in time-scales work, design projections (or parameter subspace constraints) that preserve early-phase simple components while selectively amplifying hard-to-learn modes (e.g., higher teacher harmonics). Provide theory showing the projection modifies the singular perturbation structure to shorten specific plateaus, and implement a practical algorithm using low-rank or spherical constraints on second-layer weights.",
        "Multi-Class and Multi-Index Extensions: Canonical Learning Order in High Dimensions\nGeneralize the \u201ccanonical learning order\u201d scenario from single-index regression to multi-class classification and multi-index teachers, connecting to separable-data implicit bias in the multi-class case. Derive predicted sequences of learned components (classwise margins or teacher subspace directions), identify when the order breaks (counterexamples), and map these failures to measurable quantities such as class imbalance and inter-index correlation."
      ],
      "generated_ideas_raw": "1. **Bridging Mean-Field PDE and NTK via a Controlled \u201cFeature-Learning\u201d Expansion**  \nDevelop a perturbative framework that starts from the NTK (lazy) limit and adds the first nontrivial mean-field corrections from distributional dynamics (DD) for two-layer networks. Derive explicit error terms predicting when NTK training deviates from true SGD/gradient flow, and validate by matching the predicted deviation time with observed plateau-to-drop transitions.\n\n2. **A Timescale-Resolved Theory of Early Stopping Beyond the Lazy Regime**  \nUse the singularly perturbed dynamical-system viewpoint (learning time-scales) to formalize \u201coptimal stopping\u201d rules that differ across phases (e.g., constant/linear/higher-order components). Propose a measurable, training-time diagnostic (e.g., based on evolving alignment statistics or empirical NTK spectrum drift) that triggers stopping at phase boundaries and compare it to classical NTK-based early stopping.\n\n3. **Universal Dynamical Mean Field Theory for Two-Layer Networks with Random Design and Nonlinear Targets**  \nCombine the AMP-based high-dimensional flow characterization (random data universality) with two-layer mean-field dynamics to obtain a rigorous DMFT description for gradient flow under single-index and multi-index teacher models. The key contribution is a universality theorem showing that macroscopic learning curves and phase transitions depend only on low-order moments (aspect ratio, activation statistics), not on the full covariate distribution.\n\n4. **Implicit Bias in Two-Layer Classification: Margin Dynamics Across Lazy-to-Mean-Field Regimes**  \nExtend max-margin implicit bias results (separable logistic) to two-layer networks by tracking how the classifier margin evolves under (i) kernel-like lazy training and (ii) feature-learning mean-field training. Prove conditions under which the learned decision boundary converges to an RKHS max-margin separator (NTK regime) versus a different margin notion induced by evolving features (DD regime), and quantify the transition in terms of scaling.\n\n5. **Intermittency Mechanisms: Predicting Plateau Lengths from Spectral Gaps and Teacher Harmonics**  \nIn the single-index setting, relate plateau durations to gaps in the relevant kernel/feature operator spectrum (e.g., NTK principal components vs. mean-field effective operator). Produce explicit asymptotic formulas for plateau lengths and drop times as functions of (a) activation, (b) data aspect ratio, and (c) teacher expansion coefficients, then test predictions by controlled synthetic experiments.\n\n6. **Scaling Laws that Optimize Feature Learning While Avoiding Lazy-Training Degradation in ConvNets/MLPs**  \nBuild on the \u201clazy training is a scaling choice\u201d insight by designing and analyzing intermediate scalings that maximize representation movement subject to stability constraints. Provide an actionable recipe for selecting width-dependent learning rates/initialization variances that provably yield nontrivial DD feature learning while maintaining exponential loss decrease, and evaluate on vision benchmarks where lazy training underperforms.\n\n7. **A Coupled NTK\u2013DD Hybrid Model for Finite-Width Training Dynamics**  \nPropose a hybrid dynamical model where early training is governed by nearly-constant NTK dynamics, but later training transitions to mean-field DD with a slowly evolving kernel. Fit the model\u2019s coupling parameters using finite-width statistics (e.g., empirical kernel drift and parameter displacement norms) and show it predicts training curves and generalization better than pure NTK or pure mean-field approximations.\n\n8. **Stochastic Gradient Noise as an Effective Temperature in Distributional Dynamics**  \nExtend noisy SGD convergence results in the mean-field PDE framework by explicitly incorporating minibatch-induced diffusion and deriving an effective \u201ctemperature\u201d that depends on batch size and data distribution. Quantify how this temperature changes barrier crossing and phase transitions between learning plateaus, and propose a principled batch-size schedule that accelerates transitions without sacrificing final generalization.\n\n9. **Projected/Constrained Training to Selectively Accelerate Late-Phase Components**  \nMotivated by projected SGD analyses in time-scales work, design projections (or parameter subspace constraints) that preserve early-phase simple components while selectively amplifying hard-to-learn modes (e.g., higher teacher harmonics). Provide theory showing the projection modifies the singular perturbation structure to shorten specific plateaus, and implement a practical algorithm using low-rank or spherical constraints on second-layer weights.\n\n10. **Multi-Class and Multi-Index Extensions: Canonical Learning Order in High Dimensions**  \nGeneralize the \u201ccanonical learning order\u201d scenario from single-index regression to multi-class classification and multi-index teachers, connecting to separable-data implicit bias in the multi-class case. Derive predicted sequences of learned components (classwise margins or teacher subspace directions), identify when the order breaks (counterexamples), and map these failures to measurable quantities such as class imbalance and inter-index correlation.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Bridging Mean-Field PDE and NTK via a Controlled \u201cFeature-Learning\u201d Expansion\nDevelop a perturbative framework that starts from the NTK (lazy) limit and adds the first nontrivial mean-field correction",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "A Timescale-Resolved Theory of Early Stopping Beyond the Lazy Regime\nUse the singularly perturbed dynamical-system viewpoint (learning time-scales) to formalize \u201coptimal stopping\u201d rules that differ ac",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Universal Dynamical Mean Field Theory for Two-Layer Networks with Random Design and Nonlinear Targets\nCombine the AMP-based high-dimensional flow characterization (random data universality) with two-l",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Implicit Bias in Two-Layer Classification: Margin Dynamics Across Lazy-to-Mean-Field Regimes\nExtend max-margin implicit bias results (separable logistic) to two-layer networks by tracking how the clas",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Intermittency Mechanisms: Predicting Plateau Lengths from Spectral Gaps and Teacher Harmonics\nIn the single-index setting, relate plateau durations to gaps in the relevant kernel/feature operator spec",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Scaling Laws that Optimize Feature Learning While Avoiding Lazy-Training Degradation in ConvNets/MLPs\nBuild on the \u201clazy training is a scaling choice\u201d insight by designing and analyzing intermediate s",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "A Coupled NTK\u2013DD Hybrid Model for Finite-Width Training Dynamics\nPropose a hybrid dynamical model where early training is governed by nearly-constant NTK dynamics, but later training transitions to me",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Stochastic Gradient Noise as an Effective Temperature in Distributional Dynamics\nExtend noisy SGD convergence results in the mean-field PDE framework by explicitly incorporating minibatch-induced diff",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Projected/Constrained Training to Selectively Accelerate Late-Phase Components\nMotivated by projected SGD analyses in time-scales work, design projections (or parameter subspace constraints) that pres",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Multi-Class and Multi-Index Extensions: Canonical Learning Order in High Dimensions\nGeneralize the \u201ccanonical learning order\u201d scenario from single-index regression to multi-class classification and mu",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 39,
      "paper_title": "1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities",
      "contribution": "Demonstrates that dramatically increasing network depth (up to 1024 layers) in a self-supervised, goal-conditioned contrastive RL setup yields large quantitative gains and qualitatively new goal-reaching behaviors that shallower agents cannot discover.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11630,
      "output_tokens": 1152,
      "predecessor_details": [
        {
          "success": true,
          "title": "CURL: Contrastive Unsupervised Representations for ...",
          "url": "https://arxiv.org/abs/2004.04136",
          "content": "[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)\n\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\n# Computer Science > Machine Learning\n\n**arXiv:2004.04136** (cs)\n\n\\[Submitted on 8 Apr 2020 ( [v1](https://arxiv.org/abs/2004.04136v1)), last revised 21 Sep 2020 (this version, v4)\\]\n\n# Title:CURL: Contrastive Unsupervised Representations for Reinforcement Learning\n\nAuthors: [Aravind Srinivas](https://arxiv.org/search/cs?searchtype=author&query=Aravind), [Michael Laskin](https://arxiv.org/search/cs?searchtype=author&query=Laskin,+M), [Pieter Abbeel](https://arxiv.org/search/cs?searchtype=author&query=Abbeel,+P)\n\nView a PDF of the paper titled CURL: Contrastive Unsupervised Representations for Reinforcement Learning, by Aravind Srinivas and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2004.04136)\n\n> Abstract:We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at [this https URL](https://github.com/MishaLaskin/curl).\n\n| | |\n| --- | --- |\n| Comments: | First two authors contributed equally, website: [this https URL](https://mishalaskin.github.io/curl) code: [this https URL](https://github.com/MishaLaskin/curl) |\n| Subjects: | Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2004.04136](https://arxiv.org/abs/2004.04136) \\[cs.LG\\] |\n| (or [arXiv:2004.04136v4](https://arxiv.org/abs/2004.04136v4) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2004.04136](https://doi.org/10.48550/arXiv.2004.04136) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Michael Laskin \\[ [view email](https://arxiv.org/show-email/cb6b0ab7/2004.04136)\\] **[\\[v1\\]](https://arxiv.org/abs/2004.04136v1)**\nWed, 8 Apr 2020 17:40:43 UTC (4,829 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2004.04136v2)**\nTue, 28 Apr 2020 17:54:47 UTC (5,056 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2004.04136v3)**\nTue, 7 Jul 2020 16:37:04 UTC (10,150 KB)\n**\\[v4\\]**\nMon, 21 Sep 2020 15:34:30 UTC (5,115 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled CURL: Contrastive Unsupervised Representations for Reinforcement Learning, by Aravind Srinivas and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2004.04136)\n- [TeX Source](https://arxiv.org/src/2004.04136)\n- [Other Formats](https://arxiv.org/format/2004.04136)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2004.04136&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2004.04136&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2020-04](https://arxiv.org/list/cs.LG/2020-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2004.04136?context=cs) [cs.CV](https://arxiv.org/abs/2004.04136?context=cs.CV) [stat](https://arxiv.org/abs/2004.04136?context=stat) [stat.ML](https://arxiv.org/abs/2004.04136?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2004.04136)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2004.04136)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2004.04136)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2004.html#abs-2004-04136) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2004-04136)\n\n[Aravind Srinivas](https://dblp.uni-trier.de/search/author?author=Aravind%20Srinivas) [Pieter Abbeel](https://dblp.uni-trier.de/search/author?author=Pieter%20Abbeel)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2004.04136&description=CURL: Contrastive Unsupervised Representations for Reinforcement Learning) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2004.04136&title=CURL: Contrastive Unsupervised Representations for Reinforcement Learning)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2004.04136) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning (Srinivas et al., 2020)",
          "cleaned_query": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning"
        },
        {
          "success": true,
          "title": "A Simple Framework for Contrastive Learning of Visual ... - arXiv",
          "url": "https://arxiv.org/abs/2002.05709",
          "content": "[2002.05709] A Simple Framework for Contrastive Learning of Visual Representations\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2002.05709\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2002.05709**(cs)\n[Submitted on 13 Feb 2020 ([v1](https://arxiv.org/abs/2002.05709v1)), last revised 1 Jul 2020 (this version, v3)]\n# Title:A Simple Framework for Contrastive Learning of Visual Representations\nAuthors:[Ting Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+T),[Simon Kornblith](https://arxiv.org/search/cs?searchtype=author&amp;query=Kornblith,+S),[Mohammad Norouzi](https://arxiv.org/search/cs?searchtype=author&amp;query=Norouzi,+M),[Geoffrey Hinton](https://arxiv.org/search/cs?searchtype=author&amp;query=Hinton,+G)\nView a PDF of the paper titled A Simple Framework for Contrastive Learning of Visual Representations, by Ting Chen and 3 other authors\n[View PDF](https://arxiv.org/pdf/2002.05709)> > Abstract:\n> This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels. Comments:|ICML&#39;2020. Code and pretrained models at[this https URL](https://github.com/google-research/simclr)|\nSubjects:|Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)|\nCite as:|[arXiv:2002.05709](https://arxiv.org/abs/2002.05709)[cs.LG]|\n|(or[arXiv:2002.05709v3](https://arxiv.org/abs/2002.05709v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2002.05709](https://doi.org/10.48550/arXiv.2002.05709)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Ting Chen [[view email](https://arxiv.org/show-email/2290131d/2002.05709)]\n**[[v1]](https://arxiv.org/abs/2002.05709v1)**Thu, 13 Feb 2020 18:50:45 UTC (5,093 KB)\n**[[v2]](https://arxiv.org/abs/2002.05709v2)**Mon, 30 Mar 2020 15:32:51 UTC (5,047 KB)\n**[v3]**Wed, 1 Jul 2020 00:09:08 UTC (5,829 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled A Simple Framework for Contrastive Learning of Visual Representations, by Ting Chen and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2002.05709)\n* [TeX Source](https://arxiv.org/src/2002.05709)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2002.05709&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2002.05709&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2020-02](https://arxiv.org/list/cs.LG/2020-02)\nChange to browse by:\n[cs](https://arxiv.org/abs/2002.05709?context=cs)\n[cs.CV](https://arxiv.org/abs/2002.05709?context=cs.CV)\n[stat](https://arxiv.org/abs/2002.05709?context=stat)\n[stat.ML](https://arxiv.org/abs/2002.05709?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2002.05709)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2002.05709)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2002.05709)\n### [15 blog links](https://arxiv.org/tb/2002.05709)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2002.html#abs-2002-05709)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2002-05709)\n[Ting Chen]()\n[Simon Kornblith]()\n[Mohammad Norouzi]()\n[Geoffrey E. Hinton]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv featur",
          "original_query": "A Simple Framework for Contrastive Learning of Visual Representations (SimCLR) (Chen et al., 2020)",
          "cleaned_query": "A Simple Framework for Contrastive Learning of Visual Representations (SimCLR)"
        },
        {
          "success": true,
          "title": "[1707.01495] Hindsight Experience Replay",
          "url": "https://arxiv.org/abs/1707.01495",
          "content": "[1707.01495] Hindsight Experience Replay[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1707.01495\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1707.01495**(cs)\n[Submitted on 5 Jul 2017 ([v1](https://arxiv.org/abs/1707.01495v1)), last revised 23 Feb 2018 (this version, v3)]\n# Title:Hindsight Experience Replay\nAuthors:[Marcin Andrychowicz](https://arxiv.org/search/cs?searchtype=author&amp;query=Andrychowicz,+M),[Filip Wolski](https://arxiv.org/search/cs?searchtype=author&amp;query=Wolski,+F),[Alex Ray](https://arxiv.org/search/cs?searchtype=author&amp;query=Ray,+A),[Jonas Schneider](https://arxiv.org/search/cs?searchtype=author&amp;query=Schneider,+J),[Rachel Fong](https://arxiv.org/search/cs?searchtype=author&amp;query=Fong,+R),[Peter Welinder](https://arxiv.org/search/cs?searchtype=author&amp;query=Welinder,+P),[Bob McGrew](https://arxiv.org/search/cs?searchtype=author&amp;query=McGrew,+B),[Josh Tobin](https://arxiv.org/search/cs?searchtype=author&amp;query=Tobin,+J),[Pieter Abbeel](https://arxiv.org/search/cs?searchtype=author&amp;query=Abbeel,+P),[Wojciech Zaremba](https://arxiv.org/search/cs?searchtype=author&amp;query=Zaremba,+W)\nView a PDF of the paper titled Hindsight Experience Replay, by Marcin Andrychowicz and 9 other authors\n[View PDF](https://arxiv.org/pdf/1707.01495)> > Abstract:\n> Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum.\n> We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. Subjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE); Robotics (cs.RO)|\nCite as:|[arXiv:1707.01495](https://arxiv.org/abs/1707.01495)[cs.LG]|\n|(or[arXiv:1707.01495v3](https://arxiv.org/abs/1707.01495v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1707.01495](https://doi.org/10.48550/arXiv.1707.01495)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Marcin Andrychowicz [[view email](https://arxiv.org/show-email/c7a00ded/1707.01495)]\n**[[v1]](https://arxiv.org/abs/1707.01495v1)**Wed, 5 Jul 2017 17:55:53 UTC (1,023 KB)\n**[[v2]](https://arxiv.org/abs/1707.01495v2)**Mon, 10 Jul 2017 18:35:33 UTC (1,023 KB)\n**[v3]**Fri, 23 Feb 2018 10:04:20 UTC (1,238 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Hindsight Experience Replay, by Marcin Andrychowicz and 9 other authors\n* [View PDF](https://arxiv.org/pdf/1707.01495)\n* [TeX Source](https://arxiv.org/src/1707.01495)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1707.01495&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1707.01495&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2017-07](https://arxiv.org/list/cs.LG/2017-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/1707.01495?context=cs)\n[cs.AI](https://arxiv.org/abs/1707.01495?context=cs.AI)\n[cs.NE](https://arxiv.org/abs/1707.01495?context=cs.NE)\n[cs.RO](https://arxiv.org/abs/1707.01495?context=cs.RO)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1707.01495)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1707.01495)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1707.01495)\n### [4 blog links](https://arxiv.org/tb/1707.01495)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1707.html#AndrychowiczWRS17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/AndrychowiczWRS17)\n[Marcin Andrychowicz]()\n[Filip Wolski]()\n[Alex Ray]()\n[Jonas Schneider]()\n[Rachel Fong]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*(",
          "original_query": "Hindsight Experience Replay (HER) (Andrychowicz et al., 2017)",
          "cleaned_query": "Hindsight Experience Replay (HER)"
        },
        {
          "success": true,
          "title": "Diversity is All You Need: Learning Skills without a Reward Function",
          "url": "https://arxiv.org/abs/1802.06070",
          "content": "# Computer Science > Artificial Intelligence\n\n**arXiv:1802.06070** (cs)\n\n\\[Submitted on 16 Feb 2018 ( [v1](https://arxiv.org/abs/1802.06070v1)), last revised 9 Oct 2018 (this version, v6)\\]\n\n# Title:Diversity is All You Need: Learning Skills without a Reward Function\n\nAuthors: [Benjamin Eysenbach](https://arxiv.org/search/cs?searchtype=author&query=Eysenbach,+B), [Abhishek Gupta](https://arxiv.org/search/cs?searchtype=author&query=Gupta,+A), [Julian Ibarz](https://arxiv.org/search/cs?searchtype=author&query=Ibarz,+J), [Sergey Levine](https://arxiv.org/search/cs?searchtype=author&query=Levine,+S)\n\nView a PDF of the paper titled Diversity is All You Need: Learning Skills without a Reward Function, by Benjamin Eysenbach and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/1802.06070)\n\n> Abstract:Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.\n\n| | |\n| --- | --- |\n| Comments: | Videos and code for our experiments are available at: [this https URL](https://sites.google.com/view/diayn) |\n| Subjects: | Artificial Intelligence (cs.AI); Robotics (cs.RO) |\n| Cite as: | [arXiv:1802.06070](https://arxiv.org/abs/1802.06070) \\[cs.AI\\] |\n| (or [arXiv:1802.06070v6](https://arxiv.org/abs/1802.06070v6) \\[cs.AI\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1802.06070](https://doi.org/10.48550/arXiv.1802.06070) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Benjamin Eysenbach \\[ [view email](https://arxiv.org/show-email/11404bcf/1802.06070)\\] **[\\[v1\\]](https://arxiv.org/abs/1802.06070v1)**\nFri, 16 Feb 2018 18:57:57 UTC (6,821 KB)\n**[\\[v2\\]](https://arxiv.org/abs/1802.06070v2)**\nTue, 20 Feb 2018 18:45:19 UTC (7,288 KB)\n**[\\[v3\\]](https://arxiv.org/abs/1802.06070v3)**\nFri, 23 Feb 2018 18:56:13 UTC (7,466 KB)\n**[\\[v4\\]](https://arxiv.org/abs/1802.06070v4)**\nThu, 1 Mar 2018 17:10:25 UTC (6,767 KB)\n**[\\[v5\\]](https://arxiv.org/abs/1802.06070v5)**\nWed, 6 Jun 2018 23:07:09 UTC (7,600 KB)\n**\\[v6\\]**\nTue, 9 Oct 2018 23:19:52 UTC (8,523 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Diversity is All You Need: Learning Skills without a Reward Function, by Benjamin Eysenbach and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/1802.06070)\n- [TeX Source](https://arxiv.org/src/1802.06070)\n- [Other Formats](https://arxiv.org/format/1802.06070)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.AI\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1802.06070&function=prev&context=cs.AI)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1802.06070&function=next&context=cs.AI)\n\n[new](https://arxiv.org/list/cs.AI/new) \\| [recent](https://arxiv.org/list/cs.AI/recent) \\| [2018-02](https://arxiv.org/list/cs.AI/2018-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1802.06070?context=cs) [cs.RO](https://arxiv.org/abs/1802.06070?context=cs.RO)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1802.06070)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1802.06070)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1802.06070)\n\n### [2 blog links](https://arxiv.org/tb/1802.06070)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1802.html#abs-1802-06070) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1802-06070)\n\n[Benjamin Eysenbach](https://dblp.uni-trier.de/search/author?author=Benjamin%20Eysenbach) [Abhishek Gupta](https://dblp.uni-trier.de/search/author?author=Abhishek%20Gupta) [Julian Ibarz](https://dblp.uni-trier.de/search/author?author=Julian%20Ibarz) [Sergey Levine](https://dblp.uni-trier.de/search/author?author=Sergey%20Levine)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1802.06070) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Diversity Is All You Need (DIAYN) (Eysenbach et al., 2019)",
          "cleaned_query": "Diversity Is All You Need (DIAYN)"
        },
        {
          "success": true,
          "title": "Universal Value Function Approximators",
          "url": "https://proceedings.mlr.press/v37/schaul15.html",
          "content": "\\[ [edit](https://github.com/mlresearch/v37/edit/gh-pages/_posts/2015-06-01-schaul15.md)\\]\n\n# Universal Value Function Approximators\n\nTom Schaul,\u00a0Daniel Horgan,\u00a0Karol Gregor,\u00a0David Silver\n\n_Proceedings of the 32nd International Conference on Machine Learning_,\u00a0PMLR 37:1312-1320,\u00a02015.\n\n#### Abstract\n\nValue functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters \u03b8. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.\n\n#### Cite this Paper\n\n* * *\n\nBibTeX\n\n`@InProceedings{pmlr-v37-schaul15,\ntitle = {Universal Value Function Approximators},\nauthor = {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},\nbooktitle = {Proceedings of the 32nd International Conference on Machine Learning},\npages = {1312--1320},\nyear = {2015},\neditor = {Bach, Francis and Blei, David},\nvolume = {37},\nseries = {Proceedings of Machine Learning Research},\naddress = {Lille, France},\nmonth = {07--09 Jul},\npublisher = {PMLR},\npdf = {http://proceedings.mlr.press/v37/schaul15.pdf},\nurl = {https://proceedings.mlr.press/v37/schaul15.html},\nabstract = {Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters \u03b8. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.}\n}`\n\nCopy to ClipboardDownload\n\nEndnote\n\n`%0 Conference Paper\n%T Universal Value Function Approximators\n%A Tom Schaul\n%A Daniel Horgan\n%A Karol Gregor\n%A David Silver\n%B Proceedings of the 32nd International Conference on Machine Learning\n%C Proceedings of Machine Learning Research\n%D 2015\n%E Francis Bach\n%E David Blei\n%F pmlr-v37-schaul15\n%I PMLR\n%P 1312--1320\n%U https://proceedings.mlr.press/v37/schaul15.html\n%V 37\n%X Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters \u03b8. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.`\n\nCopy to ClipboardDownload\n\nRIS\n\n`TY - CPAPER\nTI - Universal Value Function Approximators\nAU - Tom Schaul\nAU - Daniel Horgan\nAU - Karol Gregor\nAU - David Silver\nBT - Proceedings of the 32nd International Conference on Machine Learning\nDA - 2015/06/01\nED - Francis Bach\nED - David Blei\nID - pmlr-v37-schaul15\nPB - PMLR\nDP - Proceedings of Machine Learning Research\nVL - 37\nSP - 1312\nEP - 1320\nL1 - http://proceedings.mlr.press/v37/schaul15.pdf\nUR - https://proceedings.mlr.press/v37/schaul15.html\nAB - Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters \u03b8. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.\nER -`\n\nCopy to ClipboardDownload\n\nAPA\n\n`Schaul, T., Horgan, D., Gregor, K. & Silver, D.. (2015). Universal Value Function Approximators. Proceedings of the 32nd International Conference on Machine Learning, in Proceedings of Machine Learning Research 37:1312-1320 Available from https://proceedings.mlr.press/v37/schaul15.html.`\n\nCopy to ClipboardDownload\n\n* * *\n\n#### Related Material\n\n- [Download PDF](http://proceedings.mlr.press/v37/schaul15.pdf)\n- [Supplementary Material](http://proceedings.mlr.press/v37/schaul15-supp.pdf)",
          "original_query": "Universal Value Function Approximators (UVFA) (Schaul et al., 2015)",
          "cleaned_query": "Universal Value Function Approximators (UVFA)"
        },
        {
          "success": true,
          "title": "Relative Scaling Laws for LLMs",
          "url": "https://arxiv.org/abs/2510.24626",
          "content": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
          "original_query": "Scaling Laws for Neural Language Models (Kaplan et al., 2020)",
          "cleaned_query": "Scaling Laws for Neural Language Models"
        }
      ],
      "generated_ideas": [
        "Goal-Conditioned CURL-UVFA with Contrastive Goal Embeddings\nCombine UVFAs with CURL by learning a joint latent space where goals (images, states, or achieved-goal observations) are embedded via contrastive learning and used as UVFA conditioning. Implement a value function \\(Q(z_s, z_g, a)\\) where \\(z_s\\) is a CURL encoder output and \\(z_g\\) is a goal encoder trained with positive pairs from goal relabeling (HER-style) and negatives from other goals in the replay buffer, targeting improved generalization to unseen goals.",
        "Contrastive Hindsight Experience Replay (C-HER) for Sparse Reward Pixels\nExtend HER by adding an auxiliary contrastive loss that pulls together embeddings of states that are \u201chindsight-successful\u201d for the relabeled goal and pushes away non-achieving states. Actionably, for each relabeled transition \\((o_t, g')\\), treat future achieved observations matching \\(g'\\) as positives for \\(o_t\\) under goal-conditioned encoding, improving representation learning and enabling faster sparse-reward learning from pixels.",
        "DIAYN Skills as Structured Negatives/Positives for CURL\nUse DIAYN\u2019s discovered skill labels \\(z\\) to structure contrastive pairs: treat two augmentations of observations from the same skill as positives, and observations from different skills as hard negatives, creating skill-aware contrastive learning. Evaluate whether this yields representations that improve hierarchical RL where a high-level policy selects DIAYN skills and a low-level policy uses CURL features.",
        "Offline Pretraining: SimCLR-to-CURL Transfer for Replay-Only RL\nPretrain an encoder on a large offline dataset of frames using SimCLR, then fine-tune with CURL jointly with an off-policy agent using only a fixed replay buffer (no new data collection). The contribution is a controlled study and method for stabilizing offline pixel-based RL by decoupling \u201crepresentation learning capacity\u201d (SimCLR) from \u201ccontrol grounding\u201d (CURL), with ablations on projection heads, batch size, and fine-tuning schedules.",
        "Goal Relabeling as Data Augmentation: HER-Driven Positive Pair Mining for SimCLR/CURL\nReinterpret HER as a mechanism to define semantic positives: two observations are positives if they correspond to the same achieved goal (e.g., same object configuration) even if visually different due to viewpoint/noise. Implement a goal-consistency classifier (or use environment-provided achieved-goal vectors when available) to mine positives inside replay buffers, then train contrastive encoders that are invariant to irrelevant variation but sensitive to goal features.",
        "Universal Skill-Value Functions: UVFA over DIAYN Latents for Rapid Task Solving\nTrain a UVFA \\(Q(s, z)\\) where \\(z\\) is the DIAYN skill index (or continuous latent), enabling fast evaluation and selection of skills to reach new external goals without re-training the low-level policy. Concretely, learn \\(Q\\) off-policy with relabeled goals (HER) and then perform planning/search in skill space (over \\(z\\)) to compose behaviors for unseen goal tasks.",
        "Relative Scaling Laws for Representation-First RL (Encoder/Data/Compute Tradeoffs)\nDevelop \u201crelative scaling laws\u201d specifically for pixel-based off-policy RL with contrastive encoders: quantify how performance scales with (i) encoder size, (ii) replay size, (iii) contrastive batch size/negatives, and (iv) environment steps, mirroring LLM relative scaling methodology. Deliver a predictive model that recommends optimal allocation under a fixed compute budget (e.g., when to spend compute on more contrastive updates vs more env interaction).",
        "Adaptive Projection Heads for Control: Task-Conditional SimCLR/CURL Projection Tuning\nBuild on SimCLR\u2019s projection-head importance by making the projection head adaptive to the RL signal: condition the projection head on action or temporal context (e.g., \\(h(z_s, a)\\)) while keeping the encoder shared. Test whether action-conditioned projection improves disentangling controllable factors, leading to better Q-learning stability and sample efficiency in environments with strong distractors.",
        "Hierarchical Exploration Curriculum: DIAYN + HER + CURL in a Single Replay Buffer\nCreate an integrated algorithm where (a) DIAYN learns diverse skills to explore, (b) HER relabels trajectories to produce goal-reaching pseudo-supervision, and (c) CURL learns invariances from pixel observations across both exploration and relabeled goal-reaching data. The key contribution is a single unified replay buffer and training loop that transitions from unsupervised skill discovery to goal-conditioned control without changing architectures, validated on long-horizon sparse tasks (e.g., pick-and-place with visual distractors)."
      ],
      "generated_ideas_raw": "1. **SimCLR-Augmented CURL for RL-Specific View Generation**  \n   Learn an augmentation policy (or distribution) tailored to each environment by optimizing CURL\u2019s contrastive objective while preserving action-relevant factors, extending SimCLR\u2019s finding that augmentation composition is critical. Concretely, train a small augmentation-controller network that selects/correlates transforms (crop, color jitter, blur, cutout) conditioned on the observation and measure downstream sample-efficiency on DMControl/Atari at 100K steps.\n\n2. **Goal-Conditioned CURL-UVFA with Contrastive Goal Embeddings**  \n   Combine UVFAs with CURL by learning a joint latent space where goals (images, states, or achieved-goal observations) are embedded via contrastive learning and used as UVFA conditioning. Implement a value function \\(Q(z_s, z_g, a)\\) where \\(z_s\\) is a CURL encoder output and \\(z_g\\) is a goal encoder trained with positive pairs from goal relabeling (HER-style) and negatives from other goals in the replay buffer, targeting improved generalization to unseen goals.\n\n3. **Contrastive Hindsight Experience Replay (C-HER) for Sparse Reward Pixels**  \n   Extend HER by adding an auxiliary contrastive loss that pulls together embeddings of states that are \u201chindsight-successful\u201d for the relabeled goal and pushes away non-achieving states. Actionably, for each relabeled transition \\((o_t, g')\\), treat future achieved observations matching \\(g'\\) as positives for \\(o_t\\) under goal-conditioned encoding, improving representation learning and enabling faster sparse-reward learning from pixels.\n\n4. **DIAYN Skills as Structured Negatives/Positives for CURL**  \n   Use DIAYN\u2019s discovered skill labels \\(z\\) to structure contrastive pairs: treat two augmentations of observations from the same skill as positives, and observations from different skills as hard negatives, creating skill-aware contrastive learning. Evaluate whether this yields representations that improve hierarchical RL where a high-level policy selects DIAYN skills and a low-level policy uses CURL features.\n\n5. **Offline Pretraining: SimCLR-to-CURL Transfer for Replay-Only RL**  \n   Pretrain an encoder on a large offline dataset of frames using SimCLR, then fine-tune with CURL jointly with an off-policy agent using only a fixed replay buffer (no new data collection). The contribution is a controlled study and method for stabilizing offline pixel-based RL by decoupling \u201crepresentation learning capacity\u201d (SimCLR) from \u201ccontrol grounding\u201d (CURL), with ablations on projection heads, batch size, and fine-tuning schedules.\n\n6. **Goal Relabeling as Data Augmentation: HER-Driven Positive Pair Mining for SimCLR/CURL**  \n   Reinterpret HER as a mechanism to define semantic positives: two observations are positives if they correspond to the same achieved goal (e.g., same object configuration) even if visually different due to viewpoint/noise. Implement a goal-consistency classifier (or use environment-provided achieved-goal vectors when available) to mine positives inside replay buffers, then train contrastive encoders that are invariant to irrelevant variation but sensitive to goal features.\n\n7. **Universal Skill-Value Functions: UVFA over DIAYN Latents for Rapid Task Solving**  \n   Train a UVFA \\(Q(s, z)\\) where \\(z\\) is the DIAYN skill index (or continuous latent), enabling fast evaluation and selection of skills to reach new external goals without re-training the low-level policy. Concretely, learn \\(Q\\) off-policy with relabeled goals (HER) and then perform planning/search in skill space (over \\(z\\)) to compose behaviors for unseen goal tasks.\n\n8. **Relative Scaling Laws for Representation-First RL (Encoder/Data/Compute Tradeoffs)**  \n   Develop \u201crelative scaling laws\u201d specifically for pixel-based off-policy RL with contrastive encoders: quantify how performance scales with (i) encoder size, (ii) replay size, (iii) contrastive batch size/negatives, and (iv) environment steps, mirroring LLM relative scaling methodology. Deliver a predictive model that recommends optimal allocation under a fixed compute budget (e.g., when to spend compute on more contrastive updates vs more env interaction).\n\n9. **Adaptive Projection Heads for Control: Task-Conditional SimCLR/CURL Projection Tuning**  \n   Build on SimCLR\u2019s projection-head importance by making the projection head adaptive to the RL signal: condition the projection head on action or temporal context (e.g., \\(h(z_s, a)\\)) while keeping the encoder shared. Test whether action-conditioned projection improves disentangling controllable factors, leading to better Q-learning stability and sample efficiency in environments with strong distractors.\n\n10. **Hierarchical Exploration Curriculum: DIAYN + HER + CURL in a Single Replay Buffer**  \n   Create an integrated algorithm where (a) DIAYN learns diverse skills to explore, (b) HER relabels trajectories to produce goal-reaching pseudo-supervision, and (c) CURL learns invariances from pixel observations across both exploration and relabeled goal-reaching data. The key contribution is a single unified replay buffer and training loop that transitions from unsupervised skill discovery to goal-conditioned control without changing architectures, validated on long-horizon sparse tasks (e.g., pick-and-place with visual distractors).",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Goal-Conditioned CURL-UVFA with Contrastive Goal Embeddings\nCombine UVFAs with CURL by learning a joint latent space where goals (images, states, or achieved-goal observations) are embedded via contra",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Contrastive Hindsight Experience Replay (C-HER) for Sparse Reward Pixels\nExtend HER by adding an auxiliary contrastive loss that pulls together embeddings of states that are \u201chindsight-successful\u201d for",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "DIAYN Skills as Structured Negatives/Positives for CURL\nUse DIAYN\u2019s discovered skill labels \\(z\\) to structure contrastive pairs: treat two augmentations of observations from the same skill as positiv",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Offline Pretraining: SimCLR-to-CURL Transfer for Replay-Only RL\nPretrain an encoder on a large offline dataset of frames using SimCLR, then fine-tune with CURL jointly with an off-policy agent using o",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Goal Relabeling as Data Augmentation: HER-Driven Positive Pair Mining for SimCLR/CURL\nReinterpret HER as a mechanism to define semantic positives: two observations are positives if they correspond to ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Universal Skill-Value Functions: UVFA over DIAYN Latents for Rapid Task Solving\nTrain a UVFA \\(Q(s, z)\\) where \\(z\\) is the DIAYN skill index (or continuous latent), enabling fast evaluation and selec",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Relative Scaling Laws for Representation-First RL (Encoder/Data/Compute Tradeoffs)\nDevelop \u201crelative scaling laws\u201d specifically for pixel-based off-policy RL with contrastive encoders: quantify how pe",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Adaptive Projection Heads for Control: Task-Conditional SimCLR/CURL Projection Tuning\nBuild on SimCLR\u2019s projection-head importance by making the projection head adaptive to the RL signal: condition th",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Hierarchical Exploration Curriculum: DIAYN + HER + CURL in a Single Replay Buffer\nCreate an integrated algorithm where (a) DIAYN learns diverse skills to explore, (b) HER relabels trajectories to prod",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 40,
      "paper_title": "Depth-Bounds for Neural Networks via the Braid Arrangement",
      "contribution": "For ReLU (and related maxout) networks compatible with the braid fan, the paper proves a non-constant lower bound \u2126(log log d) on the number of hidden layers needed to compute the maximum of d numbers, gives a combinatorial proof that max of 5 numbers needs three hidden layers under the same compatibility assumption, and supplies a tighter constructive upper bound in the maxout setting (rank-3 followed by rank-2 suffices for max of 7).",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 4,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 9432,
      "output_tokens": 910,
      "predecessor_details": [
        {
          "success": true,
          "title": "Adaptive control of the Wang-Sun four-scroll chaotic system with ...",
          "url": "https://ieeexplore.ieee.org/document/7238377/",
          "content": "Adaptive control of the Wang-Sun four-scroll chaotic system with unknown parameters \\| IEEE Conference Publication \\| IEEE Xplore\n\n### IEEE Account\n\n- [Change Username/Password](https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Update Address](https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n\n### Purchase Details\n\n- [Payment Options](https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Order History](https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [View Purchased Documents](https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp)\n\n### Profile Information\n\n- [Communications Preferences](https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Profession and Education](https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Technical Interests](https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n\n### Need Help?\n\n- **US & Canada:** +1 800 678 4333\n- **Worldwide:** +1 732 981 0060\n\n- [Contact & Support](https://ieeexplore.ieee.org/xpl/contact)\n\n- [About IEEE _Xplore_](https://ieeexplore.ieee.org/Xplorehelp/about-ieee-xplore.html)\n- [Contact Us](https://ieeexplore.ieee.org/xpl/contact)\n- [Help](https://ieeexplore.ieee.org/Xplorehelp/Help_start.html)\n- [Accessibility](https://ieeexplore.ieee.org/Xplorehelp/accessibility-statement.html)\n- [Terms of Use](https://ieeexplore.ieee.org/Xplorehelp/Help_Terms_of_Use.html)\n- [Nondiscrimination Policy](http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html)\n- [Sitemap](https://ieeexplore.ieee.org/xpl/sitemap.jsp)\n- [Privacy & Opting Out of Cookies](http://www.ieee.org/about/help/security_privacy.html)\n\nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.\n\n\u00a9 Copyright 2024 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.",
          "original_query": "Wang and Sun (2005) \u2014 reduction of CPWL representation to maxima of affine terms",
          "cleaned_query": "Wang and Sun"
        },
        {
          "success": true,
          "title": "Metadata Shaping: A Simple Approach for Knowledge-Enhanced ...",
          "url": "https://aclanthology.org/2022.findings-acl.137/",
          "content": "## [Metadata Shaping: A Simple Approach for Knowledge-Enhanced Language Models](https://aclanthology.org/2022.findings-acl.137.pdf)\n\n[Simran Arora](https://aclanthology.org/people/simran-arora/),\n[Sen Wu](https://aclanthology.org/people/sen-wu/),\n[Enci Liu](https://aclanthology.org/people/enci-liu/),\n[Christopher Re](https://aclanthology.org/people/christopher-re/)\n\n##### Abstract\n\nPopular language models (LMs) struggle to capture knowledge about rare tail facts and entities. Since widely used systems such as search and personal-assistants must support the long tail of entities that users ask about, there has been significant effort towards enhancing these base LMs with factual knowledge. We observe proposed methods typically start with a base LM and data that has been annotated with entity metadata, then change the model, by modifying the architecture or introducing auxiliary loss terms to better capture entity knowledge. In this work, we question this typical process and ask to what extent can we match the quality of model modifications, with a simple alternative: using a base LM and only changing the data. We propose metadata shaping, a method which inserts substrings corresponding to the readily available entity metadata, e.g. types and descriptions, into examples at train and inference time based on mutual information. Despite its simplicity, metadata shaping is quite effective. On standard evaluation benchmarks for knowledge-enhanced LMs, the method exceeds the base-LM baseline by an average of 4.3 F1 points and achieves state-of-the-art results. We further show the gains are on average 4.4x larger for the slice of examples containing tail vs. popular entities.\n\nAnthology ID:2022.findings-acl.137Volume:[Findings of the Association for Computational Linguistics: ACL 2022](https://aclanthology.org/volumes/2022.findings-acl/)Month:MayYear:2022Address:Dublin, IrelandEditors:[Smaranda Muresan](https://aclanthology.org/people/smaranda-muresan/),\n[Preslav Nakov](https://aclanthology.org/people/preslav-nakov/),\n[Aline Villavicencio](https://aclanthology.org/people/aline-villavicencio/)Venue:[Findings](https://aclanthology.org/venues/findings/)SIG:Publisher:Association for Computational LinguisticsNote:Pages:1733\u20131745Language:URL:[https://aclanthology.org/2022.findings-acl.137/](https://aclanthology.org/2022.findings-acl.137/)DOI:[10.18653/v1/2022.findings-acl.137](https://doi.org/10.18653/v1/2022.findings-acl.137)Bibkey:arora-etal-2022-metadataCite (ACL):Simran Arora, Sen Wu, Enci Liu, and Christopher Re. 2022. [Metadata Shaping: A Simple Approach for Knowledge-Enhanced Language Models](https://aclanthology.org/2022.findings-acl.137/). In _Findings of the Association for Computational Linguistics: ACL 2022_, pages 1733\u20131745, Dublin, Ireland. Association for Computational Linguistics.Cite (Informal):[Metadata Shaping: A Simple Approach for Knowledge-Enhanced Language Models](https://aclanthology.org/2022.findings-acl.137/) (Arora et al., Findings 2022)Copy Citation:BibTeXMarkdownMODS XMLEndnoteMore options\u2026PDF:[https://aclanthology.org/2022.findings-acl.137.pdf](https://aclanthology.org/2022.findings-acl.137.pdf)Video:[https://aclanthology.org/2022.findings-acl.137.mp4](https://aclanthology.org/2022.findings-acl.137.mp4)\n\n[PDF](https://aclanthology.org/2022.findings-acl.137.pdf) [Cite](https://aclanthology.org/aclanthology.org) [Search](https://www.semanticscholar.org/search?q=Metadata+Shaping%3A+A+Simple+Approach+for+Knowledge-Enhanced+Language+Models) [Video](https://aclanthology.org/2022.findings-acl.137.mp4) [Fix data](https://aclanthology.org/aclanthology.org)",
          "original_query": "Arora et al. (2018) \u2014 ReLU networks exactly represent CPWL functions and a log2(d+1) upper bound",
          "cleaned_query": "Arora et al."
        },
        {
          "success": true,
          "title": "AI will be a thing in Design and Research, but there are some issues ...",
          "url": "https://uxdesign.cc/ai-will-be-a-thing-in-design-and-research-but-there-are-some-issues-to-resolve-first-7bc6405fbe7b",
          "content": "Maximilian Speicher AI will be a thing in Design and Research, but there are some issues to resolve first https://uxdesign.cc/ai-will-be-a-thing-in-design-and-research-but-there-are-some-issues-to-resolve-first-7bc6405fbe7b\nAI will be a thing in Design and Research, but there are some issues to resolve first\nMaximilian Speicher\n2023-01-05T18:05:00Z\n# AI will be a thing in Design and Research, but there are some issues to resolve first\n[Maximilian Speicher](https://maxspeicher.medium.com/?source=post_page---byline--7bc6405fbe7b---------------------------------------)\n8 min read\nAug 5, 2022\nShare\n**TL;DR:** _To what extent do digital designers and user researchers in industry make use of AI-powered systems to support their work? The answer seems to be \u201cnot much,\u201d as we found in a survey with 34 practitioners. In general, there seems to be little awareness of the specific advantages AI can bring to a design process and which tools are already out there, despite significant issues with existing processes that could be well mitigated with the help of AI. However, designers and researchers are very open to the topic and would appreciate AI-powered systems to support them in both ideation and evaluation. Providing practitioners with such systems that are easily accessible and demonstrate added value holds great opportunities._\n> This article describes research that has been conducted in collaboration with [Maxim Bakaev](https://scholar.google.com/citations?user=tJg-WxMAAAAJ&hl=en), [Johanna Jagow](https://www.linkedin.com/in/johanna-jagow-208650116/), and [Sebastian Heil](https://vsr.informatik.tu-chemnitz.de/about/people/heseba/). The [research paper](https://link.springer.com/chapter/10.1007/978-3-031-09917-5_28)[] was published at the 2022 International Conference on Web Engineering (ICWE).\nPress enter or click to view image in full size\nPhoto by [DeepMind](https://unsplash.com/@deepmind?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/ai?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)\nD **esign and research are a thing.** [**Artificial intelligence**](https://en.wikipedia.org/wiki/Artificial_intelligence) **(AI) is a thing.** But they\u2019re not quite a thing together yet. There have been massive advances in AI over the past decades. Yet, designers and user researchers in the digital industry still often execute human-centered design processes manually and tediously \u2014 from clicking\u2026\nFollow\n[**Published in UX Collective**](https://uxdesign.cc/?source=post_page---post_publication_info--7bc6405fbe7b---------------------------------------)\n[565K followers](https://uxdesign.cc/followers?source=post_page---post_publication_info--7bc6405fbe7b---------------------------------------)\n\u00b7 [Last published\u00a018 hours ago]\nWe believe designers are thinkers as much as they are makers. [https://linktr.ee/uxc](https://linktr.ee/uxc)\nFollow\n[**Written by Maximilian Speicher**](https://maxspeicher.medium.com/?source=post_page---post_author_info--7bc6405fbe7b---------------------------------------)\n[1K followers](https://maxspeicher.medium.com/followers?source=post_page---post_author_info--7bc6405fbe7b---------------------------------------)\n\u00b7 [274 following](https://medium.com/@maxspeicher/following?source=post_page---post_author_info--7bc6405fbe7b---------------------------------------)\nI write about leadership, strategy, and anything product & UX \u2022 Doctor of Computer Science \u2022 ex University of Michigan \u2022 [maxspeicher.com/newsletter](http://maxspeicher.com/newsletter)\n## No responses yet\n[Help](https://help.medium.com/hc/en-us?source=post_page-----7bc6405fbe7b---------------------------------------)\n[Status](https://status.medium.com/?source=post_page-----7bc6405fbe7b---------------------------------------)\n[About](https://medium.com/about?autoplay=1&source=post_page-----7bc6405fbe7b---------------------------------------)\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----7bc6405fbe7b---------------------------------------)\n[Press](mailto:pressinquiries@medium.com)\n[Blog](https://blog.medium.com/?source=post_page-----7bc6405fbe7b---------------------------------------)\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7bc6405fbe7b---------------------------------------)\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----7bc6405fbe7b---------------------------------------)\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7bc6405fbe7b---------------------------------------)\n[Text to speech](https://speechify.com/medium?source=post_page-----7bc6405fbe7b---------------------------------------)",
          "original_query": "Bakaev et al. (2025b) \u2014 improved upper bound (\u2308log3(d\u22121)\u2309+1) for CPWL representation",
          "cleaned_query": "Bakaev et al.",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "ICPhS-15 Abstract: Hertrich et al.",
          "url": "https://www.internationalphoneticassociation.org/icphs-proceedings/ICPhS2003/p15_1715.html",
          "content": "\n To assess pitch-related brain lateralization effects, evoked magnetic\nfields in response to dichotic rippled noise (RN) were recorded by\nmeans of whole-head magneto-encephalography (MEG). Four dichotic stimulus\nconstellations (111- or 133-Hz RN to one ear, white noise to the other)\nwere applied in randomized order. The evoked fields delineated three\nstages of central-auditory processing. (1) Attention-dependent ear\nx hemisphere interactions were observed as early as the M50 field (50-80\nms post stimulus onset), presumably reflecting early streaming of auditory\ninformation. (2) The relatively late M100 field (136 ms) was significantly\nlateralized to the left hemisphere, but showed a pitch-related modulation\nof right- rather than left-hemisphere activity. The observed interaction\nbetween periodic and aperiodic signal components suggests this time\nwindow to reflect pitch-synchronous spectral evaluation. (3) A time\nwindow centered at 192 ms showed a main effect of RN pitch, presumably\ndue to sensory memory operations.\n \n \n Full Paper \n \n Bibliographic reference. \nHertrich, Ingo / Mathiak, Klaus / Lutzenberger, Werner / Ackermann, Hermann (2003):\n\"Time course and lateralization of evoked magnetic fields during complex pitch processing\",\nIn ICPhS-15, 1715-1718. \n",
          "original_query": "Hertrich et al. (2023) \u2014 conjectures and reductions tying general CPWL depth to the max primitive",
          "cleaned_query": "Hertrich et al.",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "Strategic Choices for Matching Platforms",
          "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5027950",
          "content": "[Skip to main content](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5027950#maincontent)\n\n[![PDF icon](https://static.ssrn.com/cfincludes/img/icons/icon-adobe-pdf.svg)Download This Paper](https://papers.ssrn.com/sol3/Delivery.cfm/5027950.pdf?abstractid=5027950&mirid=1)\n\n[Open PDF in Browser](https://papers.ssrn.com/sol3/Delivery.cfm/5027950.pdf?abstractid=5027950&mirid=1&type=2)\n\n[Add Paper to My Library](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5027950)\n\nShare:\n\nPermalink\n\nUsing these links will ensure access to this page indefinitely\n\n[Copy URL](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5027950)\n\n[Copy DOI](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5027950)\n\n# Strategic Choices for Matching Platforms\n\n[SMU Cox School of Business Research Paper No. 24-15](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5027950)\n\n9 PagesPosted: 11 Dec 2024Last revised: 23 Nov 2024\n\n[See all articles by Amit Basu](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=470814)\n\n## [Amit Basu](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=470814)\n\nSouthern Methodist University (SMU) - Information Technology and Operations Management Department (ITOM)\n\n## [Sreekumar R. Bhaskaran](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=343405)\n\nSouthern Methodist University (SMU) - Information Technology and Operations Management Department (ITOM)\n\n## [Rajiv Mukherjee](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=2490216)\n\nTexas A&M University - Mays Business School\n\nDate Written: November 20, 2024\n\n### Abstract\n\nOnline matching platforms dramatically enhance the ability of both individuals and organizations to find matches for their personal or business needs. At the same time, some of the key issues in the design of these platforms are not well-understood, leading to poor strategic choices by some early entrants. For matching platforms to succeed, three core services stand out as critical value drivers: search, authentication, and compatibility counseling. To maximize their value, executives planning matching platforms can learn from the strategies of online dating platforms, where quality matches and user experience are essential for success. Then, drawing upon our research on matching platforms, we lay out some of the key design choices, and provide guidance in the form of a decision framework.\n\n**Keywords:** search, compatibility, counseling, authentication, matching platforms, dating markets\n\n**Suggested Citation:** [Suggested Citation](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5027950)\n\nBasu, Amit and Bhaskaran, Sreekumar R. and Mukherjee, Rajiv, Strategic Choices for Matching Platforms (November 20, 2024). SMU Cox School of Business Research Paper No. 24-15, Available at SSRN: [https://ssrn.com/abstract=5027950](https://ssrn.com/abstract=5027950) or [http://dx.doi.org/10.2139/ssrn.5027950](https://dx.doi.org/10.2139/ssrn.5027950)\n\n### [Amit Basu](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=470814)\n\n[![Southern Methodist University (SMU) - Information Technology and Operations Management Department (ITOM)](https://papers.ssrn.com/Organizations/OrgBrandings/17978_13002.gif)](http://www.smu.edu/businesslibrary)\n\n#### Southern Methodist University (SMU) - Information Technology and Operations Management Department (ITOM) ( [email](javascript:void(0)) )\n\nDallas, TX 75275\n\nUnited States\n\n### [Sreekumar R. Bhaskaran (Contact Author)](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=343405)\n\n[![Southern Methodist University (SMU) - Information Technology and Operations Management Department (ITOM)](https://papers.ssrn.com/Organizations/OrgBrandings/17978_13002.gif)](http://www.smu.edu/businesslibrary)\n\n#### Southern Methodist University (SMU) - Information Technology and Operations Management Department (ITOM) ( [email](javascript:void(0)) )\n\nDallas, TX 75275\n\nUnited States\n\n### [Rajiv Mukherjee](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=2490216)\n\n[![Texas A&M University - Mays Business School](https://papers.ssrn.com/Organizations/OrgBrandings/193170_5868.gif)](http://mays.tamu.edu/)\n\n#### Texas A&M University - Mays Business School ( [email](javascript:void(0)) )\n\nWehner 401Q, MS 4353\n\nCollege Station, TX 77843-4218\n\nUnited States\n\n## Do you have a job opening that you would like to promote on SSRN?\n\n[Place Job Opening](https://www.ssrn.com/index.cfm/en/Announcements-Jobs/)\n\n## Paper statistics\n\nDownloads\n\n58\n\nAbstract Views\n\n333\n\nRank\n\n786,223\n\n[4 References](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5027950#paper-references-widget)\n\nPlumX Metrics\n\n## Related eJournals\n\n- [SMU Cox School of Business Research Paper Series](https://papers.ssrn.com/sol3/JELJOUR_Results.cfm?form_name=journalBrowse&journal_id=968283)\n\n\n\n[Follow](javascript:void(0);)\n\n\n\n\n\n\n\n#### SMU Cox School of Business Research Paper Series\n\n\n\nSubscribe to this free journal for more curated articles on this topic\n\n\n\n\n\n\n\nFOLLOWERS\n\n\n\n5,157\n\n\n\n\n\n\n\nPAPERS\n\n\n\n514\n\n\n\n\n\n\n\n\n\nThis Journal is curated by:\n\n\n\n**Sandy Miller** at Southern Methodist University (SMU) - SMU Cox School of Business, **Melissa Johnson** at Southern Methodist University (SMU) - SMU Cox School of Business\n\n- [Microeconomics: Search; Learning; Information Costs & Specific Knowledge; Expectation & Speculation eJournal](https://papers.ssrn.com/sol3/JELJOUR_Results.cfm?form_name=journalBrowse&journal_id=1499582)\n\n\n\n[Follow](javascript:void(0);)\n\n\n\n\n\n\n\n#### Microeconomics: Search; Learning; Information Costs & Specific Knowledge; Expectation & Speculation eJournal\n\n\n\nSubscribe to this fee journal for more curated articles on this topic\n\n\n\n\n\n\n\nFOLLOWERS\n\n\n\n775\n\n\n\n\n\n\n\nPAPERS\n\n\n\n8,463\n\n- [Operations Strategy eJournal](https://papers.ssrn.com/sol3/JELJOUR_Results.cfm?form_name=journalBrowse&journal_id=992373)\n\n\n\n[Follow](javascript:void(0);)\n\n\n\n\n\n\n\n#### Operations Strategy eJournal\n\n\n\nSubscribe to this fee journal for more curated articles on this topic\n\n\n\n\n\n\n\nFOLLOWERS\n\n\n\n744\n\n\n\n\n\n\n\nPAPERS\n\n\n\n1,643\n\n- [Technology, Operations Management & Production eJournal](https://papers.ssrn.com/sol3/JELJOUR_Results.cfm?form_name=journalBrowse&journal_id=930066)\n\n\n\n[Follow](javascript:void(0);)\n\n\n\n\n\n\n\n#### Technology, Operations Management & Production eJournal\n\n\n\nSubscribe to this fee journal for more curated articles on this topic\n\n\n\n\n\n\n\nFOLLOWERS\n\n\n\n734\n\n\n\n\n\n\n\nPAPERS\n\n\n\n2,149\n\n- [Sources of Innovation eJournal](https://papers.ssrn.com/sol3/JELJOUR_Results.cfm?form_name=journalBrowse&journal_id=2011220)\n\n\n\n[Follow](javascript:void(0);)\n\n\n\n\n\n\n\n#### Sources of Innovation eJournal\n\n\n\nSubscribe to this fee journal for more curated articles on this topic\n\n\n\n\n\n\n\nFOLLOWERS\n\n\n\n280\n\n\n\n\n\n\n\nPAPERS\n\n\n\n3,339\n\n- [Information Systems & Economics eJournal](https://papers.ssrn.com/sol3/JELJOUR_Results.cfm?form_name=journalBrowse&journal_id=1475407)\n\n\n\n[Follow](javascript:void(0);)\n\n\n\n\n\n\n\n#### Information Systems & Economics eJournal\n\n\n\nSubscribe to this fee journal for more curated articles on this topic\n\n\n\n\n\n\n\nFOLLOWERS\n\n\n\n224\n\n\n\n\n\n\n\nPAPERS\n\n\n\n15,486\n\n\n\n\n\n\n\n\n\nThis Journal is curated by:\n\n\n\n**Erik Brynjolfsson** at National Bureau of Economic Research (NBER)\n\n- [Resource Based Strategy & Policy eJournal](https://papers.ssrn.com/sol3/JELJOUR_Results.cfm?form_name=journalBrowse&journal_id=1358731)\n\n\n\n[Follow](javascript:void(0);)\n\n\n\n\n\n\n\n#### Resource Based Strategy & Policy eJournal\n\n\n\nSubscribe to this fee journal for more curated articles on this topic\n\n\n\n\n\n\n\nFOLLOWERS\n\n\n\n215\n\n\n\n\n\n\n\nPAPERS\n\n\n\n1,346\n\n\nFeedback\n\nFeedback to SSRN\n\nFeedback\u00a0(required)\n\nEmail\u00a0(required)\n\nSubmit",
          "original_query": "Mukherjee and Basu (2017) \u2014 lower-bound example: max{0,x1,x2} not representable with one hidden layer",
          "cleaned_query": "Mukherjee and Basu"
        },
        {
          "success": true,
          "title": "Generative Adversarial Networks | Request PDF - ResearchGate",
          "url": "https://www.researchgate.net/publication/263012109_Generative_Adversarial_Networks",
          "content": "- [Home](https://www.researchgate.net/directory/publications)\n- [Organizational Psychology](https://www.researchgate.net/topic/Organizational-Psychology/publications)\n- [Training](https://www.researchgate.net/topic/Training/publications)\n\nArticlePDF Available\n\n# Generative Adversarial Networks\n\n- June 2014\n- [Advances in Neural Information Processing Systems](https://www.researchgate.net/journal/Advances-in-Neural-Information-Processing-Systems-1049-5258) 3(11)\n\nDOI: [10.1145/3422622](http://dx.doi.org/10.1145/3422622)\n\nAuthors:\n\n[Ian Goodfellow](https://www.researchgate.net/profile/Ian-Goodfellow-2)\n\n- [Universit\u00e9 de Montr\u00e9al](https://www.researchgate.net/institution/Universite-de-Montreal)\n\n[Jean Pouget-Abadie](https://www.researchgate.net/scientific-contributions/Jean-Pouget-Abadie-2049681813)\n\n[Jean Pouget-Abadie](https://www.researchgate.net/scientific-contributions/Jean-Pouget-Abadie-2049681813)\n\n- This person is not on ResearchGate, or hasn't claimed this research yet.\n\n\n[Mehdi Mirza](https://www.researchgate.net/scientific-contributions/Mehdi-Mirza-2006489246)\n\n[Mehdi Mirza](https://www.researchgate.net/scientific-contributions/Mehdi-Mirza-2006489246)\n\n- This person is not on ResearchGate, or hasn't claimed this research yet.\n\n\n[Bing Xu](https://www.researchgate.net/scientific-contributions/Bing-Xu-2009213940)\n\n[Bing Xu](https://www.researchgate.net/scientific-contributions/Bing-Xu-2009213940)\n\n- This person is not on ResearchGate, or hasn't claimed this research yet.\n\n\nShow all 8 authorsHide\n\n[Download full-text PDF](https://www.researchgate.net/profile/Y-Bengio/publication/263012109_Generative_Adversarial_Networks/links/546b70220cf20dedafd5303b/Generative-Adversarial-Networks.pdf)\n\n[Read full-text](https://www.researchgate.net/publication/263012109_Generative_Adversarial_Networks#read)\n\n[Download citation](https://www.researchgate.net/publication/263012109_Generative_Adversarial_Networks/citation/download)\n\nCopy link Link copied\n\n[Read full-text](https://www.researchgate.net/publication/263012109_Generative_Adversarial_Networks#read) [Download citation](https://www.researchgate.net/publication/263012109_Generative_Adversarial_Networks/citation/download)\nCopy link Link copied\n\n## Abstract and Figures\n\nWe propose a new framework for estimating generative models via an\nadversarial process, in which we simultaneously train two models: a generative\nmodel G that captures the data distribution, and a discriminative model D that\nestimates the probability that a sample came from the training data rather than\nG. The training procedure for G is to maximize the probability of D making a\nmistake. This framework corresponds to a minimax two-player game. In the space\nof arbitrary functions G and D, a unique solution exists, with G recovering the\ntraining data distribution and D equal to 1/2 everywhere. In the case where G\nand D are defined by multilayer perceptrons, the entire system can be trained\nwith backpropagation. There is no need for any Markov chains or unrolled\napproximate inference networks during either training or generation of samples.\nExperiments demonstrate the potential of the framework through qualitative and\nquantitative evaluation of the generated samples.\n\n[Visualization of samples from the model. Rightmost column shows the nearest training example of the neighboring sample, in order to demonstrate that the model has not memorized the training set. Samples are fair random draws, not cherry-picked. Unlike most other visualizations of deep generative models, these images show actual samples from the model distributions, not conditional means given samples of hidden units. Moreover, these samples are uncorrelated because the sampling process does not depend on Markov chain mixing. a) MNIST b) TFD c) CIFAR-10 (fully connected model) d) CIFAR-10 (convolutional discriminator and \"deconvolutional\" generator)\\\n\\\n\u2026](https://www.researchgate.net/figure/sualization-of-samples-from-the-model-Rightmost-column-shows-the-nearest-training_fig2_263012109)\n\n[Digits obtained by linearly interpolating between coordinates in z space of the full model.\\\n\\\n\u2026](https://www.researchgate.net/figure/Digits-obtained-by-linearly-interpolating-between-coordinates-in-z-space-of-the-full_fig1_263012109)\n\nFigures - uploaded by [Y. Bengio](https://www.researchgate.net/profile/Y-Bengio)\n\nAuthor content\n\nAll figure content in this area was uploaded by Y. Bengio\n\nContent may be subject to copyright.\n\n**Discover the world's research**\n\n- 25+ million members\n- 160+ million publication pages\n- 2.3+ billion citations\n\n[Join for free](https://www.researchgate.net/signup.SignUp.html)\n\nContent uploaded by [Y. Bengio](https://www.researchgate.net/profile/Y-Bengio)\n\nAuthor content\n\nAll content in this area was uploaded by Y. Bengio on Nov 18, 2014\n\nContent may be subject to copyright.\n\nGenerativeAdversarialNets\n\nIanJ.Goodfellow,JeanPouget-Abadie\u2217\n\n,MehdiMirza,BingXu,DavidWarde-Farley,\n\nSherjilOzair\u2020\n\n,AaronCourville,YoshuaBengio\u2021\n\nD\u00b4\n\nepartementd\u2019informatiqueetderechercheop\u00b4\n\nerationnelle\n\nUniversit\u00b4\n\nedeMontr\u00b4\n\neal\n\nMontr\u00b4\n\neal,QCH3C3J7\n\nAbstract\n\nWeproposeanewframeworkforestimatinggenerativemodelsviaanadversar-\n\nialprocess,inwhichwesimultaneouslytraintwomodels:agenerativemodelG\n\nthatcapturesthedatadistribution,andadiscriminativemodelDthatestimates\n\ntheprobabilitythatasamplecamefromthetrainingdataratherthanG.Thetrain-\n\ningprocedureforGistomaximizetheprobabilityofDmakingamistake.This\n\nframeworkcorrespondstoaminimaxtwo-playergame.Inthespaceofarbitrary\n\nfunctionsGandD,auniquesolutionexists,withGrecoveringthetrainingdata\n\ndistributionandDequalto1\n\n2everywhere.InthecasewhereGandDarede\ufb01ned\n\nbymultilayerperceptrons,theentiresystemcanbetrainedwithbackpropagation.\n\nThereisnoneedforanyMarkovchainsorunrolledapproximateinferencenet-\n\nworksduringeithertrainingorgenerationofsamples.Experimentsdemonstrate\n\nthepotentialoftheframeworkthroughqualitativeandquantitativeevaluationof\n\nthegeneratedsamples.\n\n1Introduction\n\nThepromiseofdeeplearningistodiscoverrich,hierarchicalmodels\\[2\\]thatrepresentprobability\n\ndistributionsoverthekindsofdataencounteredinarti\ufb01cialintelligenceapplications,suchasnatural\n\nimages,audiowaveformscontainingspeech,andsymbolsinnaturallanguagecorpora.Sofar,the\n\nmoststrikingsuccessesindeeplearninghaveinvolveddiscriminativemodels,usuallythosethat\n\nmapahigh-dimensional,richsensoryinputtoaclasslabel\\[14,22\\].Thesestrikingsuccesseshave\n\nprimarilybeenbasedonthebackpropagationanddropoutalgorithms,usingpiecewiselinearunits\n\n\\[19,9,10\\]whichhaveaparticularlywell-behavedgradient.Deepgenerativemodelshavehadless\n\nofanimpact,duetothedif\ufb01cultyofapproximatingmanyintractableprobabilisticcomputationsthat\n\nariseinmaximumlikelihoodestimationandrelatedstrategies,andduetodif\ufb01cultyofleveraging\n\nthebene\ufb01tsofpiecewiselinearunitsinthegenerativecontext.Weproposeanewgenerativemodel\n\nestimationprocedurethatsidestepsthesedif\ufb01culties.1\n\nIntheproposedadversarialnetsframework,thegenerativemodelispittedagainstanadversary:a\n\ndiscriminativemodelthatlearnstodeterminewhetherasampleisfromthemodeldistributionorthe\n\ndatadistribution.Thegenerativemodelcanbethoughtofasanalogoustoateamofcounterfeiters,\n\ntryingtoproducefakecurrencyanduseitwithoutdetection,whilethediscriminativemodelis\n\nanalogoustothepolice,tryingtodetectthecounterfeitcurrency.Competitioninthisgamedrives\n\nbothteamstoimprovetheirmethodsuntilthecounterfeitsareindistiguishablefromthegenuine\n\narticles.\n\n\u2217JeanPouget-AbadieisvisitingUniversit\u00b4\n\nedeMontr\u00b4\n\nealfromEcolePolytechnique.\n\n\u2020SherjilOzairisvisitingUniversit\u00b4\n\nedeMontr\u00b4\n\nealfromIndianInstituteofTechnologyDelhi\n\n\u2021YoshuaBengioisaCIFARSeniorFellow.\n\n1Allcodeandhyperparametersavailableathttp://www.github.com/goodfeli/adversarial\n\n1\n\narXiv:1406.2661v1 \\[stat.ML\\] 10 Jun 2014\n\nThisframeworkcanyieldspeci\ufb01ctrainingalgorithmsformanykindsofmodelandoptimization\n\nalgorithm.Inthisarticle,weexplorethespecialcasewhenthegenerativemodelgeneratessamples\n\nbypassingrandomnoisethroughamultilayerperceptron,andthediscriminative",
          "original_query": "Goodfellow et al. (2013) \u2014 Maxout networks",
          "cleaned_query": "Goodfellow et al."
        }
      ],
      "generated_ideas": [
        "MI-Guided Metadata Shaping for Matching-Platform Search & Compatibility\nBuild a matchmaking recommender that uses metadata shaping to inject user/profile attributes (verified traits, preferences, dealbreakers) into query and candidate representations based on mutual information with match outcomes. Evaluate improvements in long-tail matching (rare preferences/identities) and cold-start performance versus standard embedding and feature-cross baselines, using offline logs and counterfactual evaluation.",
        "Authentication-Aware Generative Profiles via Conditional GANs\nDevelop a conditional GAN that generates realistic but *audit-friendly* synthetic user profiles conditioned on authentication states (unverified/self-reported/verified) and platform segments. Use the synthetic data to stress-test search and compatibility algorithms for bias and robustness, while ensuring the generator preserves statistical properties relevant to downstream ranking without leaking real-user identities.",
        "Closed-Loop Adaptive Control for Dynamic Matching Markets\nModel a matching platform as a dynamical system where interventions (ranking weights, exploration rate, counseling prompts) influence supply\u2013demand balance and user churn over time. Adapt ideas from adaptive control with unknown parameters to estimate latent market parameters online and stabilize key metrics (match rate, fairness constraints, retention) under shocks such as rapid cohort growth or shifting preferences.",
        "Chaos-Inspired Exploration Schedules for Recommender Ranking\nDesign exploration policies that use deterministic chaotic signals (e.g., from a multi-scroll chaotic system) to modulate candidate sampling and slate diversification. Test whether chaos-driven exploration yields better coverage of tail entities and avoids periodic \u201cecho chamber\u201d cycles compared to epsilon-greedy or Thompson sampling, especially under sparse feedback.",
        "Pitch-Lateralization-Informed Audio UX Evaluation with Generative Stimuli\nCreate a pipeline that uses GANs to generate controlled rippled-noise-like stimuli spanning pitch periodicity/aperiodicity and then measure user responses (behavioral or MEG/EEG when available) during interface sound design tasks. The contribution is an evaluation framework linking stimulus properties to cognitive load/attention proxies, enabling data-driven guidelines for notification sounds and auditory feedback.",
        "Metadata Shaping for Practitioner-Facing Design/Research Assistants\nBuild an LLM assistant for UX practitioners that injects project metadata (product domain, user segments, constraints, prior findings) into prompts via mutual-information-based shaping at both training and inference. Evaluate whether this reduces hallucinations and improves usefulness on real design tasks (ideation briefs, interview guides, synthesis) compared to vanilla prompting and retrieval-only baselines.",
        "Counseling as Control: Personalized Intervention Policies for Match Success\nOperationalize \u201ccompatibility counseling\u201d as a sequential decision problem where the system chooses nudges (reflection prompts, profile edits, conversation starters) to improve match outcomes. Use adaptive parameter estimation (from control) to personalize intervention intensity while enforcing constraints (no manipulation, opt-out), and test effectiveness via randomized experiments.",
        "Adversarial Robustness to Metadata Manipulation in Knowledge-Enhanced LMs\nStudy how metadata shaping changes the attack surface when users or sellers manipulate metadata (types/descriptions) to gain visibility in search/matching. Create an adversarial training protocol where a discriminator detects \u201cmetadata hacks\u201d while the main model learns stable decisions; benchmark on synthetic and real platform-like corpora with long-tail entities.",
        "Tail-Entity Fairness Metrics for Matching Platforms Using Knowledge-Enhanced Representations\nPropose new fairness diagnostics focused on the long tail (rare attributes, niche intents) using knowledge-enhanced LM representations produced by metadata shaping. Implement an evaluation suite that measures exposure, match quality, and error rates across tail slices, and demonstrate how different platform strategies (search vs counseling vs authentication emphasis) shift these metrics.",
        "Hybrid Neuro-Symbolic Entity Memory for Rare-Concept Matching and Search\nExtend metadata shaping with an explicit \u201centity memory\u201d that stores structured facts (types, constraints, verified fields) and learns when to rely on memory vs parametric LM knowledge. Validate on tasks that mirror matching-platform queries (e.g., \u201cfind a mentor who worked on X and prefers Y\u201d) where tail entities dominate, showing improvements in factual accuracy and controllability over LM-only approaches."
      ],
      "generated_ideas_raw": "1. **MI-Guided Metadata Shaping for Matching-Platform Search & Compatibility**\n   Build a matchmaking recommender that uses metadata shaping to inject user/profile attributes (verified traits, preferences, dealbreakers) into query and candidate representations based on mutual information with match outcomes. Evaluate improvements in long-tail matching (rare preferences/identities) and cold-start performance versus standard embedding and feature-cross baselines, using offline logs and counterfactual evaluation.\n\n2. **Authentication-Aware Generative Profiles via Conditional GANs**\n   Develop a conditional GAN that generates realistic but *audit-friendly* synthetic user profiles conditioned on authentication states (unverified/self-reported/verified) and platform segments. Use the synthetic data to stress-test search and compatibility algorithms for bias and robustness, while ensuring the generator preserves statistical properties relevant to downstream ranking without leaking real-user identities.\n\n3. **Closed-Loop Adaptive Control for Dynamic Matching Markets**\n   Model a matching platform as a dynamical system where interventions (ranking weights, exploration rate, counseling prompts) influence supply\u2013demand balance and user churn over time. Adapt ideas from adaptive control with unknown parameters to estimate latent market parameters online and stabilize key metrics (match rate, fairness constraints, retention) under shocks such as rapid cohort growth or shifting preferences.\n\n4. **Chaos-Inspired Exploration Schedules for Recommender Ranking**\n   Design exploration policies that use deterministic chaotic signals (e.g., from a multi-scroll chaotic system) to modulate candidate sampling and slate diversification. Test whether chaos-driven exploration yields better coverage of tail entities and avoids periodic \u201cecho chamber\u201d cycles compared to epsilon-greedy or Thompson sampling, especially under sparse feedback.\n\n5. **Pitch-Lateralization-Informed Audio UX Evaluation with Generative Stimuli**\n   Create a pipeline that uses GANs to generate controlled rippled-noise-like stimuli spanning pitch periodicity/aperiodicity and then measure user responses (behavioral or MEG/EEG when available) during interface sound design tasks. The contribution is an evaluation framework linking stimulus properties to cognitive load/attention proxies, enabling data-driven guidelines for notification sounds and auditory feedback.\n\n6. **Metadata Shaping for Practitioner-Facing Design/Research Assistants**\n   Build an LLM assistant for UX practitioners that injects project metadata (product domain, user segments, constraints, prior findings) into prompts via mutual-information-based shaping at both training and inference. Evaluate whether this reduces hallucinations and improves usefulness on real design tasks (ideation briefs, interview guides, synthesis) compared to vanilla prompting and retrieval-only baselines.\n\n7. **Counseling as Control: Personalized Intervention Policies for Match Success**\n   Operationalize \u201ccompatibility counseling\u201d as a sequential decision problem where the system chooses nudges (reflection prompts, profile edits, conversation starters) to improve match outcomes. Use adaptive parameter estimation (from control) to personalize intervention intensity while enforcing constraints (no manipulation, opt-out), and test effectiveness via randomized experiments.\n\n8. **Adversarial Robustness to Metadata Manipulation in Knowledge-Enhanced LMs**\n   Study how metadata shaping changes the attack surface when users or sellers manipulate metadata (types/descriptions) to gain visibility in search/matching. Create an adversarial training protocol where a discriminator detects \u201cmetadata hacks\u201d while the main model learns stable decisions; benchmark on synthetic and real platform-like corpora with long-tail entities.\n\n9. **Tail-Entity Fairness Metrics for Matching Platforms Using Knowledge-Enhanced Representations**\n   Propose new fairness diagnostics focused on the long tail (rare attributes, niche intents) using knowledge-enhanced LM representations produced by metadata shaping. Implement an evaluation suite that measures exposure, match quality, and error rates across tail slices, and demonstrate how different platform strategies (search vs counseling vs authentication emphasis) shift these metrics.\n\n10. **Hybrid Neuro-Symbolic Entity Memory for Rare-Concept Matching and Search**\n   Extend metadata shaping with an explicit \u201centity memory\u201d that stores structured facts (types, constraints, verified fields) and learns when to rely on memory vs parametric LM knowledge. Validate on tasks that mirror matching-platform queries (e.g., \u201cfind a mentor who worked on X and prefers Y\u201d) where tail entities dominate, showing improvements in factual accuracy and controllability over LM-only approaches.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "MI-Guided Metadata Shaping for Matching-Platform Search & Compatibility\nBuild a matchmaking recommender that uses metadata shaping to inject user/profile attributes (verified traits, preferences, deal",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Authentication-Aware Generative Profiles via Conditional GANs\nDevelop a conditional GAN that generates realistic but *audit-friendly* synthetic user profiles conditioned on authentication states (unve",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Closed-Loop Adaptive Control for Dynamic Matching Markets\nModel a matching platform as a dynamical system where interventions (ranking weights, exploration rate, counseling prompts) influence supply\u2013d",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Chaos-Inspired Exploration Schedules for Recommender Ranking\nDesign exploration policies that use deterministic chaotic signals (e.g., from a multi-scroll chaotic system) to modulate candidate samplin",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Pitch-Lateralization-Informed Audio UX Evaluation with Generative Stimuli\nCreate a pipeline that uses GANs to generate controlled rippled-noise-like stimuli spanning pitch periodicity/aperiodicity and",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Metadata Shaping for Practitioner-Facing Design/Research Assistants\nBuild an LLM assistant for UX practitioners that injects project metadata (product domain, user segments, constraints, prior finding",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Counseling as Control: Personalized Intervention Policies for Match Success\nOperationalize \u201ccompatibility counseling\u201d as a sequential decision problem where the system chooses nudges (reflection promp",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Adversarial Robustness to Metadata Manipulation in Knowledge-Enhanced LMs\nStudy how metadata shaping changes the attack surface when users or sellers manipulate metadata (types/descriptions) to gain v",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Tail-Entity Fairness Metrics for Matching Platforms Using Knowledge-Enhanced Representations\nPropose new fairness diagnostics focused on the long tail (rare attributes, niche intents) using knowledge-",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Hybrid Neuro-Symbolic Entity Memory for Rare-Concept Matching and Search\nExtend metadata shaping with an explicit \u201centity memory\u201d that stores structured facts (types, constraints, verified fields) and",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 41,
      "paper_title": "Tighter CMI-Based Generalization Bounds via Stochastic Projection and Quantization",
      "contribution": "Introduce a new CMI-style generalization bound that injects stochastic projection and lossy compression (quantization) into the CMI super-sample framework to obtain strictly tighter, non\u2011vacuous O(1/\u221an) guarantees on instances where prior MI/CMI bounds fail, and to argue that memorization is not necessary for good generalization.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 11793,
      "output_tokens": 1036,
      "predecessor_details": [
        {
          "success": true,
          "title": "Reasoning About Generalization via Conditional Mutual Information",
          "url": "https://arxiv.org/abs/2001.09122",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "Reasoning about generalization via conditional mutual information",
          "cleaned_query": "Reasoning about generalization via conditional mutual information"
        },
        {
          "success": true,
          "title": "Information-theoretic analysis of generalization capability of learning ...",
          "url": "https://arxiv.org/abs/1705.07809",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1705.07809** (cs)\n\n\\[Submitted on 22 May 2017 ( [v1](https://arxiv.org/abs/1705.07809v1)), last revised 6 Nov 2017 (this version, v2)\\]\n\n# Title:Information-theoretic analysis of generalization capability of learning algorithms\n\nAuthors: [Aolin Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+A), [Maxim Raginsky](https://arxiv.org/search/cs?searchtype=author&query=Raginsky,+M)\n\nView a PDF of the paper titled Information-theoretic analysis of generalization capability of learning algorithms, by Aolin Xu and Maxim Raginsky\n\n[View PDF](https://arxiv.org/pdf/1705.07809)\n\n> Abstract:We derive upper bounds on the generalization error of a learning algorithm in terms of the mutual information between its input and output. The bounds provide an information-theoretic understanding of generalization in learning problems, and give theoretical guidelines for striking the right balance between data fit and generalization by controlling the input-output mutual information. We propose a number of methods for this purpose, among which are algorithms that regularize the ERM algorithm with relative entropy or with random noise. Our work extends and leads to nontrivial improvements on the recent results of Russo and Zou.\n\n| | |\n| --- | --- |\n| Comments: | Final version, accepted to NIPS 2017 |\n| Subjects: | Machine Learning (cs.LG); Information Theory (cs.IT); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1705.07809](https://arxiv.org/abs/1705.07809) \\[cs.LG\\] |\n| | (or [arXiv:1705.07809v2](https://arxiv.org/abs/1705.07809v2) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1705.07809](https://doi.org/10.48550/arXiv.1705.07809) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Maxim Raginsky \\[ [view email](https://arxiv.org/show-email/53d0d258/1705.07809)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1705.07809v1)**\nMon, 22 May 2017 15:38:22 UTC (30 KB)\n\n**\\[v2\\]**\nMon, 6 Nov 2017 18:58:37 UTC (21 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Information-theoretic analysis of generalization capability of learning algorithms, by Aolin Xu and Maxim Raginsky\n\n- [View PDF](https://arxiv.org/pdf/1705.07809)\n- [TeX Source](https://arxiv.org/src/1705.07809)\n- [Other Formats](https://arxiv.org/format/1705.07809)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1705.07809&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1705.07809&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2017-05](https://arxiv.org/list/cs.LG/2017-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1705.07809?context=cs)\n\n[cs.IT](https://arxiv.org/abs/1705.07809?context=cs.IT)\n\n[math](https://arxiv.org/abs/1705.07809?context=math)\n\n[math.IT](https://arxiv.org/abs/1705.07809?context=math.IT)\n\n[stat](https://arxiv.org/abs/1705.07809?context=stat)\n\n[stat.ML](https://arxiv.org/abs/1705.07809?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1705.07809)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1705.07809)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1705.07809)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1705.html#XuR17) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/XuR17)\n\n[Aolin Xu](https://dblp.uni-trier.de/search/author?author=Aolin%20Xu)\n\n[Maxim Raginsky](https://dblp.uni-trier.de/search/author?author=Maxim%20Raginsky)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1705.07809&description=Information-theoretic analysis of generalization capability of learning algorithms) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1705.07809&title=Information-theoretic analysis of generalization capability of learning algorithms)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1705.07809) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Information-theoretic analysis of generalization capability of learning algorithms",
          "cleaned_query": "Information-theoretic analysis of generalization capability of learning algorithms"
        },
        {
          "success": true,
          "title": "Information Complexity of Stochastic Convex Optimization - arXiv",
          "url": "https://arxiv.org/abs/2402.09327",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2402.09327** (cs)\n\n\\[Submitted on 14 Feb 2024 ( [v1](https://arxiv.org/abs/2402.09327v1)), last revised 18 Jul 2024 (this version, v2)\\]\n\n# Title:Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization\n\nAuthors: [Idan Attias](https://arxiv.org/search/cs?searchtype=author&query=Attias,+I), [Gintare Karolina Dziugaite](https://arxiv.org/search/cs?searchtype=author&query=Dziugaite,+G+K), [Mahdi Haghifam](https://arxiv.org/search/cs?searchtype=author&query=Haghifam,+M), [Roi Livni](https://arxiv.org/search/cs?searchtype=author&query=Livni,+R), [Daniel M. Roy](https://arxiv.org/search/cs?searchtype=author&query=Roy,+D+M)\n\nView a PDF of the paper titled Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization, by Idan Attias and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2402.09327)\n\n> Abstract:In this work, we investigate the interplay between memorization and learning in the context of \\\\emph{stochastic convex optimization} (SCO). We define memorization via the information a learning algorithm reveals about its training data points. We then quantify this information using the framework of conditional mutual information (CMI) proposed by Steinke and Zakynthinou (2020). Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering an open question posed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded setting and under strong convexity, every learner with an excess error $\\\\varepsilon$ has CMI bounded below by $\\\\Omega(1/\\\\varepsilon^2)$ and $\\\\Omega(1/\\\\varepsilon)$, respectively. We further demonstrate the essential role of memorization in learning problems in SCO by designing an adversary capable of accurately identifying a significant fraction of the training samples in specific SCO problems. Finally, we enumerate several implications of our results, such as a limitation of generalization bounds based on CMI and the incompressibility of samples in SCO problems.\n\n| | |\n| --- | --- |\n| Comments: | 41 Pages, To appear in ICML 2024 |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2402.09327](https://arxiv.org/abs/2402.09327) \\[cs.LG\\] |\n| | (or [arXiv:2402.09327v2](https://arxiv.org/abs/2402.09327v2) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2402.09327](https://doi.org/10.48550/arXiv.2402.09327) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Mahdi Haghifam \\[ [view email](https://arxiv.org/show-email/0f61040c/2402.09327)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2402.09327v1)**\nWed, 14 Feb 2024 17:17:30 UTC (180 KB)\n\n**\\[v2\\]**\nThu, 18 Jul 2024 17:37:59 UTC (186 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization, by Idan Attias and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2402.09327)\n- [TeX Source](https://arxiv.org/src/2402.09327)\n- [Other Formats](https://arxiv.org/format/2402.09327)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2402.09327&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2402.09327&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2024-02](https://arxiv.org/list/cs.LG/2024-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2402.09327?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2402.09327)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2402.09327)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2402.09327)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2402.09327&description=Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2402.09327&title=Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2402.09327) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Information complexity of stochastic convex optimization: Applications to generalization, memorization, and tracing",
          "cleaned_query": "Information complexity of stochastic convex optimization: Applications to generalization, memorization, and tracing"
        },
        {
          "success": true,
          "title": "[2302.04925] Information Theoretic Lower Bounds for ... - arXiv",
          "url": "https://arxiv.org/abs/2302.04925",
          "content": "[2302.04925] Information Theoretic Lower Bounds for Information Theoretic Upper Bounds\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nIn just 5 minutes help us improve arXiv:\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2302.04925\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2302.04925**(cs)\n[Submitted on 9 Feb 2023 ([v1](https://arxiv.org/abs/2302.04925v1)), last revised 14 Jan 2024 (this version, v2)]\n# Title:Information Theoretic Lower Bounds for Information Theoretic Upper Bounds\nAuthors:[Roi Livni](https://arxiv.org/search/cs?searchtype=author&amp;query=Livni,+R)\nView a PDF of the paper titled Information Theoretic Lower Bounds for Information Theoretic Upper Bounds, by Roi Livni\n[View PDF](https://arxiv.org/pdf/2302.04925)[HTML (experimental)](https://arxiv.org/html/2302.04925v2)> > Abstract:\n> We examine the relationship between the mutual information between the output model and the empirical sample and the generalization of the algorithm in the context of stochastic convex optimization. Despite increasing interest in information-theoretic generalization bounds, it is uncertain if these bounds can provide insight into the exceptional performance of various learning algorithms. Our study of stochastic convex optimization reveals that, for true risk minimization, dimension-dependent mutual information is necessary. This indicates that existing information-theoretic generalization bounds fall short in capturing the generalization capabilities of algorithms like SGD and regularized ERM, which have dimension-independent sample complexity. Subjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2302.04925](https://arxiv.org/abs/2302.04925)[cs.LG]|\n|(or[arXiv:2302.04925v2](https://arxiv.org/abs/2302.04925v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2302.04925](https://doi.org/10.48550/arXiv.2302.04925)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Roi Livni [[view email](https://arxiv.org/show-email/382587a5/2302.04925)]\n**[[v1]](https://arxiv.org/abs/2302.04925v1)**Thu, 9 Feb 2023 20:42:36 UTC (51 KB)\n**[v2]**Sun, 14 Jan 2024 08:18:25 UTC (49 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Information Theoretic Lower Bounds for Information Theoretic Upper Bounds, by Roi Livni\n* [View PDF](https://arxiv.org/pdf/2302.04925)\n* [HTML (experimental)](https://arxiv.org/html/2302.04925v2)\n* [TeX Source](https://arxiv.org/src/2302.04925)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2302.04925&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2302.04925&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2023-02](https://arxiv.org/list/cs.LG/2023-02)\nChange to browse by:\n[cs](https://arxiv.org/abs/2302.04925?context=cs)\n[stat](https://arxiv.org/abs/2302.04925?context=stat)\n[stat.ML](https://arxiv.org/abs/2302.04925?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2302.04925)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2302.04925)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2302.04925)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2302.04925)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Information theoretic lower bounds for information theoretic upper bounds",
          "cleaned_query": "Information theoretic lower bounds for information theoretic upper bounds"
        },
        {
          "success": true,
          "title": "Rate-Distortion Theoretic Generalization Bounds for Stochastic ...",
          "url": "https://arxiv.org/abs/2203.02474",
          "content": "[2203.02474] Rate-Distortion Theoretic Generalization Bounds for Stochastic Learning Algorithms\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:2203.02474\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:2203.02474**(stat)\n[Submitted on 4 Mar 2022 ([v1](https://arxiv.org/abs/2203.02474v1)), last revised 29 Jun 2022 (this version, v2)]\n# Title:Rate-Distortion Theoretic Generalization Bounds for Stochastic Learning Algorithms\nAuthors:[Milad Sefidgaran](https://arxiv.org/search/stat?searchtype=author&amp;query=Sefidgaran,+M),[Amin Gohari](https://arxiv.org/search/stat?searchtype=author&amp;query=Gohari,+A),[Ga\u00ebl Richard](https://arxiv.org/search/stat?searchtype=author&amp;query=Richard,+G),[Umut \u015eim\u015fekli](https://arxiv.org/search/stat?searchtype=author&amp;query=\u015eim\u015fekli,+U)\nView a PDF of the paper titled Rate-Distortion Theoretic Generalization Bounds for Stochastic Learning Algorithms, by Milad Sefidgaran and 3 other authors\n[View PDF](https://arxiv.org/pdf/2203.02474)> > Abstract:\n> Understanding generalization in modern machine learning settings has been one of the major challenges in statistical learning theory. In this context, recent years have witnessed the development of various generalization bounds suggesting different complexity notions such as the mutual information between the data sample and the algorithm output, compressibility of the hypothesis space, and the fractal dimension of the hypothesis space. While these bounds have illuminated the problem at hand from different angles, their suggested complexity notions might appear seemingly unrelated, thereby restricting their high-level impact. In this study, we prove novel generalization bounds through the lens of rate-distortion theory, and explicitly relate the concepts of mutual information, compressibility, and fractal dimensions in a single mathematical framework. Our approach consists of (i) defining a generalized notion of compressibility by using source coding concepts, and (ii) showing that the `compression error rate&#39; can be linked to the generalization error both in expectation and with high probability. We show that in the `lossless compression&#39; setting, we recover and improve existing mutual information-based bounds, whereas a `lossy compression&#39; scheme allows us to link generalization to the rate-distortion dimension -- a particular notion of fractal dimension. Our results bring a more unified perspective on generalization and open up several future research directions. Comments:|Accepted for presentation at the Conference on Learning Theory (COLT) 2022|\nSubjects:|Machine Learning (stat.ML); Information Theory (cs.IT); Machine Learning (cs.LG)|\nCite as:|[arXiv:2203.02474](https://arxiv.org/abs/2203.02474)[stat.ML]|\n|(or[arXiv:2203.02474v2](https://arxiv.org/abs/2203.02474v2)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.2203.02474](https://doi.org/10.48550/arXiv.2203.02474)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Milad Sefidgaran [[view email](https://arxiv.org/show-email/d46464f6/2203.02474)]\n**[[v1]](https://arxiv.org/abs/2203.02474v1)**Fri, 4 Mar 2022 18:12:31 UTC (61 KB)\n**[v2]**Wed, 29 Jun 2022 20:48:42 UTC (65 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Rate-Distortion Theoretic Generalization Bounds for Stochastic Learning Algorithms, by Milad Sefidgaran and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2203.02474)\n* [TeX Source](https://arxiv.org/src/2203.02474)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2203.02474&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2203.02474&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2022-03](https://arxiv.org/list/stat.ML/2022-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2203.02474?context=cs)\n[cs.IT](https://arxiv.org/abs/2203.02474?context=cs.IT)\n[cs.LG](https://arxiv.org/abs/2203.02474?context=cs.LG)\n[math](https://arxiv.org/abs/2203.02474?context=math)\n[math.IT](https://arxiv.org/abs/2203.02474?context=math.IT)\n[stat](https://arxiv.org/abs/2203.02474?context=stat)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2203.02474)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2203.02474)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2203.02474)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. ",
          "original_query": "Rate-distortion theoretic generalization bounds for stochastic learning algorithms",
          "cleaned_query": "Rate-distortion theoretic generalization bounds for stochastic learning algorithms"
        },
        {
          "success": true,
          "title": "(PDF) Extensions of Lipschitz maps into a Hilbert space",
          "url": "https://www.researchgate.net/publication/235008656_Extensions_of_Lipschitz_maps_into_a_Hilbert_space",
          "content": "(PDF) Extensions of Lipschitz maps into a Hilbert space\nArticlePDF Available\n# Extensions of Lipschitz maps into a Hilbert space\n* January 1984\nDOI:[10.1090/conm/026/737400](https://doi.org/10.1090/conm/026/737400)\nAuthors:\n[![William B Johnson at Texas A&amp;M University](https://i1.rgstatic.net/ii/profile.image/276985067655169-1443049794181_Q64/William-Johnson-69.jpg)](profile/William-Johnson-69)\n[William B Johnson](profile/William-Johnson-69)\n* [Texas A&amp;M University](https://www.researchgate.net/institution/Texas-A-M-University)\n[![Joram Lindenstrauss](https://c5.rgstatic.net/m/448675030402/images/icons/icons/author-avatar.svg)](scientific-contributions/Joram-Lindenstrauss-3234684)\n[Joram Lindenstrauss](scientific-contributions/Joram-Lindenstrauss-3234684)\n[Joram Lindenstrauss](scientific-contributions/Joram-Lindenstrauss-3234684)\n* This person is not on ResearchGate, or hasn't claimed this research yet.\n![](https://i1.rgstatic.net/publication/235008656_Extensions_of_Lipschitz_maps_into_a_Hilbert_space/links/55e9abf908aeb65162649527/smallpreview.png)\n[Download full-text PDF](profile/William-Johnson-69/publication/235008656_Extensions_of_Lipschitz_maps_into_a_Hilbert_space/links/55e9abf908aeb65162649527/Extensions-of-Lipschitz-maps-into-a-Hilbert-space.pdf)[Read full-text](publication/235008656_Extensions_of_Lipschitz_maps_into_a_Hilbert_space#read)\n[Download full-text PDF](https://www.researchgate.net/profile/William-Johnson-69/publication/235008656_Extensions_of_Lipschitz_maps_into_a_Hilbert_space/links/55e9abf908aeb65162649527/Extensions-of-Lipschitz-maps-into-a-Hilbert-space.pdf)\n[Read full-text](publication/235008656_Extensions_of_Lipschitz_maps_into_a_Hilbert_space#read)\n[Download citation](https://www.researchgate.net/publication/235008656_Extensions_of_Lipschitz_maps_into_a_Hilbert_space/citation/download)\nCopy linkLink copied\n[\nRead full-text\n](publication/235008656_Extensions_of_Lipschitz_maps_into_a_Hilbert_space#read)[\nDownload citation\n](https://www.researchgate.net/publication/235008656_Extensions_of_Lipschitz_maps_into_a_Hilbert_space/citation/download)\nCopy linkLink copied\n![ResearchGate Logo](images/icons/svgicons/researchgate-logo-white.svg)\n**Discover the world's research**\n* 25+ million members\n* 160+ million publication pages\n* 2.3+ billion citations[Join for free](signup.SignUp.html)\n[](publication/235008656_Extensions_of_Lipschitz_maps_into_a_Hilbert_space#read-preview)\nContent uploaded by[William B Johnson](profile/William-Johnson-69)\nAuthor content\nAll content in this area was uploaded by William B Johnson on Sep 04, 2015\nContent may be subject to copyright.\n... Random projection (RP), sometimes also referred to as sketching, is a dimensionality reduction technique that has traditionally been used to speed up Euclidean distance-based algorithms such as k-means clustering [7] or k-nearest neighbors [24]. It is based on the Johnson-Lindenstrauss Lemma (JLL), which states that a set of points in a large dimensional space can be projected into a lower dimensional space while approximately preserving the distance between the points, see[25]for more details. However, in recent years, RPs have been proven to be useful in solving mathematical programs more efficiently. ...\n... Next, we use Eq. (24)-(25)and Eq. (20)-(21) to bound Eq. (22)- (23). ...\n[Sparse sub-gaussian random projections for semidefinite programming relaxations](publication/397261127_Sparse_sub-gaussian_random_projections_for_semidefinite_programming_relaxations)\nArticle\nFull-text available\n* Nov 2025\n* J GLOBAL OPTIM\n* [Monse Guedes-Ayala](https://www.researchgate.net/scientific-contributions/Monse-Guedes-Ayala-2284884777)\n* [Pierre-Louis Poirion](https://www.researchgate.net/profile/Pierre-Louis-Poirion)\n* [Lars Schewe](https://www.researchgate.net/scientific-contributions/Lars-Schewe-14986450)\n* [Akiko Takeda](https://www.researchgate.net/profile/Akiko-Takeda-2)\nRandom projection, a dimensionality reduction technique, has been found useful in recent years for reducing the size of optimization problems. In this paper, we explore the use of sparse sub-gaussian random projections to approximate semidefinite programming (SDP) problems by reducing the size of matrix variables, thereby solving the original problem with much less computational effort. We provide some theoretical bounds on the quality of the projection in terms of feasibility and optimality that explicitly depend on the sparsity parameter of the projector. We investigate the performance of the approach for semidefinite relaxations appearing in polynomial optimization, with a focus on combinatorial optimization problems. In particular, we apply our method to the semidefinite relaxations of Maxcut and Max-2-sat. We show that for large unweighted graphs, we can obtain a good bound by solving a projection of the semidefinite relaxation of Maxcut. We also explore how to apply our method to find the stability number of four classes of imperfect graphs by solving a projection of the second level of the Lasserre Hierarchy. Overall, our computational experiments show that semidefinite programming problems appearing as relaxations of combinatorial optimization problems can be approximately solved using random projections as long as the number of constraints is not too large.\n[View](publication/397261127_Sparse_sub-gaussian_random_projections_for_semidefinite_programming_relaxations)\nShow abstract\n... Recall V K s = {v \u2208R K | \u2225v\u2225 0 \u2264s} is the set of s-sparse vectors. Remark 3. From Johnson-Lindenstrauss Lemma[43], for any \u03b4\u2208(0, 1), any \u03c4\u2208(0, 1), and any finite vector set |V| &lt; \u221e, if the number of rows m \u2265O \u03b4\u22122 \u00b7log( |V| \u03c4) , then the compressed matrix \u03a6generated by Assumption 1 satisfies (V, \u03b4)-RIP with probability at least 1 \u2212\u03c4. Now, we are poised to present the first result on training loss defined in (2). ...\n... The proof of Theorem 2 is given in Appendix A.2. Here, the lower bound condition on the number of rows m ensures that the generated compressed matrix \u03a6is (3s, \u03b4)-RIP with probability at least 1 \u2212\u03c4by considering a \u03b4/2-net cover of set V = V K 3s \u2229B 2 (0; 1) from Johnson-Lindenstrauss Lemma[43]. Moreover, since the number of rows m required in Theorem 2 is greater than the one required in Theorem 1, term \u2225\u03a6y \u2212W x\u2225 2 can be further upper bounded using the uncompressed version (1 + \u03b4)\u2225 y \u2212Zx\u2225 2 with probability at least 1 \u2212\u03c4. ...\n[Solving Sparse \\\\&amp;&amp; High-Dimensional-Output Regression via Compression](publication/397201126_Solving_Sparse_High-Dimensional-Output_Regression_via_Compression)\nConference Paper\n* Jan 2024\n* [Zhehui Chen](https://www.researchgate.net/scientific-contributions/Zhehui-Chen-2295548072)\n* [Renyuan Li](https://www.researchgate.net/scientific-contributions/Renyuan-Li-2295547816)\n* [Guanyi Wang](https://www.researchgate.net/scientific-contributions/Guanyi-Wang-2235445783)\n[View](publication/397201126_Solving_Sparse_High-Dimensional-Output_Regression_via_Compression)\n... We apply an insight that for ICL, much of the information in the embedding m is redundant (we do not need a 4096-dimensional \u03b2to fit 20 demonstrations). Hence, we project m to a much lower dimensional space via a random matrix projection while preserving the necessary information, following the Johnson-Lindenstrauss lemma [21,33], precisely represented as a projection matrix P \u2208R d\u00d7d \u2032with each entry i.i.d. from N (0, 1/d \u2032). ...\n... Random projection matrix. We recall the Johnson-Lindenstrauss (JL) lemma [21,33]. Theorem C.1 (Johnson-Lindenstrauss Lemma). ...\n[DETAIL: Task DEmonsTration Attribution for Interpretable In-context Learning](publication/397200633_DETAIL_Task_DEmonsTration_Attribution_for_Interpretable_In-context_Learning)\nConference Paper\n* Jan 2024\n* [Xiaoqiang Lin](https://www.researchgate.net/profile/Xiaoqiang-Lin-2)\n* [Bryan Kian Hsiang Low](https://www.researchgate.net/scientific-contributions/Bryan-Kian-Hsiang-Low-2152750739)\n* [Alok Prakash](https://www.researchgate.net/profile/Alok-Pr",
          "original_query": "Extensions of Lipschitz mappings into a Hilbert space (Johnson\u2013Lindenstrauss type results)",
          "cleaned_query": "Extensions of Lipschitz mappings into a Hilbert space (Johnson\u2013Lindenstrauss type results)"
        }
      ],
      "generated_ideas": [
        "CMI-Optimal Stochastic Convex Optimizers via Explicit Information Budgets\nDesign and analyze a variant of SGD/regularized ERM for strongly convex, Lipschitz-bounded SCO that enforces an explicit *conditional mutual information (CMI) budget* (e.g., by calibrated output perturbations or KL-regularized update kernels). Prove upper bounds matching Attias et al.\u2019s lower bounds (up to constants/log factors), yielding an algorithmic \u201cPareto frontier\u201d between excess risk and CMI.",
        "Dimension-Free Generalization via Geometry-Aware CMI (Beyond Mutual Information)\nPropose a refined information measure that conditions on low-dimensional geometric structure (e.g., intrinsic dimension / rate-distortion dimension of the hypothesis trajectory) to address Livni\u2019s finding that plain mutual information must be dimension-dependent for true risk minimization in SCO. Show formally that this geometry-aware CMI can yield dimension-independent generalization bounds for classes where SGD empirically generalizes, and characterize when it still fails.",
        "Rate\u2013Distortion Characterization of Memorization in SCO\nUnify Attias et al.\u2019s memorization-via-CMI notion with Sefidgaran et al.\u2019s lossy compression framework by defining \u201cmemorization distortion\u201d (how well an adversary can reconstruct training points from the model). Derive a rate\u2013distortion function for SCO learners and prove that achieving small excess risk requires a minimum reconstruction rate, making memorization lower bounds quantitatively comparable across tasks.",
        "Adversary-Driven Benchmarks: Constructing SCO Instances that Maximize Sample Identifiability\nBuild a systematic generator of SCO problem instances where any learner attaining excess error \u2264 \u03b5 enables an adversary to identify (or strongly correlate with) a large fraction of training samples, extending Attias et al.\u2019s adversarial identification results. Provide tight instance-dependent identifiability rates and validate them empirically for SGD, noisy SGD, and KL-regularized ERM.",
        "JL-Sketched Training as Controlled Information Release\nApply Johnson\u2013Lindenstrauss (JL) random projections to the data (or gradients/features) during SCO training and analyze how sketch dimension controls CMI and rate\u2013distortion complexity. Prove tradeoffs among sketch dimension, optimization accuracy, and information leakage, yielding practical guidance for choosing projection sizes to reduce memorization while maintaining accuracy.",
        "Lipschitz Extension Theory for Out-of-Distribution Generalization Bounds\nUse Lipschitz extension results (Johnson\u2013Lindenstrauss/Lindenstrauss-style extension into Hilbert spaces) to model learned predictors as partial Lipschitz maps defined on training data and extended to the whole domain. Derive generalization bounds where the complexity term is the minimal Lipschitz extension constant plus a CMI (or rate\u2013distortion) term, separating \u201cgeometric extrapolation difficulty\u201d from \u201cmemorization.\u201d",
        "CMI Along the Optimization Path: Trajectory Information Complexity of SGD\nDefine and estimate the *pathwise* CMI between the dataset and the entire iterate trajectory (or a subsampled trajectory) rather than only the final model. Prove sharper generalization and memorization bounds that depend on how quickly information is accumulated over iterations, enabling early-stopping rules derived from an information growth criterion.",
        "Instance-Dependent CMI Lower Bounds via Local Curvature and Condition Number\nExtend Attias et al.\u2019s CMI lower bounds by making them instance-dependent through local strong convexity, smoothness, and condition number of the risk. Show that some data distributions force higher CMI than others for the same \u03b5, and derive matching upper bounds for tailored algorithms (e.g., preconditioned/noisy methods) that minimize CMI given curvature.",
        "Practical Estimation of CMI/Rate\u2013Distortion for Large Models Using Variational Coding\nDevelop scalable estimators for CMI and rate\u2013distortion dimension of learned hypotheses by fitting a learned compressor (e.g., variational codes for weights or logits) and measuring achievable codelength vs distortion. Validate whether these estimated quantities predict generalization and memorization (membership inference success) across optimization choices like noise, projection, and KL regularization.",
        "Information-Theoretic Regularization that Targets Identifiability Rather Than MI\nMotivated by the gap highlighted by Livni (MI bounds may be uninformative), define a regularizer that directly penalizes *sample identifiability* (e.g., via differentiable proxies for reconstruction of training points from the model) while staying compatible with CMI/rate\u2013distortion analysis. Prove that controlling identifiability yields meaningful generalization guarantees in regimes where MI-based bounds are necessarily dimension-dependent, and demonstrate improved resistance to the adversaries constructed in SCO."
      ],
      "generated_ideas_raw": "1. **CMI-Optimal Stochastic Convex Optimizers via Explicit Information Budgets**  \nDesign and analyze a variant of SGD/regularized ERM for strongly convex, Lipschitz-bounded SCO that enforces an explicit *conditional mutual information (CMI) budget* (e.g., by calibrated output perturbations or KL-regularized update kernels). Prove upper bounds matching Attias et al.\u2019s lower bounds (up to constants/log factors), yielding an algorithmic \u201cPareto frontier\u201d between excess risk and CMI.\n\n2. **Dimension-Free Generalization via Geometry-Aware CMI (Beyond Mutual Information)**  \nPropose a refined information measure that conditions on low-dimensional geometric structure (e.g., intrinsic dimension / rate-distortion dimension of the hypothesis trajectory) to address Livni\u2019s finding that plain mutual information must be dimension-dependent for true risk minimization in SCO. Show formally that this geometry-aware CMI can yield dimension-independent generalization bounds for classes where SGD empirically generalizes, and characterize when it still fails.\n\n3. **Rate\u2013Distortion Characterization of Memorization in SCO**  \nUnify Attias et al.\u2019s memorization-via-CMI notion with Sefidgaran et al.\u2019s lossy compression framework by defining \u201cmemorization distortion\u201d (how well an adversary can reconstruct training points from the model). Derive a rate\u2013distortion function for SCO learners and prove that achieving small excess risk requires a minimum reconstruction rate, making memorization lower bounds quantitatively comparable across tasks.\n\n4. **Adversary-Driven Benchmarks: Constructing SCO Instances that Maximize Sample Identifiability**  \nBuild a systematic generator of SCO problem instances where any learner attaining excess error \u2264 \u03b5 enables an adversary to identify (or strongly correlate with) a large fraction of training samples, extending Attias et al.\u2019s adversarial identification results. Provide tight instance-dependent identifiability rates and validate them empirically for SGD, noisy SGD, and KL-regularized ERM.\n\n5. **JL-Sketched Training as Controlled Information Release**  \nApply Johnson\u2013Lindenstrauss (JL) random projections to the data (or gradients/features) during SCO training and analyze how sketch dimension controls CMI and rate\u2013distortion complexity. Prove tradeoffs among sketch dimension, optimization accuracy, and information leakage, yielding practical guidance for choosing projection sizes to reduce memorization while maintaining accuracy.\n\n6. **Lipschitz Extension Theory for Out-of-Distribution Generalization Bounds**  \nUse Lipschitz extension results (Johnson\u2013Lindenstrauss/Lindenstrauss-style extension into Hilbert spaces) to model learned predictors as partial Lipschitz maps defined on training data and extended to the whole domain. Derive generalization bounds where the complexity term is the minimal Lipschitz extension constant plus a CMI (or rate\u2013distortion) term, separating \u201cgeometric extrapolation difficulty\u201d from \u201cmemorization.\u201d\n\n7. **CMI Along the Optimization Path: Trajectory Information Complexity of SGD**  \nDefine and estimate the *pathwise* CMI between the dataset and the entire iterate trajectory (or a subsampled trajectory) rather than only the final model. Prove sharper generalization and memorization bounds that depend on how quickly information is accumulated over iterations, enabling early-stopping rules derived from an information growth criterion.\n\n8. **Instance-Dependent CMI Lower Bounds via Local Curvature and Condition Number**  \nExtend Attias et al.\u2019s CMI lower bounds by making them instance-dependent through local strong convexity, smoothness, and condition number of the risk. Show that some data distributions force higher CMI than others for the same \u03b5, and derive matching upper bounds for tailored algorithms (e.g., preconditioned/noisy methods) that minimize CMI given curvature.\n\n9. **Practical Estimation of CMI/Rate\u2013Distortion for Large Models Using Variational Coding**  \nDevelop scalable estimators for CMI and rate\u2013distortion dimension of learned hypotheses by fitting a learned compressor (e.g., variational codes for weights or logits) and measuring achievable codelength vs distortion. Validate whether these estimated quantities predict generalization and memorization (membership inference success) across optimization choices like noise, projection, and KL regularization.\n\n10. **Information-Theoretic Regularization that Targets Identifiability Rather Than MI**  \nMotivated by the gap highlighted by Livni (MI bounds may be uninformative), define a regularizer that directly penalizes *sample identifiability* (e.g., via differentiable proxies for reconstruction of training points from the model) while staying compatible with CMI/rate\u2013distortion analysis. Prove that controlling identifiability yields meaningful generalization guarantees in regimes where MI-based bounds are necessarily dimension-dependent, and demonstrate improved resistance to the adversaries constructed in SCO.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "CMI-Optimal Stochastic Convex Optimizers via Explicit Information Budgets\nDesign and analyze a variant of SGD/regularized ERM for strongly convex, Lipschitz-bounded SCO that enforces an explicit *cond",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Dimension-Free Generalization via Geometry-Aware CMI (Beyond Mutual Information)\nPropose a refined information measure that conditions on low-dimensional geometric structure (e.g., intrinsic dimension",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Rate\u2013Distortion Characterization of Memorization in SCO\nUnify Attias et al.\u2019s memorization-via-CMI notion with Sefidgaran et al.\u2019s lossy compression framework by defining \u201cmemorization distortion\u201d (ho",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Adversary-Driven Benchmarks: Constructing SCO Instances that Maximize Sample Identifiability\nBuild a systematic generator of SCO problem instances where any learner attaining excess error \u2264 \u03b5 enables ",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "JL-Sketched Training as Controlled Information Release\nApply Johnson\u2013Lindenstrauss (JL) random projections to the data (or gradients/features) during SCO training and analyze how sketch dimension cont",
          "is_match": true
        },
        {
          "idea_idx": 5,
          "idea_text": "Lipschitz Extension Theory for Out-of-Distribution Generalization Bounds\nUse Lipschitz extension results (Johnson\u2013Lindenstrauss/Lindenstrauss-style extension into Hilbert spaces) to model learned pred",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "CMI Along the Optimization Path: Trajectory Information Complexity of SGD\nDefine and estimate the *pathwise* CMI between the dataset and the entire iterate trajectory (or a subsampled trajectory) rath",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Instance-Dependent CMI Lower Bounds via Local Curvature and Condition Number\nExtend Attias et al.\u2019s CMI lower bounds by making them instance-dependent through local strong convexity, smoothness, and c",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Practical Estimation of CMI/Rate\u2013Distortion for Large Models Using Variational Coding\nDevelop scalable estimators for CMI and rate\u2013distortion dimension of learned hypotheses by fitting a learned compr",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Information-Theoretic Regularization that Targets Identifiability Rather Than MI\nMotivated by the gap highlighted by Livni (MI bounds may be uninformative), define a regularizer that directly penalize",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 42,
      "paper_title": "A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning",
      "contribution": "Introduce a local data-attribution framework for online RL (PPO) using gradient-similarity-based influence from recent buffers, and leverage it to diagnose learning and to iteratively filter experiences (IIF) to speed and stabilize training.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 11565,
      "output_tokens": 814,
      "predecessor_details": [
        {
          "success": true,
          "title": "[1707.06347] Proximal Policy Optimization Algorithms - arXiv",
          "url": "https://arxiv.org/abs/1707.06347",
          "content": "[1707.06347] Proximal Policy Optimization Algorithms[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1707.06347\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1707.06347**(cs)\n[Submitted on 20 Jul 2017 ([v1](https://arxiv.org/abs/1707.06347v1)), last revised 28 Aug 2017 (this version, v2)]\n# Title:Proximal Policy Optimization Algorithms\nAuthors:[John Schulman](https://arxiv.org/search/cs?searchtype=author&amp;query=Schulman,+J),[Filip Wolski](https://arxiv.org/search/cs?searchtype=author&amp;query=Wolski,+F),[Prafulla Dhariwal](https://arxiv.org/search/cs?searchtype=author&amp;query=Dhariwal,+P),[Alec Radford](https://arxiv.org/search/cs?searchtype=author&amp;query=Radford,+A),[Oleg Klimov](https://arxiv.org/search/cs?searchtype=author&amp;query=Klimov,+O)\nView a PDF of the paper titled Proximal Policy Optimization Algorithms, by John Schulman and 4 other authors\n[View PDF](https://arxiv.org/pdf/1707.06347)> > Abstract:\n> We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a &#34;surrogate&#34; objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time. Subjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:1707.06347](https://arxiv.org/abs/1707.06347)[cs.LG]|\n|(or[arXiv:1707.06347v2](https://arxiv.org/abs/1707.06347v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1707.06347](https://doi.org/10.48550/arXiv.1707.06347)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: John Schulman [[view email](https://arxiv.org/show-email/b7d77275/1707.06347)]\n**[[v1]](https://arxiv.org/abs/1707.06347v1)**Thu, 20 Jul 2017 02:32:33 UTC (2,178 KB)\n**[v2]**Mon, 28 Aug 2017 09:20:06 UTC (2,537 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Proximal Policy Optimization Algorithms, by John Schulman and 4 other authors\n* [View PDF](https://arxiv.org/pdf/1707.06347)\n* [TeX Source](https://arxiv.org/src/1707.06347)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1707.06347&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1707.06347&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2017-07](https://arxiv.org/list/cs.LG/2017-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/1707.06347?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1707.06347)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1707.06347)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1707.06347)\n### [18 blog links](https://arxiv.org/tb/1707.06347)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1707.html#SchulmanWDRK17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/SchulmanWDRK17)\n[John Schulman]()\n[Filip Wolski]()\n[Prafulla Dhariwal]()\n[Alec Radford]()\n[Oleg Klimov]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/la",
          "original_query": "Proximal Policy Optimization Algorithms (PPO)",
          "cleaned_query": "Proximal Policy Optimization Algorithms (PPO)"
        },
        {
          "success": true,
          "title": "Understanding Black-box Predictions via Influence Functions - arXiv",
          "url": "https://arxiv.org/abs/1703.04730",
          "content": "# Statistics > Machine Learning\n\n**arXiv:1703.04730** (stat)\n\n\\[Submitted on 14 Mar 2017 ( [v1](https://arxiv.org/abs/1703.04730v1)), last revised 29 Dec 2020 (this version, v3)\\]\n\n# Title:Understanding Black-box Predictions via Influence Functions\n\nAuthors: [Pang Wei Koh](https://arxiv.org/search/stat?searchtype=author&query=Koh,+P+W), [Percy Liang](https://arxiv.org/search/stat?searchtype=author&query=Liang,+P)\n\nView a PDF of the paper titled Understanding Black-box Predictions via Influence Functions, by Pang Wei Koh and Percy Liang\n\n[View PDF](https://arxiv.org/pdf/1703.04730)\n\n> Abstract:How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.\n\n| | |\n| --- | --- |\n| Comments: | International Conference on Machine Learning, 2017. (This version adds more historical references and fixes typos.) |\n| Subjects: | Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:1703.04730](https://arxiv.org/abs/1703.04730) \\[stat.ML\\] |\n| (or [arXiv:1703.04730v3](https://arxiv.org/abs/1703.04730v3) \\[stat.ML\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1703.04730](https://doi.org/10.48550/arXiv.1703.04730) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Pang Wei Koh \\[ [view email](https://arxiv.org/show-email/ed31f4a2/1703.04730)\\] **[\\[v1\\]](https://arxiv.org/abs/1703.04730v1)**\nTue, 14 Mar 2017 21:07:01 UTC (4,753 KB)\n**[\\[v2\\]](https://arxiv.org/abs/1703.04730v2)**\nMon, 10 Jul 2017 02:31:54 UTC (4,538 KB)\n**\\[v3\\]**\nTue, 29 Dec 2020 22:40:43 UTC (4,831 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Understanding Black-box Predictions via Influence Functions, by Pang Wei Koh and Percy Liang\n\n- [View PDF](https://arxiv.org/pdf/1703.04730)\n- [TeX Source](https://arxiv.org/src/1703.04730)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1703.04730&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1703.04730&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2017-03](https://arxiv.org/list/stat.ML/2017-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1703.04730?context=cs) [cs.AI](https://arxiv.org/abs/1703.04730?context=cs.AI) [cs.LG](https://arxiv.org/abs/1703.04730?context=cs.LG) [stat](https://arxiv.org/abs/1703.04730?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1703.04730)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1703.04730)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1703.04730)\n\n### [3 blog links](https://arxiv.org/tb/1703.04730)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1703.04730) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Understanding Black-box Predictions via Influence Functions",
          "cleaned_query": "Understanding Black-box Predictions via Influence Functions"
        },
        {
          "success": true,
          "title": "Estimating Training Data Influence by Tracing Gradient Descent - arXiv",
          "url": "https://arxiv.org/abs/2002.08484",
          "content": "[2002.08484] Estimating Training Data Influence by Tracing Gradient Descent\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2002.08484\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2002.08484**(cs)\n[Submitted on 19 Feb 2020 ([v1](https://arxiv.org/abs/2002.08484v1)), last revised 14 Nov 2020 (this version, v3)]\n# Title:Estimating Training Data Influence by Tracing Gradient Descent\nAuthors:[Garima Pruthi](https://arxiv.org/search/cs?searchtype=author&amp;query=Pruthi,+G),[Frederick Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+F),[Mukund Sundararajan](https://arxiv.org/search/cs?searchtype=author&amp;query=Sundararajan,+M),[Satyen Kale](https://arxiv.org/search/cs?searchtype=author&amp;query=Kale,+S)\nView a PDF of the paper titled Estimating Training Data Influence by Tracing Gradient Descent, by Garima Pruthi and 3 other authors\n[View PDF](https://arxiv.org/pdf/2002.08484)> > Abstract:\n> We introduce a method called TracIn that computes the influence of a training example on a prediction made by the model. The idea is to trace how the loss on the test point changes during the training process whenever the training example of interest was utilized. We provide a scalable implementation of TracIn via: (a) a first-order gradient approximation to the exact computation, (b) saved checkpoints of standard training procedures, and (c) cherry-picking layers of a deep neural network. In contrast with previously proposed methods, TracIn is simple to implement; all it needs is the ability to work with gradients, checkpoints, and loss functions. The method is general. It applies to any machine learning model trained using stochastic gradient descent or a variant of it, agnostic of architecture, domain and task. We expect the method to be widely useful within processes that study and improve training data. Comments:|NeurIPS 2020|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2002.08484](https://arxiv.org/abs/2002.08484)[cs.LG]|\n|(or[arXiv:2002.08484v3](https://arxiv.org/abs/2002.08484v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2002.08484](https://doi.org/10.48550/arXiv.2002.08484)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Frederick Liu [[view email](https://arxiv.org/show-email/eedc9108/2002.08484)]\n**[[v1]](https://arxiv.org/abs/2002.08484v1)**Wed, 19 Feb 2020 22:40:32 UTC (6,349 KB)\n**[[v2]](https://arxiv.org/abs/2002.08484v2)**Mon, 13 Jul 2020 22:52:31 UTC (6,331 KB)\n**[v3]**Sat, 14 Nov 2020 18:47:35 UTC (6,423 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Estimating Training Data Influence by Tracing Gradient Descent, by Garima Pruthi and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2002.08484)\n* [TeX Source](https://arxiv.org/src/2002.08484)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2002.08484&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2002.08484&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2020-02](https://arxiv.org/list/cs.LG/2020-02)\nChange to browse by:\n[cs](https://arxiv.org/abs/2002.08484?context=cs)\n[stat](https://arxiv.org/abs/2002.08484?context=stat)\n[stat.ML](https://arxiv.org/abs/2002.08484?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2002.08484)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2002.08484)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2002.08484)\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2002.html#abs-2002-08484)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2002-08484)\n[Frederick Liu]()\n[Mukund Sundararajan]()\n[Satyen Kale]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2002.08484)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "TracIn: Estimating Training Data Influence by Tracking Gradients",
          "cleaned_query": "TracIn: Estimating Training Data Influence by Tracking Gradients"
        },
        {
          "success": true,
          "title": "[1511.05952] Prioritized Experience Replay - arXiv",
          "url": "https://arxiv.org/abs/1511.05952",
          "content": "[1511.05952] Prioritized Experience Replay\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1511.05952\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1511.05952**(cs)\n[Submitted on 18 Nov 2015 ([v1](https://arxiv.org/abs/1511.05952v1)), last revised 25 Feb 2016 (this version, v4)]\n# Title:Prioritized Experience Replay\nAuthors:[Tom Schaul](https://arxiv.org/search/cs?searchtype=author&amp;query=Schaul,+T),[John Quan](https://arxiv.org/search/cs?searchtype=author&amp;query=Quan,+J),[Ioannis Antonoglou](https://arxiv.org/search/cs?searchtype=author&amp;query=Antonoglou,+I),[David Silver](https://arxiv.org/search/cs?searchtype=author&amp;query=Silver,+D)\nView a PDF of the paper titled Prioritized Experience Replay, by Tom Schaul and 3 other authors\n[View PDF](https://arxiv.org/pdf/1511.05952)> > Abstract:\n> Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games. Comments:|Published at ICLR 2016|\nSubjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:1511.05952](https://arxiv.org/abs/1511.05952)[cs.LG]|\n|(or[arXiv:1511.05952v4](https://arxiv.org/abs/1511.05952v4)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1511.05952](https://doi.org/10.48550/arXiv.1511.05952)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Tom Schaul [[view email](https://arxiv.org/show-email/8b0fb15a/1511.05952)]\n**[[v1]](https://arxiv.org/abs/1511.05952v1)**Wed, 18 Nov 2015 20:54:44 UTC (1,194 KB)\n**[[v2]](https://arxiv.org/abs/1511.05952v2)**Thu, 19 Nov 2015 18:38:04 UTC (1,197 KB)\n**[[v3]](https://arxiv.org/abs/1511.05952v3)**Thu, 7 Jan 2016 01:53:42 UTC (1,217 KB)\n**[v4]**Thu, 25 Feb 2016 17:55:31 UTC (1,217 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Prioritized Experience Replay, by Tom Schaul and 3 other authors\n* [View PDF](https://arxiv.org/pdf/1511.05952)\n* [TeX Source](https://arxiv.org/src/1511.05952)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1511.05952&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1511.05952&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2015-11](https://arxiv.org/list/cs.LG/2015-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/1511.05952?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1511.05952)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1511.05952)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1511.05952)\n### [9 blog links](https://arxiv.org/tb/1511.05952)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/conf/iclr/iclr2016.html#SchaulQAS15)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/conf/iclr/SchaulQAS15)\n[Tom Schaul]()\n[John Quan]()\n[Ioannis Antonoglou]()\n[David Silver]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1511.05952)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Prioritized Experience Replay",
          "cleaned_query": "Prioritized Experience Replay"
        },
        {
          "success": true,
          "title": "Data Shapley: Equitable Valuation of Data for Machine Learning",
          "url": "https://arxiv.org/abs/1904.02868",
          "content": "# Statistics > Machine Learning\n\n**arXiv:1904.02868** (stat)\n\n\\[Submitted on 5 Apr 2019 ( [v1](https://arxiv.org/abs/1904.02868v1)), last revised 10 Jun 2019 (this version, v2)\\]\n\n# Title:Data Shapley: Equitable Valuation of Data for Machine Learning\n\nAuthors: [Amirata Ghorbani](https://arxiv.org/search/stat?searchtype=author&query=Ghorbani,+A), [James Zou](https://arxiv.org/search/stat?searchtype=author&query=Zou,+J)\n\nView a PDF of the paper titled Data Shapley: Equitable Valuation of Data for Machine Learning, by Amirata Ghorbani and James Zou\n\n[View PDF](https://arxiv.org/pdf/1904.02868)\n\n> Abstract:As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on $n$ data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley value uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (stat.ML); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:1904.02868](https://arxiv.org/abs/1904.02868) \\[stat.ML\\] |\n| (or [arXiv:1904.02868v2](https://arxiv.org/abs/1904.02868v2) \\[stat.ML\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1904.02868](https://doi.org/10.48550/arXiv.1904.02868) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Amirata Ghorbani \\[ [view email](https://arxiv.org/show-email/b338fc99/1904.02868)\\] **[\\[v1\\]](https://arxiv.org/abs/1904.02868v1)**\nFri, 5 Apr 2019 04:54:10 UTC (2,772 KB)\n**\\[v2\\]**\nMon, 10 Jun 2019 08:10:40 UTC (3,761 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Data Shapley: Equitable Valuation of Data for Machine Learning, by Amirata Ghorbani and James Zou\n\n- [View PDF](https://arxiv.org/pdf/1904.02868)\n- [TeX Source](https://arxiv.org/src/1904.02868)\n- [Other Formats](https://arxiv.org/format/1904.02868)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1904.02868&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1904.02868&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2019-04](https://arxiv.org/list/stat.ML/2019-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1904.02868?context=cs) [cs.AI](https://arxiv.org/abs/1904.02868?context=cs.AI) [cs.LG](https://arxiv.org/abs/1904.02868?context=cs.LG) [stat](https://arxiv.org/abs/1904.02868?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1904.02868)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1904.02868)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1904.02868)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1904.02868) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Data Shapley: Equitable Valuation of Data for Machine Learning",
          "cleaned_query": "Data Shapley: Equitable Valuation of Data for Machine Learning"
        }
      ],
      "generated_ideas": [
        "Influence-Weighted Prioritized Experience Replay\nUse TracIn-style gradient dot products to estimate each transition\u2019s influence on a held-out objective (e.g., validation return or specific failure states) and prioritize replay by positive influence rather than TD-error alone. Compare against classic PER on Atari and continuous control, measuring sample efficiency and robustness to noisy rewards.",
        "Data Shapley for RL Trajectories (Trajectory Shapley)\nExtend Data Shapley from i.i.d. supervised points to temporally correlated RL trajectories by defining each \u201cplayer\u201d as a trajectory segment or episode and the utility as final policy performance under PPO. Develop an efficient Monte Carlo estimator that reuses PPO checkpoints to avoid full retraining for each coalition sample.",
        "Checkpoint Influence Diagnostics for PPO Instability\nAdapt TracIn to PPO by tracing which on-policy minibatches (or collected rollouts) most influence later policy collapses or reward hacking behaviors. The deliverable is a debugging tool that flags harmful rollouts and identifies whether failures stem from advantage estimation, clipping, or value-function drift.",
        "Influence-Regularized PPO Updates\nAdd a regularizer to the PPO objective that penalizes updates where a small subset of rollouts has disproportionate influence on the policy gradient (estimated via influence functions or TracIn approximations). This aims to reduce sensitivity to outlier trajectories and improve training stability in sparse-reward or high-variance settings.",
        "Learning Replay Priorities by Meta-Optimizing PPO Return\nTrain a small \u201cpriority network\u201d that assigns replay weights to stored transitions; update it to maximize downstream PPO performance using gradient-through-checkpoint approximations (TracIn-like) as a credit assignment signal. This turns replay prioritization into a learnable module, benchmarked against fixed heuristics like TD-error and novelty.",
        "Influence-Based Poisoning and Defense in Deep RL\nConstruct minimally perturbed trajectory injections (analogous to influence-function training-set attacks) that steer PPO toward specific bad behaviors while remaining hard to detect. Then design defenses that downweight high-influence suspicious rollouts (via TracIn signals) and evaluate attack/defense tradeoffs on standard control suites.",
        "Counterfactual Data Removal (\u201cUnlearning\u201d) for PPO with Influence Estimates\nDevelop an approximate unlearning procedure for RL that removes the effect of specific episodes (e.g., unsafe demonstrations, corrupted rollouts) from a trained PPO agent using influence-function parameter updates rather than full retraining. Validate by measuring how quickly targeted behaviors disappear and how much overall performance is preserved.",
        "Multi-Objective Influence Attribution for Safety-Critical States\nCompute per-transition influence not just on average return but on targeted metrics (constraint violations, rare catastrophic states) by defining multiple test losses and tracing which training experiences drive each. Use this to curate replay buffers that improve safety metrics without sacrificing performance, comparing to naive constraint-penalty tuning.",
        "Layer-Selective TracIn for Efficient RL Data Valuation\nBorrow TracIn\u2019s \u201ccherry-pick layers\u201d idea to value experiences using only gradients from selected PPO submodules (e.g., policy head vs. value head) to cut computation. Study which layers yield the best fidelity-cost tradeoff and whether policy-head influence better predicts behavioral changes than full-network influence.",
        "Shapley-Guided Data Acquisition Policies for On-Policy RL\nTreat environment interaction as a data acquisition problem: use online estimates of trajectory Shapley (or influence) to decide which initial states, perturbations, or curriculum levels to sample next. Implement a sampling controller that biases rollout collection toward high-marginal-value regions and quantify gains in sample efficiency and generalization."
      ],
      "generated_ideas_raw": "1. **Influence-Weighted Prioritized Experience Replay**\n   Use TracIn-style gradient dot products to estimate each transition\u2019s influence on a held-out objective (e.g., validation return or specific failure states) and prioritize replay by positive influence rather than TD-error alone. Compare against classic PER on Atari and continuous control, measuring sample efficiency and robustness to noisy rewards.\n\n2. **Data Shapley for RL Trajectories (Trajectory Shapley)**\n   Extend Data Shapley from i.i.d. supervised points to temporally correlated RL trajectories by defining each \u201cplayer\u201d as a trajectory segment or episode and the utility as final policy performance under PPO. Develop an efficient Monte Carlo estimator that reuses PPO checkpoints to avoid full retraining for each coalition sample.\n\n3. **Checkpoint Influence Diagnostics for PPO Instability**\n   Adapt TracIn to PPO by tracing which on-policy minibatches (or collected rollouts) most influence later policy collapses or reward hacking behaviors. The deliverable is a debugging tool that flags harmful rollouts and identifies whether failures stem from advantage estimation, clipping, or value-function drift.\n\n4. **Influence-Regularized PPO Updates**\n   Add a regularizer to the PPO objective that penalizes updates where a small subset of rollouts has disproportionate influence on the policy gradient (estimated via influence functions or TracIn approximations). This aims to reduce sensitivity to outlier trajectories and improve training stability in sparse-reward or high-variance settings.\n\n5. **Learning Replay Priorities by Meta-Optimizing PPO Return**\n   Train a small \u201cpriority network\u201d that assigns replay weights to stored transitions; update it to maximize downstream PPO performance using gradient-through-checkpoint approximations (TracIn-like) as a credit assignment signal. This turns replay prioritization into a learnable module, benchmarked against fixed heuristics like TD-error and novelty.\n\n6. **Influence-Based Poisoning and Defense in Deep RL**\n   Construct minimally perturbed trajectory injections (analogous to influence-function training-set attacks) that steer PPO toward specific bad behaviors while remaining hard to detect. Then design defenses that downweight high-influence suspicious rollouts (via TracIn signals) and evaluate attack/defense tradeoffs on standard control suites.\n\n7. **Counterfactual Data Removal (\u201cUnlearning\u201d) for PPO with Influence Estimates**\n   Develop an approximate unlearning procedure for RL that removes the effect of specific episodes (e.g., unsafe demonstrations, corrupted rollouts) from a trained PPO agent using influence-function parameter updates rather than full retraining. Validate by measuring how quickly targeted behaviors disappear and how much overall performance is preserved.\n\n8. **Multi-Objective Influence Attribution for Safety-Critical States**\n   Compute per-transition influence not just on average return but on targeted metrics (constraint violations, rare catastrophic states) by defining multiple test losses and tracing which training experiences drive each. Use this to curate replay buffers that improve safety metrics without sacrificing performance, comparing to naive constraint-penalty tuning.\n\n9. **Layer-Selective TracIn for Efficient RL Data Valuation**\n   Borrow TracIn\u2019s \u201ccherry-pick layers\u201d idea to value experiences using only gradients from selected PPO submodules (e.g., policy head vs. value head) to cut computation. Study which layers yield the best fidelity-cost tradeoff and whether policy-head influence better predicts behavioral changes than full-network influence.\n\n10. **Shapley-Guided Data Acquisition Policies for On-Policy RL**\n   Treat environment interaction as a data acquisition problem: use online estimates of trajectory Shapley (or influence) to decide which initial states, perturbations, or curriculum levels to sample next. Implement a sampling controller that biases rollout collection toward high-marginal-value regions and quantify gains in sample efficiency and generalization.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Influence-Weighted Prioritized Experience Replay\nUse TracIn-style gradient dot products to estimate each transition\u2019s influence on a held-out objective (e.g., validation return or specific failure sta",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Data Shapley for RL Trajectories (Trajectory Shapley)\nExtend Data Shapley from i.i.d. supervised points to temporally correlated RL trajectories by defining each \u201cplayer\u201d as a trajectory segment or ep",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Checkpoint Influence Diagnostics for PPO Instability\nAdapt TracIn to PPO by tracing which on-policy minibatches (or collected rollouts) most influence later policy collapses or reward hacking behavior",
          "is_match": true
        },
        {
          "idea_idx": 3,
          "idea_text": "Influence-Regularized PPO Updates\nAdd a regularizer to the PPO objective that penalizes updates where a small subset of rollouts has disproportionate influence on the policy gradient (estimated via in",
          "is_match": true
        },
        {
          "idea_idx": 4,
          "idea_text": "Learning Replay Priorities by Meta-Optimizing PPO Return\nTrain a small \u201cpriority network\u201d that assigns replay weights to stored transitions; update it to maximize downstream PPO performance using grad",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Influence-Based Poisoning and Defense in Deep RL\nConstruct minimally perturbed trajectory injections (analogous to influence-function training-set attacks) that steer PPO toward specific bad behaviors",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Counterfactual Data Removal (\u201cUnlearning\u201d) for PPO with Influence Estimates\nDevelop an approximate unlearning procedure for RL that removes the effect of specific episodes (e.g., unsafe demonstrations",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Multi-Objective Influence Attribution for Safety-Critical States\nCompute per-transition influence not just on average return but on targeted metrics (constraint violations, rare catastrophic states) b",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Layer-Selective TracIn for Efficient RL Data Valuation\nBorrow TracIn\u2019s \u201ccherry-pick layers\u201d idea to value experiences using only gradients from selected PPO submodules (e.g., policy head vs. value hea",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Shapley-Guided Data Acquisition Policies for On-Policy RL\nTreat environment interaction as a data acquisition problem: use online estimates of trajectory Shapley (or influence) to decide which initial",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 43,
      "paper_title": "High-dimensional neuronal activity from low-dimensional latent dynamics: a solvable model",
      "contribution": "Shows analytically and empirically that low-dimensional recurrent latent dynamics can produce high-dimensional observed neural activity (after neuronal nonlinearities), and introduces a provably interpretable latent\u2011variable method (NCE) to recover the latent dimensionality from recordings.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11295,
      "output_tokens": 996,
      "predecessor_details": [
        {
          "success": true,
          "title": "Revisiting the high-dimensional geometry of population responses ...",
          "url": "https://www.biorxiv.org/content/10.1101/2024.02.16.580726v1",
          "content": "[Skip to main content](https://www.biorxiv.org/content/10.1101/2024.02.16.580726v1#main-content)\n\nNew Results\n\n# Revisiting the high-dimensional geometry of population responses in visual cortex\n\n[View ORCID Profile](http://orcid.org/0000-0002-5793-2517) Dean A.Pospisil, [View ORCID Profile](http://orcid.org/0000-0002-3638-8831) Jonathan W.Pillow\n\ndoi: https://doi.org/10.1101/2024.02.16.580726\n\nDean A. Pospisil\n\naPrinceton Neuroscience Institute, Princeton University, Princeton, NJ, 08544\n\n- [Find this author on Google Scholar](https://www.biorxiv.org/lookup/google-scholar?link_type=googlescholar&gs_type=author&author%5B0%5D=Dean%2BA.%2BPospisil%2B)\n- [Find this author on PubMed](https://www.biorxiv.org/lookup/external-ref?access_num=Pospisil%20DA&link_type=AUTHORSEARCH)\n- [Search for this author on this site](https://www.biorxiv.org/search/author1%3ADean%2BA.%2BPospisil%2B)\n- [ORCID record for Dean A. Pospisil](http://orcid.org/0000-0002-5793-2517)\n- For correspondence:\n[dp4846{at}princeton.edu](https://www.biorxiv.org/cdn-cgi/l/email-protection#86e2f6b2beb2b0fde7f2fbf6f4efe8e5e3f2e9e8a8e3e2f3)\n\nJonathan W. Pillow\n\naPrinceton Neuroscience Institute, Princeton University, Princeton, NJ, 08544\n\n- [Find this author on Google Scholar](https://www.biorxiv.org/lookup/google-scholar?link_type=googlescholar&gs_type=author&author%5B0%5D=Jonathan%2BW.%2BPillow%2B)\n- [Find this author on PubMed](https://www.biorxiv.org/lookup/external-ref?access_num=Pillow%20JW&link_type=AUTHORSEARCH)\n- [Search for this author on this site](https://www.biorxiv.org/search/author1%3AJonathan%2BW.%2BPillow%2B)\n- [ORCID record for Jonathan W. Pillow](http://orcid.org/0000-0002-3638-8831)\n\n- [Abstract](https://www.biorxiv.org/content/10.1101/2024.02.16.580726v1)\n- [Full Text](https://www.biorxiv.org/content/10.1101/2024.02.16.580726v1.full-text)\n- [Info/History](https://www.biorxiv.org/content/10.1101/2024.02.16.580726v1.article-info)\n- [Metrics](https://www.biorxiv.org/content/10.1101/2024.02.16.580726v1.article-metrics)\n- [Data/Code](https://www.biorxiv.org/content/10.1101/2024.02.16.580726v1.external-links)\n- [Preview PDF](https://www.biorxiv.org/content/10.1101/2024.02.16.580726v1.full.pdf+html)\n\n![Loading](https://www.biorxiv.org/sites/all/modules/contrib/panels_ajax_tab/images/loading.gif)\n\n## Abstract\n\nRecent advances in large-scale recording technology have spurred exciting new inquiries into the high-dimensional geometry of the neural code. However, characterizing this geometry from noisy neural responses, particularly in datasets with more neurons than trials, poses major statistical challenges. We address this problem by developing new tools for the accurate estimation of high-dimensional signal geometry. We apply these tools to investigate the geometry of representations in mouse primary visual cortex. Previous work has argued that these representations exhibit a power law, in which the _n_\u2019th principal component falls off as 1 _/n_. Here we show that response geometry in V1 is better described by a broken power law, in which two different exponents govern the falloff of early and late modes of population activity. Our analysis reveals that later modes decay more rapidly than previously suggested, resulting in a substantially larger fraction of signal variance contained in the early modes of population activity. We examined the signal representations of the early population modes and found them to have higher fidelity than even the most reliable neurons. Intriguingly there are many population modes not captured by classic models of primary visual cortex indicating there is highly redundant yet poorly characterized tuning across neurons. Furthermore, inhibitory neurons tend to co-activate in response to stimuli that drive the early modes consistent with a role in sharpening population level tuning. Overall, our novel and broadly applicable approach overturns prior results and reveals striking structure in a population sensory representation.\n\n**Significance Statement** The nervous system encodes the visual environment across millions of neurons. Such high-dimensional signals are difficult to estimate\u2014and consequently\u2014to characterize. We address this challenge with a novel statistical method that revises past conceptions of the complexity of encoding in primary visual cortex. We discover population encoding is dominated by approximately ten features while additional features account for much less of the representation than previously thought. Many dominant features are not explained by classic models indicating highly redundant encoding of poorly characterized nonlinear image features. Interestingly, inhibitory neurons respond in unison to dominant features consistent with a role in sharpening population representation. Overall, we discover striking properties of population visual representation with novel, broadly applicable, statistical tools.\n\n### Competing Interest Statement\n\nThe authors have declared no competing interest.\n\n## Footnotes\n\n- Please declare any competing interests here.\n\n- [https://doi.org/10.25378/janelia.6845348.v4](https://doi.org/10.25378/janelia.6845348.v4)\n\n\nCopyright\n\nThe copyright holder for this preprint is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under a [CC-BY-NC-ND 4.0 International license](http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n[Back to top](https://www.biorxiv.org/content/10.1101/2024.02.16.580726v1#page)\n\n[Previous](https://www.biorxiv.org/content/10.1101/2024.02.16.580691v1)[Next](https://www.biorxiv.org/content/10.1101/2024.02.16.580699v1)\n\nPosted\u00a0February 21, 2024.\n\n[Download PDF](https://www.biorxiv.org/content/10.1101/2024.02.16.580726v1.full.pdf)\n\n[Data/Code](https://www.biorxiv.org/content/early/2024/02/21/2024.02.16.580726.external-links)\n\n[Email](https://www.biorxiv.org/)\n\nThank you for your interest in spreading the word about bioRxiv.\n\nNOTE: Your email address is requested solely to identify you as the sender of this article.\n\nYour Email \\*\n\nYour Name \\*\n\nSend To \\*\n\nEnter multiple addresses on separate lines or separate them with commas.\n\nYou are going to email the following [Revisiting the high-dimensional geometry of population responses in visual cortex](https://www.biorxiv.org/content/10.1101/2024.02.16.580726v1)\n\nMessage Subject (Your Name) has forwarded a page to you from bioRxiv\n\nMessage Body (Your Name) thought you would like to see this page from the bioRxiv website.\n\nYour Personal Message\n\nCAPTCHA\n\nThis question is for testing whether or not you are a human visitor and to prevent automated spam submissions.\n\n[Share](https://www.biorxiv.org/)\n\nRevisiting the high-dimensional geometry of population responses in visual cortex\n\nDean A.Pospisil, Jonathan W.Pillow\n\nbioRxiv 2024.02.16.580726; doi: https://doi.org/10.1101/2024.02.16.580726\n\nShare This Article:Copy\n\n[![Twitter logo](https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/twitter.png)](http://twitter.com/share?url=https%3A//www.biorxiv.org/content/10.1101/2024.02.16.580726v1&text=Revisiting%20the%20high-dimensional%20geometry%20of%20population%20responses%20in%20visual%20cortex)[![Facebook logo](https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/fb-blue.png)](http://www.facebook.com/sharer.php?u=https%3A//www.biorxiv.org/content/10.1101/2024.02.16.580726v1&t=Revisiting%20the%20high-dimensional%20geometry%20of%20population%20responses%20in%20visual%20cortex)[![LinkedIn logo](https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/linkedin-32px.png)](http://www.linkedin.com/shareArticle?mini=true&url=https%3A//www.biorxiv.org/content/10.1101/2024.02.16.580726v1&title=Revisiting%20the%20high-dimensional%20geometry%20of%20population%20responses%20in%20visual%20cortex&summary=&source=bioRxiv)[![Mendeley logo](https://www.biorxiv.org/sites/all/modules/highwire/highwire/images/mendeley.png)](http://www.mendeley.com/import/?url=https%3A//www.b",
          "original_query": "High-dimensional geometry of population responses in visual cortex",
          "cleaned_query": "High-dimensional geometry of population responses in visual cortex"
        },
        {
          "success": true,
          "title": "Neural Manifolds for the Control of Movement - ScienceDirect.com",
          "url": "https://www.sciencedirect.com/science/article/pii/S0896627317304634",
          "content": "Perspective Neural Manifolds for the Control of Movement The analysis of neural dynamics in several brain cortices has consistently uncovered low-dimensional manifolds that capture a significant fraction of neural variability. These neural manifolds are spanned by specific patterns of correlated neural activity, the \u201cneural modes.\u201d We discuss a model for neural control of movement in which the time-dependent activation of these neural modes is the generator of motor behavior. This manifold-based view of motor cortex may lead to a better understanding of how the brain controls movement. Previous Next Main Text Since the work of Herbert Jasper ( Jasper et\u00a0al., 1958) and Ed Evarts ( Evarts, 1968), cortical function has been studied by recording single-neuron activity while animals perform a variety of behaviors, including decision making ( Newsome et\u00a0al., 1989), sensation ( Wurtz, 1969), and movement ( Georgopoulos et\u00a0al., 1982, Humphrey et\u00a0al., 1970). In the motor system, the main focus of this article, single neuron studies typically involved recordings during repeated, stereotypical movements. Many of these experiments sought explicit representations that relate single-neuron activity to specific movement covariates, including but not limited to target position, endpoint and joint kinematics, endpoint forces, and muscle activity ( Evarts, 1968, Georgopoulos et\u00a0al., 1982, Humphrey et\u00a0al., 1970, Morrow et\u00a0al., 2007, Thach, 1978). Although some of these efforts involved the decoding of population activity ( Georgopoulos et\u00a0al., 1982), they were restricted to models of non-interacting neurons whose individual activity was associated with specific movement covariates. However, some of these studies also identified single neurons whose activity did not represent movement parameters ( Churchland and Shenoy, 2007, Fetz, 1992, Scott, 2008). If neurons in primary motor cortex (M1) were to represent movement parameters, those representations ought to be most evident in corticomotoneuronal (CM) cells, which make direct connections onto\u00a0spinal motoneurons ( Fetz, 1992). Yet, many of these CM cells do not represent any specific movement covariate ( Fetz et\u00a0al., 1989). The ultimate role of M1 is to generate movement, not to represent it ( Churchland et\u00a0al., 2012, Cisek, 2006, Scott, 2004); thus, it is not surprising that many M1 neurons do not relate to any single movement covariate. The search for representations at the single-neuron level might actually divert us from understanding the neural control of movement. Early neural network simulations indicated that individual neurons need not explicitly encode movement covariates when the goal of M1 population activity is to generate realistic muscle activation patterns ( Fetz, 1992). The role of neurons that do not explicitly represent any movement covariate can be explained by recent work based on optimal feedback control theory, which postulates that the goal of motor cortex is to produce a desired movement and force, taking into account the state of the muscles. This hypothesis avoids the need for explicit representation of movement covariates by single neurons, though some neurons may still represent\u00a0movement covariates or high-level task characteristics as a byproduct of the necessary control signals ( Scott, 2008, Todorov, 2000). Recent and accelerating technical developments provide the experimental tools for monitoring the activity of large numbers of neurons, as well as the statistical and modeling tools for analyzing how these neural populations perform the computations necessary to plan and execute movement ( Gao and Ganguli, 2015). The challenge of understanding the neural control of movement by analyzing neural population activity is formidable, as population activity in any specific area not only reflects its intrinsic dynamics, but must also respond to its inputs and generate output projections based on the computations being performed ( Sussillo et\u00a0al., 2015). A simplification arises from the fact that neural computations are based on the joint activity of interconnected neurons ( Fetz, 1992, Hatsopoulos et\u00a0al., 1998, Shenoy et\u00a0al., 2013); the resulting population activity is thus likely constrained by the connectivity of the underlying network. Here we argue that the underlying network connectivity constrains these possible patterns of population activity ( Okun et\u00a0al., 2015, Sadtler et\u00a0al., 2014, Tsodyks et\u00a0al., 1999) and that the possible patterns are confined to a low-dimensional manifold ( Stopfer et\u00a0al., 2003, Yu et\u00a0al., 2009) spanned by a few independent patterns that we call \u201cneural modes.\u201d These neural modes capture a significant fraction of population covariance. It is the activation of these neural modes, rather than the activity of single neurons, that provides the basic building blocks of neural dynamics and function ( Luczak et\u00a0al., 2015, Sadtler et\u00a0al., 2014, Shenoy et\u00a0al., 2013). We thus propose a generative model of the activity of individual neurons based on the activation of neural modes, and explain how the parameters of the model can be identified using dimensionality reduction methods. We then review work showing that\u00a0these neural modes span task-specific neural manifolds in premotor and motor cortices. We propose that neural manifolds spanned by a surprisingly small number of neural modes are likely to simplify the neural control of movement and speculate on the potential learning mechanisms underlying the emergence of this low-dimensional organization. From Single Neurons to Neural Manifolds Current multi-electrode arrays (MEAs) allow for the simultaneous recording of about a hundred neurons. This is much more than the small numbers recorded with single electrodes but still a tiny fraction of the total number of neurons involved in movement generation. Despite this limitation, brain-machine interfaces (BMIs) based on these MEAs are able to predict reasonably well many behavioral variables ( Carmena et\u00a0al., 2003, Ethier et\u00a0al., 2012, Serruya et\u00a0al., 2002). What is the underlying reason for this success? Intuitively, it is\u00a0the high degree of correlation and redundancy across individual neural activity. This intuition has been recently made precise in elegant arguments on the low dimensionality of the stereotypical motor behaviors used in most motor control studies ( Gao\u00a0and Ganguli, 2015). The relatively small number of independent signals needed to control behavior during the execution\u00a0of such tasks only requires a small number of independent neural signals. These neural signals are the \u201clatent variables\u201d ( Cunningham and Yu, 2014) that describe the dynamics of the \u201cneural modes.\u201d The participation of individual neurons in neural modes is illustrated in Figure\u00a01 A. Note that each neural mode includes a large fraction of the neurons in the population and that a given neuron can participate in several neural modes. In this view, the time-dependent activity of individual neurons is simply a reflection of the latent variables ( Figure\u00a01 B) ( Kaufman et\u00a0al., 2016, Kobak et\u00a0al., 2016, Macke et\u00a0al., 2011). Consider the \u201cneural space\u201d in\u00a0 Figure\u00a01 C; each axis represents the activity of one of the N \u00a0recorded neurons (here, N = 3). Assuming that network connectivity constrains the possible patterns of population activity ( Okun et\u00a0al., 2015, Sadtler et\u00a0al., 2014, Tsodyks et\u00a0al., 1999), the population dynamics will not explore the full high-dimensional neural space but will instead remain confined to a low-dimensional surface within the full space, the \u201cneural manifold.\u201d In the simplest linear case, the neural manifold is flat, as is the hyperplane in Figure\u00a01 C, spanned by the two neural modes, u1 and u2. Download : Download high-res image (319KB) Download : Download full-size image Figure\u00a01. The Neural Manifold Hypothesis (A) Latent variables as a generative model for population activity. The relative area of the blue/green regions in each neuron represents the relative magnitude of ",
          "original_query": "Neural manifolds for the control of movement",
          "cleaned_query": "Neural manifolds for the control of movement"
        },
        {
          "success": true,
          "title": "Shaping dynamics with multiple populations in low-rank recurrent ...",
          "url": "https://arxiv.org/abs/2007.02062",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: An emerging paradigm proposes that neural computations can be understood at\nthe level of dynamical systems that govern low-dimensional trajectories of\ncollective neural activity. How the connectivity structure of a network\ndetermines the emergent dynamical system however remains to be clarified. Here\nwe consider a novel class of models, Gaussian-mixture low-rank recurrent\nnetworks, in which the rank of the connectivity matrix and the number of\nstatistically-defined populations are independent hyper-parameters. We show\nthat the resulting collective dynamics form a dynamical system, where the rank\nsets the dimensionality and the population structure shapes the dynamics. In\nparticular, the collective dynamics can be described in terms of a simplified\neffective circuit of interacting latent variables. While having a single,\nglobal population strongly restricts the possible dynamics, we demonstrate that\nif the number of populations is large enough, a rank-R network can approximate\nany R-dimensional dynamical system.\n \n \n \n \n Submission history From: Manuel Beiran [ view email]\n \n [v1] \n Sat, 4 Jul 2020 10:13:04 UTC (4,568 KB) [v2] \nTue, 17 Nov 2020 08:40:09 UTC (4,620 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > q-bio > arXiv:2007.02062\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Quantitative Biology > Neurons and Cognition\n\n arXiv:2007.02062 (q-bio)\n [Submitted on 4 Jul 2020 (v1), last revised 17 Nov 2020 (this version, v2)]\n\n Title: Shaping dynamics with multiple populations in low-rank recurrent networks\n\n Authors: Manuel Beiran, Alexis Dubreuil, Adrian Valente, Francesca Mastrogiuseppe, Srdjan Ostojic\n Download PDF\n Abstract: An emerging paradigm proposes that neural computations can be understood at the level of dynamical systems that govern low-dimensional trajectories of collective neural activity. How the connectivity structure of a network determines the emergent dynamical system however remains to be clarified. Here we consider a novel class of models, Gaussian-mixture low-rank recurrent networks, in which the rank of the connectivity matrix and the number of statistically-defined populations are independent hyper-parameters. We show that the resulting collective dynamics form a dynamical system, where the rank sets the dimensionality and the population structure shapes the dynamics. In particular, the collective dynamics can be described in terms of a simplified effective circuit of interacting latent variables. While having a single, global population strongly restricts the possible dynamics, we demonstrate that if the number of populations is large enough, a rank-R network can approximate any R-dimensional dynamical system.\n Comments: 29 pages, 7 figures \n Subjects: Neurons and Cognition (q-bio.NC) \n Cite as: arXiv:2007.02062 [q-bio.NC] \n (or arXiv:2007.02062v2 [q-bio.NC] for this version)\n https://doi.org/10.48550/arXiv.2007.02062 \n Focus to learn more \n arXiv-issued DOI via DataCite \n https://doi.org/10.1162/neco_a_01381 \n Related DOI: Focus to learn more \n DOI(s) linking to related resources \n \n\n Submission history\n\n From: Manuel Beiran [view email]\n [v1] Sat, 4 Jul 2020 10:13:04 UTC (4,568 KB)\n [v2] Tue, 17 Nov 2020 08:40:09 UTC (4,620 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * Other formats\n (license)\n Current browse context:\n q-bio.NC\n < prev | next >\n new | recent | 2007\n Change to browse by:\n q-bio\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs and how to get involved.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Shaping dynamics with multiple populations in low-rank recurrent networks",
          "cleaned_query": "Shaping dynamics with multiple populations in low-rank recurrent networks"
        },
        {
          "success": true,
          "title": "Random Features for Kernel Approximation: Algorithms, Theory",
          "url": "https://arxiv.org/abs/2004.11154",
          "content": "# Statistics > Machine Learning\n\n**arXiv:2004.11154** (stat)\n\n\\[Submitted on 23 Apr 2020 ( [v1](https://arxiv.org/abs/2004.11154v1)), last revised 11 Jul 2021 (this version, v5)\\]\n\n# Title:Random Features for Kernel Approximation: A Survey on Algorithms, Theory, and Beyond\n\nAuthors: [Fanghui Liu](https://arxiv.org/search/stat?searchtype=author&query=Liu,+F), [Xiaolin Huang](https://arxiv.org/search/stat?searchtype=author&query=Huang,+X), [Yudong Chen](https://arxiv.org/search/stat?searchtype=author&query=Chen,+Y), [Johan A.K. Suykens](https://arxiv.org/search/stat?searchtype=author&query=Suykens,+J+A)\n\nView a PDF of the paper titled Random Features for Kernel Approximation: A Survey on Algorithms, Theory, and Beyond, by Fanghui Liu and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2004.11154)\n\n> Abstract:Random features is one of the most popular techniques to speed up kernel methods in large-scale problems. Related works have been recognized by the NeurIPS Test-of-Time award in 2017 and the ICML Best Paper Finalist in 2019. The body of work on random features has grown rapidly, and hence it is desirable to have a comprehensive overview on this topic explaining the connections among various algorithms and theoretical results. In this survey, we systematically review the work on random features from the past ten years. First, the motivations, characteristics and contributions of representative random features based algorithms are summarized according to their sampling schemes, learning procedures, variance reduction properties and how they exploit training data. Second, we review theoretical results that center around the following key question: how many random features are needed to ensure a high approximation quality or no loss in the empirical/expected risks of the learned estimator. Third, we provide a comprehensive evaluation of popular random features based algorithms on several large-scale benchmark datasets and discuss their approximation quality and prediction performance for classification. Last, we discuss the relationship between random features and modern over-parameterized deep neural networks (DNNs), including the use of high dimensional random features in the analysis of DNNs as well as the gaps between current theoretical and empirical results. This survey may serve as a gentle introduction to this topic, and as a users' guide for practitioners interested in applying the representative algorithms and understanding theoretical results under various technical assumptions. We hope that this survey will facilitate discussion on the open problems in this topic, and more importantly, shed light on future research directions.\n\n| | |\n| --- | --- |\n| Comments: | Short version will be published on IEEE TPAMI |\n| Subjects: | Machine Learning (stat.ML); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2004.11154](https://arxiv.org/abs/2004.11154) \\[stat.ML\\] |\n| | (or [arXiv:2004.11154v5](https://arxiv.org/abs/2004.11154v5) \\[stat.ML\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2004.11154](https://doi.org/10.48550/arXiv.2004.11154) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Fanghui Liu \\[ [view email](https://arxiv.org/show-email/f3d87cff/2004.11154)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2004.11154v1)**\nThu, 23 Apr 2020 13:44:48 UTC (3,879 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2004.11154v2)**\nFri, 24 Apr 2020 20:32:47 UTC (1 KB) _(withdrawn)_\n\n**[\\[v3\\]](https://arxiv.org/abs/2004.11154v3)**\nSat, 4 Jul 2020 19:26:26 UTC (3,916 KB)\n\n**[\\[v4\\]](https://arxiv.org/abs/2004.11154v4)**\nTue, 16 Mar 2021 11:13:28 UTC (5,037 KB)\n\n**\\[v5\\]**\nSun, 11 Jul 2021 18:59:32 UTC (5,023 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Random Features for Kernel Approximation: A Survey on Algorithms, Theory, and Beyond, by Fanghui Liu and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2004.11154)\n- [TeX Source](https://arxiv.org/src/2004.11154)\n- [Other Formats](https://arxiv.org/format/2004.11154)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2004.11154&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2004.11154&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2020-04](https://arxiv.org/list/stat.ML/2020-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2004.11154?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2004.11154?context=cs.LG)\n\n[stat](https://arxiv.org/abs/2004.11154?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2004.11154)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2004.11154)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2004.11154)\n\n### [1 blog link](https://arxiv.org/tb/2004.11154)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2004.11154&description=Random Features for Kernel Approximation: A Survey on Algorithms, Theory, and Beyond) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2004.11154&title=Random Features for Kernel Approximation: A Survey on Algorithms, Theory, and Beyond)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLab",
          "original_query": "Kernel methods for deep learning (random feature / arc-cosine kernel literature)",
          "cleaned_query": "Kernel methods for deep learning (random feature"
        },
        {
          "success": true,
          "title": "Learning identifiable and interpretable latent models of ...",
          "url": "https://arxiv.org/abs/2011.04798",
          "content": "[2011.04798] Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:2011.04798\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:2011.04798**(stat)\n[Submitted on 9 Nov 2020]\n# Title:Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE\nAuthors:[Ding Zhou](https://arxiv.org/search/stat?searchtype=author&amp;query=Zhou,+D),[Xue-Xin Wei](https://arxiv.org/search/stat?searchtype=author&amp;query=Wei,+X)\nView a PDF of the paper titled Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE, by Ding Zhou and 1 other authors\n[View PDF](https://arxiv.org/pdf/2011.04798)> > Abstract:\n> The ability to record activities from hundreds of neurons simultaneously in the brain has placed an increasing demand for developing appropriate statistical techniques to analyze such data. Recently, deep generative models have been proposed to fit neural population responses. While these methods are flexible and expressive, the downside is that they can be difficult to interpret and identify. To address this problem, we propose a method that integrates key ingredients from latent models and traditional neural encoding models. Our method, pi-VAE, is inspired by recent progress on identifiable variational auto-encoder, which we adapt to make appropriate for neuroscience applications. Specifically, we propose to construct latent variable models of neural activity while simultaneously modeling the relation between the latent and task variables (non-neural variables, e.g. sensory, motor, and other externally observable states). The incorporation of task variables results in models that are not only more constrained, but also show qualitative improvements in interpretability and identifiability. We validate pi-VAE using synthetic data, and apply it to analyze neurophysiological datasets from rat hippocampus and macaque motor cortex. We demonstrate that pi-VAE not only fits the data better, but also provides unexpected novel insights into the structure of the neural codes. Subjects:|Machine Learning (stat.ML); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)|\nCite as:|[arXiv:2011.04798](https://arxiv.org/abs/2011.04798)[stat.ML]|\n|(or[arXiv:2011.04798v1](https://arxiv.org/abs/2011.04798v1)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.2011.04798](https://doi.org/10.48550/arXiv.2011.04798)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\nJournalreference:|NeurIPS 2020|\n## Submission history\nFrom: Ding Zhou [[view email](https://arxiv.org/show-email/6bb71a1d/2011.04798)]\n**[v1]**Mon, 9 Nov 2020 22:00:38 UTC (14,098 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE, by Ding Zhou and 1 other authors\n* [View PDF](https://arxiv.org/pdf/2011.04798)\n* [TeX Source](https://arxiv.org/src/2011.04798)\n[![license icon](https://arxiv.org/icons/licenses/by-sa-4.0.png)view license](http://creativecommons.org/licenses/by-sa/4.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2011.04798&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2011.04798&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2020-11](https://arxiv.org/list/stat.ML/2020-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/2011.04798?context=cs)\n[cs.LG](https://arxiv.org/abs/2011.04798?context=cs.LG)\n[q-bio](https://arxiv.org/abs/2011.04798?context=q-bio)\n[q-bio.NC](https://arxiv.org/abs/2011.04798?context=q-bio.NC)\n[stat](https://arxiv.org/abs/2011.04798?context=stat)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2011.04798)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2011.04798)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2011.04798)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2011.04798)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-vae",
          "cleaned_query": "Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-vae"
        },
        {
          "success": true,
          "title": "Deep Equals Shallow for ReLU Networks in Kernel Regimes",
          "url": "https://arxiv.org/abs/2009.14397",
          "content": "# Statistics > Machine Learning\n\n**arXiv:2009.14397** (stat)\n\n\\[Submitted on 30 Sep 2020 ( [v1](https://arxiv.org/abs/2009.14397v1)), last revised 26 Aug 2021 (this version, v4)\\]\n\n# Title:Deep Equals Shallow for ReLU Networks in Kernel Regimes\n\nAuthors: [Alberto Bietti](https://arxiv.org/search/stat?searchtype=author&query=Bietti,+A), [Francis Bach](https://arxiv.org/search/stat?searchtype=author&query=Bach,+F)\n\nView a PDF of the paper titled Deep Equals Shallow for ReLU Networks in Kernel Regimes, by Alberto Bietti and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2009.14397)\n\n> Abstract:Deep networks are often considered to be more expressive than shallow ones in terms of approximation. Indeed, certain functions can be approximated by deep networks provably more efficiently than by shallow ones, however, no tractable algorithms are known for learning such deep models. Separately, a recent line of work has shown that deep networks trained with gradient descent may behave like (tractable) kernel methods in a certain over-parameterized regime, where the kernel is determined by the architecture and initialization, and this paper focuses on approximation for such kernels. We show that for ReLU activations, the kernels derived from deep fully-connected networks have essentially the same approximation properties as their shallow two-layer counterpart, namely the same eigenvalue decay for the corresponding integral operator. This highlights the limitations of the kernel framework for understanding the benefits of such deep architectures. Our main theoretical result relies on characterizing such eigenvalue decays through differentiability properties of the kernel function, which also easily applies to the study of other kernels defined on the sphere.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (stat.ML); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2009.14397](https://arxiv.org/abs/2009.14397) \\[stat.ML\\] |\n| | (or [arXiv:2009.14397v4](https://arxiv.org/abs/2009.14397v4) \\[stat.ML\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2009.14397](https://doi.org/10.48550/arXiv.2009.14397) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Alberto Bietti \\[ [view email](https://arxiv.org/show-email/9f59015f/2009.14397)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2009.14397v1)**\nWed, 30 Sep 2020 02:37:43 UTC (47 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2009.14397v2)**\nFri, 9 Oct 2020 16:54:31 UTC (48 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/2009.14397v3)**\nWed, 17 Mar 2021 22:25:45 UTC (60 KB)\n\n**\\[v4\\]**\nThu, 26 Aug 2021 15:49:40 UTC (61 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Deep Equals Shallow for ReLU Networks in Kernel Regimes, by Alberto Bietti and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2009.14397)\n- [TeX Source](https://arxiv.org/src/2009.14397)\n- [Other Formats](https://arxiv.org/format/2009.14397)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nstat.ML\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2009.14397&function=prev&context=stat.ML)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2009.14397&function=next&context=stat.ML)\n\n[new](https://arxiv.org/list/stat.ML/new) \\| [recent](https://arxiv.org/list/stat.ML/recent) \\| [2020-09](https://arxiv.org/list/stat.ML/2020-09)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2009.14397?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2009.14397?context=cs.LG)\n\n[stat](https://arxiv.org/abs/2009.14397?context=stat)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2009.14397)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2009.14397)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2009.14397)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2009.14397&description=Deep Equals Shallow for ReLU Networks in Kernel Regimes) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2009.14397&title=Deep Equals Shallow for ReLU Networks in Kernel Regimes)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2009.14397) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Deep equals shallow for ReLU networks in kernel regimes",
          "cleaned_query": "Deep equals shallow for ReLU networks in kernel regimes"
        }
      ],
      "generated_ideas": [
        "Task-Conditioned pi-VAE for Separating Early vs Late Population Modes\nExtend pi-VAE with an explicit prior that factorizes latents into two groups corresponding to early high-fidelity modes and late rapidly-decaying modes, while conditioning both on task/stimulus variables. Test whether the early latents align with robust stimulus/task features and whether the late latents capture redundant or context-dependent structure (e.g., state, arousal), quantified via identifiability and cross-session stability.",
        "Inhibitory Co-activation as a Low-Rank \u201cSharpening\u201d Control Signal\nBuild and fit a low-rank recurrent network with multiple populations (excitatory/inhibitory subpopulations) where inhibitory population activity modulates the gain/curvature of early latent modes. Use the effective latent-variable circuit from Gaussian-mixture low-rank theory to generate falsifiable predictions: specific inhibitory latent trajectories should precede or coincide with increases in discriminability along early sensory modes.",
        "Random-Feature Neural Decoders with Sample-Efficient High-Dimensional Geometry Estimation\nCombine the high-dimensional geometry estimation tools (noise-aware eigen-spectrum estimation) with variance-reduced random-feature training to build decoders that remain accurate when neurons \u226b trials. Benchmark against linear decoders and deep nets in kernel regimes, explicitly relating decoder performance to the broken-power-law spectrum and reporting how many random features are needed per spectral regime.",
        "Cross-Area Comparison: Do Sensory and Motor Codes Share Spectral \u201cBreaks\u201d?\nApply the broken-power-law spectrum analysis to motor cortex manifold datasets and test whether movement-related population activity also shows two scaling regimes (e.g., planning vs execution modes). Link spectral breaks to behavioral epochs by computing time-resolved spectra and asking whether the \u201cearly-mode dominance\u201d increases during high-precision control periods predicted by manifold-based movement theories.",
        "From Spectrum to Mechanism: Inferring Population Structure in Low-Rank RNNs from Data\nDevelop an inverse method that takes an empirically observed eigen-spectrum (including a broken power law) and fits a Gaussian-mixture low-rank recurrent model whose population structure reproduces it. The actionable output is an estimated number of effective subpopulations and their interaction signs/strengths, which can be compared to cell-type labels (e.g., inhibitory subclasses) and validated via held-out dynamical predictions.",
        "Closed-Loop BMI Learning Constrained to Early High-Fidelity Modes\nImplement a BMI decoder/controller that explicitly restricts control to the early, high-fidelity manifold dimensions (estimated with noise-corrected geometry), then gradually relaxes constraints to include later modes. Test the hypothesis (from manifold control perspectives) that restricting to dominant modes accelerates learning and stabilizes performance, while later-mode inclusion enables finer adaptation but at increased variability.",
        "Time-Varying Broken Power Laws: Tracking Representational Compression During Learning or State Changes\nExtend the geometry estimation to a state-space or sliding-window framework to estimate how the two spectral exponents and breakpoint evolve with learning, attention, or arousal. Pair with pi-VAE task/state conditioning to attribute spectral changes to specific latent factors, yielding a concrete readout of \u201crepresentational compression/expansion\u201d dynamics and predictions for how inhibitory co-activation should covary with the breakpoint."
      ],
      "generated_ideas_raw": "1. **Broken-Power-Law\u2013Aware Dimensionality Selection for Neural Manifolds**  \nDevelop a principled estimator that detects the \u201cknee\u201d between early/late eigenmodes (broken power law) and uses it to choose manifold dimensionality for downstream neural dynamics/decoding. Validate on V1 and M1 datasets by comparing prediction/control performance (e.g., kinematics decoding) against standard variance-threshold and single-exponent power-law baselines.\n\n2. **Task-Conditioned pi-VAE for Separating Early vs Late Population Modes**  \nExtend pi-VAE with an explicit prior that factorizes latents into two groups corresponding to early high-fidelity modes and late rapidly-decaying modes, while conditioning both on task/stimulus variables. Test whether the early latents align with robust stimulus/task features and whether the late latents capture redundant or context-dependent structure (e.g., state, arousal), quantified via identifiability and cross-session stability.\n\n3. **Inhibitory Co-activation as a Low-Rank \u201cSharpening\u201d Control Signal**  \nBuild and fit a low-rank recurrent network with multiple populations (excitatory/inhibitory subpopulations) where inhibitory population activity modulates the gain/curvature of early latent modes. Use the effective latent-variable circuit from Gaussian-mixture low-rank theory to generate falsifiable predictions: specific inhibitory latent trajectories should precede or coincide with increases in discriminability along early sensory modes.\n\n4. **Discovering \u201cMissing\u201d V1 Features via Kernel Eigenfunctions Matched to Neural PCs**  \nConstruct kernel models whose eigenfunctions are directly fit to the empirically-estimated population eigenmodes (especially dominant modes not captured by classic V1 models). Use random-feature approximations to scale learning and then interpret learned features by stimulus optimization (maximize projection onto each learned eigenfunction) to characterize the nonlinear image statistics driving those modes.\n\n5. **Random-Feature Neural Decoders with Sample-Efficient High-Dimensional Geometry Estimation**  \nCombine the high-dimensional geometry estimation tools (noise-aware eigen-spectrum estimation) with variance-reduced random-feature training to build decoders that remain accurate when neurons \u226b trials. Benchmark against linear decoders and deep nets in kernel regimes, explicitly relating decoder performance to the broken-power-law spectrum and reporting how many random features are needed per spectral regime.\n\n6. **Cross-Area Comparison: Do Sensory and Motor Codes Share Spectral \u201cBreaks\u201d?**  \nApply the broken-power-law spectrum analysis to motor cortex manifold datasets and test whether movement-related population activity also shows two scaling regimes (e.g., planning vs execution modes). Link spectral breaks to behavioral epochs by computing time-resolved spectra and asking whether the \u201cearly-mode dominance\u201d increases during high-precision control periods predicted by manifold-based movement theories.\n\n7. **From Spectrum to Mechanism: Inferring Population Structure in Low-Rank RNNs from Data**  \nDevelop an inverse method that takes an empirically observed eigen-spectrum (including a broken power law) and fits a Gaussian-mixture low-rank recurrent model whose population structure reproduces it. The actionable output is an estimated number of effective subpopulations and their interaction signs/strengths, which can be compared to cell-type labels (e.g., inhibitory subclasses) and validated via held-out dynamical predictions.\n\n8. **Testing Kernel-Regime Limits: When Do Deep Models Recover Late-Mode Structure?**  \nUsing the \u201cdeep equals shallow in kernel regimes\u201d insight, design experiments where models are forced out of kernel behavior (e.g., finite width, feature learning) and test whether this preferentially improves prediction of late population modes versus early modes. Quantify whether feature learning is specifically needed to capture the redundant/poorly-characterized tuning seen in late V1 modes.\n\n9. **Closed-Loop BMI Learning Constrained to Early High-Fidelity Modes**  \nImplement a BMI decoder/controller that explicitly restricts control to the early, high-fidelity manifold dimensions (estimated with noise-corrected geometry), then gradually relaxes constraints to include later modes. Test the hypothesis (from manifold control perspectives) that restricting to dominant modes accelerates learning and stabilizes performance, while later-mode inclusion enables finer adaptation but at increased variability.\n\n10. **Time-Varying Broken Power Laws: Tracking Representational Compression During Learning or State Changes**  \nExtend the geometry estimation to a state-space or sliding-window framework to estimate how the two spectral exponents and breakpoint evolve with learning, attention, or arousal. Pair with pi-VAE task/state conditioning to attribute spectral changes to specific latent factors, yielding a concrete readout of \u201crepresentational compression/expansion\u201d dynamics and predictions for how inhibitory co-activation should covary with the breakpoint.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Task-Conditioned pi-VAE for Separating Early vs Late Population Modes\nExtend pi-VAE with an explicit prior that factorizes latents into two groups corresponding to early high-fidelity modes and late r",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Inhibitory Co-activation as a Low-Rank \u201cSharpening\u201d Control Signal\nBuild and fit a low-rank recurrent network with multiple populations (excitatory/inhibitory subpopulations) where inhibitory populati",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Random-Feature Neural Decoders with Sample-Efficient High-Dimensional Geometry Estimation\nCombine the high-dimensional geometry estimation tools (noise-aware eigen-spectrum estimation) with variance-r",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Cross-Area Comparison: Do Sensory and Motor Codes Share Spectral \u201cBreaks\u201d?\nApply the broken-power-law spectrum analysis to motor cortex manifold datasets and test whether movement-related population a",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "From Spectrum to Mechanism: Inferring Population Structure in Low-Rank RNNs from Data\nDevelop an inverse method that takes an empirically observed eigen-spectrum (including a broken power law) and fit",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Closed-Loop BMI Learning Constrained to Early High-Fidelity Modes\nImplement a BMI decoder/controller that explicitly restricts control to the early, high-fidelity manifold dimensions (estimated with n",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Time-Varying Broken Power Laws: Tracking Representational Compression During Learning or State Changes\nExtend the geometry estimation to a state-space or sliding-window framework to estimate how the t",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 44,
      "paper_title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks",
      "contribution": "The paper presents a novel training approach for Spiking Neural Networks that utilizes adaptive surrogate gradients and a guiding policy to enhance performance in sequential reinforcement learning tasks.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10409,
      "output_tokens": 1068,
      "predecessor_details": [
        {
          "success": true,
          "title": "Networks of spiking neurons: The third generation of neural network ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S0893608097000117",
          "content": "[Skip to main content](https://www.sciencedirect.com/www.sciencedirect.com#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/www.sciencedirect.com#screen-reader-main-title)\n\n- [Access through\u00a0**your organization**](https://www.sciencedirect.com/user/institution/login?targetUrl=%2Fscience%2Farticle%2Fpii%2FS0893608097000117)\n- [Purchase PDF](https://www.sciencedirect.com/getaccess/pii/S0893608097000117/purchase)\n\nSearch ScienceDirect\n\n## Article preview\n\n- [Abstract](https://www.sciencedirect.com/www.sciencedirect.com#preview-section-abstract)\n- [References (74)](https://www.sciencedirect.com/www.sciencedirect.com#preview-section-references)\n- [Cited by (2769)](https://www.sciencedirect.com/www.sciencedirect.com#preview-section-cited-by)\n\n## [Neural Networks](https://www.sciencedirect.com/journal/neural-networks)\n\n[Volume 10, Issue 9](https://www.sciencedirect.com/journal/neural-networks/vol/10/issue/9), December 1997, Pages 1659-1671\n\n# Contributed article Networks of spiking neurons: The third generation of neural network models\n\nAuthor links open overlay panelWolfgangMaass\n\nShow more\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/S0893-6080(97)00011-7](https://doi.org/10.1016/S0893-6080(97)00011-7) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S0893608097000117&orderBeanReset=true)\n\n## Abstract\n\nThe computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.\n\nRecommended articles\n\n- W. Bialek _et al._\n\n### [Reliability and information transmission in spiking neurons](https://www.sciencedirect.com/science/article/pii/016622369290005S)\n\n\n\n\n### Trends in Neuroscience\n\n\n\n(1992)\n\n- K.T. Judd _et al._\n\n### [Pulse propagation networks: a neural network model that uses temporal coding by action potentials](https://www.sciencedirect.com/science/article/pii/089360809390017Q)\n\n\n\n\n### Neural Networks\n\n\n\n(1993)\n\n- M. Leshno _et al._\n\n### [Multilayer feedforward networks with a nonpolynomial activation function can approximate any function](https://www.sciencedirect.com/science/article/pii/S0893608005801315)\n\n\n\n\n### Neural Networks\n\n\n\n(1993)\n\n- E.T. Rolls\n\n### [Brain mechanism for invariant visual recognition and learning](https://www.sciencedirect.com/science/article/pii/0376635794900620)\n\n\n\n\n### Behavioural Processes\n\n\n\n(1994)\n\n- W. Softky\n\n### [Sub-millisecond coincidence detection in active dendritic tree](https://www.sciencedirect.com/science/article/pii/0306452294901546)\n\n\n\n\n### Neuroscience\n\n\n\n(1994)\n\n- J.G. Taylor _et al._\n\n### [Mathematical analysis of a competitive network for attention](https://www.sciencedirect.com/science/article/pii/S0924650908700436)\n\n- M. Abeles\n\n### Corticonics: Neural circuits of the cerebral cortex\n\n\n(1991)\n\n- M. Abeles _et al._\n\n### Spatio-temporal firing patterns in the frontal cortex of behaving monkeys\n\n\n\n\n### Journal of Neurophilosiology\n\n\n\n(1993)\n\n- S.K. Aityan _et al._\n\n### Paradigm, logical performance, and training of recurrent refractory neural networks\n\n\n\n\n### Neural, Parallel & Scientific Computations\n\n\n\n(1993)\n\n\nM.A. Arbib\nW. Bair _et al._\n\n### Reliable temporal modulation in cortical spike trains in the awake monkey\n\nE. Bienenstock\n\n### A model of neocortex\n\n### Network: Computation in Neural Systems\n\n(1995)\n\nJ.M. Bower _et al._\n\n### The book of GENESIS: exploring realistic neural models with the General Neural Simulation System\n\n(1995)\n\nP.S. Churchland _et al._\n\n### The computational brain\n\n(1993)\n\nM.C. Crair _et al._\n\n### Non-Boltzmann dynamics in networks of spiking neurons\n\nB. DasGupta _et al._\n\n### The power of approximating: a comparison of activation functions\n\nM.R. DeYong _et al._\n\n### The design, fabrication, and test of a new VLSI hybrid analog-digital neural processing element\n\n### IEEE Transcripts on Neural Networks\n\n(1992)\n\nR.J. Douglas _et al._\n\n### Recurrent excitation in neocortical circuits\n\n### Science\n\n(1995)\n\nD. Ferster _et al._\n\n### Cracking the neuronal code\n\n### Science\n\n(1995)\n\nW. Gerstner\n\n### Associative memory in a network of \u201cbiological neurons\u201d\n\nW. Gerstner\n\n### Time structure of the activity in neural network models\n\n### Physics Review E\n\n(1995)\n\nW. Gerstner _et al._\n\n### How to describe neuronal activity: spikes, rates, or assemblies\n\nW. Gerstner _et al._\n\n### A biologically motivated and analytically soluble model of collective oscillations in the cortex: I. Theory of weak locking\n\n### Biological Cybernetics\n\n(1993)\n\nP.W. Goldberg _et al._\n\n### Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers\n\n### Machine Learning\n\n(1995)\n\nHerrmann, M., Hertz, J. A., & Pr\u00fcgel-Bennett, A. (in press). Analysis of synfire chains. Nordita...\nJ.J. Hopfield\n\n### Pattern recognition computation using action potential timing for stimulus representations\n\n### Nature\n\n(1995)\n\nJ.J. Hopfield _et al._\n\n### Rapid local synchronization of action potentials: towards computation with coupled integrate-and-fire neurons\n\nT. Horinchi _et al._\n\n### A delay-line based motion detection chip\n\nA. Jahnke _et al._\n\n### A SIMD/dataflow architecture for a neurocomputer for spike-processing neural networks (NESPINN)\n\n### MicroNeuro\n\n(1996)\n\nC.T. Jiu _et al._\n\n### An analog VLSI time-encoded pattern classifier\n\nD. Johnston _et al._\n\n### Foundations of cellular neurophisiology\n\n(1995)\n\nKarpinski, M., & Macintyre, A. (in press). Polynomial bounds for VC-dimension of sigmoidal and general Pfaffian neural...\nR. Kempter _et al._\n\n### Temporal coding in the sub-millisecond range: model of barn owl auditory pathway\n\nC. Koch _et al._\n\n### Multiplying with synapses and neurons\n\nP. Koiran\n\n### VC-dimension in circuit complexity\n\nJ. Kr\u00fcger _et al._\n\n### Multielectrode investigation of monkey striate cortex: spike train correlations in the infragranular layers\n\n### Journal of Neurophysiology\n\n(1988)\n\nView more references\n\n- ### [Deep learning in spiking neural networks](https://www.sciencedirect.com/science/article/pii/S0893608018303332)\n\n\n\n2019, Neural Networks\n\n\n\nShow abstract\n\n\n\nIn recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans.\n\n\n\nNeurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier ",
          "original_query": "Networks of spiking neurons: The third generation of neural network models",
          "cleaned_query": "Networks of spiking neurons: The third generation of neural network models"
        },
        {
          "success": true,
          "title": "MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization",
          "url": "https://arxiv.org/abs/2511.12199",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2511.12199** (cs)\n\n\\[Submitted on 15 Nov 2025\\]\n\n# Title:MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization\n\nAuthors: [Runhao Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang,+R), [Chengzhi Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang,+C), [Rui Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan,+R), [Huajin Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+H)\n\nView a PDF of the paper titled MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization, by Runhao Jiang and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2511.12199) [HTML (experimental)](https://arxiv.org/html/2511.12199v1)\n\n> Abstract:The surrogate gradient (SG) method has shown significant promise in enhancing the performance of deep spiking neural networks (SNNs), but it also introduces vulnerabilities to adversarial attacks. Although spike coding strategies and neural dynamics parameters have been extensively studied for their impact on robustness, the critical role of gradient magnitude, which reflects the model's sensitivity to input perturbations, remains underexplored. In SNNs, the gradient magnitude is primarily determined by the interaction between the membrane potential distribution (MPD) and the SG function. In this study, we investigate the relationship between the MPD and SG and its implications for improving the robustness of SNNs. Our theoretical analysis reveals that reducing the proportion of membrane potential lying within the gradient-available range of the SG function effectively mitigates the sensitivity of SNNs to input perturbations. Building upon this insight, we propose a novel MPD-driven surrogate gradient regularization (MPD-SGR) method, which enhances robustness by explicitly regularizing the MPD based on its interaction with the SG function. Extensive experiments across multiple image classification benchmarks and diverse network architectures confirm that the MPD-SGR method significantly enhances the resilience of SNNs to adversarial perturbations and exhibits strong generalizability across diverse network configurations, SG function variants, and spike encoding schemes.\n\n| | |\n| --- | --- |\n| Comments: | Accepted by AAAI 2026 |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2511.12199](https://arxiv.org/abs/2511.12199) \\[cs.LG\\] |\n| (or [arXiv:2511.12199v1](https://arxiv.org/abs/2511.12199v1) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2511.12199](https://doi.org/10.48550/arXiv.2511.12199) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Runhao Jiang \\[ [view email](https://arxiv.org/show-email/e9b8905a/2511.12199)\\] **\\[v1\\]**\nSat, 15 Nov 2025 13:12:20 UTC (464 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization, by Runhao Jiang and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2511.12199)\n- [HTML (experimental)](https://arxiv.org/html/2511.12199v1)\n- [TeX Source](https://arxiv.org/src/2511.12199)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2511.12199&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2511.12199&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2025-11](https://arxiv.org/list/cs.LG/2025-11)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2511.12199?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2511.12199)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2511.12199)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2511.12199)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2511.12199) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Surrogate gradient learning in spiking neural networks",
          "cleaned_query": "Surrogate gradient learning in spiking neural networks"
        },
        {
          "success": true,
          "title": "Elucidating the theoretical underpinnings of surrogate gradient ...",
          "url": "https://arxiv.org/abs/2404.14964",
          "content": "[2404.14964] Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2404.14964\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Neural and Evolutionary Computing\n**arXiv:2404.14964**(cs)\n[Submitted on 23 Apr 2024 ([v1](https://arxiv.org/abs/2404.14964v1)), last revised 17 Nov 2024 (this version, v3)]\n# Title:Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks\nAuthors:[Julia Gygax](https://arxiv.org/search/cs?searchtype=author&amp;query=Gygax,+J),[Friedemann Zenke](https://arxiv.org/search/cs?searchtype=author&amp;query=Zenke,+F)\nView a PDF of the paper titled Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks, by Julia Gygax and Friedemann Zenke\n[View PDF](https://arxiv.org/pdf/2404.14964)> > Abstract:\n> Training spiking neural networks to approximate universal functions is essential for studying information processing in the brain and for neuromorphic computing. Yet the binary nature of spikes poses a challenge for direct gradient-based training. Surrogate gradients have been empirically successful in circumventing this problem, but their theoretical foundation remains elusive. Here, we investigate the relation of surrogate gradients to two theoretically well-founded approaches. On the one hand, we consider smoothed probabilistic models, which, due to the lack of support for automatic differentiation, are impractical for training multi-layer spiking neural networks but provide derivatives equivalent to surrogate gradients for single neurons. On the other hand, we investigate stochastic automatic differentiation, which is compatible with discrete randomness but has not yet been used to train spiking neural networks. We find that the latter gives surrogate gradients a theoretical basis in stochastic spiking neural networks, where the surrogate derivative matches the derivative of the neuronal escape noise function. This finding supports the effectiveness of surrogate gradients in practice and suggests their suitability for stochastic spiking neural networks. However, surrogate gradients are generally not gradients of a surrogate loss despite their relation to stochastic automatic differentiation. Nevertheless, we empirically confirm the effectiveness of surrogate gradients in stochastic multi-layer spiking neural networks and discuss their relation to deterministic networks as a special case. Our work gives theoretical support to surrogate gradients and the choice of a suitable surrogate derivative in stochastic spiking neural networks. Comments:|26 pages, 7 figures + 3 supplementary figures|\nSubjects:|Neural and Evolutionary Computing (cs.NE); Neurons and Cognition (q-bio.NC)|\nCite as:|[arXiv:2404.14964](https://arxiv.org/abs/2404.14964)[cs.NE]|\n|(or[arXiv:2404.14964v3](https://arxiv.org/abs/2404.14964v3)[cs.NE]for this version)|\n|[https://doi.org/10.48550/arXiv.2404.14964](https://doi.org/10.48550/arXiv.2404.14964)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Friedemann Zenke [[view email](https://arxiv.org/show-email/594cfc94/2404.14964)]\n**[[v1]](https://arxiv.org/abs/2404.14964v1)**Tue, 23 Apr 2024 12:20:09 UTC (2,940 KB)\n**[[v2]](https://arxiv.org/abs/2404.14964v2)**Thu, 6 Jun 2024 12:48:18 UTC (2,977 KB)\n**[v3]**Sun, 17 Nov 2024 18:42:05 UTC (3,038 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks, by Julia Gygax and Friedemann Zenke\n* [View PDF](https://arxiv.org/pdf/2404.14964)\n* [TeX Source](https://arxiv.org/src/2404.14964)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.NE\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2404.14964&amp;function=prev&amp;context=cs.NE) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2404.14964&amp;function=next&amp;context=cs.NE)\n[new](https://arxiv.org/list/cs.NE/new)|[recent](https://arxiv.org/list/cs.NE/recent)|[2024-04](https://arxiv.org/list/cs.NE/2024-04)\nChange to browse by:\n[cs](https://arxiv.org/abs/2404.14964?context=cs)\n[q-bio](https://arxiv.org/abs/2404.14964?context=q-bio)\n[q-bio.NC](https://arxiv.org/abs/2404.14964?context=q-bio.NC)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2404.14964)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2404.14964)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2404.14964)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, commun",
          "original_query": "Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks",
          "cleaned_query": "Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks"
        },
        {
          "success": true,
          "title": "Spiking Neural Network Actor\u2013Critic Reinforcement Learning with Temporal Coding and Reward-Modulated Plasticity",
          "url": "https://link.springer.com/article/10.3103/S0027134924702400?error=cookies_not_supported&code=20757227-5dd3-4b44-8a76-2a80bd079586",
          "content": "# Spiking Neural Network Actor\u2013Critic Reinforcement Learning with Temporal Coding and Reward-Modulated Plasticity\n\n- Published: 22 March 2025\n\n- Volume\u00a079,\u00a0pages S944\u2013S952, (2024)\n- [Cite this article](https://link.springer.com/article/10.3103/S0027134924702400?error=cookies_not_supported&code=20757227-5dd3-4b44-8a76-2a80bd079586#citeas)\n\n[![](https://media.springernature.com/w144/springer-static/cover-hires/journal/11972?as=webp)Moscow University Physics Bulletin](https://link.springer.com/journal/11972) [Aims and scope](https://link.springer.com/journal/11972/aims-and-scope)\n\n### Abstract\n\nThe article presents an algorithm for adjusting the weights of the spike neural network of the actor\u2013critic architecture. A feature of the algorithm is the use of time coding of input data. The critic neuron is applied to calculate the change in the expected value of the action performed based on the difference in spike times received by the critic when processing the previous and current states. The change in the weights of the synaptic connections of the actor and critic neurons is carried out under the influence of local plasticity (spike\u2013timing-dependent plasticity), in which the change in weight depends on the received value of the expected reward. The proposed learning algorithm was tested to solve the problem of holding a cart pole, in which it demonstrated its effectiveness. The proposed algorithm is an important step towards the implementation of reinforcement learning algorithms for spiking neural networks on neuromorphic computing devices.\n\nThis is a preview of subscription content, [log in via an institution](https://wayf.springernature.com?redirect_uri=https%3A%2F%2Flink.springer.com%2Farticle%2F10.3103%2FS0027134924702400%3Ferror%3Dcookies_not_supported%26code%3D20757227-5dd3-4b44-8a76-2a80bd079586) to check access.\n\n## Access this article\n\n[Log in via an institution](https://wayf.springernature.com?redirect_uri=https%3A%2F%2Flink.springer.com%2Farticle%2F10.3103%2FS0027134924702400%3Ferror%3Dcookies_not_supported%26code%3D20757227-5dd3-4b44-8a76-2a80bd079586)\n\n## Subscribe and save\n\nSpringer+ Basic\n\n\u20ac32.70 /Month\n\n- Get 10 units per month\n- Download Article/Chapter or eBook\n- 1 Unit = 1 Article or 1 Chapter\n- Cancel anytime\n\n[Subscribe now](https://link.springer.com/product/springer-plus)\n\n## Buy Now\n\nBuy article PDF 39,95 \u20ac\n\nPrice includes VAT (Australia)\n\nInstant access to the full article PDF.\n\n[Institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/journals)\n\n**Fig. 1**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig1_HTML.png)\n\n**Fig. 2**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig2_HTML.png)\n\n**Fig. 3**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig3_HTML.png)\n\n**Fig. 4**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig4_HTML.png)\n\n**Fig. 5**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig5_HTML.png)\n\n**Fig. 6**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig6_HTML.png)\n\n**Fig. 7**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig7_HTML.png)\n\n**Fig. 8**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig8_HTML.png)\n\n**Fig. 9**\n\n![](https://media.springernature.com/m312/springer-static/image/art%3A10.3103%2FS0027134924702400/MediaObjects/11972_2025_8749_Fig9_HTML.png)\n\n### Explore related subjects\n\nDiscover the latest articles and news from researchers in related subjects, suggested using machine learning.\n\n- [Spike-timing-dependent plasticity](https://link.springer.com/subjects/spike-timing-dependent-plasticity)\n- [Neural encoding](https://link.springer.com/subjects/neural-encoding)\n- [Neural decoding](https://link.springer.com/subjects/neural-decoding)\n- [Synaptic plasticity](https://link.springer.com/subjects/synaptic-plasticity)\n- [Computational Neuroscience](https://link.springer.com/subjects/computational-neuroscience)\n- [Stochastic Learning and Adaptive Control](https://link.springer.com/subjects/stochastic-learning-and-adaptive-control)\n\n## Notes\n\n1. https://motivnt.ru/neurochip-altai.\n\n2. https://scikit-learn.org/stable/modules/preprocessing.html.\n\n\n## REFERENCES\n\n01. J. Zhu, T. Zhang, Yu. Yang, and R. Huang, Appl. Phys. Rev. **7**, 11312 (2020). [https://doi.org/10.1063/1.5118217](https://doi.org/10.1063/1.5118217)\n\n [Article](https://doi.org/10.1063%2F1.5118217) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=&journal=Appl.%20Phys.%20Rev.&doi=10.1063%2F1.5118217&volume=7&publication_year=2020&author=Zhu%2CJ.&author=Zhang%2CT.&author=Yang%2CYu.&author=Huang%2CR.)\n\n02. D. Ielmini and S. Menzel, in _Resistive Switching_, Ed. by D. Ielmini and R. Waser (Wiley, 2016), p. 317. [https://doi.org/10.1002/9783527680870.ch11](https://doi.org/10.1002/9783527680870.ch11)\n\n [Book](https://doi.org/10.1002%2F9783527680870.ch11) [MATH](http://www.emis.de/MATH-item?1416.65577) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Resistive%20Switching&doi=10.1002%2F9783527680870.ch11&publication_year=2016&author=Ielmini%2CD.&author=Menzel%2CS.)\n\n03. Yu. V. Pershin and M. Di Ventra, Neural Networks **23**, 881 (2010). [https://doi.org/10.1016/j.neunet.2010.05.001](https://doi.org/10.1016/j.neunet.2010.05.001)\n\n [Article](https://doi.org/10.1016%2Fj.neunet.2010.05.001) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=&journal=Neural%20Networks&doi=10.1016%2Fj.neunet.2010.05.001&volume=23&publication_year=2010&author=Pershin%2CYu.%20V.&author=Di%20Ventra%2CM.)\n\n04. K. Berggren, Q. Xia, K. K. Likharev, D. B. Strukov, H. Jiang, T. Mikolajick, D. Querlioz, M. Salinga, J. R. Erickson, Sh. Pi, F. Xiong, P. Lin, C. Li, Yu. Chen, Sh. Xiong, B. D. Hoskins, M. W. Daniels, A. Madhavan, J. A. Liddle, J. J. Mcclelland, Yu. Yang, J. Rupp, S. S. Nonnenmann, K.-T. Cheng, N. Gong, M. A. Lastras-Monta\u00f1o, A. A. Talin, A. Salleo, B. J. Shastri, T. F. De Lima, P. Prucnal, A. N. Tait, Yi. Shen, H. Meng, Ch. Roques-Carmes, Z. Cheng, H. Bhaskaran, D. Jariwala, H. Wang, J. M. Shainline, K. Segall, J. J. Yang, K. Roy, S. Datta, and A. Raychowdhury, Nanotechnology **32**, 012002 (2020). [https://doi.org/10.1088/1361-6528/aba70f](https://doi.org/10.1088/1361-6528/aba70f)\n\n05. B. Rajendran, A. Sebastian, M. Schmuker, N. Srinivasa, and E. Eleftheriou, IEEE Signal Process. Mag. **36**, 97 (2019). [https://doi.org/10.1109/msp.2019.2933719](https://doi.org/10.1109/msp.2019.2933719)\n\n [Article](https://doi.org/10.1109%2Fmsp.2019.2933719) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=&journal=IEEE%20Signal%20Process.%20Mag.&doi=10.1109%2Fmsp.2019.2933719&volume=36&publication_year=2019&author=Rajendran%2CB.&author=Sebastian%2CA.&author=Schmuker%2CM.&author=Srinivasa%2CN.&author=Eleftheriou%2CE.)\n\n06. P. A. Merolla, J. V. Arthur, R. Alvarez-Icaza, A. S. Cassidy, J. Sawada, F. Akopyan, B. L. Jackson, N. Imam, Ch. Guo, Yu. Nakamura, B. Brezzo, I. Vo, S. K. Esser, R. Appuswamy, B. Taba, A. Amir, M. D. Flickner, W. P. Risk, R. Manohar, and D. S. Modha, Science **345**, 668 (2014). [https://doi.org/10.1126/science.1254642](https://doi.org/10.1126/science.1254642)\n\n [Article](https://doi.org/10.1126%2Fscience.1254642) [ADS](http://adsabs.harvard.edu/cgi-bin/nph-data_query?link_type=ABSTRACT&bibcode=2014Sci...345..668M) [Google Scholar](http://scholar.google.com/scholar_lookup?&title=&journal=Science&doi=10.1126%2Fscience.1254642&volume=345&",
          "original_query": "Reinforcement learning using a continuous time actor-critic framework with spiking neurons",
          "cleaned_query": "Reinforcement learning using a continuous time actor-critic framework with spiking neurons"
        },
        {
          "success": true,
          "title": "[2204.02372] Jump-Start Reinforcement Learning - arXiv",
          "url": "https://arxiv.org/abs/2204.02372",
          "content": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
          "original_query": "Jump-start reinforcement learning",
          "cleaned_query": "Jump-start reinforcement learning"
        },
        {
          "success": true,
          "title": "Recurrent Experience Replay in Distributed Reinforcement Learning",
          "url": "https://openreview.net/forum?id=r1lyTjAqYX",
          "content": "[![back arrow](https://openreview.net/images/arrow_left.svg)Go to **ICLR 2019 Conference** homepage](https://openreview.net/group?id=ICLR.cc/2019/Conference)\n\n\u00d7\n\n## Recurrent Experience Replay in Distributed Reinforcement Learning [![](https://openreview.net/images/pdf_icon_blue.svg)](https://openreview.net/pdf?id=r1lyTjAqYX)\n\n## Blind Submission by Conference \u2022 Recurrent Experience Replay in Distributed Reinforcement Learning\n\n[Steven Kapturowski](https://openreview.net/profile?email=skapturowski%40google.com), [Georg Ostrovski](https://openreview.net/profile?email=ostrovski%40google.com), [John Quan](https://openreview.net/profile?email=johnquan%40google.com), [Remi Munos](https://openreview.net/profile?email=munos%40google.com), [Will Dabney](https://openreview.net/profile?email=wdabney%40google.com)\n\nPublished: 20 Dec 2018, Last Modified: 05 May 2023ICLR 2019 Conference Blind SubmissionReaders: Everyone[Show Bibtex](https://openreview.net/forum?id=r1lyTjAqYX) [Show Revisions](https://openreview.net/revisions?id=r1lyTjAqYX)\n\nKeywords: RNN, LSTM, experience replay, distributed training, reinforcement learning\n\nTL;DR: Investigation on combining recurrent neural networks and experience replay leading to state-of-the-art agent on both Atari-57 and DMLab-30 using single set of hyper-parameters.\n\nAbstract: Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the effects of parameter lag resulting in representational drift and recurrent state staleness and empirically derive an improved training strategy. Using a single network architecture and fixed set of hyper-parameters, the resulting agent, Recurrent Replay Distributed DQN, quadruples the previous state of the art on Atari-57, and matches the state of the art on DMLab-30. It is the first agent to exceed human-level performance in 52 of the 57 Atari games.\n\nCode: [![Papers with Code](https://openreview.net/images/pwc_icon.svg) 3 community implementations](https://paperswithcode.com/paper/?openreview=r1lyTjAqYX)\n\nData: [Arcade Learning Environment](https://paperswithcode.com/dataset/arcade-learning-environment), [DQN Replay Dataset](https://paperswithcode.com/dataset/dqn-replay-dataset)\n\n* * *\n\nReply Type:\n\nall\n\n- Select All\n- Paper765 Meta Review\n- Paper765 Official Review\n- Paper765 Official Comment\n- Paper765 Public Comment\n\nAuthor:\n\neverybody\n\n- Select All\n- Paper765 Authors\n- Paper765 AnonReviewer1\n- Paper765 AnonReviewer2\n- Paper765 AnonReviewer3\n- Paper765 Area Chair1\n- (anonymous)\n\nVisible To:\n\nall readers\n\n- Select All\n- everyone\n\nHidden From:\n\nnobody\n\n- Select All\n- everyone\n\n22 Replies\n\n[\\[\u2013\\]](https://openreview.net/forum?id=r1lyTjAqYX) [\\[+\\]](https://openreview.net/forum?id=r1lyTjAqYX)\n\n## Valuable insights on training reinforcement learning with recurrent neural networks at scale\n\n## Meta Review of Paper765 by Area Chair1 \u2022 Valuable insights on training reinforcement learning with recurrent neural networks at scale\n\nICLR 2019 Conference Paper765 Area Chair1\n\n13 Dec 2018, 16:33 (modified: 20 Dec 2018, 20:08)ICLR 2019 Conference Paper765 Meta ReviewReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=BkeeQlUelV)\n\nMetareview: The paper proposes a new distributed DQN algorithm that combines recurrent neural networks with distributed prioritized replay memory. The authors systematically compare three types of initialization strategies for training the recurrent models. The thorough investigation is cited as a valuable contribution by all reviewers, with reviewer 1 noting that the study would be of interest to \"anyone using recurrent networks on RL tasks\". Empirical results on Atari and DMLab are impressive.\nThe reviewers noted several weaknesses in their original reviews. These included issues of clarity, a need for more detailed ablation studies, and need to more carefully document the empirical setup. A further question was raised on whether the empirical results could be complemented with theoretical or conceptual insights.\nThe authors carefully addressed all concerns raised during the reviewing and rebuttal period. They took exceptional care to clarify their writing, document experiment details, and ran a large set of additional experiments as suggested by the reviewers. The AC feels that the review period for the paper was particularly productive and would like to thank the reviewers and authors.\nThe reviewers and AC agree that the paper makes a significant contribution to the field and should be accepted.\n\nRecommendation: Accept (Poster)\n\nConfidence: 4: The area chair is confident but not absolutely certain\n\n[\\[\u2013\\]](https://openreview.net/forum?id=r1lyTjAqYX) [\\[+\\]](https://openreview.net/forum?id=r1lyTjAqYX)\n\n## All Reviewers\n\n## Official Comment by Paper765 Authors \u2022 All Reviewers\n\nICLR 2019 Conference Paper765 Authors\n\n20 Nov 2018, 12:41ICLR 2019 Conference Paper765 Official CommentReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=SkxyvPTZ0X)\n\nComment: We first want to thank all the reviewers and commenters for their close reading and constructive feedback. We have revised or expanded most of our experiments and attempted to clarify the text in various locations. Additionally, as requested we are including (in the appendix) a figure comparing the sample efficiency of R2D2 with Rainbow, Ape-X, and Reactor on Atari. As detailed in our individual responses we have extended the ablations and rerun our experiments to address some concerns. This resulted in slightly improved performance, which is now averaged over three seeds.\nWe intend to add further ablation results (on life-loss signal) and our own rerun of IMPALA with matched action-set before the revision period ends.\n\n[\\[\u2013\\]](https://openreview.net/forum?id=r1lyTjAqYX) [\\[+\\]](https://openreview.net/forum?id=r1lyTjAqYX)\n\n## All Reviewers\n\n## Official Comment by Paper765 Authors \u2022 All Reviewers\n\nICLR 2019 Conference Paper765 Authors\n\n27 Nov 2018, 10:31ICLR 2019 Conference Paper765 Official CommentReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=ryx6VXJoCm)\n\nComment: We thank all reviewers once again for the careful reading of the paper and the helpful comments.\nWe have updated our ablations, now including two life-loss ablations, and provided complete details on the feed-forward ablation including human-normalized scores and sample efficiency data.\nFinally, we have updated the paper with our own re-run of the IMPALA agent on DMLab with the new action-set and longer training time, for a fairer comparison with R2D2. To explore the potential of our agent further, we also added a version of R2D2 more closely matching the Deep IMPALA architecture (deep ResNet + asymmetric reward clipping). Both our re-run of IMPALA and R2D2 achieve new SOTA scores on DMLab-30. We intend to report all DMLab-30 scores at 10B environment frames, but have restricted to 5B frames for this revision as these runs have not all completed at the time of the revision deadline.\n\n[\\[\u2013\\]](https://openreview.net/forum?id=r1lyTjAqYX) [\\[+\\]](https://openreview.net/forum?id=r1lyTjAqYX)\n\n## Re: All Reviewers\n\n## Official Comment by Paper765 AnonReviewer1 \u2022 Re: All Reviewers\n\nICLR 2019 Conference Paper765 AnonReviewer1\n\n27 Nov 2018, 17:02ICLR 2019 Conference Paper765 Official CommentReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=S1xbJJBj0Q)\n\nComment: Thanks, very interesting! Could you please just add a short description of R2D2+ in Table 1's caption, for the final version?\n\n[\\[\u2013\\]](https://openreview.net/forum?id=r1lyTjAqYX) [\\[+\\]](https://openreview.net/forum?id=r1lyTjAqYX)\n\n## Re: All Reviewers\n\n## Official Comment by Paper765 Authors \u2022 Re: All Reviewers\n\nICLR 2019 Conference Paper765 Authors\n\n29 Nov 2018, 08:55ICLR 2019 Conference Paper765 Official CommentReaders: Everyone [Show Revisions](https://openreview.net/revisions?id=S1x50y_aRm)\n\nComment",
          "original_query": "Recurrent experience replay in distributed reinforcement learning",
          "cleaned_query": "Recurrent experience replay in distributed reinforcement learning"
        }
      ],
      "generated_ideas": [
        "Escape-Noise-Calibrated Surrogate Derivatives for Robustness\u2013Accuracy Trade-offs\n- Use the stochastic autodiff interpretation (surrogate derivative \u2248 escape-noise derivative) to *fit* neuron-wise surrogate derivatives from an explicit escape-noise model whose parameters are learned (or scheduled) to match desired MPD statistics. Evaluate whether calibrated surrogates reduce gradient masking and improve certified/empirical robustness compared to fixed surrogates under the MPD-SGR framework.",
        "Temporal-Coding Jump-Start for SNN Policies Using Spike-Timing Priors\n- Extend \u201cjump-start\u201d RL to spiking agents by pretraining (or directly initializing) actor synapses from a library of temporal-coding primitives (e.g., time-to-first-spike encoders for velocity/position features) before reward learning. Test whether initializing the SNN to produce useful spike-time orderings reduces exploration cost and improves early performance on sparse-reward tasks.",
        "Dendrite-Inspired Multi-Compartment MPD Regularization for Single-Neuron Temporal Functions\n- Build a multi-compartment spiking neuron model (proximal/distal integration) and derive compartment-specific MPD-SGR targets to keep only select compartments within the surrogate active range. Demonstrate that this yields compact implementations of temporally sensitive functions (in the spirit of Maass\u2019s \u201csingle spiking neuron advantage\u201d) while improving adversarial robustness relative to equivalent single-compartment networks.",
        "Certified Robustness Bounds for Surrogate-Trained SNNs from MPD Mass Constraints\n- Develop theoretical bounds linking (i) the fraction of membrane potential within the surrogate-gradient support and (ii) Lipschitz-like sensitivity of spike outputs to bounded input perturbations over time. Produce a practical certificate computed from logged MPD histograms per layer/time step, validating tightness against PGD/AutoAttack-style adversaries on spiking image and event-stream benchmarks.",
        "Robustness-Aware Encoding: Learning Spike Encoders that Shape MPD\n- Co-train the input spike encoder (latency/time-to-first-spike, population coding, or learned event filters) with an MPD objective so encoded streams push membrane potentials away from the surrogate\u2019s gradient-available interval except when informative. Measure gains in adversarial robustness and corruption robustness (noise, jitter, missing events), and analyze which encoding families best control MPD without harming accuracy/energy.",
        "Distributional Regularization of Eligibility Traces for Reward-Modulated STDP\n- Combine MPD-driven regularization with actor\u2013critic eligibility traces by constraining the distribution of pre/post spike-time differences that drive STDP updates (e.g., penalize regimes producing high-variance, high-sensitivity updates). Implement a \u201ctrace distribution regularizer\u201d that stabilizes learning under observation attacks and nonstationary rewards, and compare to entropy regularization and gradient clipping in SG-trained SNN-RL.",
        "Attack-Adapted Surrogate Gradients: Online Shaping of Surrogate Support Under Adversarial Pressure\n- Create an adaptive surrogate function whose active window width/center is adjusted online based on detected MPD shifts and attack indicators (e.g., sudden rise in gradient norms or atypical MPD mass near threshold). Evaluate whether dynamically shrinking/relocating surrogate support preserves clean accuracy while preventing adversaries from exploiting consistently high-gradient membrane regimes.",
        "Energy\u2013Robustness Co-Optimization in Deep SNNs via Joint Spike-Rate and MPD Constraints\n- Formulate a multi-objective training scheme that jointly regularizes (a) spike counts (energy proxy) and (b) MPD mass in surrogate-active regions (robustness proxy), yielding Pareto-front models. Report trade-offs on neuromorphic-relevant hardware simulators (latency/energy) and robustness suites, identifying regimes where modest MPD constraints deliver large robustness gains with minimal spike-rate increase."
      ],
      "generated_ideas_raw": "1. **Adversarially Robust Spiking Actor\u2013Critic via MPD-SGR on Neuromorphic Control**\n   - Integrate MPD-SGR into the temporal-coding, reward-modulated STDP actor\u2013critic SNN to explicitly control membrane potential mass inside the surrogate-gradient active window during policy/value updates. Benchmark robustness to observation and dynamics adversarial perturbations on CartPole, Walker2D, and noisy event-based control, comparing against standard SG-trained SNN policies and rate-based baselines.\n\n2. **Escape-Noise-Calibrated Surrogate Derivatives for Robustness\u2013Accuracy Trade-offs**\n   - Use the stochastic autodiff interpretation (surrogate derivative \u2248 escape-noise derivative) to *fit* neuron-wise surrogate derivatives from an explicit escape-noise model whose parameters are learned (or scheduled) to match desired MPD statistics. Evaluate whether calibrated surrogates reduce gradient masking and improve certified/empirical robustness compared to fixed surrogates under the MPD-SGR framework.\n\n3. **Replay-Aware Surrogate Learning: Correcting Gradient Drift from Stale Recurrent States in SNN-RL**\n   - Adapt R2D2\u2019s analysis of recurrent state staleness to spiking recurrent agents by logging spike traces/membrane states and replaying them with correction factors (e.g., importance weights or state re-initialization via stored membrane potential snapshots). Quantify how MPD shifts between data collection and replay affect surrogate-gradient magnitude, and propose a \u201cMPD-consistent replay\u201d rule that regularizes MPD to remain stable across lag.\n\n4. **Temporal-Coding Jump-Start for SNN Policies Using Spike-Timing Priors**\n   - Extend \u201cjump-start\u201d RL to spiking agents by pretraining (or directly initializing) actor synapses from a library of temporal-coding primitives (e.g., time-to-first-spike encoders for velocity/position features) before reward learning. Test whether initializing the SNN to produce useful spike-time orderings reduces exploration cost and improves early performance on sparse-reward tasks.\n\n5. **Dendrite-Inspired Multi-Compartment MPD Regularization for Single-Neuron Temporal Functions**\n   - Build a multi-compartment spiking neuron model (proximal/distal integration) and derive compartment-specific MPD-SGR targets to keep only select compartments within the surrogate active range. Demonstrate that this yields compact implementations of temporally sensitive functions (in the spirit of Maass\u2019s \u201csingle spiking neuron advantage\u201d) while improving adversarial robustness relative to equivalent single-compartment networks.\n\n6. **Certified Robustness Bounds for Surrogate-Trained SNNs from MPD Mass Constraints**\n   - Develop theoretical bounds linking (i) the fraction of membrane potential within the surrogate-gradient support and (ii) Lipschitz-like sensitivity of spike outputs to bounded input perturbations over time. Produce a practical certificate computed from logged MPD histograms per layer/time step, validating tightness against PGD/AutoAttack-style adversaries on spiking image and event-stream benchmarks.\n\n7. **Robustness-Aware Encoding: Learning Spike Encoders that Shape MPD**\n   - Co-train the input spike encoder (latency/time-to-first-spike, population coding, or learned event filters) with an MPD objective so encoded streams push membrane potentials away from the surrogate\u2019s gradient-available interval except when informative. Measure gains in adversarial robustness and corruption robustness (noise, jitter, missing events), and analyze which encoding families best control MPD without harming accuracy/energy.\n\n8. **Distributional Regularization of Eligibility Traces for Reward-Modulated STDP**\n   - Combine MPD-driven regularization with actor\u2013critic eligibility traces by constraining the distribution of pre/post spike-time differences that drive STDP updates (e.g., penalize regimes producing high-variance, high-sensitivity updates). Implement a \u201ctrace distribution regularizer\u201d that stabilizes learning under observation attacks and nonstationary rewards, and compare to entropy regularization and gradient clipping in SG-trained SNN-RL.\n\n9. **Attack-Adapted Surrogate Gradients: Online Shaping of Surrogate Support Under Adversarial Pressure**\n   - Create an adaptive surrogate function whose active window width/center is adjusted online based on detected MPD shifts and attack indicators (e.g., sudden rise in gradient norms or atypical MPD mass near threshold). Evaluate whether dynamically shrinking/relocating surrogate support preserves clean accuracy while preventing adversaries from exploiting consistently high-gradient membrane regimes.\n\n10. **Energy\u2013Robustness Co-Optimization in Deep SNNs via Joint Spike-Rate and MPD Constraints**\n   - Formulate a multi-objective training scheme that jointly regularizes (a) spike counts (energy proxy) and (b) MPD mass in surrogate-active regions (robustness proxy), yielding Pareto-front models. Report trade-offs on neuromorphic-relevant hardware simulators (latency/energy) and robustness suites, identifying regimes where modest MPD constraints deliver large robustness gains with minimal spike-rate increase.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Escape-Noise-Calibrated Surrogate Derivatives for Robustness\u2013Accuracy Trade-offs\n- Use the stochastic autodiff interpretation (surrogate derivative \u2248 escape-noise derivative) to *fit* neuron-wise surr",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Temporal-Coding Jump-Start for SNN Policies Using Spike-Timing Priors\n- Extend \u201cjump-start\u201d RL to spiking agents by pretraining (or directly initializing) actor synapses from a library of temporal-cod",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Dendrite-Inspired Multi-Compartment MPD Regularization for Single-Neuron Temporal Functions\n- Build a multi-compartment spiking neuron model (proximal/distal integration) and derive compartment-specif",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Certified Robustness Bounds for Surrogate-Trained SNNs from MPD Mass Constraints\n- Develop theoretical bounds linking (i) the fraction of membrane potential within the surrogate-gradient support and (",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Robustness-Aware Encoding: Learning Spike Encoders that Shape MPD\n- Co-train the input spike encoder (latency/time-to-first-spike, population coding, or learned event filters) with an MPD objective so",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Distributional Regularization of Eligibility Traces for Reward-Modulated STDP\n- Combine MPD-driven regularization with actor\u2013critic eligibility traces by constraining the distribution of pre/post spik",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Attack-Adapted Surrogate Gradients: Online Shaping of Surrogate Support Under Adversarial Pressure\n- Create an adaptive surrogate function whose active window width/center is adjusted online based on ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Energy\u2013Robustness Co-Optimization in Deep SNNs via Joint Spike-Rate and MPD Constraints\n- Formulate a multi-objective training scheme that jointly regularizes (a) spike counts (energy proxy) and (b) M",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 45,
      "paper_title": "Class-wise Balancing Data Replay for Federated Class-Incremental Learning",
      "contribution": "Introduce FedCBDR, a privacy-preserving, global-perspective replay pipeline that (1) reconstructs class-level pseudo features for coordinated, class-balanced exemplar sampling across heterogeneous clients and (2) applies task-aware temperature scaling to mitigate class imbalance and overconfidence between replayed and new classes.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 7,
      "input_tokens": 10502,
      "output_tokens": 933,
      "predecessor_details": [
        {
          "success": true,
          "title": "iCaRL: Incremental Classifier and Representation Learning - arXiv",
          "url": "https://arxiv.org/abs/1611.07725",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:1611.07725** (cs)\n\n\\[Submitted on 23 Nov 2016 ( [v1](https://arxiv.org/abs/1611.07725v1)), last revised 14 Apr 2017 (this version, v2)\\]\n\n# Title:iCaRL: Incremental Classifier and Representation Learning\n\nAuthors: [Sylvestre-Alvise Rebuffi](https://arxiv.org/search/cs?searchtype=author&query=Rebuffi,+S), [Alexander Kolesnikov](https://arxiv.org/search/cs?searchtype=author&query=Kolesnikov,+A), [Georg Sperl](https://arxiv.org/search/cs?searchtype=author&query=Sperl,+G), [Christoph H. Lampert](https://arxiv.org/search/cs?searchtype=author&query=Lampert,+C+H)\n\nView a PDF of the paper titled iCaRL: Incremental Classifier and Representation Learning, by Sylvestre-Alvise Rebuffi and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/1611.07725)\n\n> Abstract:A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail.\n\n| | |\n| --- | --- |\n| Comments: | Accepted paper at CVPR 2017 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1611.07725](https://arxiv.org/abs/1611.07725) \\[cs.CV\\] |\n| | (or [arXiv:1611.07725v2](https://arxiv.org/abs/1611.07725v2) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1611.07725](https://doi.org/10.48550/arXiv.1611.07725) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Christoph H. Lampert \\[ [view email](https://arxiv.org/show-email/c441c561/1611.07725)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1611.07725v1)**\nWed, 23 Nov 2016 10:24:11 UTC (1,155 KB)\n\n**\\[v2\\]**\nFri, 14 Apr 2017 16:41:02 UTC (1,165 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled iCaRL: Incremental Classifier and Representation Learning, by Sylvestre-Alvise Rebuffi and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/1611.07725)\n- [TeX Source](https://arxiv.org/src/1611.07725)\n- [Other Formats](https://arxiv.org/format/1611.07725)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1611.07725&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1611.07725&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2016-11](https://arxiv.org/list/cs.CV/2016-11)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1611.07725?context=cs)\n\n[cs.LG](https://arxiv.org/abs/1611.07725?context=cs.LG)\n\n[stat](https://arxiv.org/abs/1611.07725?context=stat)\n\n[stat.ML](https://arxiv.org/abs/1611.07725?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1611.07725)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1611.07725)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1611.07725)\n\n### [1 blog link](https://arxiv.org/tb/1611.07725)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1611.html#RebuffiKL16) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/RebuffiKL16)\n\n[Sylvestre-Alvise Rebuffi](https://dblp.uni-trier.de/search/author?author=Sylvestre-Alvise%20Rebuffi)\n\n[Alexander Kolesnikov](https://dblp.uni-trier.de/search/author?author=Alexander%20Kolesnikov)\n\n[Christoph H. Lampert](https://dblp.uni-trier.de/search/author?author=Christoph%20H.%20Lampert)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1611.07725&description=iCaRL: Incremental Classifier and Representation Learning) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1611.07725&title=iCaRL: Incremental Classifier and Representation Learning)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1611.07725) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "icarl: Incremental classifier and representation learning",
          "cleaned_query": "icarl: Incremental classifier and representation learning"
        },
        {
          "success": true,
          "title": "Federated Class-Incremental Learning with Hierarchical Generative Prototypes",
          "url": "https://arxiv.org/html/2406.02447v4",
          "content": "# Federated Class-Incremental Learning with Hierarchical Generative Prototypes\n\nRiccardo Salami\u2217 \u00a0\u00a0 Pietro Buzzega\u2217 \u00a0\u00a0 Matteo Mosconi \u00a0\u00a0 Mattia Verasani \u00a0\u00a0 Simone Calderara\nAImageLab - University of Modena and Reggio Emilia, Modena, Italy\nname.surname@unimore.it\n\n###### Abstract\n\nFederated Learning (FL) aims at unburdening the training of deep models by distributing computation across multiple devices (clients) while safeguarding data privacy. On top of that, Federated Continual Learning (FCL) also accounts for data distribution evolving over time, mirroring the dynamic nature of real-world environments. While previous studies have identified Catastrophic Forgetting and Client Drift as primary causes of performance degradation in FCL, we shed light on the importance of Incremental Bias and Federated Bias, which cause models to prioritize classes that are recently introduced or locally predominant, respectively. Our proposal constrains both biases in the last layer by efficiently fine-tuning a pre-trained backbone using learnable prompts, resulting in clients that produce less biased representations and more biased classifiers. Therefore, instead of solely relying on parameter aggregation, we leverage generative prototypes to effectively balance the predictions of the global model. Our proposed methodology significantly improves the current State Of The Art across six datasets, each including three different scenarios.\n\n\\*\\*footnotetext: Equal contribution\n\n## 1 Introduction\n\nThe traditional paradigm in Deep Learning necessitates accessing large-scale datasets all at once, which hinders scalability and raises significant privacy concerns, especially when sensitive data is involved. Although distributing training across many devices could be an effective solution, there is still no effective mechanism for blending the resulting trained models into a single unified one. Federated Learning (FL)\u00a0[mcmahan2017communication](https://arxiv.org/html/2406.02447v4#bib.bib37)addresses this challenge through a centralized server that coordinates distributed devices, aiming to create a single unified model while minimizing communication costs.\n\nFederated Class-Incremental Learning (FCIL)\u00a0[yoon2021federated](https://arxiv.org/html/2406.02447v4#bib.bib52); [dong2022federated](https://arxiv.org/html/2406.02447v4#bib.bib7); [zhang2023target](https://arxiv.org/html/2406.02447v4#bib.bib56)takes a step further and couples distributed training with Online Learning, tolerating distribution shifts in the data over time. This presents new challenges, as deep models learning online (without relying on old examples) experience severe performance degradation due to Catastrophic Forgetting\u00a0[mccloskey1989catastrophic](https://arxiv.org/html/2406.02447v4#bib.bib35). In FCIL, the training process unfolds in tasks, each of which shifts the data distribution by introducing new categories. Each task is divided into communication rounds, wherein the local models train on their private data distribution. After local training, each client may transmit information to the orchestrator (server), which creates a global model and redistributes it to all clients. In the literature, some methodologies account for architectural heterogeneity ( _i.e_., heterogeneous FL\u00a0[diao2021heterofl](https://arxiv.org/html/2406.02447v4#bib.bib5); [kim2022depthfl](https://arxiv.org/html/2406.02447v4#bib.bib19); [ilhan2023scalefl](https://arxiv.org/html/2406.02447v4#bib.bib15)), while others aim to enhance the performance of local models without necessarily converging to a global one ( _i.e_., personalized FL\u00a0[collins2021exploiting](https://arxiv.org/html/2406.02447v4#bib.bib4); [ma2022layer](https://arxiv.org/html/2406.02447v4#bib.bib32); [oh2022fedbabu](https://arxiv.org/html/2406.02447v4#bib.bib38)). Instead, we follow the original FCIL setting as presented in\u00a0[dong2022federated](https://arxiv.org/html/2406.02447v4#bib.bib7), with the goal of training a single global model in a distributed way.\n\nWhen learning on a sequence of tasks, the model struggles the most at differentiating classes from distinct tasks, whereas it works well at separating those within the same one. Albeit one would intuitively link such behavior to Catastrophic Forgetting, it primarily occurs because tasks are learned separately, and some classes are never seen simultaneously\u00a0[kim2022theoretical](https://arxiv.org/html/2406.02447v4#bib.bib18). This results in a phenomenon known in the Incremental Learning literature as bias, which favors recently introduced classes\u00a0[wu2019large](https://arxiv.org/html/2406.02447v4#bib.bib51). We refer to this as Incremental Bias (IB). IB emerges because new classification heads are optimized independently, without concurrent access to previous classes. As a result, gradient updates are disproportionately influenced by the new classes, causing imbalance in the classifier\u2019s output.\n\nFigure 1: Federated bias. Histogram of the clients\u2019 responses computed on the global test set (\u03b2=0.05\ud835\udefd0.05\\\\beta=0.05italic\\_\u03b2 = 0.05). Class indexes have been rearranged for improved visualization (left). The entropy of the response histograms, averaged on all clients, compared with FL performance (right).\n\nThe authors of\u00a0[luo2021no](https://arxiv.org/html/2406.02447v4#bib.bib31)observe a similar tendency in the Federated Learning scenario: since clients train exclusively on their local datasets, they exhibit a bias towards their local label distribution. We refer to this effect as Federated Bias (FB). In contrast to the well-known Client Drift\u00a0[gao2022feddc](https://arxiv.org/html/2406.02447v4#bib.bib9); [karimireddy2020scaffold](https://arxiv.org/html/2406.02447v4#bib.bib17); [zhao2018federated](https://arxiv.org/html/2406.02447v4#bib.bib59), which causes misalignment between the clients\u2019 learned parameters, Federated Bias affects the clients\u2019 responses. Specifically, FB induces clients\u2019 outputs to diverge in different directions, mirroring the patterns of their local label distributions. Also, the strength of FB increases with growing heterogeneity, suggesting a correlation with declining performance (see [Section2](https://arxiv.org/html/2406.02447v4#S2)). To relieve such an effect, we constrain FB to the last layer by leveraging a frozen pre-trained backbone and efficiently fine-tuning it via prompt learning\u00a0[li2021prefix](https://arxiv.org/html/2406.02447v4#bib.bib26). Ideally, prompting keeps the clients\u2019 representations close to the pre-training optimum (hence, close to each other), thus minimizing their Federated Bias. In [Section4.3](https://arxiv.org/html/2406.02447v4#S4.SS3), we experimentally verify that prompting leads to reduced bias in the feature space w.r.t. fine-tuning all parameters. This confines the impact of FB to the last layer, providing the centralized server with less biased representations. On top of that, prompt-based methodologies i) have demonstrated SOTA results in Class-Incremental Learning\u00a0[wang2022learning](https://arxiv.org/html/2406.02447v4#bib.bib50); [wang2022dualprompt](https://arxiv.org/html/2406.02447v4#bib.bib49); [smith2023coda](https://arxiv.org/html/2406.02447v4#bib.bib44)and ii) adapt only a small portion of the clients\u2019 parameters, improving communication efficiency in distributed scenarios\u00a0[zhao2023fedprompt](https://arxiv.org/html/2406.02447v4#bib.bib57); [liu2023fedet](https://arxiv.org/html/2406.02447v4#bib.bib29).\n\nThe authors of\u00a0[wu2019large](https://arxiv.org/html/2406.02447v4#bib.bib51); [zhang2023slca](https://arxiv.org/html/2406.02447v4#bib.bib55); [luo2021no](https://arxiv.org/html/2406.02447v4#bib.bib31)address either IB or FB by fine-tuning the classification layer (where such biases are the most evident) on IID data samples. To meet the privacy requirements of FL, which prohibit transferring real data, we follow recent studies\u00a0[zhang2023slca](https://arxiv.org/html/2406.02447v4#bib.bib55); [luo2021no](https://arxiv.org/html/2406.02447v4#bib.bib31)and leverage lat",
          "original_query": "Federated class-incremental learning",
          "cleaned_query": "Federated class-incremental learning"
        },
        {
          "success": true,
          "title": "FedProK: Trustworthy Federated Class-Incremental Learning ... - arXiv",
          "url": "https://arxiv.org/abs/2405.02685",
          "content": "\n View PDF \n HTML (experimental) \nFederated Class-Incremental Learning (FCIL) focuses on continually transferring the previous knowledge to learn new classes in dynamic Federated Learning (FL). However, existing methods do not consider the trustworthiness of FCIL, i.e., improving continual utility, privacy, and efficiency simultaneously, which is greatly influenced by catastrophic forgetting and data heterogeneity among clients. To address this issue, we propose FedProK (Federated Prototypical Feature Knowledge Transfer), leveraging prototypical feature as a novel representation of knowledge to perform spatial-temporal knowledge transfer. Specifically, FedProK consists of two components: (1) feature translation procedure on the client side by temporal knowledge transfer from the learned classes and (2) prototypical knowledge fusion on the server side by spatial knowledge transfer among clients. Extensive experiments conducted in both synchronous and asynchronous settings demonstrate that our FedProK outperforms the other state-of-the-art methods in three perspectives of trustworthiness, validating its effectiveness in selectively transferring spatial-temporal knowledge.\n \n \n Submission history From: Xin Gao [ view email] [v1] \n Sat, 4 May 2024 14:57:09 UTC (3,661 KB) \n ||||I|||| Skip to main content\n We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate\n > cs > arXiv:2405.02685\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Machine Learning\n\n arXiv:2405.02685 (cs)\n [Submitted on 4 May 2024]\n\n Title: FedProK: Trustworthy Federated Class-Incremental Learning via Prototypical Feature Knowledge Transfer\n\n Authors: Xin Gao, Xin Yang, Hao Yu, Yan Kang, Tianrui Li\n View a PDF of the paper titled FedProK: Trustworthy Federated Class-Incremental Learning via Prototypical Feature Knowledge Transfer, by Xin Gao and 4 other authors\n View PDF HTML (experimental)\n Abstract: Federated Class-Incremental Learning (FCIL) focuses on continually transferring the previous knowledge to learn new classes in dynamic Federated Learning (FL). However, existing methods do not consider the trustworthiness of FCIL, i.e., improving continual utility, privacy, and efficiency simultaneously, which is greatly influenced by catastrophic forgetting and data heterogeneity among clients. To address this issue, we propose FedProK (Federated Prototypical Feature Knowledge Transfer), leveraging prototypical feature as a novel representation of knowledge to perform spatial-temporal knowledge transfer. Specifically, FedProK consists of two components: (1) feature translation procedure on the client side by temporal knowledge transfer from the learned classes and (2) prototypical knowledge fusion on the server side by spatial knowledge transfer among clients. Extensive experiments conducted in both synchronous and asynchronous settings demonstrate that our FedProK outperforms the other state-of-the-art methods in three perspectives of trustworthiness, validating its effectiveness in selectively transferring spatial-temporal knowledge.\n Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)\n Cite as: arXiv:2405.02685 [cs.LG] \n (or arXiv:2405.02685v1 [cs.LG] for this version) \n \n\n Submission history\n\n From: Xin Gao [view email]\n [v1] Sat, 4 May 2024 14:57:09 UTC (3,661 KB)\n Full-text links:\n\n Access Paper:\n\n View a PDF of the paper titled FedProK: Trustworthy Federated Class-Incremental Learning via Prototypical Feature Knowledge Transfer, by Xin Gao and 4 other authors\n * View PDF\n * HTML (experimental)\n * TeX Source\n * Other Formats\n view license\n Current browse context:\n cs.LG\n < prev | next >\n new | recent | 2405\n Change to browse by:\n cs\n cs.AI\n cs.NE\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export BibTeX citation Loading...\n\n BibTeX formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n DagsHub Toggle\n DagsHub (What is DagsHub?)\n GotitPub Toggle\n Gotit.pub (What is GotitPub?)\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Spaces Toggle\n TXYZ.AI (What is TXYZ.AI?)\n Related Papers\n\n Recommenders and Search Tools\n\n Link to Influence Flower\n Influence Flower (What are Influence Flowers?)\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n IArxiv recommender toggle\n IArxiv Recommender (What is IArxiv?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Fedprok: Trustworthy federated class-incremental learning via prototypical feature knowledge transfer",
          "cleaned_query": "Fedprok: Trustworthy federated class-incremental learning via prototypical feature knowledge transfer"
        },
        {
          "success": true,
          "title": "Federated Class Incremental Learning: A Pseudo Feature Based ...",
          "url": "https://link.springer.com/chapter/10.1007/978-981-96-0963-5_21",
          "content": "References Babakniya, S., Fabian, Z., He, C., Soltanolkotabi, M., Avestimehr, S.: A data-free approach to mitigate catastrophic forgetting in federated class incremental learning for vision tasks. Advances in Neural Information Processing Systems 36 (2024) \n Google Scholar \u00a0\n Castro, F.M., Mar\u00edn-Jim\u00e9nez, M.J., Guil, N., Schmid, C., Alahari, K.: End-to-end incremental learning. In: Proceedings of the European conference on computer vision (ECCV). pp. 233\u2013248 (2018) \n Google Scholar \u00a0\n Dong, J., Li, H., Cong, Y., Sun, G., Zhang, Y., Van\u00a0Gool, L.: No one left behind: Real-world federated class-incremental learning. IEEE Transactions on Pattern Analysis and Machine Intelligence (2023) \n Google Scholar \u00a0\n Dong, J., Wang, L., Fang, Z., Sun, G., Xu, S., Wang, X., Zhu, Q.: Federated class-incremental learning. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10164\u201310173 (2022) \n Google Scholar \u00a0\n Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. Advances in neural information processing systems 27 (2014) \n Google Scholar \u00a0\n He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770\u2013778 (2016) \n Google Scholar \u00a0\n Hou, S., Pan, X., Loy, C.C., Wang, Z., Lin, D.: Learning a unified classifier incrementally via rebalancing. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 831\u2013839 (2019) \n Google Scholar \u00a0\n Imteaj, A., Thakker, U., Wang, S., Li, J., Amini, M.H.: A survey on federated learning for resource-constrained iot devices. IEEE Internet Things J. 9 (1), 1\u201324 (2021) Article \u00a0\n \n Google Scholar \u00a0\n Kone\u010dn\u1ef3, J., McMahan, H.B., Yu, F.X., Richt\u00e1rik, P., Suresh, A.T., Bacon, D.: Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492 (2016) Krizhevsky, A., Hinton, G., et\u00a0al.: Learning multiple layers of features from tiny images (2009) \n Google Scholar \u00a0\n Le, Y., Yang, X.: Tiny imagenet visual recognition challenge. CS 231N 7 (7), 3 (2015) \n Google Scholar \u00a0\n Li, T., Sahu, A.K., Zaheer, M., Sanjabi, M., Talwalkar, A., Smith, V.: Federated optimization in heterogeneous networks. Proceedings of Machine learning and systems 2, 429\u2013450 (2020) \n Google Scholar \u00a0\n Masana, M., Liu, X., Twardowski, B., Menta, M., Bagdanov, A.D., Van De Weijer, J.: Class-incremental learning: survey and performance evaluation on image classification. IEEE Trans. Pattern Anal. Mach. Intell. 45 (5), 5513\u20135533 (2022) Article \u00a0\n \n Google Scholar \u00a0\n McCloskey, M., Cohen, N.J.: Catastrophic interference in connectionist networks: The sequential learning problem. In: Psychology of learning and motivation, vol.\u00a024, pp. 109\u2013165. Elsevier (1989) \n Google Scholar \u00a0\n McMahan, B., Moore, E., Ramage, D., Hampson, S., y\u00a0Arcas, B.A.: Communication-efficient learning of deep networks from decentralized data. In: Artificial intelligence and statistics. pp. 1273\u20131282. PMLR (2017) \n Google Scholar \u00a0\n Petit, G., Popescu, A., Schindler, H., Picard, D., Delezoide, B.: Fetril: Feature translation for exemplar-free class-incremental learning. In: Proceedings of the IEEE/CVF winter conference on applications of computer vision. pp. 3911\u20133920 (2023) \n Google Scholar \u00a0\n Psaltis, A., Chatzikonstantinou, C., Patrikakis, C.Z., Daras, P.: Fedrcil: Federated knowledge distillation for representation based contrastive incremental learning. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3463\u20133472 (2023) \n Google Scholar \u00a0\n Qayyum, A., Ahmad, K., Ahsan, M.A., Al-Fuqaha, A., Qadir, J.: Collaborative federated learning for healthcare: Multi-modal covid-19 diagnosis at the edge. IEEE Open Journal of the Computer Society 3, 172\u2013184 (2022) Article \u00a0\n \n Google Scholar \u00a0\n Qi, D., Zhao, H., Li, S.: Better generative replay for continual federated learning. arXiv preprint arXiv:2302.13001 (2023) Ravaglia, L., Rusci, M., Nadalini, D., Capotondi, A., Conti, F., Benini, L.: A tinyml platform for on-device continual learning with quantized latent replays. IEEE Journal on Emerging and Selected Topics in Circuits and Systems 11 (4), 789\u2013802 (2021) Article \u00a0\n \n Google Scholar \u00a0\n Rebuffi, S.A., Kolesnikov, A., Sperl, G., Lampert, C.H.: icarl: Incremental classifier and representation learning. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. pp. 2001\u20132010 (2017) \n Google Scholar \u00a0\n Reddi, S., Charles, Z., Zaheer, M., Garrett, Z., Rush, K., Kone\u010dn\u1ef3, J., Kumar, S., McMahan, H.B.: Adaptive federated optimization. arXiv preprint arXiv:2003.00295 (2020) Schlimmer, J.C., Fisher, D.: A case study of incremental concept induction. In: Proceedings of the Fifth AAAI National Conference on Artificial Intelligence. pp. 496\u2013501 (1986) \n Google Scholar \u00a0\n Smith, J., Hsu, Y.C., Balloch, J., Shen, Y., Jin, H., Kira, Z.: Always be dreaming: A new approach for data-free class-incremental learning-supplementary materials (appendix)- \n Google Scholar \u00a0\n Wold, S., Esbensen, K., Geladi, P.: Principal component analysis. Chemom. Intell. Lab. Syst. 2 (1\u20133), 37\u201352 (1987) Article \u00a0\n \n Google Scholar \u00a0\n Yu, L., Twardowski, B., Liu, X., Herranz, L., Wang, K., Cheng, Y., Jui, S., Weijer, J.v.d.: Semantic drift compensation for class-incremental learning. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 6982\u20136991 (2020) \n Google Scholar \u00a0\n Zhang, J., Chen, C., Zhuang, W., Lyu, L.: Target: Federated class-continual learning via exemplar-free distillation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4782\u20134793 (2023) \n Google Scholar \u00a0\n Zhao, B., Xiao, X., Gan, G., Zhang, B., Xia, S.T.: Maintaining discrimination and fairness in class incremental learning. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 13208\u201313217 (2020) \n Google Scholar \u00a0\n Class-incremental learning via dual augmentation: Zhu, F., Cheng, Z., Zhang, X.Y., Liu, C.l. Adv. Neural. Inf. Process. Syst. 34, 14306\u201314318 (2021) \n Google Scholar \u00a0\n Zhu, F., Zhang, X.Y., Wang, C., Yin, F., Liu, C.L.: Prototype augmentation and self-supervision for incremental learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5871\u20135880 (2021) \n Google Scholar \u00a0\n Zhu, K., Zhai, W., Cao, Y., Luo, J., Zha, Z.J.: Self-sustaining representation expansion for non-exemplar class-incremental learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 9296\u20139305 (2022) \n Google Scholar \u00a0\n Download references",
          "original_query": "Federated class incremental learning: A pseudo feature based approach without exemplars",
          "cleaned_query": "Federated class incremental learning: A pseudo feature based approach without exemplars"
        },
        {
          "success": true,
          "title": "Towards Efficient Replay in Federated Incremental Learning - arXiv",
          "url": "https://arxiv.org/abs/2403.05890",
          "content": "\n Download PDF \n HTML (experimental) \nIn Federated Learning (FL), the data in each client is typically assumed fixed or static. However, data often comes in an incremental manner in real-world applications, where the data domain may increase dynamically. In this work, we study catastrophic forgetting with data heterogeneity in Federated Incremental Learning (FIL) scenarios where edge clients may lack enough storage space to retain full data. We propose to employ a simple, generic framework for FIL named Re-Fed, which can coordinate each client to cache important samples for replay. More specifically, when a new task arrives, each client first caches selected previous samples based on their global and local importance. Then, the client trains the local model with both the cached samples and the samples from the new task. Theoretically, we analyze the ability of Re-Fed to discover important samples for replay thus alleviating the catastrophic forgetting problem. Moreover, we empirically show that Re-Fed achieves competitive performance compared to state-of-the-art methods.\n \n \n Submission history From: Yichen Li [ view email] [v1] \n Sat, 9 Mar 2024 12:04:56 UTC (1,838 KB) \n ||||I|||| Skip to main content\n We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate\n > cs > arXiv:2403.05890\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Machine Learning\n\n arXiv:2403.05890 (cs)\n [Submitted on 9 Mar 2024]\n\n Title: Towards Efficient Replay in Federated Incremental Learning\n\n Authors: Yichen Li, Qunwei Li, Haozhao Wang, Ruixuan Li, Wenliang Zhong, Guannan Zhang\n Download a PDF of the paper titled Towards Efficient Replay in Federated Incremental Learning, by Yichen Li and 5 other authors\n Download PDF HTML (experimental)\n Abstract: In Federated Learning (FL), the data in each client is typically assumed fixed or static. However, data often comes in an incremental manner in real-world applications, where the data domain may increase dynamically. In this work, we study catastrophic forgetting with data heterogeneity in Federated Incremental Learning (FIL) scenarios where edge clients may lack enough storage space to retain full data. We propose to employ a simple, generic framework for FIL named Re-Fed, which can coordinate each client to cache important samples for replay. More specifically, when a new task arrives, each client first caches selected previous samples based on their global and local importance. Then, the client trains the local model with both the cached samples and the samples from the new task. Theoretically, we analyze the ability of Re-Fed to discover important samples for replay thus alleviating the catastrophic forgetting problem. Moreover, we empirically show that Re-Fed achieves competitive performance compared to state-of-the-art methods.\n Subjects: Machine Learning (cs.LG) ; Distributed, Parallel, and Cluster Computing (cs.DC)\n Cite as: arXiv:2403.05890 [cs.LG] \n (or arXiv:2403.05890v1 [cs.LG] for this version) \n \n\n Submission history\n\n From: Yichen Li [view email]\n [v1] Sat, 9 Mar 2024 12:04:56 UTC (1,838 KB)\n Full-text links:\n\n Access Paper:\n\n Download a PDF of the paper titled Towards Efficient Replay in Federated Incremental Learning, by Yichen Li and 5 other authors\n * Download PDF\n * HTML (experimental)\n * TeX Source\n * Other Formats\n view license\n Current browse context:\n cs.LG\n < prev | next >\n new | recent | 2403\n Change to browse by:\n cs\n cs.DC\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export BibTeX citation Loading...\n\n BibTeX formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n DagsHub Toggle\n DagsHub (What is DagsHub?)\n GotitPub Toggle\n Gotit.pub (What is GotitPub?)\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Spaces Toggle\n TXYZ.AI (What is TXYZ.AI?)\n Related Papers\n\n Recommenders and Search Tools\n\n Link to Influence Flower\n Influence Flower (What are Influence Flowers?)\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n IArxiv recommender toggle\n IArxiv Recommender (What is IArxiv?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Towards efficient replay in federated incremental learning",
          "cleaned_query": "Towards efficient replay in federated incremental learning"
        }
      ],
      "generated_ideas": [
        "Bias-Calibrated Nearest-Mean Classifier for Federated iCaRL\nDevelop a federated variant of iCaRL\u2019s nearest-mean-of-exemplars classifier where the server learns per-task and per-client calibration parameters to counter Incremental Bias (IB) and Federated Bias (FB). Use only shared class prototypes (means/covariances) to fit a lightweight calibration layer that adjusts distances/logits before prediction, and evaluate on FCIL benchmarks under strong label-skew.",
        "Client-Adaptive Prompting with Server-Side Prototype Regularization\nExtend prompt-tuning FCIL by learning a small client-specific prompt (personalization) while constraining representation drift using server-maintained global class prototypes. Each round, clients optimize prompts to match global prototypes for old classes (distillation in feature space) and to separate new classes, aiming to reduce client drift without aggregating large backbone updates.",
        "Hierarchical Prototype Replay with Storage Budgets (Replay\u2013Prototype Hybrid)\nCombine Re-Fed\u2019s sample caching with hierarchical generative prototypes by allocating each client\u2019s memory budget between (a) a tiny cache of real samples and (b) compressed prototype statistics (multi-resolution: class \u2192 superclass \u2192 task). Train with mixed replay where prototypes generate pseudo-features and cached samples anchor calibration, explicitly optimizing the budget split for accuracy vs. memory/communication.",
        "Asynchronous FCIL via Temporal Prototype Translation\nBuild an asynchronous FCIL algorithm that uses FedProK-style temporal feature translation but operates when clients are at different task indices. The server maintains a \u201ctime-indexed\u201d prototype bank and learns translation operators to map prototypes across task time, enabling late clients to benefit from future-learned structure without direct access to those data.",
        "Privacy-Utility Tradeoff for Prototype Sharing with Formal Leakage Tests\nPropose a systematic privacy evaluation protocol for prototype-based FCIL (generative prototypes, prototypical features) by designing membership/property inference attacks tailored to prototypes and pseudo-features. Add defense mechanisms (e.g., DP noise on prototype moments, randomized subspace projection, or secure aggregation of sufficient statistics) and quantify the utility drop per privacy level.",
        "Prototype Uncertainty Modeling to Reduce Federated Bias\nReplace single-point class prototypes with probabilistic prototypes (mean + low-rank covariance or diagonal variance) estimated locally and fused on the server. At inference and distillation, weight client contributions by prototype uncertainty and downweight overconfident, locally-dominant classes to mitigate FB; measure improvements in long-tail and non-IID label settings.",
        "Exemplar Selection in FCIL Using Cross-Client Influence Scores\nGeneralize iCaRL\u2019s herding/exemplar selection to federated settings by computing a communication-light \u201cinfluence score\u201d for candidate cached samples based on their alignment with global prototypes and their expected benefit to other clients. Each client selects replay items that maximize global representational coverage (not just local), and the server periodically updates selection criteria using aggregated statistics.",
        "Debiased Last-Layer Training with Prototype-Constrained Logit Rebalancing\nTarget IB directly by training only the classifier head with a constraint that matches class-wise logit distributions to those implied by global prototypes (e.g., enforce equalized margins or temperature-scaled logits across tasks). Implement as a small convex/bi-level optimization step on the server after each round, using only prototype summaries, to correct the \u201crecent-class advantage.\u201d",
        "Multi-Modal Federated Class-Incremental Learning with Shared Prototype Spaces\nExtend prototype-based FCIL to multi-modal data (e.g., image + text or clinical signals + metadata) by learning a shared embedding space where each modality contributes modality-specific prototypes that are aligned via contrastive/prototypical losses. Clients missing modalities can still learn incrementally by distilling from the shared prototype space, enabling robustness to heterogeneous sensors across clients.",
        "Communication-Efficient Prototype Compression via Learned Subspaces\nReduce FCIL communication by learning a global low-dimensional subspace (e.g., via federated PCA/autoencoder on prototype features) and transmitting only subspace coefficients for prototypes and prompt updates. Compare against raw prototype sharing and parameter aggregation, quantifying accuracy vs. bandwidth for long task sequences and many clients under strict uplink constraints."
      ],
      "generated_ideas_raw": "1. **Bias-Calibrated Nearest-Mean Classifier for Federated iCaRL**\n   Develop a federated variant of iCaRL\u2019s nearest-mean-of-exemplars classifier where the server learns per-task and per-client calibration parameters to counter Incremental Bias (IB) and Federated Bias (FB). Use only shared class prototypes (means/covariances) to fit a lightweight calibration layer that adjusts distances/logits before prediction, and evaluate on FCIL benchmarks under strong label-skew.\n\n2. **Client-Adaptive Prompting with Server-Side Prototype Regularization**\n   Extend prompt-tuning FCIL by learning a small client-specific prompt (personalization) while constraining representation drift using server-maintained global class prototypes. Each round, clients optimize prompts to match global prototypes for old classes (distillation in feature space) and to separate new classes, aiming to reduce client drift without aggregating large backbone updates.\n\n3. **Hierarchical Prototype Replay with Storage Budgets (Replay\u2013Prototype Hybrid)**\n   Combine Re-Fed\u2019s sample caching with hierarchical generative prototypes by allocating each client\u2019s memory budget between (a) a tiny cache of real samples and (b) compressed prototype statistics (multi-resolution: class \u2192 superclass \u2192 task). Train with mixed replay where prototypes generate pseudo-features and cached samples anchor calibration, explicitly optimizing the budget split for accuracy vs. memory/communication.\n\n4. **Asynchronous FCIL via Temporal Prototype Translation**\n   Build an asynchronous FCIL algorithm that uses FedProK-style temporal feature translation but operates when clients are at different task indices. The server maintains a \u201ctime-indexed\u201d prototype bank and learns translation operators to map prototypes across task time, enabling late clients to benefit from future-learned structure without direct access to those data.\n\n5. **Privacy-Utility Tradeoff for Prototype Sharing with Formal Leakage Tests**\n   Propose a systematic privacy evaluation protocol for prototype-based FCIL (generative prototypes, prototypical features) by designing membership/property inference attacks tailored to prototypes and pseudo-features. Add defense mechanisms (e.g., DP noise on prototype moments, randomized subspace projection, or secure aggregation of sufficient statistics) and quantify the utility drop per privacy level.\n\n6. **Prototype Uncertainty Modeling to Reduce Federated Bias**\n   Replace single-point class prototypes with probabilistic prototypes (mean + low-rank covariance or diagonal variance) estimated locally and fused on the server. At inference and distillation, weight client contributions by prototype uncertainty and downweight overconfident, locally-dominant classes to mitigate FB; measure improvements in long-tail and non-IID label settings.\n\n7. **Exemplar Selection in FCIL Using Cross-Client Influence Scores**\n   Generalize iCaRL\u2019s herding/exemplar selection to federated settings by computing a communication-light \u201cinfluence score\u201d for candidate cached samples based on their alignment with global prototypes and their expected benefit to other clients. Each client selects replay items that maximize global representational coverage (not just local), and the server periodically updates selection criteria using aggregated statistics.\n\n8. **Debiased Last-Layer Training with Prototype-Constrained Logit Rebalancing**\n   Target IB directly by training only the classifier head with a constraint that matches class-wise logit distributions to those implied by global prototypes (e.g., enforce equalized margins or temperature-scaled logits across tasks). Implement as a small convex/bi-level optimization step on the server after each round, using only prototype summaries, to correct the \u201crecent-class advantage.\u201d\n\n9. **Multi-Modal Federated Class-Incremental Learning with Shared Prototype Spaces**\n   Extend prototype-based FCIL to multi-modal data (e.g., image + text or clinical signals + metadata) by learning a shared embedding space where each modality contributes modality-specific prototypes that are aligned via contrastive/prototypical losses. Clients missing modalities can still learn incrementally by distilling from the shared prototype space, enabling robustness to heterogeneous sensors across clients.\n\n10. **Communication-Efficient Prototype Compression via Learned Subspaces**\n   Reduce FCIL communication by learning a global low-dimensional subspace (e.g., via federated PCA/autoencoder on prototype features) and transmitting only subspace coefficients for prototypes and prompt updates. Compare against raw prototype sharing and parameter aggregation, quantifying accuracy vs. bandwidth for long task sequences and many clients under strict uplink constraints.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Bias-Calibrated Nearest-Mean Classifier for Federated iCaRL\nDevelop a federated variant of iCaRL\u2019s nearest-mean-of-exemplars classifier where the server learns per-task and per-client calibration para",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Client-Adaptive Prompting with Server-Side Prototype Regularization\nExtend prompt-tuning FCIL by learning a small client-specific prompt (personalization) while constraining representation drift using",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Hierarchical Prototype Replay with Storage Budgets (Replay\u2013Prototype Hybrid)\nCombine Re-Fed\u2019s sample caching with hierarchical generative prototypes by allocating each client\u2019s memory budget between (",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Asynchronous FCIL via Temporal Prototype Translation\nBuild an asynchronous FCIL algorithm that uses FedProK-style temporal feature translation but operates when clients are at different task indices. ",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Privacy-Utility Tradeoff for Prototype Sharing with Formal Leakage Tests\nPropose a systematic privacy evaluation protocol for prototype-based FCIL (generative prototypes, prototypical features) by des",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Prototype Uncertainty Modeling to Reduce Federated Bias\nReplace single-point class prototypes with probabilistic prototypes (mean + low-rank covariance or diagonal variance) estimated locally and fuse",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Exemplar Selection in FCIL Using Cross-Client Influence Scores\nGeneralize iCaRL\u2019s herding/exemplar selection to federated settings by computing a communication-light \u201cinfluence score\u201d for candidate ca",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Debiased Last-Layer Training with Prototype-Constrained Logit Rebalancing\nTarget IB directly by training only the classifier head with a constraint that matches class-wise logit distributions to those",
          "is_match": true
        },
        {
          "idea_idx": 8,
          "idea_text": "Multi-Modal Federated Class-Incremental Learning with Shared Prototype Spaces\nExtend prototype-based FCIL to multi-modal data (e.g., image + text or clinical signals + metadata) by learning a shared e",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Communication-Efficient Prototype Compression via Learned Subspaces\nReduce FCIL communication by learning a global low-dimensional subspace (e.g., via federated PCA/autoencoder on prototype features) ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 46,
      "paper_title": "Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain",
      "contribution": "The authors show that convolutional recurrent encoders trained on realistic, temporally-structured whisker simulator data \u2014 using both supervised and tactile-specific contrastive self-supervision \u2014 produce internal representations that closely match neural activity in rodent somatosensory cortex, and that recurrence and task performance predict neural alignment.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 11324,
      "output_tokens": 978,
      "predecessor_details": [
        {
          "success": true,
          "title": "Using goal-driven deep learning models to understand sensory cortex",
          "url": "https://www.researchgate.net/publication/322970579_Commentary_Using_goal-driven_deep_learning_models_to_understand_sensory_cortex",
          "content": "(PDF) Commentary: Using goal-driven deep learning models to understand sensory cortex\nArticlePDF Available\n# Commentary: Using goal-driven deep learning models to understand sensory cortex\n* January 2018\n* [Frontiers in Computational Neuroscience](journal/Frontiers-in-Computational-Neuroscience-1662-5188)12\nDOI:[10.3389/fncom.2018.00004](https://doi.org/10.3389/fncom.2018.00004)\n* License\n* [CC BY 4.0](https://www.researchgate.net/deref/https://creativecommons.org/licenses/by/4.0/)\nAuthors:\n[![Qiulei Dong at Chinese Academy of Sciences](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)](profile/Qiulei-Dong)\n[Qiulei Dong](profile/Qiulei-Dong)\n* [Chinese Academy of Sciences](https://www.researchgate.net/institution/Chinese_Academy_of_Sciences)\n[![Hong Wang](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)](scientific-contributions/Hong-Wang-2138571028)\n[Hong Wang](scientific-contributions/Hong-Wang-2138571028)\n[Hong Wang](scientific-contributions/Hong-Wang-2138571028)\n* This person is not on ResearchGate, or hasn't claimed this research yet.\n[![Zhanyi hu at Institute of Automation, Chinese Academy of Sciences](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)](profile/Zhanyi-Hu)\n[Zhanyi hu](profile/Zhanyi-Hu)\n* [Institute of Automation, Chinese Academy of Sciences](https://www.researchgate.net/institution/Institute-of-Automation-Chinese-Academy-of-Sciences)\n![](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)\n[Download full-text PDF](publication/322970579_Commentary_Using_goal-driven_deep_learning_models_to_understand_sensory_cortex/fulltext/5a7a4a12a6fdccebdd8197ba/Commentary-Using-goal-driven-deep-learning-models-to-understand-sensory-cortex.pdf)[Read full-text](publication/322970579_Commentary_Using_goal-driven_deep_learning_models_to_understand_sensory_cortex#read)\n[Download full-text PDF](https://www.researchgate.net/publication/322970579_Commentary_Using_goal-driven_deep_learning_models_to_understand_sensory_cortex/fulltext/5a7a4a12a6fdccebdd8197ba/Commentary-Using-goal-driven-deep-learning-models-to-understand-sensory-cortex.pdf)\n[Read full-text](publication/322970579_Commentary_Using_goal-driven_deep_learning_models_to_understand_sensory_cortex#read)\n[Download citation](https://www.researchgate.net/publication/322970579_Commentary_Using_goal-driven_deep_learning_models_to_understand_sensory_cortex/citation/download)\nCopy linkLink copied\n[\nRead full-text\n](publication/322970579_Commentary_Using_goal-driven_deep_learning_models_to_understand_sensory_cortex#read)[\nDownload citation\n](https://www.researchgate.net/publication/322970579_Commentary_Using_goal-driven_deep_learning_models_to_understand_sensory_cortex/citation/download)\nCopy linkLink copied\n![ResearchGate Logo](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mN89OjNfwAJDwOxHEacwgAAAABJRU5ErkJggg==)\n**Discover the world's research**\n* 25+ million members\n* 160+ million publication pages\n* 2.3+ billion citations[Join for free](signup.SignUp.html)\n[](publication/322970579_Commentary_Using_goal-driven_deep_learning_models_to_understand_sensory_cortex#read-preview)\nAvailable via license:[CC BY 4.0](deref/https://creativecommons.org/licenses/by/4.0/)\nContent may be subject to copyright.\nGENERAL COMMENTARY\npublished: 19 January 2018\ndoi: 10.3389/fncom.2018.00004\nFrontiers in Computational Neuroscience | www.frontiersin.org1January 2018 | Volume 12 | Article 4\nEdited by:\nGuenther Palm,\nUniversity of Ulm, Germany\nReviewed by:\nThomas Wennekers,\nPlymouth University, United Kingdom\n\\*Correspondence:\nZhanyi Hu\nhuzy@nlpr.ia.ac.cn\nReceived:13 September 2017\nAccepted:05 January 2018\nPublished:19 January 2018\nCitation:\nDong Q, Wang H and Hu Z (2018)\nCommentary: Using goal-driven deep\nlearning models to understand\nsensory cortex.\nFront. Comput. Neurosci. 12:4.\ndoi: 10.3389/fncom.2018.00004\nCommentary: Using goal-driven deep\nlearning models to understand\nsensory cortex\nQiuleiDong1,2,3,HongWang2andZhanyiHu1,2,3\n\\*\n1National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China,\n2University of Chinese Academy of Sciences, Beijing, China,3CAS Center for Excellence in Brain Science and Intelligence\nTechnology, Beijing, China\nKeywords: goal-driven deep learning models, hierarchical convolutional neural network, IT neuron, categorization,\nconvergent features\nA commentary on\nUsing goal-driven deep learning models to understand sensory cortex\nby Yamins, D., and DiCarlo, J. (2016). Nat. Neurosci. 19, 356\u2013365. doi: 10.1038/nn.4244\nRecently, a goal-driven modeling approach of sensory cortex is proposed inYamins and DiCarlo\n(2016). The basic idea of this approach is to \ufb01rst optimize a hierarchical convolutional neural\nnetwork (HCNN) for performing an ethologically relevant task, then once the network parameters\nhave been \ufb01xed, to compare the outputs of di\ufb00erent layers of the network to neural data. The success\nof this approach is exempli\ufb01ed by the results inYamins et al. (2014), where a 4-layer HCNN, called\nHMO, was used to predict IT neuron spikes on image object stimuli. Notably by only optimizing\nthe 8-way image categorization performances, not only can the top output layer of the HMO\nquantitatively predict IT neuron responses, but its penultimate layer can also automatically predict\nV4 neuron responses. InHong et al. (2016), under the same approach, a 6-layer HCNN was trained\non ImageNet (Russakovsky et al., 2015) (a benchmark dataset for image object categorization\nin the computer vision \ufb01eld, containing 1.3 million category-labeled training images of 1,000\ndi\ufb00erent categories) to successfully predict category-orthogonal object properties along the ventral\nstream. Another demonstrative example is the work inKhaligh-Razavi and Kriegeskorte (2014),\nshowing that when the 10-category representational dissimilarity matrices were used together with\nthe outputs of all the 8 layers of the AlexNet inKrizhevsky et al. (2012), called the IT-geometry\nsupervised layer, its outputs could su\ufb03ciently explain IT data.\nHere in this commentary, we would say that this goal-driven approach, although with some\nnotable successes and great potential for understanding sensory cortex, could be not as general\nas the authors (Yamins and DiCarlo, 2016) advocate, and its general use should be taken with\nspecial care. This is because as shown inLi et al. (2016), the 4 di\ufb00erent HCNNs, with the\nsame AlexNet architecture trained with the same dataset (ImageNet) but only from di\ufb00erent\nrandom initializations, learned both convergent and divergent features although the 4 HCNNs have\nachieved the similar categorization performances: their top-1 accuracies are 58.65, 58.73, 58.79,\nand 58.84% respectively, which are also similar to the top-1 performance of 59.3% reported in\nthe original study (Krizhevsky et al., 2012). In other words, some convergent features, which are\nindividually similar or related via a linear transformation, are reliably learnt by the 4 HCNNs, yet\nother divergent features are not consistently learnt. In particular, the features at downstream layers\nare more divergent than convergent among the 4 HCNNs. The divergence is particularly marked\nby two aspects: (1) The responses of neurons at higher layers in one network were impossible to\nbe linearly mapped to the responses of the neurons at the same layer in other networks (Table 1\n[\n](deref/https://www.frontiersin.org/journals/computational-neuroscience)[\n](deref/https://www.frontiersin.org/journals/computational-neuroscience#editorial-board)[\n](deref/https://www.frontiersin.org/journals/computational-neuroscience#editorial-board)[\n](deref/https://www.frontiersin.org/journals/computational-neuroscience#editorial-board)[\n](deref",
          "original_query": "Using goal-driven deep learning models to understand sensory cortex",
          "cleaned_query": "Using goal-driven deep learning models to understand sensory cortex"
        },
        {
          "success": true,
          "title": "A Machine Learning Approach for Precipitation Nowcasting",
          "url": "https://papers.nips.cc/paper/5955-convolutional-lstm-network-a-machine-learning-approach-for-precipitation-nowcasting",
          "content": "[Bibtex](https://papers.nips.cc/paper_files/paper/2015/file/07563a3fe3bbe7e3ba84431ad9d055af-Bibtex.bib) [Metadata](https://papers.nips.cc/paper_files/paper/2015/file/07563a3fe3bbe7e3ba84431ad9d055af-Metadata.json) [Paper](https://papers.nips.cc/paper_files/paper/2015/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf) [Reviews](https://papers.nips.cc/paper_files/paper/2015/file/07563a3fe3bbe7e3ba84431ad9d055af-Reviews.html) [Supplemental](https://papers.nips.cc/paper_files/paper/2015/file/07563a3fe3bbe7e3ba84431ad9d055af-Supplemental.zip)\n\n## Abstract\n\nThe goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.\n\nDo not remove: This comment is monitored to verify that the site is working properly",
          "original_query": "Convolutional LSTM network: A machine learning approach for spatio-temporal data",
          "cleaned_query": "Convolutional LSTM network: A machine learning approach for spatio-temporal data"
        },
        {
          "success": true,
          "title": "Representation Learning with Contrastive Predictive Coding",
          "url": "https://arxiv.org/abs/1807.03748",
          "content": "[1807.03748] Representation Learning with Contrastive Predictive Coding\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1807.03748\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1807.03748**(cs)\n[Submitted on 10 Jul 2018 ([v1](https://arxiv.org/abs/1807.03748v1)), last revised 22 Jan 2019 (this version, v2)]\n# Title:Representation Learning with Contrastive Predictive Coding\nAuthors:[Aaron van den Oord](https://arxiv.org/search/cs?searchtype=author&amp;query=van+den+Oord,+A),[Yazhe Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y),[Oriol Vinyals](https://arxiv.org/search/cs?searchtype=author&amp;query=Vinyals,+O)\nView a PDF of the paper titled Representation Learning with Contrastive Predictive Coding, by Aaron van den Oord and 2 other authors\n[View PDF](https://arxiv.org/pdf/1807.03748)> > Abstract:\n> While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments. Subjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:1807.03748](https://arxiv.org/abs/1807.03748)[cs.LG]|\n|(or[arXiv:1807.03748v2](https://arxiv.org/abs/1807.03748v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1807.03748](https://doi.org/10.48550/arXiv.1807.03748)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: A\u00e4ron van den Oord [[view email](https://arxiv.org/show-email/c82bee1d/1807.03748)]\n**[[v1]](https://arxiv.org/abs/1807.03748v1)**Tue, 10 Jul 2018 16:52:11 UTC (2,982 KB)\n**[v2]**Tue, 22 Jan 2019 18:47:12 UTC (2,993 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Representation Learning with Contrastive Predictive Coding, by Aaron van den Oord and 2 other authors\n* [View PDF](https://arxiv.org/pdf/1807.03748)\n* [TeX Source](https://arxiv.org/src/1807.03748)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1807.03748&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1807.03748&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2018-07](https://arxiv.org/list/cs.LG/2018-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/1807.03748?context=cs)\n[stat](https://arxiv.org/abs/1807.03748?context=stat)\n[stat.ML](https://arxiv.org/abs/1807.03748?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1807.03748)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1807.03748)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1807.03748)\n### [4 blog links](https://arxiv.org/tb/1807.03748)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1807.html#abs-1807-03748)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1807-03748)\n[A\u00e4ron van den Oord]()\n[Yazhe Li]()\n[Oriol Vinyals]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1807.03748)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Representation Learning with Contrastive Predictive Coding",
          "cleaned_query": "Representation Learning with Contrastive Predictive Coding"
        },
        {
          "success": true,
          "title": "A dynamical model for generating synthetic data to quantify active ...",
          "url": "https://www.pnas.org/doi/abs/10.1073/pnas.2011905118",
          "content": "- Contents\n- - [Information & Authors](https://www.pnas.org/doi/abs/10.1073/pnas.2011905118#core-collateral-info)\n- [Metrics & Citations](https://www.pnas.org/doi/abs/10.1073/pnas.2011905118#core-collateral-metrics)\n- [View Options](https://www.pnas.org/doi/abs/10.1073/pnas.2011905118#core-collateral-fulltext-options)\n- [References](https://www.pnas.org/doi/abs/10.1073/pnas.2011905118#core-collateral-references)\n- [Media](https://www.pnas.org/doi/abs/10.1073/pnas.2011905118#core-collateral-media)\n- [Share](https://www.pnas.org/doi/abs/10.1073/pnas.2011905118#core-collateral-share)\n\n## Significance\n\nIn recent years, increasing effort has been made to study sensorimotor integration within the context of the animal\u2019s body and the environment. However, it is challenging to collect neurophysiological data under naturalistic conditions, and simulations have become an important part of neuroscience research. We developed a simulation framework, _WHISKiT Physics_, to model the dynamics of a complete sensorimotor system\u2014the rodent vibrissal array\u2014operating under ethologically relevant conditions. The tight link between sensing and motor control and the discrete somatotopy throughout the sensory pathway make the vibrissal array ideal to study sensorimotor circuits. _WHISKiT Physics_ makes this widely used model system accessible to sensory research as well as to other disciplines including information theory, reinforcement learning, and artificial intelligence.\n\n## Abstract\n\nAs it becomes possible to simulate increasingly complex neural networks, it becomes correspondingly important to model the sensory information that animals actively acquire: the biomechanics of sensory acquisition directly determines the sensory input and therefore neural processing. Here, we exploit the tractable mechanics of the well-studied rodent vibrissal (\u201cwhisker\u201d) system to present a model that can simulate the signals acquired by a full sensor array actively sampling the environment. Rodents actively \u201cwhisk\u201d \u223c60 vibrissae (whiskers) to obtain tactile information, and this system is therefore ideal to study closed-loop sensorimotor processing. The simulation framework presented here, _WHISKiT Physics_, incorporates realistic morphology of the rat whisker array to predict the time-varying mechanical signals generated at each whisker base during sensory acquisition. Single-whisker dynamics were optimized based on experimental data and then validated against free tip oscillations and dynamic responses to collisions. The model is then extrapolated to include all whiskers in the array, incorporating each whisker\u2019s individual geometry. Simulation examples in laboratory and natural environments demonstrate that _WHISKiT Physics_ can predict input signals during various behaviors, currently impossible in the biological animal. In one exemplary use of the model, the results suggest that active whisking increases in-plane whisker bending compared to passive stimulation and that principal component analysis can reveal the relative contributions of whisker identity and mechanics at each whisker base to the vibrissotactile response. These results highlight how interactions between array morphology and individual whisker geometry and dynamics shape the signals that the brain must process.\n\n## Continue Reading\n\n[VIEW PDF](https://www.pnas.org/doi/reader/10.1073/pnas.2011905118) [FULL TEXT](https://www.pnas.org/doi/full/10.1073/pnas.2011905118)\n\n## Data Availability\n\nAll study data are included in the article and/or supporting information.\n\n## Acknowledgments\n\nWe thank Dr. Sara A. Solla (Northwestern University) for comments and insightful discussions that greatly improved the final version of the manuscript.\n\n## Supporting Information\n\nAppendix (PDF)\n\n- [Download](https://www.pnas.org/doi/suppl/10.1073/pnas.2011905118/suppl_file/pnas.2011905118.sapp.pdf)\n- 970.50 KB\n\nDataset\\_S01 (CSV)\n\n- [Download](https://www.pnas.org/doi/suppl/10.1073/pnas.2011905118/suppl_file/pnas.2011905118.sd01.csv)\n- 1.96 KB\n\n[iframe](https://iframe.videodelivery.net/eyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiJhYmFjOTA0MDg1MTI2OWFlNjZiN2Q1MjMyMDI5Y2IzNCIsImV4cCI6MTcyMDE0MDcxMiwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.xVELhuc0uYshjEqzh6s_kauQ8_anKXlEWaUPFLJ98gsrCXF7UBpxk5qfT3YdDScp3D-zQ1panpvUJMO54ceXrlZww-1D1hi6Y82m8vvVMvdx96qUp-HrvMqu97qF4rkzMz4_8LcsK2Tjv-f8ecAuHZGBPrHY7o_jX870WD3gF8TjId3Yew6rrx-Fp6LP71AXaYbPuX1Yfy_LLkgIzOpJIkPV_o_2PR-vAs30xhPIShFJVLHYbW4SHFbJBB3KNONwoFcewR3QgnmGkaIspd8ir7ZCaocDivnQ9MijT6rWokHtq4wKye4ju3hPdag6WiDw_88WLXtcYgKbTdaivqPSkg?poster=https%3A%2F%2Fvideodelivery.net%2FeyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiJhYmFjOTA0MDg1MTI2OWFlNjZiN2Q1MjMyMDI5Y2IzNCIsImV4cCI6MTcyMDE0MDcxMiwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.xVELhuc0uYshjEqzh6s_kauQ8_anKXlEWaUPFLJ98gsrCXF7UBpxk5qfT3YdDScp3D-zQ1panpvUJMO54ceXrlZww-1D1hi6Y82m8vvVMvdx96qUp-HrvMqu97qF4rkzMz4_8LcsK2Tjv-f8ecAuHZGBPrHY7o_jX870WD3gF8TjId3Yew6rrx-Fp6LP71AXaYbPuX1Yfy_LLkgIzOpJIkPV_o_2PR-vAs30xhPIShFJVLHYbW4SHFbJBB3KNONwoFcewR3QgnmGkaIspd8ir7ZCaocDivnQ9MijT6rWokHtq4wKye4ju3hPdag6WiDw_88WLXtcYgKbTdaivqPSkg%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D4.0635s)\n\nMovie S1.\n\n**Visualization of the passive stimulation experiment (Scenario 1).** A vertical peg was simulated to move from rostral to caudal through the middle of the immobile right array. See SI Appendix for the full caption.\n\n- [Download](https://www.pnas.org/doi/suppl/10.1073/pnas.2011905118/suppl_file/pnas.2011905118.sm01.mp4)\n- 2.20 MB\n\n[iframe](https://iframe.videodelivery.net/eyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiI3YjdlMzc4M2NkYWQyNjE0Njc1M2JlNjEzMTIwODlkZCIsImV4cCI6MTcyMDE0MDcxMiwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.XG_xOHHz3AacyMcF_iMkQHXow0Q76AsvNCipkw2aXMmpitTqa8nOCXs0Af132jcEOpq-_VwtDvbaNA2CCoLXOKsIBVeq4NqkWtiNaReRsCB0NjBfF_S-K4LRL2mdhMg9dS7ZWwHeVSeNqa5-MaZ_CuKmbGmIHZXgiwRVJZFOUaHfdpWaSccMFljnEogd3MqS-GyGrquFctFE-BhqXFF2_Room7cBefw4Vv_W64e50MaWTo22LclSk4CaVqDY9nSbgw3iOa9P3V-FJk3KjWJII-4xhqJ3mZLtCGuW8ZomGr95vGvaietn82C_YvRCSw0unEllSM72oC1jd5HXb8E7MA?poster=https%3A%2F%2Fvideodelivery.net%2FeyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiI3YjdlMzc4M2NkYWQyNjE0Njc1M2JlNjEzMTIwODlkZCIsImV4cCI6MTcyMDE0MDcxMiwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.XG_xOHHz3AacyMcF_iMkQHXow0Q76AsvNCipkw2aXMmpitTqa8nOCXs0Af132jcEOpq-_VwtDvbaNA2CCoLXOKsIBVeq4NqkWtiNaReRsCB0NjBfF_S-K4LRL2mdhMg9dS7ZWwHeVSeNqa5-MaZ_CuKmbGmIHZXgiwRVJZFOUaHfdpWaSccMFljnEogd3MqS-GyGrquFctFE-BhqXFF2_Room7cBefw4Vv_W64e50MaWTo22LclSk4CaVqDY9nSbgw3iOa9P3V-FJk3KjWJII-4xhqJ3mZLtCGuW8ZomGr95vGvaietn82C_YvRCSw0unEllSM72oC1jd5HXb8E7MA%2Fthumbnails%2Fthumbnail.jpg%3Ftime%3D10.0s)\n\nMovie S2.\n\n**Visualization of active whisking against two vertical pegs (Scenario 2).** Same as in Movie S1, but instead of a single sweep through the entire array, the peg oscillates back and forth between its start and end position (in the middle of the array) to repeatedly stimulate the array eight times per second (8Hz). This scenario was carefully designed to replicate as closely as possible the stimulation distances, velocities, and frequencies associated with active whisking (Scenario 3).\n\n- [Download](https://www.pnas.org/doi/suppl/10.1073/pnas.2011905118/suppl_file/pnas.2011905118.sm02.mp4)\n- 2.82 MB\n\n[iframe](https://iframe.videodelivery.net/eyJraWQiOiI3YjgzNTg3NDZlNWJmNDM0MjY5YzEwZTYwMDg0ZjViYiIsImFsZyI6IlJTMjU2In0.eyJzdWIiOiI4YWMxZGNjOWFlMWU2NDZhMzZjZmU5NmMxNDk2ZjljYSIsImV4cCI6MTcyMDE0MDcxMiwia2lkIjoiN2I4MzU4NzQ2ZTViZjQzNDI2OWMxMGU2MDA4NGY1YmIifQ.VwAVxl9lx1AisXXMYwAU3BIm68me_k3Zpo01fs8NPN_wm5Ygg-tf8Y93QtLzf4N2PIzir9dKJQ8QjelfycFsp3IcpCQkfh1J1TarvTjTH8cdxhtNziC5II2TAKyMKoVWMfrgvaK-JFJ3dJa1w6a9dYvkA9N57k729R4Ue1wncls75bzUrecd6jHjy0VSW5R06FyeKY37hS4rtPJhl0nlj0bKVDegjNu-CdLNRLzPZ3zqhKPx6JLPqQS2LyOGE-3J8FNnTsd15amf-NHppVLe4Aa8",
          "original_query": "WHISKiT-Physics / physics-based whisker simulator (Cheung et al., 2019)",
          "cleaned_query": "WHISKiT-Physics"
        },
        {
          "success": true,
          "title": "Young Children's Haptic Exploratory Procedures - PMC - NIH",
          "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC3166994/",
          "content": "\n \n \n \n \n. Author manuscript; available in PMC: 2012 Dec 1. \n Published in final edited form as: J Exp Child Psychol. 2011 Jul 23;110(4):592\u2013602. doi: 10.1016/j.jecp.2011.06.007 \n \n Abstract \n Adults vary their haptic exploratory behavior reliably with variation both in the sensory input and in the task goals. Little is known about the development of these connections between perceptual goals and exploratory behaviors. Thirty-six children 3, 4, and 5 years of age and 20 adults completed a haptic intra-modal match-to-sample task. Participants were instructed to feel the shape, texture, rigidity, or weight of a sample object, and then asked to find which of three test objects matched the sample on that specific property. Hand movements were examined to determine whether children produced the same exploratory procedures while gathering perceptual information about each property as adults who searched for the same kind of information. Children demonstrated that they had good haptic abilities in two ways: they matched the sample objects on the specified perceptual dimension at near ceiling levels, and they produced the same hand movement patterns to find the same properties as adults. \n The human hand is a powerful tool. The sensory and motor subsystems that serve the hand allow for quick and efficient interactions with the world. The hand\u2019s sensory system is designed to learn about the perceptual features of objects and is made up of cutaneous, thermal, and kinesthetic sensors. The hand\u2019s motor system is designed to interact with and manipulate objects ( Klatzky &amp; Lederman, 2002; Lederman &amp; Klatzky, 2009). The sensory and motor systems are not independent. How the hands are moved determines what sensory information is obtained. The sensory input obtained guides further hand movements and importantly, constrains what can be perceived. \n When adults search for perceptual information, their hand movement patterns (i.e., the haptic exploratory behaviors) are efficient, rapid, and systematic (e.g., Klatzky, Lederman, &amp; Metzger, 1985). Lederman, Klatzky, and their colleagues have extensively studied the haptic exploratory abilities of adults. Lederman and Klatzky (1987) recorded the hand and finger movements of adults attempting to extract perceptual information about specific object properties \u2013 shape, texture, hardness, weight, temperature, volume, part motion, and specific function \u2013 using haptics alone. Subjects were instructed to find which of three test objects was the best match for a standard object on one of these eight specific dimensions. None of the test objects was identical to the standard and participants were told to ignore all other properties of the objects. The participants\u2019 object matching choices and the hand movements that preceded them were recorded. The researchers identified eight stereotyped hand movement patterns or \u201cexploratory procedures,\u201d each associated with a specific task goal. For example, adults asked to match objects by texture generally produced \u201clateral motion\u201d \u2013 movements of the fingers lateral to the object\u2019s surface. In contrast, adults asked to match objects by shape produced \u201ccontour following\u201d \u2013 tracing the object\u2019s contours with their fingertips. Hand movements during haptic exploration varied reliably both with the sensory input and with the kind of information participants had been asked to seek. The consistent relations between particular hand movements and particular goals suggested that observing how participants moved their hands during haptic tasks could reveal the goals behind their exploratory behavior ( Lederman &amp; Klatzky, 1987). \n Little is known about the development of the connections between particular perceptual goals and particular exploratory manual behaviors observed in adults. In the present study, we ask whether young children perform the same hand movements as adults perform to achieve the same perceptual goals. There are very few previous studies on this question. Bushnell and Boudreau (1991, 1993, 1998) developed (but did not test) a set of hypotheses about when in development the exploratory procedures in Lederman and Klatzky\u2019s (1987) taxonomy might become available to infants and young children due to age-typical advances in motor behavior and attention. For example, these authors suggested that infants should not be able to produce the contour-following exploratory procedure associated with haptic perception of shape until 9 or 10 months of age, because it is not until this age that infants acquire the ability to move their two hands independently. Bushnell and Boudreau (1991, 1993, 1998) suggested that children might have the capacity to display mature haptic exploratory movements when they reached the preschool period. \n However, there are few data on whether preschool-aged children do employ adult-like exploratory procedures. In fact, there are few data on children\u2019s hand movements of any kind during haptic object exploration. Instead, research on the development of haptic perception has largely focused on children\u2019s ability to use haptic information for object recognition either in haptics or in vision. A number of studies have concluded that young children have poor haptic perception. However, this conclusion has been based on findings of deficits in young children\u2019s ability to transfer information between haptics and vision in inter-modal object recognition tasks. These studies have not reported how children moved their hands during haptic exploration (e.g., Milner &amp; Bryant, 1970; Rose, Blank, &amp; Bridger, 1972). \n In contrast, a number of studies have looked at what children do with their hands during visual object exploration. Ruff (1982, 1984, 1986, 1989; Ruff &amp; Kohler, 1978) carried out a number of studies documenting how infants handle objects during visual exploration, but there is no evidence that the set of hand movements produced by infants in the service of visual exploration overlaps with the set of hand movements that serve haptic perception. Klatzky, Lederman, and Mankinen (2005) asked preschool-aged children to make decisions about the appropriateness of a tool to perform a certain task \u2013for example, could a spoon be used to carry a piece of candy. Children were visually presented with the tools and also allowed to handle them. The children were more likely to haptically explore objects when the important object property was rigidity than when it was shape. Moreover, when the children chose to explore haptically, they tested the objects for rigidity using the same exploratory procedure as adults use for assessing rigidity (i.e., pressure; Lederman &amp; Klatzky, 1987). \n Only three previous studies have documented what children do with their hands during haptic exploration. Schwarzer, Kufer, and Wilkening (1999) reported that children aged 3 to 9 years produced adult-like exploratory procedures. Following haptic exploration, children in this study haptically grouped novel objects that simultaneously varied in shape, texture, weight, and size. Almost all participants categorized the objects analytically - that is, on a single perceptual dimension \u2013 as opposed to holistically. In at least 60% of their trials, 3- to 5-year-old children produced enclosure, lateral motion, and contour following \u2013 three adult-like exploratory procedures. However, whereas adults used enclosure and contour following to find shape information, the children in this study produced these two movements at high frequencies before categorizing objects by texture. It is possible that the children in Schwarzer et al. (1999) produced something similar to enclosure and contour following but that these movements were not really the same as those produced by adults; or that children obtained shape information from these two procedures but failed to use it during categorization. It is also possible that exploratory procedures and perceptual goals are not as strongly and distinctive",
          "original_query": "Haptic exploratory procedures and active touch (Lederman & Klatzky, 2009)",
          "cleaned_query": "Haptic exploratory procedures and active touch",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "Functional organization in mouse barrel cortex - ScienceDirect",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/000689937990564X",
          "content": "First page preview Acute whisker removal reduces neuronal activity in barrels of mouse SmI cortex Morphology of Golgi-impregnated neurons in mouse cortical barrels following whisker damage at different postnatal ages Modality and topographic properties of single neurons of cat's somatic sensory cortex Cortical neuronal mechanisms in flutter-vibration studied in unanesthetized monkeys. Neuronal periodicity and frequency discrimination There are more references available in the full text version of this article. Cited by (142) 2019, Journal of Chemical Neuroanatomy The CA pups showed no overt signs of craniofacial abnormalities or general brain morphology. The PMBSF barrels appeared darkly CO reactive with no distorted boundaries or disruptions to the organization of the barrels, and appeared typical of that reported generally for mice (Woolsey and van der Loos, 1970; Simons and Woolsey, 1979). The PMBSF was sharply distinct from the adjacent cortex on three sides, the posterior, posteromedial and posterolateral borders, but the anterior border was less distinct. 2019, Neuroscience and Biobehavioral Reviews In each area, cortical and subcortical, the receptive topography of the periphery is maintained in several modality maps. This means that the neighboring cells in the skin project to neighboring groups of neurons in areas such as the thalamus and cortex (Lenz et al., 2002; Simons and Woolsey, 1979). After the primary sensory cortex, the signal is send to secondary, higher-order areas. 2014, NeuroImage Whiskers actively move at high frequency and are thought to have a similar function to the human fingertips for sensing surface texture (Petersen, 2007). Functional and anatomical mapping studies have established that each whisker makes a preferential connection to a single barrel (Grinvald and Hildesheim, 2004; Simons and Woolsey, 1979; Welker, 1976). In rodents, barrels in the large barrel field, referred to as the posterior medial barrel sub-field (PMBSF), are arranged topographically in 5 rows (A\u2013E) with 5 arcs (1\u20135). 2009, Brain Research The approximate locations of core auditory, visual, and somatosensory (barrel) cortices were estimated (see Figs. 1, 3 and 4 for examples) using the mouse atlas of Franklin and Paxinos (1996) and architectonic studies (Caviness and Frost, 1975; Ma et al., 1989; Mangini and Pearlman, 1980; Wree et al., 1983). In parietal cortex, analyses were focussed on the region with clear barrel structure in sections stained for CYO, corresponding to the physiological primary somatosensory barrel field (Simons and Woolsey, 1979). In occipital cortex, the region in which layer IV stained most densely for cytochrome oxidase is considered to correspond to the physiologically defined primary visual cortex in the mouse (Schuett et al., 2002). Work supported by NIH Grant EH 01255. Dr. Simons is supported by NIH Training Grant NS 07057. We are grateful to Drs. A. Pearlman, T. Sandel and N. Suga for use of their equipment. ||||I||||JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article\n * Journals & Books\n * \n * Search\n My Account Sign in\n * Access through your institution\n * Purchase PDF\n Search ScienceDirect\n\n Article preview\n\n * References (15)\n * Cited by (142)\n\n Brain Research\n\n Volume 165, Issue 2 , 13 April 1979, Pages 327-332\n\n Functional organization in mouse barrel cortex\n\n Author links open overlay panel Daniel J. Simons *, Thomas A. Woolsey *\n Show more\n Add to Mendeley\n Share\n Cite\n https://doi.org/10.1016/0006-8993(79)90564-X Get rights and content\n\n First page preview\n\n Click to open first page preview\n View PDF\n Recommended articles\n\n References (15)\n\n * Axelrad H. et al.\n\n Responses evoked in mouse and rat SI cortex by vibrissa stimulation\n\n Neurosci. Lett.\n\n (1976)\n * Welker C.\n\n Microelectrode delineation of fine grain somatotopic organization of SmI cerebral neocortex in albino rat\n\n Brain Research\n\n (1971)\n * Woolsey T.A. et al.\n\n The structural organization of layer IV in the somatosensory region (SI) of mouse cerebral cortex: the description of a cortical field composed of discrete cytoarchitectonic units\n\n Brain Research\n\n (1970)\n * Durham D. et al.\n\n Acute whisker removal reduces neuronal activity in barrels of mouse SmI cortex\n\n J. comp. Neurol.\n\n (1978)\n * Harris R.M. et al.\n\n Morphology of Golgi-impregnated neurons in mouse cortical barrels following whisker damage at different postnatal ages\n\n Brain Research\n\n (1979)\n * Mountcastle V.B.\n\n Modality and topographic properties of single neurons of cat's somatic sensory cortex\n\n J. Neurophysiol.\n\n (1957)\n * Mountcastle V.B. et al.\n\n Cortical neuronal mechanisms in flutter-vibration studied in unanesthetized monkeys. Neuronal periodicity and frequency discrimination\n\n J. Neurophysiol.\n\n (1969)\n There are more references available in the full text version of this article.\n\n Cited by (142)\n\n * Changes to the somatosensory barrel cortex in C57BL/6J mice at early adulthood (56 days post-natal) following prenatal alcohol exposure\n\n 2019, Journal of Chemical Neuroanatomy\n Citation Excerpt :\n\n The CA pups showed no overt signs of craniofacial abnormalities or general brain morphology. The PMBSF barrels appeared darkly CO reactive with no distorted boundaries or disruptions to the organization of the barrels, and appeared typical of that reported generally for mice (Woolsey and van der Loos, 1970; Simons and Woolsey, 1979). The PMBSF was sharply distinct from the adjacent cortex on three sides, the posterior, posteromedial and posterolateral borders, but the anterior border was less distinct.\n\n Show abstract\n\n Children with Fetal Alcohol Spectrum Disorder (FASD) have impaired sensory processing skills as a result of neurodevelopmental anomalies. The somatosensory barrel field of rodent brain is a readily accessible model for studying the effects of alcohol exposure. Within the barrel field, the posterior medial barrel subfield (PMBSF) receives sensory inputs from the large vibrissae on the contralateral face. This study reports on the consequence of prenatal exposure to alcohol on the somatosensory cortices of mice later in life. Two control groups, a sucrose and a non-treated control, were also examined. At postnatal day (PND) 56 the cerebral hemisphere of mice from each group were processed for cytochrome oxidase reactivity. In contrast to previous studies, there were no significant differences in the mean areas of: (I) the PMBSF enclosure, (II) the PMBSF barrels, (III) the individual PMBSF barrels and (IV) the septal portion of the PMBSF in the alcohol group compared to the controls. However barrel sizes in rows D and E in the alcohol group were significantly reduced, indicating an alcohol-induced damage on the barrel development and which may reduce the amount of the cortex devoted to processing somatosensory input\u2013 a common defect seen in children with FASD.\n\n * Multisensory cortical processing and dysfunction across the neuropsychiatric spectrum\n\n 2019, Neuroscience and Biobehavioral Reviews\n Citation Excerpt :\n\n In each area, cortical and subcortical, the receptive topography of the periphery is maintained in several modality maps. This means that the neighboring cells in the skin project to neighboring groups of neurons in areas such as the thalamus and cortex (Lenz et al., 2002; Simons and Woolsey, 1979). After the primary sensory cortex, the signal is send to secondary, higher-order areas.\n\n Show abstract\n\n Sensory processing is affected in multiple neuropsychiatric disorders like schizophrenia and autism spectrum disorders. Genetic and environmental factors guide the formation and fine-tuning of brain circuitry necessary to receive, organize, and respond to sensory input in order to behave in a meaningful and consistent manner. During certain developmental stages the brain is sensitive to intrinsic and external factors. For example, disturbed expression levels of certain ri",
          "original_query": "The functional organization of barrel cortex / reviews of rodent somatosensory processing",
          "cleaned_query": "The functional organization of barrel cortex"
        }
      ],
      "generated_ideas": [
        "Goal-Driven Whisker-Cortex Models Trained on WHISKiT Physics\n- Train hierarchical ConvNet/ConvLSTM models on tactile signals generated by WHISKiT Physics while optimizing ethological tasks (e.g., edge orientation, gap crossing, object localization). Evaluate which layers best predict neuronal responses across the whisker-to-barrel-cortex pathway and whether task choice changes the emergence of barrel-like topography.",
        "Contrastive Predictive Coding for Active Tactile Sequences in Whisking\n- Apply CPC to multi-whisker time series (forces/moments at whisker bases) to learn latent states that predict future contact events during active exploration. Test whether CPC latents linearly decode object properties (texture/shape) and better match barrel-cortex representational geometry than supervised-only training.",
        "Closed-Loop Sensorimotor RL: Learning Whisking Policies that Maximize Predictive Information\n- Use WHISKiT Physics as the environment and train an agent to control whisking parameters (amplitude, phase, set-point) with a reward combining task success and CPC-style predictability of future sensory input. Quantify how learned policies trade off exploration vs exploitation and compare emergent \u201cexploratory procedures\u201d to those observed in humans/rodents.",
        "ConvLSTM Nowcasting of Tactile Flow: Predicting Contact Maps Across the Whisker Array\n- Recast multi-whisker signals as a spatiotemporal \u201ctactile radar\u201d grid (whisker identity \u00d7 time) and train ConvLSTM models to nowcast near-future contact forces during ongoing whisking. Evaluate whether accurate nowcasting reduces sample complexity for downstream recognition and provides a computational account of predictive coding in somatosensory cortex.",
        "Synthetic-to-Biological Transfer: Domain Adaptation from WHISKiT Physics to Real Barrel Cortex Data\n- Build a training pipeline where models are pre-trained on WHISKiT Physics synthetic signals and then adapted to limited real recordings using feature-level alignment (e.g., contrastive domain adaptation). Measure gains in predicting barrel-cortex activity and identify which simulated factors (surface friction, whisker stiffness, motor noise) are most critical for transfer.",
        "Developmental Emergence of Exploratory Procedures via Curriculum Learning\n- Implement a staged training curriculum mirroring children\u2019s increasing motor/control capacity (limited degrees of freedom early, richer control later) and assess when specific exploration primitives emerge (e.g., \u201clateral motion\u201d analogs for texture, \u201ccontour following\u201d for shape). Compare learned behavior clusters with the taxonomy of haptic exploratory procedures and test predictions about which constraints delay mature strategies.",
        "Task-Conditional \u201cVibrissal IT\u201d: Convergent Feature Units for Object Identity vs Properties\n- Train goal-driven networks on multiple tactile objectives (categorization, texture discrimination, rigidity estimation) and search for units that become selective to category while being invariant to orthogonal properties (and vice versa), paralleling ventral-stream findings. Use representational similarity analysis to test whether different objectives yield separable pathways analogous to \u201cwhat\u201d vs \u201chow\u201d processing within somatosensory hierarchies.",
        "Self-Supervised Prediction of Mechanical Latents from Partial Whisker Observations\n- Train CPC/sequence models to infer missing whisker channels (dropped/occluded whiskers) and latent environment properties (contact location, surface roughness) from incomplete array data. This yields a concrete computational hypothesis for robustness after whisker damage and can be evaluated against known activity reductions and remapping effects in barrel cortex after whisker removal.",
        "Unified Spatiotemporal Contrastive Learning Across Weather and Tactile Dynamics\n- Develop a shared framework where ConvLSTM backbones are trained with CPC-style objectives on both radar precipitation sequences and whisker-contact sequences to identify modality-agnostic principles of spatiotemporal prediction. The contribution is a controlled cross-domain study testing which architectural components (convolutional recurrence, negative sampling strategy, horizon length) are truly general versus domain-specific."
      ],
      "generated_ideas_raw": "1. **Goal-Driven Whisker-Cortex Models Trained on WHISKiT Physics**\n   - Train hierarchical ConvNet/ConvLSTM models on tactile signals generated by WHISKiT Physics while optimizing ethological tasks (e.g., edge orientation, gap crossing, object localization). Evaluate which layers best predict neuronal responses across the whisker-to-barrel-cortex pathway and whether task choice changes the emergence of barrel-like topography.\n\n2. **Contrastive Predictive Coding for Active Tactile Sequences in Whisking**\n   - Apply CPC to multi-whisker time series (forces/moments at whisker bases) to learn latent states that predict future contact events during active exploration. Test whether CPC latents linearly decode object properties (texture/shape) and better match barrel-cortex representational geometry than supervised-only training.\n\n3. **Closed-Loop Sensorimotor RL: Learning Whisking Policies that Maximize Predictive Information**\n   - Use WHISKiT Physics as the environment and train an agent to control whisking parameters (amplitude, phase, set-point) with a reward combining task success and CPC-style predictability of future sensory input. Quantify how learned policies trade off exploration vs exploitation and compare emergent \u201cexploratory procedures\u201d to those observed in humans/rodents.\n\n4. **ConvLSTM Nowcasting of Tactile Flow: Predicting Contact Maps Across the Whisker Array**\n   - Recast multi-whisker signals as a spatiotemporal \u201ctactile radar\u201d grid (whisker identity \u00d7 time) and train ConvLSTM models to nowcast near-future contact forces during ongoing whisking. Evaluate whether accurate nowcasting reduces sample complexity for downstream recognition and provides a computational account of predictive coding in somatosensory cortex.\n\n5. **Synthetic-to-Biological Transfer: Domain Adaptation from WHISKiT Physics to Real Barrel Cortex Data**\n   - Build a training pipeline where models are pre-trained on WHISKiT Physics synthetic signals and then adapted to limited real recordings using feature-level alignment (e.g., contrastive domain adaptation). Measure gains in predicting barrel-cortex activity and identify which simulated factors (surface friction, whisker stiffness, motor noise) are most critical for transfer.\n\n6. **Developmental Emergence of Exploratory Procedures via Curriculum Learning**\n   - Implement a staged training curriculum mirroring children\u2019s increasing motor/control capacity (limited degrees of freedom early, richer control later) and assess when specific exploration primitives emerge (e.g., \u201clateral motion\u201d analogs for texture, \u201ccontour following\u201d for shape). Compare learned behavior clusters with the taxonomy of haptic exploratory procedures and test predictions about which constraints delay mature strategies.\n\n7. **Task-Conditional \u201cVibrissal IT\u201d: Convergent Feature Units for Object Identity vs Properties**\n   - Train goal-driven networks on multiple tactile objectives (categorization, texture discrimination, rigidity estimation) and search for units that become selective to category while being invariant to orthogonal properties (and vice versa), paralleling ventral-stream findings. Use representational similarity analysis to test whether different objectives yield separable pathways analogous to \u201cwhat\u201d vs \u201chow\u201d processing within somatosensory hierarchies.\n\n8. **Topographic Regularizers to Induce Barrel-Like Maps in Deep Networks**\n   - Add explicit locality and wiring-cost constraints (e.g., 2D sheet embedding with distance-penalized connections) to models trained on whisker-array inputs, encouraging topographic organization. Test whether such constraints improve prediction of known barrel cortex somatotopy and whether they produce realistic reorganization patterns after simulated whisker trimming/removal.\n\n9. **Self-Supervised Prediction of Mechanical Latents from Partial Whisker Observations**\n   - Train CPC/sequence models to infer missing whisker channels (dropped/occluded whiskers) and latent environment properties (contact location, surface roughness) from incomplete array data. This yields a concrete computational hypothesis for robustness after whisker damage and can be evaluated against known activity reductions and remapping effects in barrel cortex after whisker removal.\n\n10. **Unified Spatiotemporal Contrastive Learning Across Weather and Tactile Dynamics**\n   - Develop a shared framework where ConvLSTM backbones are trained with CPC-style objectives on both radar precipitation sequences and whisker-contact sequences to identify modality-agnostic principles of spatiotemporal prediction. The contribution is a controlled cross-domain study testing which architectural components (convolutional recurrence, negative sampling strategy, horizon length) are truly general versus domain-specific.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Goal-Driven Whisker-Cortex Models Trained on WHISKiT Physics\n- Train hierarchical ConvNet/ConvLSTM models on tactile signals generated by WHISKiT Physics while optimizing ethological tasks (e.g., edge",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Contrastive Predictive Coding for Active Tactile Sequences in Whisking\n- Apply CPC to multi-whisker time series (forces/moments at whisker bases) to learn latent states that predict future contact eve",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Closed-Loop Sensorimotor RL: Learning Whisking Policies that Maximize Predictive Information\n- Use WHISKiT Physics as the environment and train an agent to control whisking parameters (amplitude, phas",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "ConvLSTM Nowcasting of Tactile Flow: Predicting Contact Maps Across the Whisker Array\n- Recast multi-whisker signals as a spatiotemporal \u201ctactile radar\u201d grid (whisker identity \u00d7 time) and train ConvLS",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Synthetic-to-Biological Transfer: Domain Adaptation from WHISKiT Physics to Real Barrel Cortex Data\n- Build a training pipeline where models are pre-trained on WHISKiT Physics synthetic signals and th",
          "is_match": true
        },
        {
          "idea_idx": 5,
          "idea_text": "Developmental Emergence of Exploratory Procedures via Curriculum Learning\n- Implement a staged training curriculum mirroring children\u2019s increasing motor/control capacity (limited degrees of freedom ea",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Task-Conditional \u201cVibrissal IT\u201d: Convergent Feature Units for Object Identity vs Properties\n- Train goal-driven networks on multiple tactile objectives (categorization, texture discrimination, rigidit",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Self-Supervised Prediction of Mechanical Latents from Partial Whisker Observations\n- Train CPC/sequence models to infer missing whisker channels (dropped/occluded whiskers) and latent environment prop",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Unified Spatiotemporal Contrastive Learning Across Weather and Tactile Dynamics\n- Develop a shared framework where ConvLSTM backbones are trained with CPC-style objectives on both radar precipitation ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 47,
      "paper_title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism",
      "contribution": "Introduce Elastic Multimodal Parallelism (EMP) and ElasticMM, a serving system that decouples multimodal inference stages, performs modality-aware load balancing, and elastically adjusts per-stage parallelism and caching to greatly reduce TTFT and improve throughput for MLLMs.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 11162,
      "output_tokens": 1055,
      "predecessor_details": [
        {
          "success": true,
          "title": "DistServe: Disaggregating Prefill and Decoding for ...",
          "url": "https://arxiv.org/abs/2401.09670",
          "content": "[2401.09670] DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2401.09670\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Distributed, Parallel, and Cluster Computing\n**arXiv:2401.09670**(cs)\n[Submitted on 18 Jan 2024 ([v1](https://arxiv.org/abs/2401.09670v1)), last revised 6 Jun 2024 (this version, v3)]\n# Title:DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving\nAuthors:[Yinmin Zhong](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhong,+Y),[Shengyu Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+S),[Junda Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+J),[Jianbo Hu](https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+J),[Yibo Zhu](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+Y),[Xuanzhe Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+X),[Xin Jin](https://arxiv.org/search/cs?searchtype=author&amp;query=Jin,+X),[Hao Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+H)\nView a PDF of the paper titled DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving, by Yinmin Zhong and 7 other authors\n[View PDF](https://arxiv.org/pdf/2401.09670)[HTML (experimental)](https://arxiv.org/html/2401.09670v3)> > Abstract:\n> DistServe improves the performance of large language models (LLMs) serving by disaggregating the prefill and decoding computation. Existing LLM serving systems colocate the two phases and batch the computation of prefill and decoding across all users and requests. We find that this strategy not only leads to strong prefill-decoding interferences but also couples the resource allocation and parallelism plans for both phases. LLM applications often emphasize individual latency for each phase: time to first token (TTFT) for the prefill phase and time per output token (TPOT) of each request for the decoding phase. In the presence of stringent latency requirements, existing systems have to prioritize one latency over the other, or over-provision compute resources to meet both.\n> DistServe assigns prefill and decoding computation to different GPUs, hence eliminating prefill-decoding interferences. Given the application&#39;s TTFT and TPOT requirements, DistServe co-optimizes the resource allocation and parallelism strategy tailored for each phase. DistServe also places the two phases according to the serving cluster&#39;s bandwidth to minimize the communication caused by disaggregation. As a result, DistServe significantly improves LLM serving performance in terms of the maximum rate that can be served within both TTFT and TPOT constraints on each GPU. Our evaluations show that on various popular LLMs, applications, and latency requirements, DistServe can serve 7.4x more requests or 12.6x tighter SLO, compared to state-of-the-art systems, while staying within latency constraints for &gt; 90% of requests. Comments:|OSDI 2024|\nSubjects:|Distributed, Parallel, and Cluster Computing (cs.DC)|\nCite as:|[arXiv:2401.09670](https://arxiv.org/abs/2401.09670)[cs.DC]|\n|(or[arXiv:2401.09670v3](https://arxiv.org/abs/2401.09670v3)[cs.DC]for this version)|\n|[https://doi.org/10.48550/arXiv.2401.09670](https://doi.org/10.48550/arXiv.2401.09670)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Yinmin Zhong [[view email](https://arxiv.org/show-email/f746e30c/2401.09670)]\n**[[v1]](https://arxiv.org/abs/2401.09670v1)**Thu, 18 Jan 2024 01:03:38 UTC (141 KB)\n**[[v2]](https://arxiv.org/abs/2401.09670v2)**Tue, 19 Mar 2024 06:20:25 UTC (142 KB)\n**[v3]**Thu, 6 Jun 2024 15:50:51 UTC (273 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving, by Yinmin Zhong and 7 other authors\n* [View PDF](https://arxiv.org/pdf/2401.09670)\n* [HTML (experimental)](https://arxiv.org/html/2401.09670v3)\n* [TeX Source](https://arxiv.org/src/2401.09670)\n[![license icon](https://arxiv.org/icons/licenses/by-sa-4.0.png)view license](http://creativecommons.org/licenses/by-sa/4.0/)\nCurrent browse context:\ncs.DC\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2401.09670&amp;function=prev&amp;context=cs.DC) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2401.09670&amp;function=next&amp;context=cs.DC)\n[new](https://arxiv.org/list/cs.DC/new)|[recent](https://arxiv.org/list/cs.DC/recent)|[2024-01](https://arxiv.org/list/cs.DC/2024-01)\nChange to browse by:\n[cs](https://arxiv.org/abs/2401.09670?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2401.09670)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2401.09670)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2401.09670)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with commu",
          "original_query": "DistServe: Disaggregating prefill and decoding for goodput-optimized large language model serving",
          "cleaned_query": "DistServe: Disaggregating prefill and decoding for goodput-optimized large language model serving"
        },
        {
          "success": true,
          "title": "LoongServe: Efficiently Serving Long-Context Large Language ...",
          "url": "https://www.researchgate.net/publication/385860137_LoongServe_Efficiently_Serving_Long-Context_Large_Language_Models_with_Elastic_Sequence_Parallelism",
          "content": "To read the full-text of this research, you can request a copy directly from the authors.... First, a MAAS is backed by fast GPU-GPU/CPU compute fabrics [42], which are 100-400 Gbps RDMA and even 16 Tbps NVLink [29,6,22], much faster than SSD and comparable to host-GPU link. The fabric is used for data transfer during serving and we found that they are under-utilized (up to 14.4% of total bandwidth), even in network-heavy workloads like P/D disaggregation [47,68,31,16, 64] GPU HBM required Figure 1: The timeline of request incoming rate of a real-world AzureConv [11] trace (a), its computation (b) and memory requirements (c) when serving this workload without SLO violation. ...... To realize the above zigzag scheduling, we formulate the problem as follows. Note that like existing works [36, 64, 66], the scheduling assumes a first-come-first-serve (FCFS) policy. For ease of presentation, we first assume non-LLM and then extend to LLM. ...... First, the execution time of equal-sized batches varies due to different sequence lengths, so our formulas and constraints described above cannot be directly applied to LLM serving. Fortunately, the prefill time of a layer has been shown to be approximately linear to the total batched token size [47, 64], so we can add a regulation parameter for each request to address this issue when scaling prefill-only serving (e.g., autoscaling a prefill instance in P/D disaggregation). A more tricky case involves handling decode, e.g., when scaling instances that combine prefill and decode, or scaling a decode instance in P/D disaggregation. ... Dingyan Zhang Haotian Wang Yang Liu Haibo Chen Model autoscaling is the key mechanism to achieve serverless model-as-a-service, but it faces a fundamental trade-off between scaling speed and storage/memory usage to cache parameters, and cannot meet frequent scaling requirements across multiple hosts. The key problem is that data plane performance is slow, and scaled instances remain stopped while parameters are loading. We first show that data plane can be made fast with no/O(1) caching by loading parameters through the compute network between GPUs because: (1) its speed is comparable host cache and is underutilized; (2) scaling multiple instances requires no or O(1) caching with network-optimized multicast. Second, autoscaling can be made live by breaking the scaling abstraction from a coarse-grained instance-level to a fine-grained layer-level. This allows us to offload the layer computation from the overloaded serving instances to the scaled instance with cooperative execution, thus handles cases even when the compute network is not sufficiently fast. Our system BLITZSCALE reduces the serving tail latencies by up to 86% without caching, and we achieve comparable performance (or even better) to an optimal setup where all the parameters are cached at all the host for autoscaling.... Therefore, deciding which requests to reject becomes a core issue in overloadoriented scheduling. Our main objective is to maximize overall throughput while adhering to SLOs, a concept referred to as goodput in other research [8, 14]. Our approach differs in that only requests that fully complete their execution are counted in the measure of goodput. ...... This further disaggregation leads to problems in dynamically adjusting the number of nodes in each group, as a static parallelism setting can result in low utilization across the cluster. Recent research [14] proposes elastic sequence parallelism to dynamically scale up or down the SP group. Although possible, this adds complexity to our architecture. ...... Dataset and Workload Building upon previous research [15,8, 14], we selected or designed the datasets as outlined in Table 1. In addition to utilizing public datasets, we generated a batch of simulated data featuring predefined lengths and prefix cache ratios for our experiments. ... Ruoyu Qin Zheming Li Weiran He Xinran Xu Mooncake is the serving platform for Kimi, a leading LLM service provided by Moonshot AI. It features a KVCache-centric disaggregated architecture that separates the prefill and decoding clusters. It also leverages the underutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a disaggregated cache of KVCache. The core of Mooncake is its KVCache-centric scheduler, which balances maximizing overall effective throughput while meeting latency-related Service Level Objectives (SLOs). Unlike traditional studies that assume all requests will be processed, Mooncake faces challenges due to highly overloaded scenarios. To mitigate these, we developed a prediction-based early rejection policy. Experiments show that Mooncake excels in long-context scenarios. Compared to the baseline method, Mooncake can achieve up to a 525% increase in throughput in certain simulated scenarios while adhering to SLOs. Under real workloads, Mooncake's innovative architecture enables Kimi to handle 75% more requests.... This diversity poses significant challenges to the LLM serving system, the computational and memory requirements for different requests can vary by order of magnitude. Two concurrent works, Infinite-LLM [94] and LoongServe [95], address this challenge using similar ideas: they employ SP to segment requests of different context lengths into smaller, manageable pieces and distribute these pieces across the entire cluster for scheduling. ...... It optimizes Ring Attention by token permutation, which reduces load-imbalance among SP nodes caused by causal masking. [94, 95], SP-attention mechanism optimized for the auto-regressive decode phase of LLM serving. In the decode phase, Q length is one and KV is already distributed. ...... \u2022 LoongServe [95], on the other hand, proposes Elastic Sequence Parallelism (ESP) to dynamically adjust the degree of parallelism for an inference request with minimal overhead. ESP facilitates two optimization strategies: (1) reducing the degree of sequence parallelism after the prefill phase and maintaining a lower degree of parallelism during the decode phase, as this phase requires less computation (per auto-regressive step); (2) increasing the degree of sequence parallelism during the auto-regressive phase as the sequence length grows, which is particularly promising when the LLM is expected to generate long output sequences. ... Pai Zeng Zhenyu Ning Jieru Zhao Yizhou Shan We survey the large language model (LLM) serving area to understand the intricate dynamics between cost-efficiency and accuracy, which is magnified by the growing need for longer contextual understanding when deploying models at a massive scale. Our findings reveal that works in this space optimize along three distinct but conflicting goals: improving serving context length (C), improving serving accuracy (A), and improving serving performance (P). Drawing inspiration from the CAP theorem in databases, we propose a CAP principle for LLM serving, which suggests that any optimization can improve at most two of these three goals simultaneously. Our survey categorizes existing works within this framework. We find the definition and continuity of user-perceived measurement metrics are crucial in determining whether a goal has been met, akin to prior CAP databases in the wild. We recognize the CAP principle for LLM serving as a guiding principle, rather than a formal theorem, to inform designers of the inherent and dynamic trade-offs in serving models. As serving accuracy and performance have been extensively studied, this survey focuses on works that extend serving context length and address the resulting challenges.... State-of-the-art approaches use a KVCache-centric approach to handle memory throttling [29, 51, 39,45]. When a GPU lacks sufficient HBM for incoming requests, the system adjusts the KVCache for active requests-dropping it, swapping it out, or migrating it to an available spare GPU to make room for waiting requests (detailed in \u00a72.3). ...... As we have mentioned before,",
          "original_query": "LoongServe: Efficiently serving long-context large language models with elastic sequence parallelism",
          "cleaned_query": "LoongServe: Efficiently serving long-context large language models with elastic sequence parallelism"
        },
        {
          "success": true,
          "title": "ModServe: Scalable and Resource-Efficient Large Multimodal ...",
          "url": "https://arxiv.org/html/2502.00937v2",
          "content": "\\\\newmdenv\n\n\\[\nbackgroundcolor=mygray,\nlinecolor=black,\nlinewidth=1pt,\nroundcorner=3pt,\nnobreak=true,\n\\]keyfinding\n\n# ModServe: Scalable and Resource-Efficient Large Multimodal Model Serving\n\nHaoran Qiu\nMicrosoft Azure Research\n[haoran.qiu@microsoft.com](mailto:haoran.qiu@microsoft.com),\u00a0Anish Biswas\nMicrosoft Research India\n[t-anibiswas@microsoft.com](mailto:t-anibiswas@microsoft.com),\u00a0Zihan Zhao\nUniversity of Virginia\n[rxy6cc@virginia.edu](mailto:rxy6cc@virginia.edu),\u00a0Jayashree Mohan\nMicrosoft Research India\n[jamohan@microsoft.com](mailto:jamohan@microsoft.com),\u00a0Alind Khare\nMicrosoft M365 Research\n[alindkhare@microsoft.com](mailto:alindkhare@microsoft.com),\u00a0Esha Choukse\nMicrosoft Azure Research\n[esha.choukse@microsoft.com](mailto:esha.choukse@microsoft.com),\u00a0\u00cd\u00f1igo Goiri\nMicrosoft Azure Research\n[inigog@microsoft.com](mailto:inigog@microsoft.com),\u00a0Zeyu Zhang\nUniversity of Virginia\n[qxc4fh@virginia.edu](mailto:qxc4fh@virginia.edu),\u00a0Haiying Shen\nUniversity of Virginia\n[hs6ms@virginia.edu](mailto:hs6ms@virginia.edu),\u00a0Chetan Bansal\nMicrosoft M365 Research\n[chetanb@microsoft.com](mailto:chetanb@microsoft.com),\u00a0Ramachandran Ramjee\nMicrosoft Research India\n[ramjee@microsoft.com](mailto:ramjee@microsoft.com)\u00a0and\u00a0Rodrigo Fonseca\nMicrosoft Azure Research\n[fonseca.rodrigo@microsoft.com](mailto:fonseca.rodrigo@microsoft.com)\n\n###### Abstract.\n\nLarge multimodal models (LMMs) demonstrate impressive capabilities in understanding images, videos, and audio beyond text.\nHowever, efficiently serving LMMs in production environments poses significant challenges due to their complex architectures and heterogeneous characteristics across their multi-stage inference pipelines.\n\nWe present the first comprehensive systems analysis of two prominent LMM architectures, decoder-only and cross-attention, across six representative open-source models, revealing key systems design implications.\nWe also present an in-depth analysis of production LMM inference traces, uncovering unique workload characteristics, including variable, heavy-tailed request distributions and bursty traffic patterns.\n\nBased on these insights, we propose ModServe, a modular LMM serving system that decouples stages for independent optimization and adaptive scaling. ModServe dynamically reconfigures stages and handles bursty traffic with modality-aware scheduling and autoscaling to meet tail latency SLOs while minimizing costs.\nModServe achieves 3.3\u20135.5\u00d7\\\\times\u00d7 higher throughput (leading to 25\u201341.3% cost saving) while meeting SLOs on a 128-GPU cluster with production traces.\n\n## 1\\. Introduction\n\nThe rapid advancement in generative AI has led to the development of large multimodal models (LMMs) capable of processing inputs across various modalities such as text, image, video, and audio.\nThese models have demonstrated remarkable capabilities in tasks like image captioning\u00a0(Chen et\u00a0al., [2022](https://arxiv.org/html/2502.00937v2#bib.bib6); Mokady et\u00a0al., [2021](https://arxiv.org/html/2502.00937v2#bib.bib36); Hu et\u00a0al., [2023](https://arxiv.org/html/2502.00937v2#bib.bib18)), visual question answering\u00a0(Schwenk et\u00a0al., [2022](https://arxiv.org/html/2502.00937v2#bib.bib47); Shao et\u00a0al., [2023](https://arxiv.org/html/2502.00937v2#bib.bib48)), and multimodal dialogue systems\u00a0(Li et\u00a0al., [2024b](https://arxiv.org/html/2502.00937v2#bib.bib26); Chen et\u00a0al., [2024b](https://arxiv.org/html/2502.00937v2#bib.bib9); Team et\u00a0al., [2024](https://arxiv.org/html/2502.00937v2#bib.bib53)).\nThis has led to a rapid adoption of LMMs in production services, including online applications where latency service-level objectives (SLOs) are critical.\n\nUnlike traditional large language models (LLMs) that process purely textual inputs using a single component, a decoder-based transformer architecture\u00a0(Waswani et\u00a0al., [2017](https://arxiv.org/html/2502.00937v2#bib.bib56)), LMMs handle fundamentally different types of inputs, each requiring distinct processing approaches.\nThis heterogeneity introduces unique serving complexities that demand novel analysis and serving strategies.\nFor Image-Text-to-Text models\u00a0(HuggingFace, [2024c](https://arxiv.org/html/2502.00937v2#bib.bib22)), the inference pipeline consists of multiple specialized stages:\nimage preprocessing to transform raw images into tensor representations, image encoding to convert these tensors into image tokens, and a language model backend that combines text prompts with image tokens to generate text outputs.\nCurrently, these stages are typically served as a monolithic system\u00a0(Kwon et\u00a0al., [2023](https://arxiv.org/html/2502.00937v2#bib.bib24); Wolf et\u00a0al., [2020](https://arxiv.org/html/2502.00937v2#bib.bib57); Aminabadi et\u00a0al., [2022](https://arxiv.org/html/2502.00937v2#bib.bib5)), where all components are integrated within a single serving instance and scaled together as a unified entity.\n\nFigure 1. Impact of image workload on LMM inference TTFT for state-of-the-art implementation of Llama3.2-11B on vLLM vs. ModServe with an 8-A100 GPU server.\nThe \u201cMonolith\u201d setup deploys the full model using 8 GPUs while the \u201cDecoupled\u201d setup deploys the LLM backend on 4 GPUs and four image encoders on the other 4 GPUs.\n\nUnfortunately, existing monolithic inference serving systems fail to account for multimodality, making them unable to scale effectively while meeting time-to-first-token (TTFT) SLOs, which now include image processing and encoding times.\n[Figure1](https://arxiv.org/html/2502.00937v2#S1.F1) shows how a monolithic deployment struggles to scale as the number of images per request increases (a common scenario in multi-image or video workloads) resulting in sharp TTFT degradation.\nAs a result, image-heavy requests can result in head-of-line (HoL) blocking, reducing system responsiveness and causing overprovisioning.\n\nOur Work.\nIn this paper, we present the first comprehensive systems analysis of two leading LMM architectures:\ncross-attention ( _CroAttn_) and decoder-only ( _DecOnly_), on both open-source LMMs and novel production LMM inference traces in Azure datacenters.\nWe analyze their multi-stage inference pipelines, performance-resource tradeoffs, and production workload patterns, including variable request rates, diverse multimodal inputs, and bursty traffic.\nWe focus on Image-Text-to-Text but our insights extend to other multimodal scenarios, such as Video-Text-to-Text, where videos are processed as image frame sequences\u00a0(Li et\u00a0al., [2024c](https://arxiv.org/html/2502.00937v2#bib.bib27)), and Audio-Text-to-Text tasks\u00a0(HuggingFace, [2024a](https://arxiv.org/html/2502.00937v2#bib.bib20)), which share similar model architectures with the models we study.\n\nOur analysis identifies three key insights for optimizing LMM inference.\nFirst, different LMM inference stages exhibit diverse performance characteristics and varying sensitivity to resource and model configurations ( _e.g._, batching and model sharding), necessitating _decoupled execution_.\nSecond, image encoding is a major bottleneck for TTFT, requiring efficient _encoder parallelization_ to reduce both latency and HoL blocking.\nFinally, production multimodal traffic exhibits distinct bursty patterns driven by increased images per request, highlighting the need for _modality-aware routing_ strategies to manage bursts and mitigate tail latency spikes.\n\nBased on these insights, we propose ModServe, a novel modular architecture for scalable and resource-efficient LMM serving which directly addresses the challenges identified in our analysis.\nModServe separates image- and text-specific inference stages into distinct instances for decoupled execution.\nIn ModServe, _Image Instances_ handle image preprocessing and encoding, while _Text Instances_ manage LLM prefill and decoding ( [Figure1](https://arxiv.org/html/2502.00937v2#S1.F1)).\nText-only requests are served by _Text Instances_, whereas image-text requests go through _Image Instances_ where images are converted to tokens before being forwarded to _Text Instances_ for text generatio",
          "original_query": "ModServe: Scalable and resource-efficient large multimodal model serving",
          "cleaned_query": "ModServe: Scalable and resource-efficient large multimodal model serving"
        },
        {
          "success": true,
          "title": "Hydragen: High-Throughput LLM Inference with Shared Prefixes",
          "url": "https://arxiv.org/abs/2402.05099",
          "content": "[2402.05099] Hydragen: High-Throughput LLM Inference with Shared Prefixes\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2402.05099\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2402.05099**(cs)\n[Submitted on 7 Feb 2024 ([v1](https://arxiv.org/abs/2402.05099v1)), last revised 13 May 2024 (this version, v2)]\n# Title:Hydragen: High-Throughput LLM Inference with Shared Prefixes\nAuthors:[Jordan Juravsky](https://arxiv.org/search/cs?searchtype=author&amp;query=Juravsky,+J),[Bradley Brown](https://arxiv.org/search/cs?searchtype=author&amp;query=Brown,+B),[Ryan Ehrlich](https://arxiv.org/search/cs?searchtype=author&amp;query=Ehrlich,+R),[Daniel Y. Fu](https://arxiv.org/search/cs?searchtype=author&amp;query=Fu,+D+Y),[Christopher R\u00e9](https://arxiv.org/search/cs?searchtype=author&amp;query=R\u00e9,+C),[Azalia Mirhoseini](https://arxiv.org/search/cs?searchtype=author&amp;query=Mirhoseini,+A)\nView a PDF of the paper titled Hydragen: High-Throughput LLM Inference with Shared Prefixes, by Jordan Juravsky and 5 other authors\n[View PDF](https://arxiv.org/pdf/2402.05099)[HTML (experimental)](https://arxiv.org/html/2402.05099v2)> > Abstract:\n> Transformer-based large language models (LLMs) are now deployed to hundreds of millions of users. LLM inference is commonly performed on batches of sequences that share a prefix, such as few-shot examples or a chatbot system prompt. Decoding in this large-batch setting can be bottlenecked by the attention operation, which reads large key-value (KV) caches from memory and computes inefficient matrix-vector products for every sequence in the batch. In this work, we introduce Hydragen, a hardware-aware exact implementation of attention with shared prefixes. Hydragen computes attention over the shared prefix and unique suffixes separately. This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and enabling the use of hardware-friendly matrix multiplications. Our method can improve end-to-end CodeLlama-13b throughput by up to 32x against competitive baselines, with speedup growing with the batch size and shared prefix length. Hydragen also enables the use of very long shared contexts: with a large batch size, increasing the prefix length from 1K to 16K tokens decreases Hydragen throughput by less than 15%, while the throughput of baselines drops by over 90%. Hydragen generalizes beyond simple prefix-suffix decomposition and can be applied to tree-based prompt sharing patterns, allowing us to further reduce inference time on competitive programming problems by 55%. Subjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:2402.05099](https://arxiv.org/abs/2402.05099)[cs.LG]|\n|(or[arXiv:2402.05099v2](https://arxiv.org/abs/2402.05099v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2402.05099](https://doi.org/10.48550/arXiv.2402.05099)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jordan Juravsky [[view email](https://arxiv.org/show-email/607e24aa/2402.05099)]\n**[[v1]](https://arxiv.org/abs/2402.05099v1)**Wed, 7 Feb 2024 18:53:01 UTC (1,750 KB)\n**[v2]**Mon, 13 May 2024 08:49:44 UTC (1,141 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Hydragen: High-Throughput LLM Inference with Shared Prefixes, by Jordan Juravsky and 5 other authors\n* [View PDF](https://arxiv.org/pdf/2402.05099)\n* [HTML (experimental)](https://arxiv.org/html/2402.05099v2)\n* [TeX Source](https://arxiv.org/src/2402.05099)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2402.05099&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2402.05099&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2024-02](https://arxiv.org/list/cs.LG/2024-02)\nChange to browse by:\n[cs](https://arxiv.org/abs/2402.05099?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2402.05099)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2402.05099)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2402.05099)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info",
          "original_query": "Hydragen: High-throughput LLM inference with shared prefixes",
          "cleaned_query": "Hydragen: High-throughput LLM inference with shared prefixes"
        },
        {
          "success": true,
          "title": "Orca: A Distributed Serving System for Transformer-Based ...",
          "url": "https://www.semanticscholar.org/paper/Orca%3A-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6",
          "content": "[PDF] Orca: A Distributed Serving System for Transformer-Based Generative Models | Semantic Scholar\n[Skip to search form](#search-form)[Skip to main content](#main-content)[Skip to account menu](#account-menu)\n[Semantic ScholarSemantic Scholar's Logo](https://www.semanticscholar.org/)\nSearch 231,260,004 papers from all fields of science\nSearch\n* Corpus ID: 251734964# Orca: A Distributed Serving System for Transformer-Based Generative Models\n```\n@inproceedings{Yu2022OrcaAD,\ntitle={Orca: A Distributed Serving System for Transformer-Based Generative Models},\nauthor={Gyeong-In Yu and Joo Seong Jeong},\nbooktitle={USENIX Symposium on Operating Systems Design and Implementation},\nyear={2022},\nurl={https://api.semanticscholar.org/CorpusID:251734964}\n}\n```\n* [Gyeong-In Yu](https://www.semanticscholar.org/author/Gyeong-In-Yu/40928126),[Joo Seong Jeong](https://www.semanticscholar.org/author/Joo-Seong-Jeong/13705521)\n* Publishedin[USENIX Symposium on Operating\u2026](https://www.semanticscholar.org/venue?name=USENIX%20Symposium%20on%20Operating%20Systems%20Design%20and%20Implementation)2022\n* Computer Science\nTLDR\nThis paper proposes iteration-level scheduling, a new scheduling mechanism that schedules execution at the granularity of iteration (instead of request) where the scheduler invokes the execution engine to run only a single iteration of the model on the batch.Expand\n[View Paper](https://www.usenix.org/conference/osdi22/presentation/yu)\n[usenix.org](https://www.usenix.org/system/files/osdi22-yu.pdf)\nSave to LibrarySave\nCreate AlertAlert\nCite\nShare\n507 Citations\n[\nHighly Influential Citations\n](#citing-papers)[](https://www.semanticscholar.org/faq#influential-citations)\n73\n[\nBackground Citations\n](#citing-papers)\n100\n[\nMethods Citations\n](#citing-papers)\n81\n[\nResults Citations\n](#citing-papers)\n3\n[View All](#citing-papers)\n## Figures and Tables from this paper\n* [\n![figure 1](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/4-Figure1-1.png)\nfigure 1](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/0)\n* [\n![table 1](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/10-Table1-1.png)\ntable 1](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/1)\n* [\n![figure 2](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/5-Figure2-1.png)\nfigure 2](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/2)\n* [\n![figure 3](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/5-Figure3-1.png)\nfigure 3](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/3)\n* [\n![figure 4](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/6-Figure4-1.png)\nfigure 4](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/4)\n* [\n![figure 5](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/7-Figure5-1.png)\nfigure 5](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/5)\n* [\n![figure 6](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/8-Figure6-1.png)\nfigure 6](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/6)\n* [\n![figure 7](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/8-Figure7-1.png)\nfigure 7](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/7)\n* [\n![figure 8](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/10-Figure8-1.png)\nfigure 8](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/8)\n* [\n![figure 9](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/12-Figure9-1.png)\nfigure 9](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/9)\n* [\n![figure 10](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/12-Figure10-1.png)\nfigure 10](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/10)\n* [\n![figure 11](https://figures.semanticscholar.org/9a51dce39542d3448138f1eae2f9cf153a207a9f/13-Figure11-1.png)\nfigure 11](https://www.semanticscholar.org/paper/Orca:-A-Distributed-Serving-System-for-Generative-Yu-Jeong/9d7a75601e0e50dd68d40cfb8ef0e891dad797a6/figure/11)\n## Topics\nAI-Generated\n[Iteration-level Scheduling(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/68263405938?corpusId=251734964)[FasterTransformer(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/23316458425?corpusId=251734964)[Serving System(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/40452359079?corpusId=251734964)[BatchMaker(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/1421560912?corpusId=251734964)[TensorFlow-Serving(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/19718902336?corpusId=251734964)[LightSeq(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/29493777737?corpusId=251734964)[TurboTransformers(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/24086097315?corpusId=251734964)[Generative Pre-trained Transformer 3(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/30153973311?corpusId=251734964)[Optimal Reciprocal Collision Avoidance(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/46371741145?corpusId=251734964)[Latency(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/32350302981?corpusId=251734964)\n## 507 Citations\nCitation Type\nHas PDF\nAuthor\nMore Filters\nMore Filters\nFilters\nSort by Most Influenced PapersSort by Citation CountSort by Recency\n[### A Survey of LLM Inference Systems\n](https://www.semanticscholar.org/paper/A-Survey-of-LLM-Inference-Systems-Pan-Li/5814cca92325e0644a15e1c7e8df5ad2922b9f77)[J. Pan](https://www.semanticscholar.org/author/J.-Pan/2261352681)[Guoliang Li](https://www.semanticscholar.org/author/Guoliang-Li/2371088227)\nComputer Science\n[ArXiv](https://www.semanticscholar.org/venue?name=ArXiv)\n* 2025\nTLDR\nThis survey reviews techniques for large language model inference, starting from operators and algorithms for request processing, then moving on to techniques for model optimization and execution, including kernel design, batching, and scheduling, before ending with techniques for memory management.Expand\n* [\n4\n](https://www.semanticscholar.org/paper/5814cca92325e0644a15e1c7e8df5ad2922b9f77#citing-papers)\n* [Highly Influenced](https://www.semanticscholar.org/paper/5814cca92325e0644a15e1c7e8df5ad2922b9f77?sort=is-influential#citing-papers)\n[[PDF]](https://www.semanticscholar.org/reader/5814cca92325e0644a15e1c7e8df5ad2922b9f77)\n* 5 Excerpts\nSave\n[### Andes: Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming Services\n](https://www.semanticscholar.org/paper/Andes:-Defining-and-Enhancing-Quality-of-Experience-Liu-Wu/b397af271897328a3111ea406c63553fb963e883)[Jiachen Liu](https://www.semanticscholar.org/author/Jiachen-Liu/2257350256)[Zhiyu Wu](https://www.semanticscholar.org/author/Zhiyu-Wu/2298290649)[Jae-Won Chung](https://www.semanticscholar.org/author/",
          "original_query": "Orca: A distributed serving system for Transformer-Based generative models",
          "cleaned_query": "Orca: A distributed serving system for Transformer-Based generative models"
        }
      ],
      "generated_ideas": [
        "Tri-Phase Disaggregation: Encode/Prefill/Decode Co-Optimization for Multimodal Serving\nExtend DistServe-style P/D disaggregation to LMM pipelines (ModServe) by explicitly separating modality encoders (image/video/audio), text prefill, and decode onto distinct GPU pools. Develop a solver that jointly allocates GPUs and parallelism per phase to satisfy TTFT (including encoding) and TPOT SLOs while minimizing inter-stage communication via topology-aware placement.",
        "Prefix-Aware DistServe: Routing Shared-Prefix Requests to Hydragen-Optimized Decode Pools\nIntegrate Hydragen\u2019s shared-prefix attention into a DistServe-like architecture by detecting shared-prefix structure (system prompts, few-shot templates, tool headers) and routing those requests to specialized decode workers. The key contribution is a scheduling/routing policy that dynamically chooses between standard attention and Hydragen kernels based on predicted prefix-sharing benefit and KV transfer overhead.",
        "Elastic Sequence Parallelism for Disaggregated Decode Under SLO Constraints\nCombine LoongServe\u2019s elastic sequence parallelism (ESP) with P/D disaggregation by allowing the decode cluster to expand/contract sequence-parallel groups per request as context length and output length evolve. Implement an online controller that uses queueing delay + per-token compute models to choose ESP degree that maximizes goodput while keeping TPOT tail within SLO.",
        "KV-Cache Fabric Offload for Disaggregated Serving with Bandwidth-Aware Placement\nBuild a KV-cache tiering mechanism that leverages underutilized GPU-GPU/CPU fabrics (as highlighted in LoongServe/Mooncake-related text) to place KV blocks across HBM, host DRAM, and remote GPU memory while maintaining DistServe\u2019s decoupling. Contribute a placement algorithm that minimizes expected attention fetch latency given network topology and predicts when remote-KV is cheaper than local eviction.",
        "Iteration-Level Scheduling Across Stages: A Unified Orca+ModServe Execution Engine\nGeneralize Orca\u2019s iteration-level scheduling to multi-stage LMM pipelines by scheduling at the granularity of \u201cstage-iteration\u201d (e.g., one encoder micro-batch step, one prefill micro-step, one decode token step). The core contribution is a global batch-maker that coordinates micro-batches across stages to reduce head-of-line blocking from image-heavy requests while preserving per-request TTFT/TPOT targets.",
        "SLO-Aware Early Rejection with Phase-Specific Completion Guarantees\nDesign an early rejection policy (inspired by Mooncake\u2019s prediction-based rejection) that reasons separately about (i) probability of meeting TTFT after prefill/encoding congestion and (ii) probability of meeting TPOT during decode. The contribution is a calibrated predictor that outputs phase-specific feasibility and a policy that only admits requests with high probability of full completion within both SLOs, improving true \u201cgoodput\u201d under overload.",
        "Multimodal Token Permutation to Balance Sequence Parallelism Under Causal Constraints\nExtend LoongServe-style token permutation (used to reduce imbalance in ring attention) to multimodal token streams where image/video tokens precede text tokens and may have different attention patterns. Propose a permutation/packing scheme that preserves correctness but reduces SP-node load skew during both prefill and decode, especially for mixed-resolution or multi-image prompts.",
        "Shared-Prefix Trees at Scale: Distributed Prompt-Graph Execution with Exact Attention\nBuild a serving primitive that groups requests into a \u201cprompt DAG\u201d (shared system prompt \u2192 shared tool instructions \u2192 per-user suffix) and executes it with Hydragen\u2019s generalization to tree-based sharing. The key contribution is a distributed algorithm that maintains exactness while partitioning the prompt DAG across GPUs and minimizing KV duplication/transfer under DistServe-like disaggregation.",
        "Goodput-Optimal Joint Batching Across Heterogeneous Requests (Long-Context + Multimodal + Shared Prefix)\nCreate a batching policy that simultaneously accounts for (a) long-context segmentation (LoongServe), (b) modality-dependent preprocessing/encoding (ModServe), and (c) shared-prefix acceleration (Hydragen), all under DistServe\u2019s separated phases. The contribution is an online optimizer that forms batches by predicted marginal goodput gain (requests completed within SLO) rather than tokens/sec, with ablations showing robustness to heavy-tailed and bursty workloads."
      ],
      "generated_ideas_raw": "1. **Tri-Phase Disaggregation: Encode/Prefill/Decode Co-Optimization for Multimodal Serving**  \n   Extend DistServe-style P/D disaggregation to LMM pipelines (ModServe) by explicitly separating modality encoders (image/video/audio), text prefill, and decode onto distinct GPU pools. Develop a solver that jointly allocates GPUs and parallelism per phase to satisfy TTFT (including encoding) and TPOT SLOs while minimizing inter-stage communication via topology-aware placement.\n\n2. **Prefix-Aware DistServe: Routing Shared-Prefix Requests to Hydragen-Optimized Decode Pools**  \n   Integrate Hydragen\u2019s shared-prefix attention into a DistServe-like architecture by detecting shared-prefix structure (system prompts, few-shot templates, tool headers) and routing those requests to specialized decode workers. The key contribution is a scheduling/routing policy that dynamically chooses between standard attention and Hydragen kernels based on predicted prefix-sharing benefit and KV transfer overhead.\n\n3. **Elastic Sequence Parallelism for Disaggregated Decode Under SLO Constraints**  \n   Combine LoongServe\u2019s elastic sequence parallelism (ESP) with P/D disaggregation by allowing the decode cluster to expand/contract sequence-parallel groups per request as context length and output length evolve. Implement an online controller that uses queueing delay + per-token compute models to choose ESP degree that maximizes goodput while keeping TPOT tail within SLO.\n\n4. **KV-Cache Fabric Offload for Disaggregated Serving with Bandwidth-Aware Placement**  \n   Build a KV-cache tiering mechanism that leverages underutilized GPU-GPU/CPU fabrics (as highlighted in LoongServe/Mooncake-related text) to place KV blocks across HBM, host DRAM, and remote GPU memory while maintaining DistServe\u2019s decoupling. Contribute a placement algorithm that minimizes expected attention fetch latency given network topology and predicts when remote-KV is cheaper than local eviction.\n\n5. **Iteration-Level Scheduling Across Stages: A Unified Orca+ModServe Execution Engine**  \n   Generalize Orca\u2019s iteration-level scheduling to multi-stage LMM pipelines by scheduling at the granularity of \u201cstage-iteration\u201d (e.g., one encoder micro-batch step, one prefill micro-step, one decode token step). The core contribution is a global batch-maker that coordinates micro-batches across stages to reduce head-of-line blocking from image-heavy requests while preserving per-request TTFT/TPOT targets.\n\n6. **SLO-Aware Early Rejection with Phase-Specific Completion Guarantees**  \n   Design an early rejection policy (inspired by Mooncake\u2019s prediction-based rejection) that reasons separately about (i) probability of meeting TTFT after prefill/encoding congestion and (ii) probability of meeting TPOT during decode. The contribution is a calibrated predictor that outputs phase-specific feasibility and a policy that only admits requests with high probability of full completion within both SLOs, improving true \u201cgoodput\u201d under overload.\n\n7. **Multimodal Token Permutation to Balance Sequence Parallelism Under Causal Constraints**  \n   Extend LoongServe-style token permutation (used to reduce imbalance in ring attention) to multimodal token streams where image/video tokens precede text tokens and may have different attention patterns. Propose a permutation/packing scheme that preserves correctness but reduces SP-node load skew during both prefill and decode, especially for mixed-resolution or multi-image prompts.\n\n8. **Shared-Prefix Trees at Scale: Distributed Prompt-Graph Execution with Exact Attention**  \n   Build a serving primitive that groups requests into a \u201cprompt DAG\u201d (shared system prompt \u2192 shared tool instructions \u2192 per-user suffix) and executes it with Hydragen\u2019s generalization to tree-based sharing. The key contribution is a distributed algorithm that maintains exactness while partitioning the prompt DAG across GPUs and minimizing KV duplication/transfer under DistServe-like disaggregation.\n\n9. **Topology-Aware Autoscaling via Layer/Stage Micro-Offloading for Live Bursts**  \n   Combine ModServe\u2019s stage decoupling with the layer-level cooperative execution idea (layer micro-offload) described in the LoongServe excerpt to handle bursty production traces without parameter preloading. The contribution is a live autoscaler that decides when to spin up \u201chelper\u201d GPUs and which layers/stages to offload over NVLink/RDMA to cut tail latency while keeping caching near O(1).\n\n10. **Goodput-Optimal Joint Batching Across Heterogeneous Requests (Long-Context + Multimodal + Shared Prefix)**  \n   Create a batching policy that simultaneously accounts for (a) long-context segmentation (LoongServe), (b) modality-dependent preprocessing/encoding (ModServe), and (c) shared-prefix acceleration (Hydragen), all under DistServe\u2019s separated phases. The contribution is an online optimizer that forms batches by predicted marginal goodput gain (requests completed within SLO) rather than tokens/sec, with ablations showing robustness to heavy-tailed and bursty workloads.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Tri-Phase Disaggregation: Encode/Prefill/Decode Co-Optimization for Multimodal Serving\nExtend DistServe-style P/D disaggregation to LMM pipelines (ModServe) by explicitly separating modality encoders ",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Prefix-Aware DistServe: Routing Shared-Prefix Requests to Hydragen-Optimized Decode Pools\nIntegrate Hydragen\u2019s shared-prefix attention into a DistServe-like architecture by detecting shared-prefix str",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Elastic Sequence Parallelism for Disaggregated Decode Under SLO Constraints\nCombine LoongServe\u2019s elastic sequence parallelism (ESP) with P/D disaggregation by allowing the decode cluster to expand/con",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "KV-Cache Fabric Offload for Disaggregated Serving with Bandwidth-Aware Placement\nBuild a KV-cache tiering mechanism that leverages underutilized GPU-GPU/CPU fabrics (as highlighted in LoongServe/Moonc",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Iteration-Level Scheduling Across Stages: A Unified Orca+ModServe Execution Engine\nGeneralize Orca\u2019s iteration-level scheduling to multi-stage LMM pipelines by scheduling at the granularity of \u201cstage-",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "SLO-Aware Early Rejection with Phase-Specific Completion Guarantees\nDesign an early rejection policy (inspired by Mooncake\u2019s prediction-based rejection) that reasons separately about (i) probability o",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Multimodal Token Permutation to Balance Sequence Parallelism Under Causal Constraints\nExtend LoongServe-style token permutation (used to reduce imbalance in ring attention) to multimodal token streams",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Shared-Prefix Trees at Scale: Distributed Prompt-Graph Execution with Exact Attention\nBuild a serving primitive that groups requests into a \u201cprompt DAG\u201d (shared system prompt \u2192 shared tool instruction",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Goodput-Optimal Joint Batching Across Heterogeneous Requests (Long-Context + Multimodal + Shared Prefix)\nCreate a batching policy that simultaneously accounts for (a) long-context segmentation (LoongS",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 48,
      "paper_title": "Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks",
      "contribution": "The paper introduces a dynamical low-rank training scheme with a novel spectral regularizer that enhances adversarial robustness while achieving significant compression in neural networks.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 3,
      "input_tokens": 11567,
      "output_tokens": 924,
      "predecessor_details": [
        {
          "success": true,
          "title": "finding efficient low-rank neural networks via matrix differential ...",
          "url": "https://arxiv.org/abs/2205.13571",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2205.13571** (cs)\n\n\\[Submitted on 26 May 2022 ( [v1](https://arxiv.org/abs/2205.13571v1)), last revised 18 Oct 2022 (this version, v2)\\]\n\n# Title:Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations\n\nAuthors: [Steffen Schotth\u00f6fer](https://arxiv.org/search/cs?searchtype=author&query=Schotth%C3%B6fer,+S), [Emanuele Zangrando](https://arxiv.org/search/cs?searchtype=author&query=Zangrando,+E), [Jonas Kusch](https://arxiv.org/search/cs?searchtype=author&query=Kusch,+J), [Gianluca Ceruti](https://arxiv.org/search/cs?searchtype=author&query=Ceruti,+G), [Francesco Tudisco](https://arxiv.org/search/cs?searchtype=author&query=Tudisco,+F)\n\nView a PDF of the paper titled Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations, by Steffen Schotth\\\\\"ofer and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2205.13571)\n\n> Abstract:Neural networks have achieved tremendous success in a large variety of applications. However, their memory footprint and computational demand can render them impractical in application settings with limited hardware or energy resources. In this work, we propose a novel algorithm to find efficient low-rank subnetworks. Remarkably, these subnetworks are determined and adapted already during the training phase and the overall time and memory resources required by both training and evaluating them are significantly reduced. The main idea is to restrict the weight matrices to a low-rank manifold and to update the low-rank factors rather than the full matrix during training. To derive training updates that are restricted to the prescribed manifold, we employ techniques from dynamic model order reduction for matrix differential equations. This allows us to provide approximation, stability, and descent guarantees. Moreover, our method automatically and dynamically adapts the ranks during training to achieve the desired approximation accuracy. The efficiency of the proposed method is demonstrated through a variety of numerical experiments on fully-connected and convolutional networks.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Numerical Analysis (math.NA); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2205.13571](https://arxiv.org/abs/2205.13571) \\[cs.LG\\] |\n| | (or [arXiv:2205.13571v2](https://arxiv.org/abs/2205.13571v2) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2205.13571](https://doi.org/10.48550/arXiv.2205.13571) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | Proceedings NeurIPS 2022 |\n\n## Submission history\n\nFrom: Francesco Tudisco \\[ [view email](https://arxiv.org/show-email/3954d9c7/2205.13571)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2205.13571v1)**\nThu, 26 May 2022 18:18:12 UTC (3,764 KB)\n\n**\\[v2\\]**\nTue, 18 Oct 2022 12:22:36 UTC (2,118 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations, by Steffen Schotth\\\\\"ofer and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2205.13571)\n- [TeX Source](https://arxiv.org/src/2205.13571)\n- [Other Formats](https://arxiv.org/format/2205.13571)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2205.13571&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2205.13571&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2022-05](https://arxiv.org/list/cs.LG/2022-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2205.13571?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2205.13571?context=cs.AI)\n\n[cs.NA](https://arxiv.org/abs/2205.13571?context=cs.NA)\n\n[math](https://arxiv.org/abs/2205.13571?context=math)\n\n[math.NA](https://arxiv.org/abs/2205.13571?context=math.NA)\n\n[stat](https://arxiv.org/abs/2205.13571?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2205.13571?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2205.13571)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2205.13571)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2205.13571)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2205.13571&description=Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2205.13571&title=Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2205.13571) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations",
          "cleaned_query": "Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations"
        },
        {
          "success": true,
          "title": "Theoretically Principled Trade-off between Robustness and Accuracy",
          "url": "https://arxiv.org/abs/1901.08573",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1901.08573** (cs)\n\n\\[Submitted on 24 Jan 2019 ( [v1](https://arxiv.org/abs/1901.08573v1)), last revised 24 Jun 2019 (this version, v3)\\]\n\n# Title:Theoretically Principled Trade-off between Robustness and Accuracy\n\nAuthors: [Hongyang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+H), [Yaodong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+Y), [Jiantao Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao,+J), [Eric P. Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing,+E+P), [Laurent El Ghaoui](https://arxiv.org/search/cs?searchtype=author&query=Ghaoui,+L+E), [Michael I. Jordan](https://arxiv.org/search/cs?searchtype=author&query=Jordan,+M+I)\n\nView a PDF of the paper titled Theoretically Principled Trade-off between Robustness and Accuracy, by Hongyang Zhang and Yaodong Yu and Jiantao Jiao and Eric P. Xing and Laurent El Ghaoui and Michael I. Jordan\n\n[View PDF](https://arxiv.org/pdf/1901.08573)\n\n> Abstract:We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of ~2,000 submissions, surpassing the runner-up approach by $11.41\\\\%$ in terms of mean $\\\\ell\\_2$ perturbation distance.\n\n| | |\n| --- | --- |\n| Comments: | Appeared in ICML 2019; the winning methodology of the NeurIPS 2018 Adversarial Vision Challenge |\n| Subjects: | Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1901.08573](https://arxiv.org/abs/1901.08573) \\[cs.LG\\] |\n| | (or [arXiv:1901.08573v3](https://arxiv.org/abs/1901.08573v3) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1901.08573](https://doi.org/10.48550/arXiv.1901.08573) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Hongyang Zhang \\[ [view email](https://arxiv.org/show-email/9640e8c4/1901.08573)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1901.08573v1)**\nThu, 24 Jan 2019 18:43:57 UTC (1,310 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1901.08573v2)**\nThu, 23 May 2019 22:04:23 UTC (1,312 KB)\n\n**\\[v3\\]**\nMon, 24 Jun 2019 07:04:11 UTC (1,313 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Theoretically Principled Trade-off between Robustness and Accuracy, by Hongyang Zhang and Yaodong Yu and Jiantao Jiao and Eric P. Xing and Laurent El Ghaoui and Michael I. Jordan\n\n- [View PDF](https://arxiv.org/pdf/1901.08573)\n- [TeX Source](https://arxiv.org/src/1901.08573)\n- [Other Formats](https://arxiv.org/format/1901.08573)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1901.08573&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1901.08573&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2019-01](https://arxiv.org/list/cs.LG/2019-01)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1901.08573?context=cs)\n\n[stat](https://arxiv.org/abs/1901.08573?context=stat)\n\n[stat.ML](https://arxiv.org/abs/1901.08573?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1901.08573)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1901.08573)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1901.08573)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1901.html#abs-1901-08573) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1901-08573)\n\n[Hongyang Zhang](https://dblp.uni-trier.de/search/author?author=Hongyang%20Zhang)\n\n[Yaodong Yu](https://dblp.uni-trier.de/search/author?author=Yaodong%20Yu)\n\n[Jiantao Jiao](https://dblp.uni-trier.de/search/author?author=Jiantao%20Jiao)\n\n[Eric P. Xing](https://dblp.uni-trier.de/search/author?author=Eric%20P.%20Xing)\n\n[Laurent El Ghaoui](https://dblp.uni-trier.de/search/author?author=Laurent%20El%20Ghaoui)\n\n\u2026\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1901.08573&description=Theoretically Principled Trade-off between Robustness and Accuracy) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1901.08573&title=Theoretically Principled Trade-off between Robustness and Accuracy)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of th",
          "original_query": "Theoretically principled trade-off between robustness and accuracy",
          "cleaned_query": "Theoretically principled trade-off between robustness and accuracy"
        },
        {
          "success": true,
          "title": "A Defense Framework for Remote Sensing Image Scene Classification",
          "url": "https://ieeexplore.ieee.org/document/9442932/",
          "content": "Perturbation-Seeking Generative Adversarial Networks: A Defense Framework for Remote Sensing Image Scene Classification | IEEE Journals &amp; Magazine | IEEE Xplore\n[**]()\n[](https://ieeexplore.ieee.org/rest/api/hpdata)\n### IEEE Account\n* [Change Username/Password]()\n* [Update Address]()\n### Purchase Details\n* [Payment Options]()\n* [Order History]()\n* [View Purchased Documents](https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp)\n### Profile Information\n* [Communications Preferences]()\n* [Profession and Education]()\n* [Technical Interests]()\n### Need Help?\n* **US &amp; Canada:**+1 800 678 4333\n* **Worldwide:**+1 732 981 0060\n* [Contact &amp; Support](https://ieeexplore.ieee.org/xpl/contact)\n* [About IEEE*Xplore*](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-ieee-xplore)\n* [Contact Us](https://ieeexplore.ieee.org/xpl/contact)\n* [Help](https://ieeexplore.ieee.org/Xplorehelp)\n* [Accessibility](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/accessibility-statement)\n* [Terms of Use](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/terms-of-use)\n* [Nondiscrimination Policy](http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html)\n* [Sitemap](https://ieeexplore.ieee.org/xpl/sitemap.jsp)\n* [Privacy &amp; Opting Out of Cookies](http://www.ieee.org/about/help/security_privacy.html)\nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.\n&copy; Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.\n**",
          "original_query": "Perturbation-seeking generative adversarial networks: A defense framework for remote sensing image scene classification",
          "cleaned_query": "Perturbation-seeking generative adversarial networks: A defense framework for remote sensing image scene classification"
        },
        {
          "success": true,
          "title": "[1412.6572] Explaining and Harnessing Adversarial Examples - arXiv",
          "url": "https://arxiv.org/abs/1412.6572",
          "content": "[1412.6572] Explaining and Harnessing Adversarial Examples\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:1412.6572\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:1412.6572**(stat)\n[Submitted on 20 Dec 2014 ([v1](https://arxiv.org/abs/1412.6572v1)), last revised 20 Mar 2015 (this version, v3)]\n# Title:Explaining and Harnessing Adversarial Examples\nAuthors:[Ian J. Goodfellow](https://arxiv.org/search/stat?searchtype=author&amp;query=Goodfellow,+I+J),[Jonathon Shlens](https://arxiv.org/search/stat?searchtype=author&amp;query=Shlens,+J),[Christian Szegedy](https://arxiv.org/search/stat?searchtype=author&amp;query=Szegedy,+C)\nView a PDF of the paper titled Explaining and Harnessing Adversarial Examples, by Ian J. Goodfellow and 2 other authors\n[View PDF](https://arxiv.org/pdf/1412.6572)> > Abstract:\n> Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks&#39; vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset. Subjects:|Machine Learning (stat.ML); Machine Learning (cs.LG)|\nCite as:|[arXiv:1412.6572](https://arxiv.org/abs/1412.6572)[stat.ML]|\n|(or[arXiv:1412.6572v3](https://arxiv.org/abs/1412.6572v3)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.1412.6572](https://doi.org/10.48550/arXiv.1412.6572)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Ian Goodfellow [[view email](https://arxiv.org/show-email/ffe3b723/1412.6572)]\n**[[v1]](https://arxiv.org/abs/1412.6572v1)**Sat, 20 Dec 2014 01:17:12 UTC (389 KB)\n**[[v2]](https://arxiv.org/abs/1412.6572v2)**Wed, 25 Feb 2015 17:25:05 UTC (688 KB)\n**[v3]**Fri, 20 Mar 2015 20:19:16 UTC (688 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Explaining and Harnessing Adversarial Examples, by Ian J. Goodfellow and 2 other authors\n* [View PDF](https://arxiv.org/pdf/1412.6572)\n* [TeX Source](https://arxiv.org/src/1412.6572)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1412.6572&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1412.6572&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2014-12](https://arxiv.org/list/stat.ML/2014-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/1412.6572?context=cs)\n[cs.LG](https://arxiv.org/abs/1412.6572?context=cs.LG)\n[stat](https://arxiv.org/abs/1412.6572?context=stat)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1412.6572)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1412.6572)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1412.6572)\n### [21 blog links](https://arxiv.org/tb/1412.6572)\n([what is this?](https://info.arxiv.org/help/trackback.html))\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1412.6572)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Explaining and harnessing adversarial examples",
          "cleaned_query": "Explaining and harnessing adversarial examples"
        },
        {
          "success": true,
          "title": "Theoretically Principled Trade-off between Robustness and Accuracy",
          "url": "https://arxiv.org/abs/1901.08573",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1901.08573** (cs)\n\n\\[Submitted on 24 Jan 2019 ( [v1](https://arxiv.org/abs/1901.08573v1)), last revised 24 Jun 2019 (this version, v3)\\]\n\n# Title:Theoretically Principled Trade-off between Robustness and Accuracy\n\nAuthors: [Hongyang Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+H), [Yaodong Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+Y), [Jiantao Jiao](https://arxiv.org/search/cs?searchtype=author&query=Jiao,+J), [Eric P. Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing,+E+P), [Laurent El Ghaoui](https://arxiv.org/search/cs?searchtype=author&query=Ghaoui,+L+E), [Michael I. Jordan](https://arxiv.org/search/cs?searchtype=author&query=Jordan,+M+I)\n\nView a PDF of the paper titled Theoretically Principled Trade-off between Robustness and Accuracy, by Hongyang Zhang and Yaodong Yu and Jiantao Jiao and Eric P. Xing and Laurent El Ghaoui and Michael I. Jordan\n\n[View PDF](https://arxiv.org/pdf/1901.08573)\n\n> Abstract:We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of ~2,000 submissions, surpassing the runner-up approach by $11.41\\\\%$ in terms of mean $\\\\ell\\_2$ perturbation distance.\n\n| | |\n| --- | --- |\n| Comments: | Appeared in ICML 2019; the winning methodology of the NeurIPS 2018 Adversarial Vision Challenge |\n| Subjects: | Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1901.08573](https://arxiv.org/abs/1901.08573) \\[cs.LG\\] |\n| | (or [arXiv:1901.08573v3](https://arxiv.org/abs/1901.08573v3) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1901.08573](https://doi.org/10.48550/arXiv.1901.08573) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Hongyang Zhang \\[ [view email](https://arxiv.org/show-email/9640e8c4/1901.08573)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1901.08573v1)**\nThu, 24 Jan 2019 18:43:57 UTC (1,310 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1901.08573v2)**\nThu, 23 May 2019 22:04:23 UTC (1,312 KB)\n\n**\\[v3\\]**\nMon, 24 Jun 2019 07:04:11 UTC (1,313 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Theoretically Principled Trade-off between Robustness and Accuracy, by Hongyang Zhang and Yaodong Yu and Jiantao Jiao and Eric P. Xing and Laurent El Ghaoui and Michael I. Jordan\n\n- [View PDF](https://arxiv.org/pdf/1901.08573)\n- [TeX Source](https://arxiv.org/src/1901.08573)\n- [Other Formats](https://arxiv.org/format/1901.08573)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1901.08573&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1901.08573&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2019-01](https://arxiv.org/list/cs.LG/2019-01)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1901.08573?context=cs)\n\n[stat](https://arxiv.org/abs/1901.08573?context=stat)\n\n[stat.ML](https://arxiv.org/abs/1901.08573?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1901.08573)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1901.08573)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1901.08573)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1901.html#abs-1901-08573) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1901-08573)\n\n[Hongyang Zhang](https://dblp.uni-trier.de/search/author?author=Hongyang%20Zhang)\n\n[Yaodong Yu](https://dblp.uni-trier.de/search/author?author=Yaodong%20Yu)\n\n[Jiantao Jiao](https://dblp.uni-trier.de/search/author?author=Jiantao%20Jiao)\n\n[Eric P. Xing](https://dblp.uni-trier.de/search/author?author=Eric%20P.%20Xing)\n\n[Laurent El Ghaoui](https://dblp.uni-trier.de/search/author?author=Laurent%20El%20Ghaoui)\n\n\u2026\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1901.08573&description=Theoretically Principled Trade-off between Robustness and Accuracy) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1901.08573&title=Theoretically Principled Trade-off between Robustness and Accuracy)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of th",
          "original_query": "Theoretically principled trade-off between robustness and accuracy",
          "cleaned_query": "Theoretically principled trade-off between robustness and accuracy"
        },
        {
          "success": true,
          "title": "Defense to Adversarial Perturbations with GAN - arXiv",
          "url": "https://arxiv.org/abs/1705.03387",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: We propose a novel technique to make neural network robust to adversarial\nexamples using a generative adversarial network. We alternately train both\nclassifier and generator networks. The generator network generates an\nadversarial perturbation that can easily fool the classifier network by using a\ngradient of each image. Simultaneously, the classifier network is trained to\nclassify correctly both original and adversarial images generated by the\ngenerator. These procedures help the classifier network to become more robust\nto adversarial perturbations. Furthermore, our adversarial training framework\nefficiently reduces overfitting and outperforms other regularization methods\nsuch as Dropout. We applied our method to supervised learning for CIFAR\ndatasets, and experimantal results show that our method significantly lowers\nthe generalization error of the network. To the best of our knowledge, this is\nthe first method which uses GAN to improve supervised learning.\n \n \n \n \n Submission history From: Hyeungill Lee [ view email]\n \n [v1] \n Tue, 9 May 2017 15:30:58 UTC (405 KB) [v2] \nFri, 26 May 2017 21:44:32 UTC (405 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > cs > arXiv:1705.03387\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Machine Learning\n\n arXiv:1705.03387 (cs)\n [Submitted on 9 May 2017 (v1), last revised 26 May 2017 (this version, v2)]\n\n Title: Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN\n\n Authors: Hyeungill Lee, Sungyeob Han, Jungwoo Lee\n Download PDF\n Abstract: We propose a novel technique to make neural network robust to adversarial examples using a generative adversarial network. We alternately train both classifier and generator networks. The generator network generates an adversarial perturbation that can easily fool the classifier network by using a gradient of each image. Simultaneously, the classifier network is trained to classify correctly both original and adversarial images generated by the generator. These procedures help the classifier network to become more robust to adversarial perturbations. Furthermore, our adversarial training framework efficiently reduces overfitting and outperforms other regularization methods such as Dropout. We applied our method to supervised learning for CIFAR datasets, and experimantal results show that our method significantly lowers the generalization error of the network. To the best of our knowledge, this is the first method which uses GAN to improve supervised learning.\n Subjects: Machine Learning (cs.LG) ; Machine Learning (stat.ML)\n Cite as: arXiv:1705.03387 [cs.LG] \n (or arXiv:1705.03387v2 [cs.LG] for this version) \n https://doi.org/10.48550/arXiv.1705.03387 \n Focus to learn more \n arXiv-issued DOI via DataCite \n \n\n Submission history\n\n From: Hyeungill Lee [view email]\n [v1] Tue, 9 May 2017 15:30:58 UTC (405 KB)\n [v2] Fri, 26 May 2017 21:44:32 UTC (405 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * Other formats\n (license)\n Current browse context:\n cs.LG\n < prev | next >\n new | recent | 1705\n Change to browse by:\n cs\n stat\n stat.ML\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n\n DBLP - CS Bibliography\n\n listing | bibtex\n Hyeungill Lee\n Sungyeob Han\n Jungwoo Lee\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n IArxiv recommender toggle\n IArxiv Recommender (What is IArxiv?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs and how to get involved.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Generative adversarial trainer: Defense to adversarial perturbations with GAN",
          "cleaned_query": "Generative adversarial trainer: Defense to adversarial perturbations with GAN"
        }
      ],
      "generated_ideas": [
        "TRADES on a Low-Rank Manifold (LR-TRADES)\n- Combine TRADES\u2019 boundary-error regularization with low-rank manifold-constrained training (updating low-rank factors instead of full weights). Study how the robustness\u2013accuracy trade-off shifts as a function of per-layer rank and TRADES coefficient, and derive conditions under which rank adaptation reduces boundary error without sacrificing natural accuracy.",
        "Adversarially-Driven Rank Adaptation\n- Modify the dynamic rank-adaptation criterion in low-rank lottery tickets to explicitly monitor *robust* loss (e.g., TRADES robust surrogate) and allocate rank where adversarial gradients concentrate. Implement a controller that increases rank in layers with high adversarial Jacobian energy and decreases rank elsewhere, then evaluate robustness/compute Pareto frontiers.",
        "Low-Rank Generative Adversarial Trainer for Cost-Aware Robustness\n- Build a GAN-based adversarial training framework where both classifier and perturbation generator are parameterized with adaptive low-rank factors to cut training cost. Benchmark whether low-rank generators can approximate strong attacks (e.g., multi-step PGD-like behavior) while maintaining end-to-end robustness comparable to full-rank adversarial training.",
        "Provable Links Between Rank, Linearity, and FGSM Vulnerability\n- Using Goodfellow et al.\u2019s \u201clinearity\u201d explanation, develop a theory connecting effective weight matrix rank (and singular spectrum) to first-order attack success (FGSM/linearized attacks). Produce actionable bounds showing how constraining singular values via low-rank factorization changes expected adversarial margin, then validate empirically across MLPs/CNNs.",
        "Layerwise Robustness Budgets via Rank\u2013Singular Value Scheduling\n- Introduce a training scheme that jointly schedules (i) TRADES penalty per layer and (ii) target rank/singular-value decay per layer to meet a global robustness constraint under compute limits. Optimize this as a constrained problem (robust error \u2264 \u03c4, FLOPs \u2264 B) and output deployable per-layer rank configurations.",
        "Robust Low-Rank Tickets for Remote Sensing Scene Classification\n- Extend low-rank lottery tickets and TRADES-style training to remote sensing scene classifiers (where textures and scale changes matter), evaluating robustness to both adversarial perturbations and domain shifts (sensor noise, compression, seasonal changes). Contribute a reproducible benchmark and show whether dynamically adapted ranks stabilize performance under these realistic perturbations.",
        "Perturbation-Seeking GANs with Manifold-Constrained Perturbations\n- Train a perturbation generator that is explicitly constrained to a low-dimensional perturbation subspace (e.g., low-rank or low-frequency factorization), making it produce \u201chard\u201d but structured adversaries. Compare against unconstrained GAN perturbations to test whether concentrating attack capacity improves robust generalization and reduces overfitting noted in GAN-based training.",
        "Low-Rank Robustness Distillation: Full-Rank Teacher \u2192 Adaptive-Rank Student\n- Train a robust full-rank model (e.g., TRADES) as a teacher, then distill into an adaptive-rank student that learns low-rank factors while matching teacher logits on clean and adversarial examples. Quantify how much rank is needed to preserve robustness and propose a practical pipeline for deploying robust models on constrained hardware.",
        "Robustness-Aware Early Stopping via Matrix Differential Equation Diagnostics\n- Use the matrix differential equation perspective from low-rank manifold training to design diagnostics (e.g., stability/energy decay of factor updates) that predict impending overfitting to adversarial training. Implement early-stopping criteria based on the evolution of factor subspaces and correlate them with TRADES boundary error on a validation adversarial set.",
        "Attack-Conditional Rank Routing (Adaptive Computation for Robust Inference)\n- Create a model that can switch ranks at inference time: low rank for clean inputs, higher rank when an input is suspected adversarial (using simple detectors like gradient norm or consistency under small noise). This yields an actionable \u201crobustness on demand\u201d system that trades extra compute only when needed, and can be evaluated against adaptive adversaries that try to evade the rank trigger."
      ],
      "generated_ideas_raw": "1. **TRADES on a Low-Rank Manifold (LR-TRADES)**\n   - Combine TRADES\u2019 boundary-error regularization with low-rank manifold-constrained training (updating low-rank factors instead of full weights). Study how the robustness\u2013accuracy trade-off shifts as a function of per-layer rank and TRADES coefficient, and derive conditions under which rank adaptation reduces boundary error without sacrificing natural accuracy.\n\n2. **Adversarially-Driven Rank Adaptation**\n   - Modify the dynamic rank-adaptation criterion in low-rank lottery tickets to explicitly monitor *robust* loss (e.g., TRADES robust surrogate) and allocate rank where adversarial gradients concentrate. Implement a controller that increases rank in layers with high adversarial Jacobian energy and decreases rank elsewhere, then evaluate robustness/compute Pareto frontiers.\n\n3. **Low-Rank Generative Adversarial Trainer for Cost-Aware Robustness**\n   - Build a GAN-based adversarial training framework where both classifier and perturbation generator are parameterized with adaptive low-rank factors to cut training cost. Benchmark whether low-rank generators can approximate strong attacks (e.g., multi-step PGD-like behavior) while maintaining end-to-end robustness comparable to full-rank adversarial training.\n\n4. **Provable Links Between Rank, Linearity, and FGSM Vulnerability**\n   - Using Goodfellow et al.\u2019s \u201clinearity\u201d explanation, develop a theory connecting effective weight matrix rank (and singular spectrum) to first-order attack success (FGSM/linearized attacks). Produce actionable bounds showing how constraining singular values via low-rank factorization changes expected adversarial margin, then validate empirically across MLPs/CNNs.\n\n5. **Layerwise Robustness Budgets via Rank\u2013Singular Value Scheduling**\n   - Introduce a training scheme that jointly schedules (i) TRADES penalty per layer and (ii) target rank/singular-value decay per layer to meet a global robustness constraint under compute limits. Optimize this as a constrained problem (robust error \u2264 \u03c4, FLOPs \u2264 B) and output deployable per-layer rank configurations.\n\n6. **Robust Low-Rank Tickets for Remote Sensing Scene Classification**\n   - Extend low-rank lottery tickets and TRADES-style training to remote sensing scene classifiers (where textures and scale changes matter), evaluating robustness to both adversarial perturbations and domain shifts (sensor noise, compression, seasonal changes). Contribute a reproducible benchmark and show whether dynamically adapted ranks stabilize performance under these realistic perturbations.\n\n7. **Perturbation-Seeking GANs with Manifold-Constrained Perturbations**\n   - Train a perturbation generator that is explicitly constrained to a low-dimensional perturbation subspace (e.g., low-rank or low-frequency factorization), making it produce \u201chard\u201d but structured adversaries. Compare against unconstrained GAN perturbations to test whether concentrating attack capacity improves robust generalization and reduces overfitting noted in GAN-based training.\n\n8. **Low-Rank Robustness Distillation: Full-Rank Teacher \u2192 Adaptive-Rank Student**\n   - Train a robust full-rank model (e.g., TRADES) as a teacher, then distill into an adaptive-rank student that learns low-rank factors while matching teacher logits on clean and adversarial examples. Quantify how much rank is needed to preserve robustness and propose a practical pipeline for deploying robust models on constrained hardware.\n\n9. **Robustness-Aware Early Stopping via Matrix Differential Equation Diagnostics**\n   - Use the matrix differential equation perspective from low-rank manifold training to design diagnostics (e.g., stability/energy decay of factor updates) that predict impending overfitting to adversarial training. Implement early-stopping criteria based on the evolution of factor subspaces and correlate them with TRADES boundary error on a validation adversarial set.\n\n10. **Attack-Conditional Rank Routing (Adaptive Computation for Robust Inference)**\n   - Create a model that can switch ranks at inference time: low rank for clean inputs, higher rank when an input is suspected adversarial (using simple detectors like gradient norm or consistency under small noise). This yields an actionable \u201crobustness on demand\u201d system that trades extra compute only when needed, and can be evaluated against adaptive adversaries that try to evade the rank trigger.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "TRADES on a Low-Rank Manifold (LR-TRADES)\n- Combine TRADES\u2019 boundary-error regularization with low-rank manifold-constrained training (updating low-rank factors instead of full weights). Study how the",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Adversarially-Driven Rank Adaptation\n- Modify the dynamic rank-adaptation criterion in low-rank lottery tickets to explicitly monitor *robust* loss (e.g., TRADES robust surrogate) and allocate rank wh",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Low-Rank Generative Adversarial Trainer for Cost-Aware Robustness\n- Build a GAN-based adversarial training framework where both classifier and perturbation generator are parameterized with adaptive lo",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Provable Links Between Rank, Linearity, and FGSM Vulnerability\n- Using Goodfellow et al.\u2019s \u201clinearity\u201d explanation, develop a theory connecting effective weight matrix rank (and singular spectrum) to ",
          "is_match": true
        },
        {
          "idea_idx": 4,
          "idea_text": "Layerwise Robustness Budgets via Rank\u2013Singular Value Scheduling\n- Introduce a training scheme that jointly schedules (i) TRADES penalty per layer and (ii) target rank/singular-value decay per layer to",
          "is_match": true
        },
        {
          "idea_idx": 5,
          "idea_text": "Robust Low-Rank Tickets for Remote Sensing Scene Classification\n- Extend low-rank lottery tickets and TRADES-style training to remote sensing scene classifiers (where textures and scale changes matter",
          "is_match": true
        },
        {
          "idea_idx": 6,
          "idea_text": "Perturbation-Seeking GANs with Manifold-Constrained Perturbations\n- Train a perturbation generator that is explicitly constrained to a low-dimensional perturbation subspace (e.g., low-rank or low-freq",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Low-Rank Robustness Distillation: Full-Rank Teacher \u2192 Adaptive-Rank Student\n- Train a robust full-rank model (e.g., TRADES) as a teacher, then distill into an adaptive-rank student that learns low-ran",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Robustness-Aware Early Stopping via Matrix Differential Equation Diagnostics\n- Use the matrix differential equation perspective from low-rank manifold training to design diagnostics (e.g., stability/e",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Attack-Conditional Rank Routing (Adaptive Computation for Robust Inference)\n- Create a model that can switch ranks at inference time: low rank for clean inputs, higher rank when an input is suspected ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 49,
      "paper_title": "QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training",
      "contribution": "QoQ-Med is the first open generalist clinical foundation model that effectively reasons across heterogeneous clinical data types.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11445,
      "output_tokens": 979,
      "predecessor_details": [
        {
          "success": true,
          "title": "DeepSeek-R1: Incentivizing Reasoning in LLMs via RL",
          "url": "https://arxiv.org/abs/2501.12948",
          "content": "[2501.12948] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2501.12948\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2501.12948**(cs)\n[Submitted on 22 Jan 2025]\n# Title:DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\nAuthors:[DeepSeek-AI](https://arxiv.org/search/cs?searchtype=author&amp;query=DeepSeek-AI),[Daya Guo](https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+D),[Dejian Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+D),[Haowei Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+H),[Junxiao Song](https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+J),[Ruoyu Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+R),[Runxin Xu](https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+R),[Qihao Zhu](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+Q),[Shirong Ma](https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+S),[Peiyi Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+P),[Xiao Bi](https://arxiv.org/search/cs?searchtype=author&amp;query=Bi,+X),[Xiaokang Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X),[Xingkai Yu](https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+X),[Yu Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+Y),[Z.F. Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+Z),[Zhibin Gou](https://arxiv.org/search/cs?searchtype=author&amp;query=Gou,+Z),[Zhihong Shao](https://arxiv.org/search/cs?searchtype=author&amp;query=Shao,+Z),[Zhuoshu Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Z),[Ziyi Gao](https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+Z),[Aixin Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+A),[Bing Xue](https://arxiv.org/search/cs?searchtype=author&amp;query=Xue,+B),[Bingxuan Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+B),[Bochao Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+B),[Bei Feng](https://arxiv.org/search/cs?searchtype=author&amp;query=Feng,+B),[Chengda Lu](https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+C),[Chenggang Zhao](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+C),[Chengqi Deng](https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+C),[Chenyu Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+C),[Chong Ruan](https://arxiv.org/search/cs?searchtype=author&amp;query=Ruan,+C),[Damai Dai](https://arxiv.org/search/cs?searchtype=author&amp;query=Dai,+D),[Deli Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+D),[Dongjie Ji](https://arxiv.org/search/cs?searchtype=author&amp;query=Ji,+D),[Erhang Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+E),[Fangyun Lin](https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+F),[Fucong Dai](https://arxiv.org/search/cs?searchtype=author&amp;query=Dai,+F),[Fuli Luo](https://arxiv.org/search/cs?searchtype=author&amp;query=Luo,+F),[Guangbo Hao](https://arxiv.org/search/cs?searchtype=author&amp;query=Hao,+G),[Guanting Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+G),[Guowei Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+G),[H. Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+H),[Han Bao](https://arxiv.org/search/cs?searchtype=author&amp;query=Bao,+H),[Hanwei Xu](https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+H),[Haocheng Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+H),[Honghui Ding](https://arxiv.org/search/cs?searchtype=author&amp;query=Ding,+H),[Huajian Xin](https://arxiv.org/search/cs?searchtype=author&amp;query=Xin,+H),[Huazuo Gao](https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+H),[Hui Qu](https://arxiv.org/search/cs?searchtype=author&amp;query=Qu,+H),[Hui Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+H),[Jianzhong Guo](https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+J),[Jiashi Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J),[Jiawei Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+J),[Jingchang Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+J),[Jingyang Yuan](https://arxiv.org/search/cs?searchtype=author&amp;query=Yuan,+J),[Junjie Qiu](https://arxiv.org/search/cs?searchtype=author&amp;query=Qiu,+J),[Junlong Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J),[J.L. Cai](https://arxiv.org/search/cs?searchtype=author&amp;query=Cai,+J),[Jiaqi Ni](https://arxiv.org/search/cs?searchtype=author&amp;query=Ni,+J),[Jian Liang](https://arxiv.org/search/cs?searchtype=author&amp;query=Liang,+J),[Jin Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+J),[Kai Dong](https://arxiv.org/search/cs?searchtype=author&amp;query=Dong,+K),[Kai Hu](https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+K),[Kaige Gao](https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+K),[Kang Guan](https://arxiv.org/search/cs?searchtype=author&amp;query=Guan,+K),[Kexin Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+K),[Kuai Yu](https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+K),[Lean Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+L),[Lecong Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+L),[Liang Zhao](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+L),[Litong Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+L),[Liyue Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+L),[Lei Xu](https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+L),[Leyi Xia](https://arxiv.org/search/cs?searchtype=author&amp;query=Xia,+L),[Mingchuan Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+M),[Minghua Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+M),[Minghui Tang](https://arxiv.org/search/cs?searchtype=author&amp;query=Tang,+M),[Meng Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+M),[Miaojun Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+M),[Mingming Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+M),[Ning Tian](https://arxiv.org/search/cs?searchtype=author&amp;query=Tian,+N),[Panpan Huang](https://arxiv.org/search/cs?searchtype=auth",
          "original_query": "Deepseek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning",
          "cleaned_query": "Deepseek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning"
        },
        {
          "success": true,
          "title": "Med-Flamingo: a Multimodal Medical Few-shot Learner",
          "url": "https://arxiv.org/abs/2307.15189",
          "content": "[2307.15189] Med-Flamingo: a Multimodal Medical Few-shot Learner\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2307.15189\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2307.15189**(cs)\n[Submitted on 27 Jul 2023]\n# Title:Med-Flamingo: a Multimodal Medical Few-shot Learner\nAuthors:[Michael Moor](https://arxiv.org/search/cs?searchtype=author&amp;query=Moor,+M),[Qian Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+Q),[Shirley Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+S),[Michihiro Yasunaga](https://arxiv.org/search/cs?searchtype=author&amp;query=Yasunaga,+M),[Cyril Zakka](https://arxiv.org/search/cs?searchtype=author&amp;query=Zakka,+C),[Yash Dalmia](https://arxiv.org/search/cs?searchtype=author&amp;query=Dalmia,+Y),[Eduardo Pontes Reis](https://arxiv.org/search/cs?searchtype=author&amp;query=Reis,+E+P),[Pranav Rajpurkar](https://arxiv.org/search/cs?searchtype=author&amp;query=Rajpurkar,+P),[Jure Leskovec](https://arxiv.org/search/cs?searchtype=author&amp;query=Leskovec,+J)\nView a PDF of the paper titled Med-Flamingo: a Multimodal Medical Few-shot Learner, by Michael Moor and 8 other authors\n[View PDF](https://arxiv.org/pdf/2307.15189)> > Abstract:\n> Medicine, by its nature, is a multifaceted domain that requires the synthesis of information across various modalities. Medical generative vision-language models (VLMs) make a first step in this direction and promise many exciting clinical applications. However, existing models typically have to be fine-tuned on sizeable down-stream datasets, which poses a significant limitation as in many medical applications data is scarce, necessitating models that are capable of learning from few examples in real-time. Here we propose Med-Flamingo, a multimodal few-shot learner adapted to the medical domain. Based on OpenFlamingo-9B, we continue pre-training on paired and interleaved medical image-text data from publications and textbooks. Med-Flamingo unlocks few-shot generative medical visual question answering (VQA) abilities, which we evaluate on several datasets including a novel challenging open-ended VQA dataset of visual USMLE-style problems. Furthermore, we conduct the first human evaluation for generative medical VQA where physicians review the problems and blinded generations in an interactive app. Med-Flamingo improves performance in generative medical VQA by up to 20\\% in clinician&#39;s rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation. We release our model, code, and evaluation app under [> this https URL\n](https://github.com/snap-stanford/med-flamingo)> . Comments:|Preprint|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2307.15189](https://arxiv.org/abs/2307.15189)[cs.CV]|\n|(or[arXiv:2307.15189v1](https://arxiv.org/abs/2307.15189v1)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2307.15189](https://doi.org/10.48550/arXiv.2307.15189)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Michael Moor [[view email](https://arxiv.org/show-email/1320f572/2307.15189)]\n**[v1]**Thu, 27 Jul 2023 20:36:02 UTC (3,182 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Med-Flamingo: a Multimodal Medical Few-shot Learner, by Michael Moor and 8 other authors\n* [View PDF](https://arxiv.org/pdf/2307.15189)\n* [TeX Source](https://arxiv.org/src/2307.15189)\n[![license icon](https://arxiv.org/icons/licenses/by-nc-sa-4.0.png)view license](http://creativecommons.org/licenses/by-nc-sa/4.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2307.15189&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2307.15189&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2023-07](https://arxiv.org/list/cs.CV/2023-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/2307.15189?context=cs)\n[cs.AI](https://arxiv.org/abs/2307.15189?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2307.15189)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2307.15189)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2307.15189)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of t",
          "original_query": "Med-flamingo: a multimodal medical few-shot learner",
          "cleaned_query": "Med-flamingo: a multimodal medical few-shot learner"
        },
        {
          "success": true,
          "title": "Biomedical Visual Instruction Tuning with Clinician ...",
          "url": "https://arxiv.org/abs/2406.13173",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2406.13173** (cs)\n\n\\[Submitted on 19 Jun 2024 ( [v1](https://arxiv.org/abs/2406.13173v1)), last revised 16 Jul 2024 (this version, v3)\\]\n\n# Title:Biomedical Visual Instruction Tuning with Clinician Preference Alignment\n\nAuthors: [Hejie Cui](https://arxiv.org/search/cs?searchtype=author&query=Cui,+H), [Lingjun Mao](https://arxiv.org/search/cs?searchtype=author&query=Mao,+L), [Xin Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang,+X), [Jieyu Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+J), [Hui Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren,+H), [Quanzheng Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+Q), [Xiang Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+X), [Carl Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+C)\n\nView a PDF of the paper titled Biomedical Visual Instruction Tuning with Clinician Preference Alignment, by Hejie Cui and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2406.13173) [HTML (experimental)](https://arxiv.org/html/2406.13173v3)\n\n> Abstract:Recent advancements in multimodal foundation models have showcased impressive capabilities in understanding and reasoning with visual and textual information. Adapting these foundation models trained for general usage to specialized domains like biomedicine requires large-scale domain-specific instruction datasets. While existing works have explored curating such datasets automatically, the resultant datasets are not explicitly aligned with domain expertise. In this work, we propose a data-centric framework, Biomedical Visual Instruction Tuning with Clinician Preference Alignment (BioMed-VITAL), that incorporates clinician preferences into both stages of generating and selecting instruction data for tuning biomedical multimodal foundation models. First, during the generation stage, we prompt the GPT-4V generator with a diverse set of clinician-selected demonstrations for preference-aligned data candidate generation. Then, during the selection phase, we train a separate selection model, which explicitly distills clinician and policy-guided model preferences into a rating function to select high-quality data for medical instruction tuning. Results show that the model tuned with the instruction-following data from our method demonstrates a significant improvement in open visual chat (18.5% relatively) and medical VQA (win rate up to 81.73%). Our instruction-following data and models are available at [this http URL](http://BioMed-VITAL.github.io).\n\n| | |\n| --- | --- |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) |\n| MSC classes: | 68T50, 68T45, 68T37, 68T05, 68T07, 68T09, |\n| ACM\u00a0classes: | I.2.7; I.2.6; I.2.10 |\n| Cite as: | [arXiv:2406.13173](https://arxiv.org/abs/2406.13173) \\[cs.CV\\] |\n| (or [arXiv:2406.13173v3](https://arxiv.org/abs/2406.13173v3) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2406.13173](https://doi.org/10.48550/arXiv.2406.13173) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Hejie Cui \\[ [view email](https://arxiv.org/show-email/de4eeb23/2406.13173)\\] **[\\[v1\\]](https://arxiv.org/abs/2406.13173v1)**\nWed, 19 Jun 2024 03:07:33 UTC (4,981 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2406.13173v2)**\nSun, 30 Jun 2024 01:22:09 UTC (4,981 KB)\n**\\[v3\\]**\nTue, 16 Jul 2024 05:56:05 UTC (4,981 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Biomedical Visual Instruction Tuning with Clinician Preference Alignment, by Hejie Cui and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2406.13173)\n- [HTML (experimental)](https://arxiv.org/html/2406.13173v3)\n- [TeX Source](https://arxiv.org/src/2406.13173)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2406.13173&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2406.13173&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-06](https://arxiv.org/list/cs.CV/2024-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2406.13173?context=cs) [cs.AI](https://arxiv.org/abs/2406.13173?context=cs.AI) [cs.CL](https://arxiv.org/abs/2406.13173?context=cs.CL) [cs.LG](https://arxiv.org/abs/2406.13173?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2406.13173)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2406.13173)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2406.13173)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2406.13173) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Biomedical visual instruction tuning with clinician preference alignment",
          "cleaned_query": "Biomedical visual instruction tuning with clinician preference alignment"
        },
        {
          "success": true,
          "title": "a Dataset for Sequential Sentence Classification in Medical Abstracts",
          "url": "https://aclanthology.org/I17-2052/",
          "content": "## [PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts](https://aclanthology.org/I17-2052.pdf)\n\n[Franck Dernoncourt](https://aclanthology.org/people/f/franck-dernoncourt/),\n[Ji Young Lee](https://aclanthology.org/people/j/ji-young-lee/)\n\n* * *\n\n##### Abstract\n\nWe present PubMed 200k RCT, a new dataset based on PubMed for sequential sentence classification. The dataset consists of approximately 200,000 abstracts of randomized controlled trials, totaling 2.3 million sentences. Each sentence of each abstract is labeled with their role in the abstract using one of the following classes: background, objective, method, result, or conclusion. The purpose of releasing this dataset is twofold. First, the majority of datasets for sequential short-text classification (i.e., classification of short texts that appear in sequences) are small: we hope that releasing a new large dataset will help develop more accurate algorithms for this task. Second, from an application perspective, researchers need better tools to efficiently skim through the literature. Automatically classifying each sentence in an abstract would help researchers read abstracts more efficiently, especially in fields where abstracts may be long, such as the medical field.\n\nAnthology ID:I17-2052Volume:[Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)](https://aclanthology.org/volumes/I17-2/)Month:NovemberYear:2017Address:Taipei, TaiwanEditors:[Greg Kondrak](https://aclanthology.org/people/g/greg-kondrak/),\n[Taro Watanabe](https://aclanthology.org/people/t/taro-watanabe/)Venue:[IJCNLP](https://aclanthology.org/venues/ijcnlp/)SIG:Publisher:Asian Federation of Natural Language ProcessingNote:Pages:308\u2013313Language:URL:[https://aclanthology.org/I17-2052](https://aclanthology.org/I17-2052)DOI:Bibkey:dernoncourt-lee-2017-pubmedCite (ACL):Franck Dernoncourt and Ji Young Lee. 2017. [PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts](https://aclanthology.org/I17-2052). In _Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)_, pages 308\u2013313, Taipei, Taiwan. Asian Federation of Natural Language Processing.Cite (Informal):[PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts](https://aclanthology.org/I17-2052) (Dernoncourt & Lee, IJCNLP 2017)Copy Citation:BibTeXMarkdownMODS XMLEndnoteMore options\u2026PDF:[https://aclanthology.org/I17-2052.pdf](https://aclanthology.org/I17-2052.pdf)Code[Franck-Dernoncourt/pubmed-rct](https://github.com/Franck-Dernoncourt/pubmed-rct)\n+\n[additional community code](https://paperswithcode.com/paper/?acl=I17-2052)Data[PubMed RCT](https://paperswithcode.com/dataset/pubmed-rct)\n\n[PDF](https://aclanthology.org/I17-2052.pdf) [Cite](https://aclanthology.org/I17-2052/) [Search](https://www.semanticscholar.org/search?q=PubMed+200k+RCT%3A+a+Dataset+for+Sequential+Sentence+Classification+in+Medical+Abstracts) [Code](https://paperswithcode.com/paper/?acl=I17-2052)\n\n* * *",
          "original_query": "Pubmed 200k RCT: a dataset for sequential sentence classification in medical abstracts",
          "cleaned_query": "Pubmed 200k RCT: a dataset for sequential sentence classification in medical abstracts"
        },
        {
          "success": true,
          "title": "Towards Reasoning in Large Language Models: A Survey",
          "url": "https://arxiv.org/abs/2212.10403",
          "content": "[2212.10403] Towards Reasoning in Large Language Models: A Survey\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nIn just 5 minutes help us improve arXiv:\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2212.10403\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2212.10403**(cs)\n[Submitted on 20 Dec 2022 ([v1](https://arxiv.org/abs/2212.10403v1)), last revised 26 May 2023 (this version, v2)]\n# Title:Towards Reasoning in Large Language Models: A Survey\nAuthors:[Jie Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+J),[Kevin Chen-Chuan Chang](https://arxiv.org/search/cs?searchtype=author&amp;query=Chang,+K+C)\nView a PDF of the paper titled Towards Reasoning in Large Language Models: A Survey, by Jie Huang and Kevin Chen-Chuan Chang\n[View PDF](https://arxiv.org/pdf/2212.10403)> > Abstract:\n> Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work. Comments:|ACL 2023 Findings, 15 pages|\nSubjects:|Computation and Language (cs.CL); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2212.10403](https://arxiv.org/abs/2212.10403)[cs.CL]|\n|(or[arXiv:2212.10403v2](https://arxiv.org/abs/2212.10403v2)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2212.10403](https://doi.org/10.48550/arXiv.2212.10403)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jie Huang [[view email](https://arxiv.org/show-email/476bc9d0/2212.10403)]\n**[[v1]](https://arxiv.org/abs/2212.10403v1)**Tue, 20 Dec 2022 16:29:03 UTC (132 KB)\n**[v2]**Fri, 26 May 2023 17:59:33 UTC (151 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Towards Reasoning in Large Language Models: A Survey, by Jie Huang and Kevin Chen-Chuan Chang\n* [View PDF](https://arxiv.org/pdf/2212.10403)\n* [TeX Source](https://arxiv.org/src/2212.10403)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2212.10403&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2212.10403&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2022-12](https://arxiv.org/list/cs.CL/2022-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/2212.10403?context=cs)\n[cs.AI](https://arxiv.org/abs/2212.10403?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2212.10403)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2212.10403)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2212.10403)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2212.10403)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Towards reasoning in large language models: A survey",
          "cleaned_query": "Towards reasoning in large language models: A survey"
        },
        {
          "success": true,
          "title": "Applying generative AI with retrieval augmented ... - ResearchGate",
          "url": "https://www.researchgate.net/publication/381436243_Applying_generative_AI_with_retrieval_augmented_generation_to_summarize_and_extract_key_clinical_information_from_electronic_health_records",
          "content": "Figures - uploaded by Ping Yu Author content All figure content in this area was uploaded by Ping Yu Content may be subject to copyright. Discover the world's research 25+ million members 160+ million publication pages 2.3+ billion citations Join for free \n \n \n \n \n \n \n Journal of Biomedical Informatics 156 (2024) 104662 Available online 14 June 2024 1532-0464/\u00a9 2024 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/). Original Research Applying generative AI with retrieval augmented generation to summarize and extract key clinical information from electronic health records Mohammad Alkhalaf a, b, Ping Yu a, *, Mengyang Yin c, Chao Deng d a School of Computing and Information Technology, University of Wollongong, Wollongong, NSW 2522, Australia b School of Computer Science, Qassim University, Qassim 51452, Saudi Arabia c Opal Healthcare, Level 11/420 George St, Sydney NSW 2000, Australia d School of Medical, Indigenous and Health Sciences, University of Wollongong, Wollongong, NSW 2522, Australia ARTICLE INFO Keywords: Generative AI Nursing notes LLAMA Malnutrition Summarization RAG ABSTRACT Background: Malnutrition is a prevalent issue in aged care facilities (RACFs), leading to adverse health outcomes. The ability to ef\ue630ciently extract key clinical information from a large volume of data in electronic health records (EHR) can improve understanding about the extent of the problem and developing effective interventions. This research aimed to test the ef\ue630cacy of zero-shot prompt engineering applied to generative arti\ue630cial intelligence (AI) models on their own and in combination with retrieval augmented generation (RAG), for the automating tasks of summarizing both structured and unstructured data in EHR and extracting important malnutrition information. Methodology: We utilized Llama 2 13B model with zero-shot prompting. The dataset comprises unstructured and structured EHRs related to malnutrition management in 40 Australian RACFs. We employed zero-shot learning to the model alone \ue630rst, then combined it with RAG to accomplish two tasks: generate structured summaries about the nutritional status of a client and extract key information about malnutrition risk factors. We utilized 25 notes in the \ue630rst task and 1,399 in the second task. We evaluated the model \u2019 s output of each task manually against a gold standard dataset. Result: The evaluation outcomes indicated that zero-shot learning applied to generative AI model is highly effective in summarizing and extracting information about nutritional status of RACFs \u2019 clients. The generated summaries provided concise and accurate representation of the original data with an overall accuracy of 93.25%. The addition of RAG improved the summarization process, leading to a 6% increase and achieving an accuracy of 99.25%. The model also proved its capability in extracting risk factors with an accuracy of 90%. However, adding RAG did not further improve accuracy in this task. Overall, the model has shown a robust performance when information was explicitly stated in the notes; however, it could encounter hallucination limitations, particularly when details were not explicitly provided. Conclusion: This study demonstrates the high performance and limitations of applying zero-shot learning to generative AI models to automatic generation of structured summarization of EHRs data and extracting key clinical information. The inclusion of the RAG approach improved the model performance and mitigated the hallucination problem. 1. Introduction Malnutrition is a signi\ue630cant health concern, which can lead to weight and muscle loss, and affect a person \u2019 s physical health, cognitive func- tions, immune function, and overall quality of life. It is prevalent and extremely harmful for the frail older people living in RACFs, and can expose these older people to higher risk of developing chronic diseases, experiencing functional decline, increasing rate of falls, hospitalization and mortality [1,2]. In older people, malnutrition could be caused by a variety of factors, including physiological changes associated with aging, chronic diseases, social isolation, and poor appetite [3,4]. The complex nature of malnutrition demands attention to its risk factors and * Corresponding author at: Centre for Digital Transformation, School of Computing and Information Technology, Faculty of Engineering and Information Sciences, North\ue630eld Ave, University of Wollongong, Wollongong, NSW 2522, Australia. E-mail address: ping@uow.edu.au (P. Yu). Contents lists available at ScienceDirect Journal of Biomedical Informatics journal homepage: www.elsevier.com/locate/yjbin https://doi.org/10.1016/j.jbi.2024.104662 Received 3 February 2024; Received in revised form 25 May 2024; Accepted 28 May 2024 \n \n \n Journal of Biomedical Informatics 156 (2024) 104662 2 implement targeted prevention and management strategies. To date, the main methods for nutrition prevention and management are regular in- person screening for malnutrition risks, regular nutritional assessment, and speci\ue630c interventions to address the risk factors [5,6]. To date, different data analytics have been applied to uncover malnutrition information from the structured EHR [7 \u2013 9]. However, these methods are inadequate, often missing crucial health information about the nutritional health status of older people that are recorded in narrative text-based notes instead of structured tables in health services [10,11]. It has been found that approximately 80 % to 90 % of health data is recorded in unstructured format [12 \u2013 14]. Consequently, it is imperative to develop an effective machine learning approach to retrieve key health information from the large volume of unstructured clinical data to understand a person \u2019 s health status. An effective method to accomplish this is to transform unstructured clinical notes to struc- tured summarization, which could turn patient information into clinical insights. [15,16]. Summarization of the important health information recorded in the free text notes of EHR could potentially enhance health data accessi- bility and analytics accuracy. This will assist nurses and doctors in ef\ue630ciently retrieving clinical information to provide timely and informed treatment decisions [17]. In addition to summarization, extracting information about risk factors for a certain disease is also essential for providing timely support and preventive intervention to stop the progression of disease-related complications. Furthermore, understanding an individual \u2019 s speci\ue630c risk factors allows for the development of personalized, tailored strate- gies to address the person \u2019 s disease-related issues [18 \u2013 20], and is important for improving client \u2019 s health outcomes. At the population level, knowledge about the prevalence of risk factors is important for government health departments and health organizations for assessing the health needs of a population, developing prevention programs, prioritizing resources to improve public health and planning policy changes, and creating the right health services [21,22]. Transforming unstructured nursing notes in EHR into structured summarization with key information has been a technical challenge for a variety of reasons, including format inconsistencies, length variations, typos, and specialized medical terms. Extracting risk factors is also a challenge because they are embedded into the clinical notes and may be subjective or depend on individual health history [23,24]. Another challenge is the high demand for domain expertise and time to develop large, annotated corpora [25]. These challenges have made automated extraction of health information through human or basic natural lan- guage processing (NLP) techniques a time-consuming and labor- intensive task [26 \u2013 29]. The latest development in NLP and more speci\ue630cally, the generative AI and large langu",
          "original_query": "Applying generative AI with retrieval augmented generation to summarize and extract key clinical information from electronic health records",
          "cleaned_query": "Applying generative AI with retrieval augmented generation to summarize and extract key clinical information from electronic health records"
        }
      ],
      "generated_ideas": [
        "Retrieval-Gated Reasoning Policies for EHR Summarization\nExtend the EHR RAG pipeline by learning an RL policy that decides *when to retrieve*, *what to retrieve* (structured vs narrative, time window), and *how much to cite* before generating a summary. Reward combines factuality (agreement with gold), brevity, and hallucination penalties, and is evaluated on malnutrition-style extraction plus general clinical summarization tasks.",
        "Evidence-Cited Multimodal VQA with Verification Rewards\nAdd a required \u201cevidence trace\u201d to Med-Flamingo answers: cited image regions (via attention/boxes) plus cited retrieved text snippets (RAG). Apply RL where rewards are given only if (a) the final answer is correct and (b) a verifier model can match claims to the cited evidence, creating a practical path to reduce hallucinations in biomedical visual QA.",
        "Few-Shot Specialty Adaptation via Preference-Aligned Data Selection\nGeneralize BioMed-VITAL\u2019s selection-model approach to *specialty-specific* few-shot settings (e.g., dermatology, radiology, pathology) by learning raters that encode each specialty\u2019s clinician preferences. Create a pipeline that generates candidate multimodal instructions from textbooks/papers, then selects a minimal set that maximizes downstream few-shot performance and safety on specialty VQA/chat benchmarks.",
        "Uncertainty-Calibrated Medical Generative VQA with Clinician Rewards\nTrain multimodal medical models to output calibrated confidence/uncertainty (e.g., probability bands, \u201cinsufficient evidence\u201d) and abstain when needed. Use clinician preference data to reward appropriate deferral and penalize overconfident errors; evaluate calibration (ECE), abstention utility, and clinician-rated helpfulness on visual USMLE and real clinical question sets.",
        "Longitudinal Multimodal Patient Summaries with Temporal Retrieval\nExtend RAG summarization from single notes to *timeline-level* summaries that integrate labs/vitals, nursing notes, and imaging reports over weeks/months. Introduce temporal retrieval operators (e.g., \u201ctrend since last month,\u201d \u201cfirst abnormal,\u201d \u201cresponse to intervention\u201d) and evaluate on structured summary accuracy plus clinical decision support tasks like malnutrition risk trajectory.",
        "Adversarial \u201cMissing Detail\u201d Benchmarks for Hallucination in Clinical Notes\nBuild a benchmark by systematically removing or obfuscating key fields in EHR notes (e.g., weight change absent, appetite unclear) and measuring whether models hallucinate specifics. Use the benchmark to compare zero-shot, RAG, instruction-tuning, and RL (with hallucination penalties) and publish standardized metrics for \u201challucination under missingness.\u201d",
        "Multimodal Clinical Instruction Generation from RCTs with Role-Based Controls\nCreate an automatic data engine that converts RCT abstracts into clinician-like multimodal instructions (e.g., \u201cGiven this chest X-ray finding and trial evidence, recommend next step\u201d) while controlling which abstract roles are allowed as evidence. Then apply preference-aligned selection (BioMed-VITAL) to curate high-quality instruction sets for training models that appropriately distinguish evidence strength and applicability.",
        "Reasoning-Token Efficiency for Medical Multimodal Models\nInspired by reasoning-focused RL, optimize models to produce *short, high-signal* rationales (or internal reasoning) that preserve accuracy while reducing verbosity and leakage risk. Train with a reward that trades off correctness, clinician-rated interpretability, and rationale length; evaluate on medical VQA and EHR summarization for cost, latency, and clinician satisfaction."
      ],
      "generated_ideas_raw": "1. **Clinician-Preference RL for Multimodal Medical Reasoning (Med-R1)**\n   Use DeepSeek-R1-style RL to directly optimize a Med-Flamingo/BioMed-VITAL backbone for clinician-preferred *reasoning quality* in medical VQA and visual chat (e.g., diagnostic justification, safety, uncertainty statements). Train a reward model from paired clinician preferences over full answers (and optionally rationales), then compare RL vs instruction-tuning-only on visual USMLE-style problems and standard medical VQA.\n\n2. **Retrieval-Gated Reasoning Policies for EHR Summarization**\n   Extend the EHR RAG pipeline by learning an RL policy that decides *when to retrieve*, *what to retrieve* (structured vs narrative, time window), and *how much to cite* before generating a summary. Reward combines factuality (agreement with gold), brevity, and hallucination penalties, and is evaluated on malnutrition-style extraction plus general clinical summarization tasks.\n\n3. **Evidence-Cited Multimodal VQA with Verification Rewards**\n   Add a required \u201cevidence trace\u201d to Med-Flamingo answers: cited image regions (via attention/boxes) plus cited retrieved text snippets (RAG). Apply RL where rewards are given only if (a) the final answer is correct and (b) a verifier model can match claims to the cited evidence, creating a practical path to reduce hallucinations in biomedical visual QA.\n\n4. **RCT-Aware Medical Answering via Abstract Role Conditioning**\n   Use PubMed 200k RCT sentence-role labels to train a retrieval/indexing system that fetches evidence conditioned on *clinical question type* (background vs methods vs results vs conclusions). Then instruction-tune and/or RL-train the model to generate answers that explicitly separate \u201cwhat the trial found\u201d from \u201chow it was studied,\u201d improving evidence-grounded medical QA and literature summarization.\n\n5. **Few-Shot Specialty Adaptation via Preference-Aligned Data Selection**\n   Generalize BioMed-VITAL\u2019s selection-model approach to *specialty-specific* few-shot settings (e.g., dermatology, radiology, pathology) by learning raters that encode each specialty\u2019s clinician preferences. Create a pipeline that generates candidate multimodal instructions from textbooks/papers, then selects a minimal set that maximizes downstream few-shot performance and safety on specialty VQA/chat benchmarks.\n\n6. **Uncertainty-Calibrated Medical Generative VQA with Clinician Rewards**\n   Train multimodal medical models to output calibrated confidence/uncertainty (e.g., probability bands, \u201cinsufficient evidence\u201d) and abstain when needed. Use clinician preference data to reward appropriate deferral and penalize overconfident errors; evaluate calibration (ECE), abstention utility, and clinician-rated helpfulness on visual USMLE and real clinical question sets.\n\n7. **Longitudinal Multimodal Patient Summaries with Temporal Retrieval**\n   Extend RAG summarization from single notes to *timeline-level* summaries that integrate labs/vitals, nursing notes, and imaging reports over weeks/months. Introduce temporal retrieval operators (e.g., \u201ctrend since last month,\u201d \u201cfirst abnormal,\u201d \u201cresponse to intervention\u201d) and evaluate on structured summary accuracy plus clinical decision support tasks like malnutrition risk trajectory.\n\n8. **Adversarial \u201cMissing Detail\u201d Benchmarks for Hallucination in Clinical Notes**\n   Build a benchmark by systematically removing or obfuscating key fields in EHR notes (e.g., weight change absent, appetite unclear) and measuring whether models hallucinate specifics. Use the benchmark to compare zero-shot, RAG, instruction-tuning, and RL (with hallucination penalties) and publish standardized metrics for \u201challucination under missingness.\u201d\n\n9. **Multimodal Clinical Instruction Generation from RCTs with Role-Based Controls**\n   Create an automatic data engine that converts RCT abstracts into clinician-like multimodal instructions (e.g., \u201cGiven this chest X-ray finding and trial evidence, recommend next step\u201d) while controlling which abstract roles are allowed as evidence. Then apply preference-aligned selection (BioMed-VITAL) to curate high-quality instruction sets for training models that appropriately distinguish evidence strength and applicability.\n\n10. **Reasoning-Token Efficiency for Medical Multimodal Models**\n   Inspired by reasoning-focused RL, optimize models to produce *short, high-signal* rationales (or internal reasoning) that preserve accuracy while reducing verbosity and leakage risk. Train with a reward that trades off correctness, clinician-rated interpretability, and rationale length; evaluate on medical VQA and EHR summarization for cost, latency, and clinician satisfaction.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Retrieval-Gated Reasoning Policies for EHR Summarization\nExtend the EHR RAG pipeline by learning an RL policy that decides *when to retrieve*, *what to retrieve* (structured vs narrative, time window)",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Evidence-Cited Multimodal VQA with Verification Rewards\nAdd a required \u201cevidence trace\u201d to Med-Flamingo answers: cited image regions (via attention/boxes) plus cited retrieved text snippets (RAG). App",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Few-Shot Specialty Adaptation via Preference-Aligned Data Selection\nGeneralize BioMed-VITAL\u2019s selection-model approach to *specialty-specific* few-shot settings (e.g., dermatology, radiology, patholog",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Uncertainty-Calibrated Medical Generative VQA with Clinician Rewards\nTrain multimodal medical models to output calibrated confidence/uncertainty (e.g., probability bands, \u201cinsufficient evidence\u201d) and ",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Longitudinal Multimodal Patient Summaries with Temporal Retrieval\nExtend RAG summarization from single notes to *timeline-level* summaries that integrate labs/vitals, nursing notes, and imaging report",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Adversarial \u201cMissing Detail\u201d Benchmarks for Hallucination in Clinical Notes\nBuild a benchmark by systematically removing or obfuscating key fields in EHR notes (e.g., weight change absent, appetite un",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Multimodal Clinical Instruction Generation from RCTs with Role-Based Controls\nCreate an automatic data engine that converts RCT abstracts into clinician-like multimodal instructions (e.g., \u201cGiven this",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Reasoning-Token Efficiency for Medical Multimodal Models\nInspired by reasoning-focused RL, optimize models to produce *short, high-signal* rationales (or internal reasoning) that preserve accuracy whi",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 50,
      "paper_title": "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free",
      "contribution": "This paper presents a systematic investigation of gating mechanisms in softmax attention variants, demonstrating that a simple head-specific sigmoid gate can enhance performance in large language models significantly.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11774,
      "output_tokens": 1005,
      "predecessor_details": [
        {
          "success": true,
          "title": "A survey on long short-term memory networks for time series ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S2212827121003796",
          "content": "A survey on long short-term memory networks for time series prediction - ScienceDirect\n[Skip to main content](#screen-reader-main-content)[Skip to article](#screen-reader-main-title)\n[![Elsevier logo](https://www.sciencedirect.com/shared-assets/24/images/elsevier-non-solus-new-grey.svg)ScienceDirect](https://www.sciencedirect.com/)\n[My account](https://www.sciencedirect.com/user/login?targetURL=/science/article/pii/S2212827121003796&amp;from=globalheader)\n[Sign in](https://www.sciencedirect.com/user/institution/login?targetURL=/science/article/pii/S2212827121003796)\n* [View**PDF**](https://www.sciencedirect.com/science/article/pii/S2212827121003796/pdf?md5=8f80db54f3725faa6cddaa3a0411df6a&amp;pid=1-s2.0-S2212827121003796-main.pdf)\n* Download full issue\nSearch ScienceDirect\n## Outline\n1. [Abstract](#abs0001)\n2. [Keywords](#keys0001)\n3. [References](#cebibsec1)\n## [Cited by (534)](#section-cited-by)\n[![Elsevier](https://www.sciencedirect.com/us-east-1/prod/39d89e3c832485a52ada0e211f012a3cc7ab704b/image/elsevier-non-solus.svg)](https://www.sciencedirect.com/journal/procedia-cirp)\n## [Procedia CIRP](https://www.sciencedirect.com/journal/procedia-cirp)\n[Volume 99](https://www.sciencedirect.com/journal/procedia-cirp/vol/99/suppl/C),2021, Pages 650-655\n[![Procedia CIRP](https://ars.els-cdn.com/content/image/1-s2.0-S2212827121X00066-cov150h.gif)](https://www.sciencedirect.com/journal/procedia-cirp/vol/99/suppl/C)\n# A survey on long short-term memory networks for time series prediction\nAuthor links open overlay panelBenjaminLindemann,TimoM\u00fcller,HannesVietz,[NasserJazdi](https://www.sciencedirect.com/author/24491873600/nasser-jazdi),[MichaelWeyrich](https://www.sciencedirect.com/author/6507626557/michael-weyrich)\nShow more\nOutline\nAdd to Mendeley\nShare\nCite\n[https://doi.org/10.1016/j.procir.2021.03.088](https://doi.org/10.1016/j.procir.2021.03.088)[Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S2212827121003796&amp;orderBeanReset=true)\nUnder a Creative Commons[license](http://creativecommons.org/licenses/by-nc-nd/4.0/)\nOpen access\n## Abstract\nRecurrent neural networks and exceedingly Long short-term memory (LSTM) have been investigated intensively in recent years due to their ability to model and predict nonlinear time-variant system dynamics. The present paper delivers a comprehensive overview of existing LSTM cell derivatives and network architectures for time series prediction. A categorization in LSTM with optimized cell state representations and LSTM with interacting cell states is proposed. The investigated approaches are evaluated against defined requirements being relevant for an accurate time series prediction. These include short-term and long-term memory behavior, the ability for multimodal and multi-step ahead predictions and the according error propagation. Sequence-to-sequence networks with partially conditioning outperform the other approaches, such as bidirectional or associative networks, and are best suited to fulfill the requirements.\n* [Previousarticlein issue](https://www.sciencedirect.com/science/article/pii/S2212827121003784)\n* [Nextarticlein issue](https://www.sciencedirect.com/science/article/pii/S2212827121003802)\n## Keywords\nRecurrent Neural Networks\nLong short-term memory\nAutoencoder\nSequence-to-Sequence Networks\nTime Series Prediction\n[View PDF](https://www.sciencedirect.com/science/article/pii/S2212827121003796/pdf?md5=8f80db54f3725faa6cddaa3a0411df6a&amp;pid=1-s2.0-S2212827121003796-main.pdf)\nSpecial issue articlesRecommended articles\n## References\n1. [1](#bbib0001)\nWilliams R.J., Zipser R.A.\nA learning algorithm for continually training neural networks\nNeural computation, 1 (1989), pp. 270-280\n[Crossref](https://doi.org/10.1162/neco.1989.1.2.270)[Google Scholar]()\n2. [2](#bbib0002)\nElman J.L.\nFinding structure in time\nCognitive science, 14 (2) (1990), pp. 179-211\n[View PDF](https://www.sciencedirect.com/science/article/pii/036402139090002E/pdf?md5=770a01058356730d8c7a365b3cc6228f&amp;pid=1-s2.0-036402139090002E-main.pdf)[View article](https://www.sciencedirect.com/science/article/pii/036402139090002E)[View in Scopus](https://www.scopus.com/inward/record.url?eid=2-s2.0-26444565569&amp;partnerID=10&amp;rel=R3.0.0)[Google Scholar]()\n3. [3](#bbib0003)\nJordan M.I.\nAttractor dynamics and parallelism in a connectionist sequential machine\nArtificial neural networks: concept learning (1990), pp. 112-127\n[View in Scopus](https://www.scopus.com/inward/record.url?eid=2-s2.0-72249111101&amp;partnerID=10&amp;rel=R3.0.0)[Google Scholar]()\n4. [4](#bbib0004)\nMotazedian, Z. and Safavi, A. A., Nonlinear and Time Varying System Identification Using a Novel Adaptive Fully Connected Recurrent Wavelet Network. in 2019 27th Iranian Conference on Electrical Engineering (ICEE), 2019, pp. 1181\u20131187.\n[Google Scholar]()\n5. [5](#bbib0005)\nChang S.,*et al.*\nDilated recurrent neural networks\nAdvances in Neural Information Processing Systems (2017), pp. 77-87\n[View in Scopus](https://www.scopus.com/inward/record.url?eid=2-s2.0-85047020041&amp;partnerID=10&amp;rel=R3.0.0)[Google Scholar]()\n6. [6](#bbib0006)\nQin, Y., Song, D., Chen, H., Cheng, W., Jiang, G., and Cottrell, G., A dual-stage attention-based recurrent neural network for time series prediction. arXiv preprint arXiv:1704.02971, 2017.\n[Google Scholar]()\n7. [7](#bbib0007)\nHochreiter S., Schmidhuber J.\nLong short-term memory\nNeural computation, 9 (8) (1997), pp. 1735-1780\n[Crossref](https://doi.org/10.1162/neco.1997.9.8.1735)[View in Scopus](https://www.scopus.com/inward/record.url?eid=2-s2.0-0031573117&amp;partnerID=10&amp;rel=R3.0.0)[Google Scholar]()\n8. [8](#bbib0008)\nCho, K. et al., Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\n[Google Scholar]()\n9. [9](#bbib0009)\nChung, J., Gulcehre, C., Cho, K., and Bengio, Y., Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\n[Google Scholar]()\n10. [10](#bbib00010)\nBritz, D., Goldie, A., Luong, M.-T., and Le, Q., Massive exploration of neural machine translation architectures. arXiv preprint arXiv:1703.03906, 2017.\n[Google Scholar]()\n11. [11](#bbib00011)\nCui, Z., Ke, R., Pu, Z., and Wang, Y., Deep bidirectional and unidirectional LSTM recurrent neural network for network-wide traffic speed prediction. arXiv preprint arXiv:1801.02143, 2018.\n[Google Scholar]()\n12. [12](#bbib00012)\nMa X., Zhang J., Du B., Ding C., Sun L.\nParallel architecture of convolutional bi-directional lstm neural networks for network-wide metro ridership prediction\nIEEE Transactions on Intelligent Transportation Systems, 20 (6) (2018), pp. 2278-2288\n[View in Scopus](https://www.scopus.com/inward/record.url?eid=2-s2.0-85069646013&amp;partnerID=10&amp;rel=R3.0.0)[Google Scholar]()\n13. [13](#bbib00013)\nXue, H., Du Huynh, Q., and Reynolds, M., SS-LSTM: A hierarchical LSTM model for pedestrian trajectory prediction. in 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), 2018, pp. 1186\u20131194.\n[Google Scholar]()\n14. [14](#bbib00014)\nSu Z., Jiang J.\nHierarchical Gated Recurrent Unit with Semantic Attention for Event Prediction\nFuture Internet, 12 (2) (2020), p. 39\n[Crossref](https://doi.org/10.3390/fi12020039)[View in Scopus](https://www.scopus.com/inward/record.url?eid=2-s2.0-85084110885&amp;partnerID=10&amp;rel=R3.0.0)[Google Scholar]()\n15. [15](#bbib00015)\nVillegas, R., Yang, J., Zou, Y., Sohn, S., Lin, X., and Lee, H., Learning to generate long-term future via hierarchical prediction. in Proceedings of the 34th International Conference on Machine Learning-Volume 70, 2017, pp. 3560\u20133569.\n[Google Scholar]()\n16. [16](#bbib00016)\nChu K.-F., Lam A.Y.S., Li V.O.K.\nDeep multi-scale convolutional LSTM network for travel demand and origin-destination predictions\nIEEE Transactions on Intelligent Transportation Systems (2019)\n[Google Scholar]()\n17. [17](#bbib00017)\nHuang C.-J., Kuo P.-",
          "original_query": "Long short-term memory",
          "cleaned_query": "Long short-term memory"
        },
        {
          "success": true,
          "title": "[1505.00387] Highway Networks - arXiv",
          "url": "https://arxiv.org/abs/1505.00387",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1505.00387** (cs)\n\n\\[Submitted on 3 May 2015 ( [v1](https://arxiv.org/abs/1505.00387v1)), last revised 3 Nov 2015 (this version, v2)\\]\n\n# Title:Highway Networks\n\nAuthors: [Rupesh Kumar Srivastava](https://arxiv.org/search/cs?searchtype=author&query=Kumar,+R), [Klaus Greff](https://arxiv.org/search/cs?searchtype=author&query=Greff,+K), [J\u00fcrgen Schmidhuber](https://arxiv.org/search/cs?searchtype=author&query=Schmidhuber,+J)\n\nView a PDF of the paper titled Highway Networks, by Rupesh Kumar Srivastava and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/1505.00387)\n\n> Abstract:There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on \"information highways\". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.\n\n| | |\n| --- | --- |\n| Comments: | 6 pages, 2 figures. Presented at ICML 2015 Deep Learning workshop. Full paper is at [arXiv:1507.06228](https://arxiv.org/abs/1507.06228) |\n| Subjects: | Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE) |\n| MSC classes: | 68T01 |\n| ACM\u00a0classes: | I.2.6; G.1.6 |\n| Cite as: | [arXiv:1505.00387](https://arxiv.org/abs/1505.00387) \\[cs.LG\\] |\n| | (or [arXiv:1505.00387v2](https://arxiv.org/abs/1505.00387v2) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1505.00387](https://doi.org/10.48550/arXiv.1505.00387) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Rupesh Kumar Srivastava \\[ [view email](https://arxiv.org/show-email/e87bb1e5/1505.00387)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1505.00387v1)**\nSun, 3 May 2015 01:56:57 UTC (311 KB)\n\n**\\[v2\\]**\nTue, 3 Nov 2015 18:15:15 UTC (319 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Highway Networks, by Rupesh Kumar Srivastava and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/1505.00387)\n- [TeX Source](https://arxiv.org/src/1505.00387)\n- [Other Formats](https://arxiv.org/format/1505.00387)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1505.00387&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1505.00387&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2015-05](https://arxiv.org/list/cs.LG/2015-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1505.00387?context=cs)\n\n[cs.NE](https://arxiv.org/abs/1505.00387?context=cs.NE)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1505.00387)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1505.00387)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1505.00387)\n\n### [2 blog links](https://arxiv.org/tb/1505.00387)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1505.html#SrivastavaGS15) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/SrivastavaGS15)\n\n[Rupesh Kumar Srivastava](https://dblp.uni-trier.de/search/author?author=Rupesh%20Kumar%20Srivastava)\n\n[Klaus Greff](https://dblp.uni-trier.de/search/author?author=Klaus%20Greff)\n\n[J\u00fcrgen Schmidhuber](https://dblp.uni-trier.de/search/author?author=J%C3%BCrgen%20Schmidhuber)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1505.00387&description=Highway Networks) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1505.00387&title=Highway Networks)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1505.00387) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Highway networks",
          "cleaned_query": "Highway networks"
        },
        {
          "success": true,
          "title": "Forgetting Transformer: Softmax Attention with a Forget Gate - arXiv",
          "url": "https://arxiv.org/abs/2503.02130",
          "content": "[2503.02130] Forgetting Transformer: Softmax Attention with a Forget Gate\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2503.02130\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2503.02130**(cs)\n[Submitted on 3 Mar 2025 ([v1](https://arxiv.org/abs/2503.02130v1)), last revised 31 Mar 2025 (this version, v2)]\n# Title:Forgetting Transformer: Softmax Attention with a Forget Gate\nAuthors:[Zhixuan Lin](https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+Z),[Evgenii Nikishin](https://arxiv.org/search/cs?searchtype=author&amp;query=Nikishin,+E),[Xu Owen He](https://arxiv.org/search/cs?searchtype=author&amp;query=He,+X+O),[Aaron Courville](https://arxiv.org/search/cs?searchtype=author&amp;query=Courville,+A)\nView a PDF of the paper titled Forgetting Transformer: Softmax Attention with a Forget Gate, by Zhixuan Lin and 3 other authors\n[View PDF](https://arxiv.org/pdf/2503.02130)[HTML (experimental)](https://arxiv.org/html/2503.02130v2)> > Abstract:\n> An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name this attention mechanism Forgetting Attention and the resulting model the Forgetting Transformer (FoX). We show that FoX outperforms the Transformer on long-context language modeling, length extrapolation, and short-context downstream tasks, while performing on par with the Transformer on long-context downstream tasks. Moreover, it is compatible with the FlashAttention algorithm and does not require any positional embeddings. Several analyses, including the needle-in-the-haystack test, show that FoX also retains the Transformer&#39;s superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet. We also introduce a &#34;Pro&#34; block design that incorporates some common architectural components in recurrent sequence models and find it significantly improves the performance of both FoX and the Transformer. Our code is available at [> this https URL\n](https://github.com/zhixuan-lin/forgetting-transformer)> . Comments:|Published as a conference paper at ICLR 2025; Fixed an issue with the attention map visualization|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)|\nCite as:|[arXiv:2503.02130](https://arxiv.org/abs/2503.02130)[cs.LG]|\n|(or[arXiv:2503.02130v2](https://arxiv.org/abs/2503.02130v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2503.02130](https://doi.org/10.48550/arXiv.2503.02130)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Zhixuan Lin [[view email](https://arxiv.org/show-email/aa584151/2503.02130)]\n**[[v1]](https://arxiv.org/abs/2503.02130v1)**Mon, 3 Mar 2025 23:35:23 UTC (8,665 KB)\n**[v2]**Mon, 31 Mar 2025 19:41:52 UTC (9,425 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Forgetting Transformer: Softmax Attention with a Forget Gate, by Zhixuan Lin and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2503.02130)\n* [HTML (experimental)](https://arxiv.org/html/2503.02130v2)\n* [TeX Source](https://arxiv.org/src/2503.02130)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2503.02130&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2503.02130&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-03](https://arxiv.org/list/cs.LG/2025-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2503.02130?context=cs)\n[cs.AI](https://arxiv.org/abs/2503.02130?context=cs.AI)\n[cs.CL](https://arxiv.org/abs/2503.02130?context=cs.CL)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.02130)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.02130)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.02130)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/aut",
          "original_query": "Forgetting transformer: Softmax attention with a forget gate",
          "cleaned_query": "Forgetting transformer: Softmax attention with a forget gate"
        },
        {
          "success": true,
          "title": "[2202.10447] Transformer Quality in Linear Time - arXiv",
          "url": "https://arxiv.org/abs/2202.10447",
          "content": "[2202.10447] Transformer Quality in Linear Time\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2202.10447\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2202.10447**(cs)\n[Submitted on 21 Feb 2022 ([v1](https://arxiv.org/abs/2202.10447v1)), last revised 27 Jun 2022 (this version, v2)]\n# Title:Transformer Quality in Linear Time\nAuthors:[Weizhe Hua](https://arxiv.org/search/cs?searchtype=author&amp;query=Hua,+W),[Zihang Dai](https://arxiv.org/search/cs?searchtype=author&amp;query=Dai,+Z),[Hanxiao Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+H),[Quoc V. Le](https://arxiv.org/search/cs?searchtype=author&amp;query=Le,+Q+V)\nView a PDF of the paper titled Transformer Quality in Linear Time, by Weizhe Hua and 3 other authors\n[View PDF](https://arxiv.org/pdf/2202.10447)> > Abstract:\n> We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head attention with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH, matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9$\\times$ on Wiki-40B and 12.1$\\times$ on PG-19 for auto-regressive language modeling, and 4.8$\\times$ on C4 for masked language modeling. Comments:|Accepted to the 39th International Conference on Machine Learning (ICML&#39;22)|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Neural and Evolutionary Computing (cs.NE)|\nCite as:|[arXiv:2202.10447](https://arxiv.org/abs/2202.10447)[cs.LG]|\n|(or[arXiv:2202.10447v2](https://arxiv.org/abs/2202.10447v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2202.10447](https://doi.org/10.48550/arXiv.2202.10447)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Weizhe Hua [[view email](https://arxiv.org/show-email/76dac607/2202.10447)]\n**[[v1]](https://arxiv.org/abs/2202.10447v1)**Mon, 21 Feb 2022 18:59:38 UTC (336 KB)\n**[v2]**Mon, 27 Jun 2022 17:59:19 UTC (661 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Transformer Quality in Linear Time, by Weizhe Hua and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2202.10447)\n* [TeX Source](https://arxiv.org/src/2202.10447)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2202.10447&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2202.10447&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2022-02](https://arxiv.org/list/cs.LG/2022-02)\nChange to browse by:\n[cs](https://arxiv.org/abs/2202.10447?context=cs)\n[cs.AI](https://arxiv.org/abs/2202.10447?context=cs.AI)\n[cs.CL](https://arxiv.org/abs/2202.10447?context=cs.CL)\n[cs.NE](https://arxiv.org/abs/2202.10447?context=cs.NE)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2202.10447)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2202.10447)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2202.10447)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2202.10447)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Transformer quality in linear time",
          "cleaned_query": "Transformer quality in linear time"
        },
        {
          "success": true,
          "title": "[2405.16039] MoEUT: Mixture-of-Experts Universal Transformers",
          "url": "https://arxiv.org/abs/2405.16039",
          "content": "[2405.16039] MoEUT: Mixture-of-Experts Universal Transformers\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2405.16039\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2405.16039**(cs)\n[Submitted on 25 May 2024 ([v1](https://arxiv.org/abs/2405.16039v1)), last revised 13 Oct 2024 (this version, v2)]\n# Title:MoEUT: Mixture-of-Experts Universal Transformers\nAuthors:[R\u00f3bert Csord\u00e1s](https://arxiv.org/search/cs?searchtype=author&amp;query=Csord\u00e1s,+R),[Kazuki Irie](https://arxiv.org/search/cs?searchtype=author&amp;query=Irie,+K),[J\u00fcrgen Schmidhuber](https://arxiv.org/search/cs?searchtype=author&amp;query=Schmidhuber,+J),[Christopher Potts](https://arxiv.org/search/cs?searchtype=author&amp;query=Potts,+C),[Christopher D. Manning](https://arxiv.org/search/cs?searchtype=author&amp;query=Manning,+C+D)\nView a PDF of the paper titled MoEUT: Mixture-of-Experts Universal Transformers, by R\\\\&#39;&#39;obert Csord\\\\&#39;&#39;as and 4 other authors\n[View PDF](https://arxiv.org/pdf/2405.16039)[HTML (experimental)](https://arxiv.org/html/2405.16039v2)> > Abstract:\n> Previous work on Universal Transformers (UTs) has demonstrated the importance of parameter sharing across layers. By allowing recurrence in depth, UTs have advantages over standard Transformers in learning compositional generalizations, but layer-sharing comes with a practical limitation of parameter-compute ratio: it drastically reduces the parameter count compared to the non-shared model with the same dimensionality. Naively scaling up the layer size to compensate for the loss of parameters makes its computational resource requirements prohibitive. In practice, no previous work has succeeded in proposing a shared-layer Transformer design that is competitive in parameter count-dominated tasks such as language modeling. Here we propose MoEUT (pronounced &#34;moot&#34;), an effective mixture-of-experts (MoE)-based shared-layer Transformer architecture, which combines several recent advances in MoEs for both feedforward and attention layers of standard Transformers together with novel layer-normalization and grouping schemes that are specific and crucial to UTs. The resulting UT model, for the first time, slightly outperforms standard Transformers on language modeling tasks such as BLiMP and PIQA, while using significantly less compute and memory. Comments:|Accepted to NeurIPS 2024|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)|\nCite as:|[arXiv:2405.16039](https://arxiv.org/abs/2405.16039)[cs.LG]|\n|(or[arXiv:2405.16039v2](https://arxiv.org/abs/2405.16039v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2405.16039](https://doi.org/10.48550/arXiv.2405.16039)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: R\u00f3bert Csord\u00e1s [[view email](https://arxiv.org/show-email/a58362a7/2405.16039)]\n**[[v1]](https://arxiv.org/abs/2405.16039v1)**Sat, 25 May 2024 03:24:32 UTC (946 KB)\n**[v2]**Sun, 13 Oct 2024 04:46:00 UTC (948 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled MoEUT: Mixture-of-Experts Universal Transformers, by R\\\\&#39;&#39;obert Csord\\\\&#39;&#39;as and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2405.16039)\n* [HTML (experimental)](https://arxiv.org/html/2405.16039v2)\n* [TeX Source](https://arxiv.org/src/2405.16039)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2405.16039&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2405.16039&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2024-05](https://arxiv.org/list/cs.LG/2024-05)\nChange to browse by:\n[cs](https://arxiv.org/abs/2405.16039?context=cs)\n[cs.AI](https://arxiv.org/abs/2405.16039?context=cs.AI)\n[cs.NE](https://arxiv.org/abs/2405.16039?context=cs.NE)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2405.16039)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2405.16039)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2405.16039)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which auth",
          "original_query": "Mixture-of-experts universal transformers",
          "cleaned_query": "Mixture-of-experts universal transformers"
        }
      ],
      "generated_ideas": [
        "1. **Partially-Conditioned FoX for Multi-Step Time-Series Forecasting**\n   Build a sequence-to-sequence forecasting model that combines Paper 1\u2019s \u201cpartially conditioned\u201d decoding with FoX\u2019s forget-gated attention (Paper 3) to explicitly control error propagation across horizons. Evaluate on multimodal, multi-step datasets (e.g., energy, traffic) with metrics that separate short-horizon accuracy from long-horizon drift, and ablate where the forget gate is applied (encoder vs decoder vs cross-attention).",
        "**Highway-Gated Universal Transformer Blocks for Stable Depth Recurrence**\n   Introduce Highway Network-style carry/transform gates (Paper 2) inside the Universal Transformer recurrence-in-depth loop used by MoEUT (Paper 5) to stabilize optimization when unrolling many recurrent depth steps. Test whether highway gating reduces the number of depth steps needed for compositional generalization while maintaining perplexity/compute tradeoffs.",
        "**Linear-Time Forgetting Attention (FoX-Flash) for Long-Context Modeling**\n   Develop a linear-time approximation of Forgetting Attention (Paper 3) by adapting FLASH\u2019s gated attention unit + linear approximation pipeline (Paper 4). The key contribution is a drop-in attention module that retains FoX\u2019s length extrapolation benefits while approaching FLASH\u2019s training/inference throughput, with benchmarking on 8K\u201364K contexts and needle-in-haystack retrieval.",
        "**MoE Routing Driven by Forget Gates for Context-Adaptive Computation**\n   Design a Mixture-of-Experts router (Paper 5) that uses statistics of FoX forget gates (Paper 3)\u2014e.g., average retention per head/token\u2014to decide which experts to activate and when. This makes compute allocation depend on \u201chow much the model chooses to remember,\u201d and can be tested for improved efficiency on mixed short/long-context workloads.",
        "**Forecasting with Learnable Memory Budgets via Gate Regularization**\n   Add an explicit \u201cmemory budget\u201d objective that regularizes LSTM forget gates (Paper 1) and FoX forget gates (Paper 3) toward sparse/structured retention over time, enabling controllable tradeoffs between accuracy and memory usage. Implement as a differentiable penalty on cumulative retention (per layer/head) and evaluate for streaming time-series forecasting under strict latency/memory constraints.",
        "**Interacting Cell-State LSTM + Forgetting Transformer Hybrid for Multimodal Signals**\n   Create a hybrid model where an interacting-cell-state LSTM encoder (Paper 1 category) produces a small set of latent state trajectories that a FoX decoder (Paper 3) attends to with forget-gated softmax. This targets multimodal sensor fusion forecasting (e.g., industrial equipment) and explicitly studies whether recurrent latent states reduce attention\u2019s need to scan long raw histories.",
        "**No-Positional-Embedding Time-Series Transformer with Learned Time-Delta Gates**\n   Extend FoX\u2019s \u201cno positional embeddings\u201d setup (Paper 3) to irregularly sampled time series by replacing positional signals with a learnable gate function of time deltas (\u2206t) that modulates attention logits (forgetting term). Compare against standard positional/continuous-time encodings, focusing on robustness to missingness and sampling-rate shifts.",
        "**Depth-Adaptive Universal Transformers Using Gate-Controlled Halting**\n   Combine UT recurrence (Paper 5) with gating signals (Highway, Paper 2; FoX-style forgetting, Paper 3) to implement a token-wise or sequence-wise halting mechanism that decides how many depth-recurrence steps to run. The contribution is an anytime model that spends more compute only on hard examples, evaluated on compositional generalization benchmarks and long-context LM.",
        "**Error-Propagation Diagnostics: Gate Attribution Across Horizons and Context Lengths**\n   Propose a unified diagnostic toolkit that attributes forecast/LM errors to \u201cretention failures\u201d by tracing gradients and activations through LSTM forget gates (Paper 1) and FoX forget gates (Paper 3). Use it to identify whether long-horizon degradation is dominated by under-retention, over-retention (interference), or routing instability (in MoEUT/FLASH variants).",
        "**Grouped/Shared-Norm Gated Attention for Robust Training at Extreme Depth and Length**\n   Extend MoEUT\u2019s UT-specific normalization/grouping insights (Paper 5) to gated attention units (Paper 4) and forgetting attention (Paper 3) by proposing a shared-norm, grouped-head scheme that keeps gate distributions stable across layers and sequence lengths. Benchmark on very deep (100+ layer equivalent) and very long-context settings, measuring both convergence stability and length extrapolation."
      ],
      "generated_ideas_raw": "1. **Partially-Conditioned FoX for Multi-Step Time-Series Forecasting**\n   Build a sequence-to-sequence forecasting model that combines Paper 1\u2019s \u201cpartially conditioned\u201d decoding with FoX\u2019s forget-gated attention (Paper 3) to explicitly control error propagation across horizons. Evaluate on multimodal, multi-step datasets (e.g., energy, traffic) with metrics that separate short-horizon accuracy from long-horizon drift, and ablate where the forget gate is applied (encoder vs decoder vs cross-attention).\n\n2. **Highway-Gated Universal Transformer Blocks for Stable Depth Recurrence**\n   Introduce Highway Network-style carry/transform gates (Paper 2) inside the Universal Transformer recurrence-in-depth loop used by MoEUT (Paper 5) to stabilize optimization when unrolling many recurrent depth steps. Test whether highway gating reduces the number of depth steps needed for compositional generalization while maintaining perplexity/compute tradeoffs.\n\n3. **Linear-Time Forgetting Attention (FoX-Flash) for Long-Context Modeling**\n   Develop a linear-time approximation of Forgetting Attention (Paper 3) by adapting FLASH\u2019s gated attention unit + linear approximation pipeline (Paper 4). The key contribution is a drop-in attention module that retains FoX\u2019s length extrapolation benefits while approaching FLASH\u2019s training/inference throughput, with benchmarking on 8K\u201364K contexts and needle-in-haystack retrieval.\n\n4. **MoE Routing Driven by Forget Gates for Context-Adaptive Computation**\n   Design a Mixture-of-Experts router (Paper 5) that uses statistics of FoX forget gates (Paper 3)\u2014e.g., average retention per head/token\u2014to decide which experts to activate and when. This makes compute allocation depend on \u201chow much the model chooses to remember,\u201d and can be tested for improved efficiency on mixed short/long-context workloads.\n\n5. **Forecasting with Learnable Memory Budgets via Gate Regularization**\n   Add an explicit \u201cmemory budget\u201d objective that regularizes LSTM forget gates (Paper 1) and FoX forget gates (Paper 3) toward sparse/structured retention over time, enabling controllable tradeoffs between accuracy and memory usage. Implement as a differentiable penalty on cumulative retention (per layer/head) and evaluate for streaming time-series forecasting under strict latency/memory constraints.\n\n6. **Interacting Cell-State LSTM + Forgetting Transformer Hybrid for Multimodal Signals**\n   Create a hybrid model where an interacting-cell-state LSTM encoder (Paper 1 category) produces a small set of latent state trajectories that a FoX decoder (Paper 3) attends to with forget-gated softmax. This targets multimodal sensor fusion forecasting (e.g., industrial equipment) and explicitly studies whether recurrent latent states reduce attention\u2019s need to scan long raw histories.\n\n7. **No-Positional-Embedding Time-Series Transformer with Learned Time-Delta Gates**\n   Extend FoX\u2019s \u201cno positional embeddings\u201d setup (Paper 3) to irregularly sampled time series by replacing positional signals with a learnable gate function of time deltas (\u2206t) that modulates attention logits (forgetting term). Compare against standard positional/continuous-time encodings, focusing on robustness to missingness and sampling-rate shifts.\n\n8. **Depth-Adaptive Universal Transformers Using Gate-Controlled Halting**\n   Combine UT recurrence (Paper 5) with gating signals (Highway, Paper 2; FoX-style forgetting, Paper 3) to implement a token-wise or sequence-wise halting mechanism that decides how many depth-recurrence steps to run. The contribution is an anytime model that spends more compute only on hard examples, evaluated on compositional generalization benchmarks and long-context LM.\n\n9. **Error-Propagation Diagnostics: Gate Attribution Across Horizons and Context Lengths**\n   Propose a unified diagnostic toolkit that attributes forecast/LM errors to \u201cretention failures\u201d by tracing gradients and activations through LSTM forget gates (Paper 1) and FoX forget gates (Paper 3). Use it to identify whether long-horizon degradation is dominated by under-retention, over-retention (interference), or routing instability (in MoEUT/FLASH variants).\n\n10. **Grouped/Shared-Norm Gated Attention for Robust Training at Extreme Depth and Length**\n   Extend MoEUT\u2019s UT-specific normalization/grouping insights (Paper 5) to gated attention units (Paper 4) and forgetting attention (Paper 3) by proposing a shared-norm, grouped-head scheme that keeps gate distributions stable across layers and sequence lengths. Benchmark on very deep (100+ layer equivalent) and very long-context settings, measuring both convergence stability and length extrapolation.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "1. **Partially-Conditioned FoX for Multi-Step Time-Series Forecasting**\n   Build a sequence-to-sequence forecasting model that combines Paper 1\u2019s \u201cpartially conditioned\u201d decoding with FoX\u2019s forget-gat",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "**Highway-Gated Universal Transformer Blocks for Stable Depth Recurrence**\n   Introduce Highway Network-style carry/transform gates (Paper 2) inside the Universal Transformer recurrence-in-depth loop ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "**Linear-Time Forgetting Attention (FoX-Flash) for Long-Context Modeling**\n   Develop a linear-time approximation of Forgetting Attention (Paper 3) by adapting FLASH\u2019s gated attention unit + linear ap",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "**MoE Routing Driven by Forget Gates for Context-Adaptive Computation**\n   Design a Mixture-of-Experts router (Paper 5) that uses statistics of FoX forget gates (Paper 3)\u2014e.g., average retention per h",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "**Forecasting with Learnable Memory Budgets via Gate Regularization**\n   Add an explicit \u201cmemory budget\u201d objective that regularizes LSTM forget gates (Paper 1) and FoX forget gates (Paper 3) toward sp",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "**Interacting Cell-State LSTM + Forgetting Transformer Hybrid for Multimodal Signals**\n   Create a hybrid model where an interacting-cell-state LSTM encoder (Paper 1 category) produces a small set of ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "**No-Positional-Embedding Time-Series Transformer with Learned Time-Delta Gates**\n   Extend FoX\u2019s \u201cno positional embeddings\u201d setup (Paper 3) to irregularly sampled time series by replacing positional ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "**Depth-Adaptive Universal Transformers Using Gate-Controlled Halting**\n   Combine UT recurrence (Paper 5) with gating signals (Highway, Paper 2; FoX-style forgetting, Paper 3) to implement a token-wi",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "**Error-Propagation Diagnostics: Gate Attribution Across Horizons and Context Lengths**\n   Propose a unified diagnostic toolkit that attributes forecast/LM errors to \u201cretention failures\u201d by tracing gr",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "**Grouped/Shared-Norm Gated Attention for Robust Training at Extreme Depth and Length**\n   Extend MoEUT\u2019s UT-specific normalization/grouping insights (Paper 5) to gated attention units (Paper 4) and f",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 51,
      "paper_title": "Learning long range dependencies through time reversal symmetry breaking",
      "contribution": "Introducing Recurrent Hamiltonian Echo Learning (RHEL) for training state space models using Hamiltonian dynamics that efficiently compute gradients without backward pass or Jacobian calculations.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 12211,
      "output_tokens": 1044,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] A Learning Algorithm for Continually Running Fully Recurrent ...",
          "url": "https://www.semanticscholar.org/paper/A-Learning-Algorithm-for-Continually-Running-Fully-Williams-Zipser/ce9a21b93ba29d4145a8ef6bf401e77f261848de",
          "content": "[PDF] A Learning Algorithm for Continually Running Fully Recurrent Neural Networks | Semantic Scholar\n[Skip to search form](#search-form)[Skip to main content](#main-content)[Skip to account menu](#account-menu)\n[Semantic ScholarSemantic Scholar's Logo](https://www.semanticscholar.org/)\nSearch 231,119,076 papers from all fields of science\nSearch\n* DOI:[10.1162/neco.1989.1.2.270](https://doi.org/10.1162/neco.1989.1.2.270)\n* Corpus ID: 14711886# A Learning Algorithm for Continually Running Fully Recurrent Neural Networks\n```\n@article{Williams1989ALA,\ntitle={A Learning Algorithm for Continually Running Fully Recurrent Neural Networks},\nauthor={Ronald J. Williams and David Zipser},\njournal={Neural Computation},\nyear={1989},\nvolume={1},\npages={270-280},\nurl={https://api.semanticscholar.org/CorpusID:14711886}\n}\n```\n* [Ronald J. Williams](https://www.semanticscholar.org/author/Ronald-J.-Williams/2116648700),[D. Zipser](https://www.semanticscholar.org/author/D.-Zipser/1895771)\n* Publishedin[Neural Computation](https://www.semanticscholar.org/venue?name=Neural%20Computation)1 June 1989\n* Computer Science\nThe exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal\u2026Expand\n[View on MIT Press](https://doi.org/10.1162/neco.1989.1.2.270)\n[gwern.net](https://www.gwern.net/docs/ai/1989-williams-2.pdf)\nSave to LibrarySave\nCreate AlertAlert\nCite\nShare\n4,863 Citations\n[\nHighly Influential Citations\n](#citing-papers)[](https://www.semanticscholar.org/faq#influential-citations)\n314\n[\nBackground Citations\n](#citing-papers)\n1,251\n[\nMethods Citations\n](#citing-papers)\n1,534\n[\nResults Citations\n](#citing-papers)\n25\n[View All](#citing-papers)\n## Topics\nAI-Generated\n[Teacher-Forced(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/24461262904?corpusId=14711886)[Recurrent Learning(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/19545723952?corpusId=14711886)[Backpropagation Through Time(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/10854005842?corpusId=14711886)[Learning Algorithm(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/65759749143?corpusId=14711886)[Recurrent Connections(opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/8006229394?corpusId=14711886)\n## 4,863 Citations\nCitation Type\nHas PDF\nAuthor\nMore Filters\nMore Filters\nFilters\nSort by RelevanceSort by Most Influenced PapersSort by Citation CountSort by Recency\n[### A learning algorithm for improved recurrent neural networks\n](https://www.semanticscholar.org/paper/A-learning-algorithm-for-improved-recurrent-neural-Chen-Yu/0d0998b0afba6e8412d98ab95ff389a363408d2c)[C. Chen](https://www.semanticscholar.org/author/C.-Chen/49750929)[L. Yu](https://www.semanticscholar.org/author/L.-Yu/2202748483)\nComputer Science\n[Proceedings of International Conference on Neural\u2026](https://www.semanticscholar.org/venue?name=Proceedings%20of%20International%20Conference%20on%20Neural%20Networks%20(ICNN%2797))\n* 1997\nTLDR\nAn improved recurrent neural network structure is proposed that allows networks to learn complex tasks that require the retention of information over time periods and compensates for the information that is missed by the traditional recurrent neural networks.Expand\n* [\n3\n](https://www.semanticscholar.org/paper/0d0998b0afba6e8412d98ab95ff389a363408d2c#citing-papers)\nSave\n[### A fast online learning algorithm for recurrent neural networks\n](https://www.semanticscholar.org/paper/A-fast-online-learning-algorithm-for-recurrent-Sun-Chen/290b5f741c552c21568ea88a23a0e683357b9282)[G. Sun](https://www.semanticscholar.org/author/G.-Sun/2148441625)[H. Chen](https://www.semanticscholar.org/author/H.-Chen/2108324032)[Ye Le](https://www.semanticscholar.org/author/Ye-Le/2132239251)\nComputer Science\nIJCNN-91-Seattle International Joint Conference\u2026\n* 1991\nTLDR\nThe authors present an algorithm that would require O(N/sup 3/) calculations to update the weights in one time step, which is faster than all other known online training algorithms.Expand\n* [\n2\n](https://www.semanticscholar.org/paper/290b5f741c552c21568ea88a23a0e683357b9282#citing-papers)\n* 2 Excerpts\nSave\n[### Learning State Space Trajectories in Recurrent Neural Networks\n](https://www.semanticscholar.org/paper/Learning-State-Space-Trajectories-in-Recurrent-Pearlmutter/34468c0aa95a7aea212d8738ab899a69b2fc14c6)[Barak A. Pearlmutter](https://www.semanticscholar.org/author/Barak-A.-Pearlmutter/1700974)\nComputer Science\n[Neural Computation](https://www.semanticscholar.org/venue?name=Neural%20Computation)\n* 1989\nTLDR\nA procedure for finding E/wij, where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and wij are the weights of that network, which seems particularly suited for temporally continuous domains.Expand\n* [\n858\n](https://www.semanticscholar.org/paper/34468c0aa95a7aea212d8738ab899a69b2fc14c6#citing-papers)\n* [\nPDF\n](https://www.semanticscholar.org/paper/34468c0aa95a7aea212d8738ab899a69b2fc14c6)\nSave\n[### An algorithm for the addition of time-delayed connections to recurrent neural networks\n](https://www.semanticscholar.org/paper/An-algorithm-for-the-addition-of-time-delayed-to-Bon%C3%A9-Crucianu/30187808365684a32768d29a93b87fc898260767)[R. Bon\u00e9](https://www.semanticscholar.org/author/R.-Bon%C3%A9/4832915)[M. Crucianu](https://www.semanticscholar.org/author/M.-Crucianu/1719698)[J. A. D. Beauville](https://www.semanticscholar.org/author/J.-A.-D.-Beauville/3232042)\nComputer Science, Mathematics\n[ESANN](https://www.semanticscholar.org/venue?name=ESANN)\n* 2000\nTLDR\nThis work supports the view that it is easier for gradient descent algorithms to find good solutions if one includes connections with time delays in the recurrent networks, and presents an algorithm that allows one to choose the right locations and delays for such connections.Expand\n* [\n6\n](https://www.semanticscholar.org/paper/30187808365684a32768d29a93b87fc898260767#citing-papers)\n* [\nPDF\n](https://www.semanticscholar.org/paper/30187808365684a32768d29a93b87fc898260767)\n* 1 Excerpt\nSave\n[### An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories\n](https://www.semanticscholar.org/paper/An-Efficient-Gradient-Based-Algorithm-for-On-Line-Williams-Peng/2ae5a5507253aa3cada113d41d35fada1e84555f)[Ronald J. Williams](https://www.semanticscholar.org/author/Ronald-J.-Williams/2116648700)[Jing Peng](https://www.semanticscholar.org/author/Jing-Peng/47918185)\nComputer Science\n[Neural Computation](https://www.semanticscholar.org/venue?name=Neural%20Computation)\n* 1990\nA novel variant of the familiar backpropagation-through-time approach to training recurrent networks is described. This algorithm is intended to be used on arbitrary recurrent networks that run\u2026Expand\n* [\n737\n](https://www.semanticscholar.org/paper/2ae5a5507253aa3cada113d41d35fada1e84555f#citing-papers)\n* [Highly Influenced](https://www.semanticscholar.org/paper/2ae5a5507253aa3cada113d41d35fada1e84555f?sort=is-influential#citing-papers)\n* 16 Excerpts\nSave\n[### A realtime learning algorithm for recurrent neural networks\n](https://www.semanticscholar.org/paper/A-realtime-learning-algorithm-for-recurrent-neural-Uchiyama-Shimohara/691597ccbf857f93584a4c0eee0c65db1fee331c)[T. Uchiyama](https://www.semanticscholar.org/author/T.-Uchiyama/69848015)[K. Shimohara](https://www.semanticscholar.org/author/K.-Shimohara/143643764)\nComputer Science\n[Systems and Computers in Japan](https://www.semanticscholar.org/venue?name=Systems%20and%20Computers%20in%20Japan)\n* 1991\nTLDR\nGradients of objective functionals are derived by variational calculus and a real-time computation method of gradients is proposed and it is expected that the real- time learning, requiring less computational complexity compared to conventional learning algorithms, will become pos",
          "original_query": "A learning algorithm for continually running fully recurrent neural networks",
          "cleaned_query": "A learning algorithm for continually running fully recurrent neural networks"
        },
        {
          "success": true,
          "title": "A cookbook for hardware-friendly implicit learning on static data",
          "url": "https://openreview.net/forum?id=aL5AlzWrf0",
          "content": "A cookbook for hardware-friendly implicit learning on static data | OpenReview\n[![back arrow](https://openreview.net/images/arrow_left.svg)Go to**NeurIPS 2024 Workshop MLNCP**homepage](https://openreview.net/group?id=NeurIPS.cc/2024/Workshop/MLNCP)\n## A cookbook for hardware-friendly implicit learning on static data\n[![Download PDF](https://openreview.net/images/pdf_icon_blue.svg)](https://openreview.net/pdf?id=aL5AlzWrf0)\n### [Maxence Ernoult](https://openreview.net/profile?id=~Maxence_Ernoult1),[Rasmus H\u00f8ier](https://openreview.net/profile?id=~Rasmus_H%C3%B8ier1),[Jack Kendall](https://openreview.net/profile?id=~Jack_Kendall1)\nPublished: 17 Oct 2024, Last Modified: 06 Dec 2024MLNCP PosterEveryone[Revisions](https://openreview.net/revisions?id=aL5AlzWrf0)[BibTeX](#)[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)\n**Keywords:**implicit differentiation, hardware, bilevel optimization, multilevel optimization, zeroth-order optimization, neuromorphic computing, energy-based models, Hopfield models, biologically plausible credit assignment\n**Abstract:**The following aims to be a pragmatic introduction to hardware-friendly learning of implicit models, which encompass a broad class of models from feedforward nets to physical systems, taking static data as inputs. Starting from first principles, we present a minimal hierarchy of independent concepts to circumvent some problems inherent to the hardware implementation of standard differentiation. This way, we avoid entangling essential ingredients with arbitrary design choices by naively listing existing algorithms and instead propose the draft of a \u201ccookbook\u201d to help the exploration of many possible combinations of these independent mechanisms.\n**Submission Number:**54\nLoading\n[OpenReview](https://openreview.net/about)is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the[OpenReview Sponsors](https://openreview.net/sponsors). \u00a92025OpenReview",
          "original_query": "A cookbook for hardware-friendly implicit learning on static data",
          "cleaned_query": "A cookbook for hardware-friendly implicit learning on static data"
        },
        {
          "success": true,
          "title": "Equilibrium Propagation Without Limits",
          "url": "https://arxiv.org/html/2511.22024v1",
          "content": "Equilibrium Propagation Without Limits\n\\\\correspondence\nelonlit@stanford.edu\\\\versionNovember 27, 2025\n# Equilibrium Propagation Without Limits\nElon Litman11Stanford University\n###### Abstract\nWe liberate Equilibrium Propagation (EP) from the limit of infinitesimal perturbations by establishing a finite-nudge foundation for local credit assignment. By modeling network states as Gibbs-Boltzmann distributions rather than deterministic points, we prove that the gradient of the difference in Helmholtz free energy between a nudged and free phase is exactly the difference in expected local energy derivatives. This validates the classic Contrastive Hebbian Learning update as an exact gradient estimator for arbitrary finite nudging, requiring neither infinitesimal approximations nor convexity. Furthermore, we derive a generalized EP algorithm based on the path integral of loss-energy covariances, enabling learning with strong error signals that standard infinitesimal approximations cannot support.\n## 1Introduction\nBackpropagation provides an efficient solution to the credit assignment problem in layered and recurrent networks> [\n[> 40\n](https://arxiv.org/html/2511.22024v1#bib.bib40)> , [> 33\n](https://arxiv.org/html/2511.22024v1#bib.bib33)> ]\nand underpins most contemporary applications of deep neural networks> [\n[> 22\n](https://arxiv.org/html/2511.22024v1#bib.bib22)> , [> 19\n](https://arxiv.org/html/2511.22024v1#bib.bib19)> , [> 14\n](https://arxiv.org/html/2511.22024v1#bib.bib14)> , [> 36\n](https://arxiv.org/html/2511.22024v1#bib.bib36)> ]\n. At the same time, the algorithm relies on a dedicated backward pass that transports errors through the network using the exact transpose of the forward weights. This weight transport requirement and the associated non-locality are widely regarded as biologically implausible and difficult to reconcile with the constraints of real neural circuits> [\n[> 7\n](https://arxiv.org/html/2511.22024v1#bib.bib7)> , [> 13\n](https://arxiv.org/html/2511.22024v1#bib.bib13)> , [> 3\n](https://arxiv.org/html/2511.22024v1#bib.bib3)> , [> 25\n](https://arxiv.org/html/2511.22024v1#bib.bib25)> , [> 42\n](https://arxiv.org/html/2511.22024v1#bib.bib42)> ]\n. Empirical and theoretical work in neuroscience instead points to synaptic plasticity rules that depend on locally available variables such as pre and postsynaptic activity> [\n[> 15\n](https://arxiv.org/html/2511.22024v1#bib.bib15)> ]\n, their timing> [\n[> 12\n](https://arxiv.org/html/2511.22024v1#bib.bib12)> , [> 6\n](https://arxiv.org/html/2511.22024v1#bib.bib6)> ]\n, and possibly a small number of modulatory signals> [\n[> 38\n](https://arxiv.org/html/2511.22024v1#bib.bib38)> ]\n.\nThis tension has motivated a broad search for learning rules that are both powerful and local. Proposals include multivariate Hebbian and three-factor rules, feedback alignment and its variants> [\n[> 24\n](https://arxiv.org/html/2511.22024v1#bib.bib24)> , [> 30\n](https://arxiv.org/html/2511.22024v1#bib.bib30)> ]\n, target propagation> [\n[> 23\n](https://arxiv.org/html/2511.22024v1#bib.bib23)> ]\n, and predictive coding style architectures that attempt to approximate error backpropagation with local computations> [\n[> 41\n](https://arxiv.org/html/2511.22024v1#bib.bib41)> , [> 27\n](https://arxiv.org/html/2511.22024v1#bib.bib27)> , [> 5\n](https://arxiv.org/html/2511.22024v1#bib.bib5)> ]\n. While these approaches relax strict weight symmetry and can achieve reasonable performance, they often lack a clean global objective, or they rely on auxiliary mechanisms whose physical or biological status is unclear> [\n[> 3\n](https://arxiv.org/html/2511.22024v1#bib.bib3)> , [> 25\n](https://arxiv.org/html/2511.22024v1#bib.bib25)> , [> 42\n](https://arxiv.org/html/2511.22024v1#bib.bib42)> ]\n.\nEnergy based models provide a natural framework in which to formulate local learning> [\n[> 17\n](https://arxiv.org/html/2511.22024v1#bib.bib17)> , [> 1\n](https://arxiv.org/html/2511.22024v1#bib.bib1)> , [> 21\n](https://arxiv.org/html/2511.22024v1#bib.bib21)> ]\n. In this setting a network is defined by an energy function over states and parameters, and its dynamics can be viewed as a relaxation process that lowers this energy> [\n[> 16\n](https://arxiv.org/html/2511.22024v1#bib.bib16)> , [> 8\n](https://arxiv.org/html/2511.22024v1#bib.bib8)> ]\n. Contrastive Hebbian Learning (CHL)> [\n[> 28\n](https://arxiv.org/html/2511.22024v1#bib.bib28)> , [> 32\n](https://arxiv.org/html/2511.22024v1#bib.bib32)> , [> 43\n](https://arxiv.org/html/2511.22024v1#bib.bib43)> ]\nand Equilibrium Propagation (EP)> [\n[> 34\n](https://arxiv.org/html/2511.22024v1#bib.bib34)> , [> 35\n](https://arxiv.org/html/2511.22024v1#bib.bib35)> ]\nare two influential schemes that exploit this structure. Both use a free phase, in which the network state is driven only by the input, and a nudged phase, in which a supervisory signal biases the system toward target configurations> [\n[> 10\n](https://arxiv.org/html/2511.22024v1#bib.bib10)> , [> 20\n](https://arxiv.org/html/2511.22024v1#bib.bib20)> , [> 31\n](https://arxiv.org/html/2511.22024v1#bib.bib31)> ]\n. Parameters are updated according to the difference between local statistics measured in the two phases, which yields a local two-phase learning rule.\nDespite their appeal, the theoretical status of these methods is incomplete. In its classical form, CHL is derived for architectures with symmetric weights and a single well defined energy function> [\n[> 28\n](https://arxiv.org/html/2511.22024v1#bib.bib28)> , [> 43\n](https://arxiv.org/html/2511.22024v1#bib.bib43)> ]\n. Under those assumptions, contrastive updates can be identified with gradients of a likelihood or related energy based objective> [\n[> 21\n](https://arxiv.org/html/2511.22024v1#bib.bib21)> ]\n. As soon as the infinitesimal limit is relaxed, which is required for noise tolerance in physical implementations and in biological circuits, the learning rule remains well defined but it is no longer obvious what global quantity it optimizes, if any> [\n[> 43\n](https://arxiv.org/html/2511.22024v1#bib.bib43)> , [> 24\n](https://arxiv.org/html/2511.22024v1#bib.bib24)> , [> 8\n](https://arxiv.org/html/2511.22024v1#bib.bib8)> ]\n. EP addresses the weight transport issue by avoiding an explicit backward pass. It couples the energy to a supervised loss via a nudging parameter and recovers the gradient of the supervised objective in the limit of an infinitesimal perturbation> [\n[> 34\n](https://arxiv.org/html/2511.22024v1#bib.bib34)> , [> 35\n](https://arxiv.org/html/2511.22024v1#bib.bib35)> , [> 10\n](https://arxiv.org/html/2511.22024v1#bib.bib10)> ]\n. In practice, however, finite nudging is required for stable learning, which introduces bias relative to the true gradient. Moreover, most analyses assume deterministic dynamics at zero temperature, in which the network state is identified with a single energy minimum> [\n[> 1\n](https://arxiv.org/html/2511.22024v1#bib.bib1)> ]\n, an idealization that is difficult to justify for complex nonconvex energy landscapes> [\n[> 26\n](https://arxiv.org/html/2511.22024v1#bib.bib26)> , [> 9\n](https://arxiv.org/html/2511.22024v1#bib.bib9)> , [> 29\n](https://arxiv.org/html/2511.22024v1#bib.bib29)> ]\n.\nThis paper develops a statistical mechanics foundation for contrastive learning that resolves these issues. Instead of treating the network as a deterministic energy minimizer, we model its state as a random variable distributed according to a Gibbs\u2013Boltzmann measure at finite temperature, defined by an energy function and a task-dependent loss. Within this framework we introduce the stochastic contrastive objective, defined as the difference in Helmholtz free energy between the nudged and free phases. This objective depends only on the underlying energy based model and the loss and is well defined for arbitrary nonlinear architectures, nonconvex energy landscapes, and finite temperature dynamics> [\n[> 26\n](https://arxiv.org/html/2511.22024v1#bib.bib26)> , [> 9\n](https://arxiv.org/html/2511.22024v1#",
          "original_query": "Equilibrium propagation: Bridging the gap between energy-based models and backpropagation",
          "cleaned_query": "Equilibrium propagation: Bridging the gap between energy-based models and backpropagation"
        },
        {
          "success": true,
          "title": "Self-learning Machines based on Hamiltonian Echo Backpropagation",
          "url": "https://arxiv.org/abs/2103.04992",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2103.04992** (cs)\n\n\\[Submitted on 8 Mar 2021 ( [v1](https://arxiv.org/abs/2103.04992v1)), last revised 7 Feb 2023 (this version, v2)\\]\n\n# Title:Self-learning Machines based on Hamiltonian Echo Backpropagation\n\nAuthors: [Victor Lopez-Pastor](https://arxiv.org/search/cs?searchtype=author&query=Lopez-Pastor,+V), [Florian Marquardt](https://arxiv.org/search/cs?searchtype=author&query=Marquardt,+F)\n\nView a PDF of the paper titled Self-learning Machines based on Hamiltonian Echo Backpropagation, by Victor Lopez-Pastor and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2103.04992)\n\n> Abstract:A physical self-learning machine can be defined as a nonlinear dynamical system that can be trained on data (similar to artificial neural networks), but where the update of the internal degrees of freedom that serve as learnable parameters happens autonomously. In this way, neither external processing and feedback nor knowledge of (and control of) these internal degrees of freedom is required. We introduce a general scheme for self-learning in any time-reversible Hamiltonian system. We illustrate the training of such a self-learning machine numerically for the case of coupled nonlinear wave fields.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Adaptation and Self-Organizing Systems (nlin.AO); Data Analysis, Statistics and Probability (physics.data-an); Optics (physics.optics) |\n| Cite as: | [arXiv:2103.04992](https://arxiv.org/abs/2103.04992) \\[cs.LG\\] |\n| | (or [arXiv:2103.04992v2](https://arxiv.org/abs/2103.04992v2) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2103.04992](https://doi.org/10.48550/arXiv.2103.04992) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | Physical Review X 13, 031020 (2023) |\n| Related DOI: | [https://doi.org/10.1103/PhysRevX.13.031020](https://doi.org/10.1103/PhysRevX.13.031020) Focus to learn more DOI(s) linking to related resources |\n\n## Submission history\n\nFrom: Victor Jose Lopez-Pastor \\[ [view email](https://arxiv.org/show-email/6f629c14/2103.04992)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2103.04992v1)**\nMon, 8 Mar 2021 18:35:06 UTC (959 KB)\n\n**\\[v2\\]**\nTue, 7 Feb 2023 15:09:15 UTC (4,737 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Self-learning Machines based on Hamiltonian Echo Backpropagation, by Victor Lopez-Pastor and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2103.04992)\n- [TeX Source](https://arxiv.org/src/2103.04992)\n- [Other Formats](https://arxiv.org/format/2103.04992)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2103.04992&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2103.04992&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-03](https://arxiv.org/list/cs.LG/2021-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2103.04992?context=cs)\n\n[nlin](https://arxiv.org/abs/2103.04992?context=nlin)\n\n[nlin.AO](https://arxiv.org/abs/2103.04992?context=nlin.AO)\n\n[physics](https://arxiv.org/abs/2103.04992?context=physics)\n\n[physics.data-an](https://arxiv.org/abs/2103.04992?context=physics.data-an)\n\n[physics.optics](https://arxiv.org/abs/2103.04992?context=physics.optics)\n\n### References & Citations\n\n- [INSPIRE HEP](https://inspirehep.net/arxiv/2103.04992)\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2103.04992)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2103.04992)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2103.04992)\n\n### [1 blog link](https://arxiv.org/tb/2103.04992)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2103.html#abs-2103-04992) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2103-04992)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2103.04992&description=Self-learning Machines based on Hamiltonian Echo Backpropagation) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2103.04992&title=Self-learning Machines based on Hamiltonian Echo Backpropagation)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2103.04992) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Self-learning machines based on hamiltonian echo backpropagation",
          "cleaned_query": "Self-learning machines based on hamiltonian echo backpropagation"
        },
        {
          "success": true,
          "title": "[1806.07366] Neural Ordinary Differential Equations",
          "url": "https://arxiv.org/abs/1806.07366",
          "content": "[1806.07366] Neural Ordinary Differential Equations\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1806.07366\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1806.07366**(cs)\n[Submitted on 19 Jun 2018 ([v1](https://arxiv.org/abs/1806.07366v1)), last revised 14 Dec 2019 (this version, v5)]\n# Title:Neural Ordinary Differential Equations\nAuthors:[Ricky T. Q. Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+R+T+Q),[Yulia Rubanova](https://arxiv.org/search/cs?searchtype=author&amp;query=Rubanova,+Y),[Jesse Bettencourt](https://arxiv.org/search/cs?searchtype=author&amp;query=Bettencourt,+J),[David Duvenaud](https://arxiv.org/search/cs?searchtype=author&amp;query=Duvenaud,+D)\nView a PDF of the paper titled Neural Ordinary Differential Equations, by Ricky T. Q. Chen and 3 other authors\n[View PDF](https://arxiv.org/pdf/1806.07366)> > Abstract:\n> We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models. Subjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)|\nCite as:|[arXiv:1806.07366](https://arxiv.org/abs/1806.07366)[cs.LG]|\n|(or[arXiv:1806.07366v5](https://arxiv.org/abs/1806.07366v5)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1806.07366](https://doi.org/10.48550/arXiv.1806.07366)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: David Duvenaud [[view email](https://arxiv.org/show-email/8698f71b/1806.07366)]\n**[[v1]](https://arxiv.org/abs/1806.07366v1)**Tue, 19 Jun 2018 17:50:12 UTC (4,832 KB)\n**[[v2]](https://arxiv.org/abs/1806.07366v2)**Wed, 3 Oct 2018 00:13:07 UTC (4,912 KB)\n**[[v3]](https://arxiv.org/abs/1806.07366v3)**Mon, 22 Oct 2018 22:06:50 UTC (4,912 KB)\n**[[v4]](https://arxiv.org/abs/1806.07366v4)**Tue, 15 Jan 2019 01:56:48 UTC (3,506 KB)\n**[v5]**Sat, 14 Dec 2019 02:01:18 UTC (3,505 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Neural Ordinary Differential Equations, by Ricky T. Q. Chen and 3 other authors\n* [View PDF](https://arxiv.org/pdf/1806.07366)\n* [TeX Source](https://arxiv.org/src/1806.07366)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1806.07366&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1806.07366&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2018-06](https://arxiv.org/list/cs.LG/2018-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1806.07366?context=cs)\n[cs.AI](https://arxiv.org/abs/1806.07366?context=cs.AI)\n[stat](https://arxiv.org/abs/1806.07366?context=stat)\n[stat.ML](https://arxiv.org/abs/1806.07366?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1806.07366)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1806.07366)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1806.07366)\n### [9 blog links](https://arxiv.org/tb/1806.07366)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1806.html#abs-1806-07366)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1806-07366)\n[Tian Qi Chen]()\n[Yulia Rubanova]()\n[Jesse Bettencourt]()\n[David Duvenaud]()\n[David K. Duvenaud]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper",
          "original_query": "Neural ordinary differential equations",
          "cleaned_query": "Neural ordinary differential equations"
        },
        {
          "success": true,
          "title": "Automatic differentiation in machine learning: a survey - arXiv",
          "url": "https://arxiv.org/abs/1502.05767",
          "content": "[1502.05767] Automatic differentiation in machine learning: a survey\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nIn just 5 minutes help us improve arXiv:\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1502.05767\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Symbolic Computation\n**arXiv:1502.05767**(cs)\n[Submitted on 20 Feb 2015 ([v1](https://arxiv.org/abs/1502.05767v1)), last revised 5 Feb 2018 (this version, v4)]\n# Title:Automatic differentiation in machine learning: a survey\nAuthors:[Atilim Gunes Baydin](https://arxiv.org/search/cs?searchtype=author&amp;query=Baydin,+A+G),[Barak A. Pearlmutter](https://arxiv.org/search/cs?searchtype=author&amp;query=Pearlmutter,+B+A),[Alexey Andreyevich Radul](https://arxiv.org/search/cs?searchtype=author&amp;query=Radul,+A+A),[Jeffrey Mark Siskind](https://arxiv.org/search/cs?searchtype=author&amp;query=Siskind,+J+M)\nView a PDF of the paper titled Automatic differentiation in machine learning: a survey, by Atilim Gunes Baydin and 3 other authors\n[View PDF](https://arxiv.org/pdf/1502.05767)> > Abstract:\n> Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply &#34;autodiff&#34;, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other&#39;s results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names &#34;dynamic computational graphs&#34; and &#34;differentiable programming&#34;. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms &#34;autodiff&#34;, &#34;automatic differentiation&#34;, and &#34;symbolic differentiation&#34; as these are encountered more and more in machine learning settings. Comments:|43 pages, 5 figures|\nSubjects:|Symbolic Computation (cs.SC); Machine Learning (cs.LG); Machine Learning (stat.ML)|\nMSCclasses:|68W30, 65D25, 68T05|\nACMclasses:|G.1.4; I.2.6|\nCite as:|[arXiv:1502.05767](https://arxiv.org/abs/1502.05767)[cs.SC]|\n|(or[arXiv:1502.05767v4](https://arxiv.org/abs/1502.05767v4)[cs.SC]for this version)|\n|[https://doi.org/10.48550/arXiv.1502.05767](https://doi.org/10.48550/arXiv.1502.05767)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\nJournalreference:|Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Research, 18(153):1--43, 2018|\n## Submission history\nFrom: Atilim Gunes Baydin [[view email](https://arxiv.org/show-email/2f8e0a6c/1502.05767)]\n**[[v1]](https://arxiv.org/abs/1502.05767v1)**Fri, 20 Feb 2015 04:20:47 UTC (70 KB)\n**[[v2]](https://arxiv.org/abs/1502.05767v2)**Sun, 19 Apr 2015 16:49:13 UTC (79 KB)\n**[[v3]](https://arxiv.org/abs/1502.05767v3)**Thu, 17 Aug 2017 16:45:07 UTC (69 KB)\n**[v4]**Mon, 5 Feb 2018 15:57:57 UTC (92 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Automatic differentiation in machine learning: a survey, by Atilim Gunes Baydin and 3 other authors\n* [View PDF](https://arxiv.org/pdf/1502.05767)\n* [TeX Source](https://arxiv.org/src/1502.05767)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.SC\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1502.05767&amp;function=prev&amp;context=cs.SC) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1502.05767&amp;function=next&amp;context=cs.SC)\n[new](https://arxiv.org/list/cs.SC/new)|[recent](https://arxiv.org/list/cs.SC/recent)|[2015-02](https://arxiv.org/list/cs.SC/2015-02)\nChange to browse by:\n[cs](https://arxiv.org/abs/1502.05767?context=cs)\n[cs.LG](https://arxiv.org/abs/1502.05767?context=cs.LG)\n[stat](https://arxiv.org/abs/1502.05767?context=stat)\n[stat.ML](https://arxiv.org/abs/1502.05767?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1502.05767)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1502.05767)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1502.05767)\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1502.html#BaydinPR15)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/BaydinPR15)\n[Atilim Gunes Baydin]()\n[Barak A. Pearlmutter]()\n[Alexey Andreyevich Radul]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore reco",
          "original_query": "Automatic differentiation in machine learning: a survey",
          "cleaned_query": "Automatic differentiation in machine learning: a survey"
        }
      ],
      "generated_ideas": [
        "Finite\u2011Nudge Equilibrium Propagation for Continual-Time Recurrent Networks\nExtend Williams & Zipser\u2019s continually running fully recurrent learning to an energy-based continuous-time RNN trained with Litman\u2019s finite\u2011nudge EP objective. Derive the exact local update for continuous-time dynamics (ODE/SDE) and validate on streaming prediction/control tasks where standard BPTT is unstable or memory-intensive.",
        "Adaptive Nudge Scheduling via Covariance Path Integrals\nTurn the \u201cpath integral of loss\u2013energy covariances\u201d (finite\u2011nudge EP) into a practical algorithm that adaptively chooses nudge magnitude and duration per sample to minimize gradient variance under a compute/energy budget. Implement a controller that uses online estimates of covariance curvature to decide when strong nudges help versus when small nudges suffice, and benchmark against fixed-nudge EP/CHL.",
        "Hamiltonian Echo Backprop Meets Finite\u2011Nudge EP in Reversible Energy Networks\nUnify Hamiltonian Echo Backpropagation (time-reversible learning) with finite\u2011nudge EP by constructing a reversible energy-based network whose forward relaxation and backward \u201cecho\u201d correspond to free and nudged phases. The contribution is a physically implementable local learning rule that remains exact for finite nudges while exploiting reversibility to reduce sampling/mixing time.",
        "Implicit Differentiation Toolkit for Energy-Based Hardware with Two-Phase Readout\nBuild a \u201ccookbook-to-code\u201d library that compiles implicit learning recipes into hardware-friendly training loops combining (i) two-phase EP/CHL measurements, (ii) implicit differentiation for fixed-point solvers, and (iii) zeroth-order fallbacks when readout noise dominates. Evaluate on static datasets with analog/noisy constraints, reporting energy-to-accuracy tradeoffs and sensitivity to solver tolerances.",
        "Neural ODEs Trained by Contrastive Free-Energy Differences (No Adjoint Needed)\nReplace the Neural ODE adjoint method with a two-phase training procedure: run the ODE to a free trajectory, then re-run with a finite supervisory nudge and update parameters using the difference in expected local energy derivatives (finite\u2011nudge EP/CHL). This yields a training method that does not require storing solver internals or backpropagating through them, targeting embedded/accelerator settings where adjoints are hard to implement.",
        "Stochastic EP for Gibbs\u2011Boltzmann State Models Under Realistic Hardware Noise\nOperationalize the Gibbs-Boltzmann state perspective from finite\u2011nudge EP by explicitly modeling state noise as an SDE and deriving learning rules that remain correct under non-thermal, colored, or state-dependent noise. Provide diagnostics to detect when the assumed distributional form breaks (e.g., heavy tails) and propose robust estimators for the phase-difference statistics.",
        "Delay-Augmented Recurrent Energy Networks for Long-Horizon Credit Assignment\nIncorporate learnable time-delayed connections (as suggested by follow-on work to Williams & Zipser) into an energy-based recurrent architecture trained with finite\u2011nudge EP. The key contribution is a concrete method to place and tune delays to improve memory and stability, evaluated on long-horizon sequence tasks where standard recurrent EP struggles to propagate credit.",
        "Local Second-Order Preconditioning for Two-Phase Learning (Hessian-Free EP)\nLeverage AD concepts (forward-over-reverse, Jacobian-vector products) to build a hardware-friendly approximate natural-gradient or Gauss\u2013Newton preconditioner using only local phase statistics and occasional vector-probe nudges. This would reduce the number of relaxation steps/epochs needed for EP/CHL to converge, while avoiding full backprop and global Hessian construction.",
        "Energy-Precision Co-Design: Choosing Solver Tolerances as Learnable Parameters\nFor Neural ODE / implicit-model training on constrained hardware, treat numerical solver tolerances (step size, stopping criteria, quantization levels) as differentiable or implicitly-optimized hyperparameters within the learning loop (bilevel optimization from the \u201ccookbook\u201d). The contribution is an actionable co-design procedure that learns when to spend compute for precision versus when coarse dynamics suffice, measured by wall-power/accuracy Pareto curves.",
        "Benchmark Suite for Continual-Time Local Learning: BPTT vs Adjoint vs EP vs Echo\nCreate a standardized benchmark that compares (i) BPTT for continually running RNNs, (ii) adjoint-based Neural ODE training, (iii) finite\u2011nudge EP/CHL, and (iv) Hamiltonian echo learning on matched tasks (streaming system ID, control, event-based sensing). Provide unified metrics: sample efficiency, relaxation/solver cost, robustness to noise, and hardware implementability, enabling principled selection of local-credit methods by regime."
      ],
      "generated_ideas_raw": "1. **Finite\u2011Nudge Equilibrium Propagation for Continual-Time Recurrent Networks**  \nExtend Williams & Zipser\u2019s continually running fully recurrent learning to an energy-based continuous-time RNN trained with Litman\u2019s finite\u2011nudge EP objective. Derive the exact local update for continuous-time dynamics (ODE/SDE) and validate on streaming prediction/control tasks where standard BPTT is unstable or memory-intensive.\n\n2. **Adaptive Nudge Scheduling via Covariance Path Integrals**  \nTurn the \u201cpath integral of loss\u2013energy covariances\u201d (finite\u2011nudge EP) into a practical algorithm that adaptively chooses nudge magnitude and duration per sample to minimize gradient variance under a compute/energy budget. Implement a controller that uses online estimates of covariance curvature to decide when strong nudges help versus when small nudges suffice, and benchmark against fixed-nudge EP/CHL.\n\n3. **Hamiltonian Echo Backprop Meets Finite\u2011Nudge EP in Reversible Energy Networks**  \nUnify Hamiltonian Echo Backpropagation (time-reversible learning) with finite\u2011nudge EP by constructing a reversible energy-based network whose forward relaxation and backward \u201cecho\u201d correspond to free and nudged phases. The contribution is a physically implementable local learning rule that remains exact for finite nudges while exploiting reversibility to reduce sampling/mixing time.\n\n4. **Implicit Differentiation Toolkit for Energy-Based Hardware with Two-Phase Readout**  \nBuild a \u201ccookbook-to-code\u201d library that compiles implicit learning recipes into hardware-friendly training loops combining (i) two-phase EP/CHL measurements, (ii) implicit differentiation for fixed-point solvers, and (iii) zeroth-order fallbacks when readout noise dominates. Evaluate on static datasets with analog/noisy constraints, reporting energy-to-accuracy tradeoffs and sensitivity to solver tolerances.\n\n5. **Neural ODEs Trained by Contrastive Free-Energy Differences (No Adjoint Needed)**  \nReplace the Neural ODE adjoint method with a two-phase training procedure: run the ODE to a free trajectory, then re-run with a finite supervisory nudge and update parameters using the difference in expected local energy derivatives (finite\u2011nudge EP/CHL). This yields a training method that does not require storing solver internals or backpropagating through them, targeting embedded/accelerator settings where adjoints are hard to implement.\n\n6. **Stochastic EP for Gibbs\u2011Boltzmann State Models Under Realistic Hardware Noise**  \nOperationalize the Gibbs-Boltzmann state perspective from finite\u2011nudge EP by explicitly modeling state noise as an SDE and deriving learning rules that remain correct under non-thermal, colored, or state-dependent noise. Provide diagnostics to detect when the assumed distributional form breaks (e.g., heavy tails) and propose robust estimators for the phase-difference statistics.\n\n7. **Delay-Augmented Recurrent Energy Networks for Long-Horizon Credit Assignment**  \nIncorporate learnable time-delayed connections (as suggested by follow-on work to Williams & Zipser) into an energy-based recurrent architecture trained with finite\u2011nudge EP. The key contribution is a concrete method to place and tune delays to improve memory and stability, evaluated on long-horizon sequence tasks where standard recurrent EP struggles to propagate credit.\n\n8. **Local Second-Order Preconditioning for Two-Phase Learning (Hessian-Free EP)**  \nLeverage AD concepts (forward-over-reverse, Jacobian-vector products) to build a hardware-friendly approximate natural-gradient or Gauss\u2013Newton preconditioner using only local phase statistics and occasional vector-probe nudges. This would reduce the number of relaxation steps/epochs needed for EP/CHL to converge, while avoiding full backprop and global Hessian construction.\n\n9. **Energy-Precision Co-Design: Choosing Solver Tolerances as Learnable Parameters**  \nFor Neural ODE / implicit-model training on constrained hardware, treat numerical solver tolerances (step size, stopping criteria, quantization levels) as differentiable or implicitly-optimized hyperparameters within the learning loop (bilevel optimization from the \u201ccookbook\u201d). The contribution is an actionable co-design procedure that learns when to spend compute for precision versus when coarse dynamics suffice, measured by wall-power/accuracy Pareto curves.\n\n10. **Benchmark Suite for Continual-Time Local Learning: BPTT vs Adjoint vs EP vs Echo**  \nCreate a standardized benchmark that compares (i) BPTT for continually running RNNs, (ii) adjoint-based Neural ODE training, (iii) finite\u2011nudge EP/CHL, and (iv) Hamiltonian echo learning on matched tasks (streaming system ID, control, event-based sensing). Provide unified metrics: sample efficiency, relaxation/solver cost, robustness to noise, and hardware implementability, enabling principled selection of local-credit methods by regime.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Finite\u2011Nudge Equilibrium Propagation for Continual-Time Recurrent Networks\nExtend Williams & Zipser\u2019s continually running fully recurrent learning to an energy-based continuous-time RNN trained with L",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Adaptive Nudge Scheduling via Covariance Path Integrals\nTurn the \u201cpath integral of loss\u2013energy covariances\u201d (finite\u2011nudge EP) into a practical algorithm that adaptively chooses nudge magnitude and dur",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Hamiltonian Echo Backprop Meets Finite\u2011Nudge EP in Reversible Energy Networks\nUnify Hamiltonian Echo Backpropagation (time-reversible learning) with finite\u2011nudge EP by constructing a reversible energy",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Implicit Differentiation Toolkit for Energy-Based Hardware with Two-Phase Readout\nBuild a \u201ccookbook-to-code\u201d library that compiles implicit learning recipes into hardware-friendly training loops combi",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Neural ODEs Trained by Contrastive Free-Energy Differences (No Adjoint Needed)\nReplace the Neural ODE adjoint method with a two-phase training procedure: run the ODE to a free trajectory, then re-run ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Stochastic EP for Gibbs\u2011Boltzmann State Models Under Realistic Hardware Noise\nOperationalize the Gibbs-Boltzmann state perspective from finite\u2011nudge EP by explicitly modeling state noise as an SDE and",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Delay-Augmented Recurrent Energy Networks for Long-Horizon Credit Assignment\nIncorporate learnable time-delayed connections (as suggested by follow-on work to Williams & Zipser) into an energy-based r",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Local Second-Order Preconditioning for Two-Phase Learning (Hessian-Free EP)\nLeverage AD concepts (forward-over-reverse, Jacobian-vector products) to build a hardware-friendly approximate natural-gradi",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Energy-Precision Co-Design: Choosing Solver Tolerances as Learnable Parameters\nFor Neural ODE / implicit-model training on constrained hardware, treat numerical solver tolerances (step size, stopping ",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Benchmark Suite for Continual-Time Local Learning: BPTT vs Adjoint vs EP vs Echo\nCreate a standardized benchmark that compares (i) BPTT for continually running RNNs, (ii) adjoint-based Neural ODE trai",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 52,
      "paper_title": "FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution",
      "contribution": "FuXi-Ocean is the first data-driven global ocean forecasting model achieving six-hourly predictions at eddy-resolving 1/12\u00b0 spatial resolution.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 4,
      "crawl_rate": 1.0,
      "ideas_generated": 4,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10277,
      "output_tokens": 1064,
      "predecessor_details": [
        {
          "success": true,
          "title": "Data Assimilation",
          "url": "https://www.hycom.org/data-assimilation",
          "content": "| |\n| --- |\n| - [Home](https://www.hycom.org/) - [Support Us](https://give.fsu.edu/coaps) - [Need Help?](javascript:void(0);) - [Ask a Question on the Forum](http://groups.google.com/a/hycom.org/group/forum) - [Trouble using THREDDS?](https://www.unidata.ucar.edu/software/tds/) - [Trouble using OPeNDAP?](http://docs.opendap.org/index.php/QuickStart) - [Trouble using NCSS?](https://docs.unidata.ucar.edu/tds/current/userguide/netcdf_subset_service_ref.html) - [Media](https://www.hycom.org/youtube) - [Data Server](https://www.hycom.org/dataserver) - [THREDDS](http://tds.hycom.org/thredds) - [HTTP / HTTPS](https://data.hycom.org/datasets) - [OPeNDAP](http://tds.hycom.org/thredds/catalog/datasets/catalog.html) - [FTP](ftp://ftp.hycom.org/datasets) - [Tools](https://www.hycom.org/tools) - [Calendar](https://www.hycom.org/tools/calendar) - [Status](https://www.hycom.org/tools/status) - [Panoply](http://www.giss.nasa.gov/tools/panoply/) - [Uptime](https://stats.uptimerobot.com/JQn3qt8AXL) - [Login / Logout](https://www.hycom.org/login) |\n\n- [About](https://www.hycom.org/about)\n\n- [HYCOM](https://www.hycom.org/hycom)\n - [Overview](https://www.hycom.org/hycom/overview)\n - [Documentation](https://www.hycom.org/hycom/documentation)\n - [Source Code](https://www.hycom.org/hycom/source-code)\n - [Contact Info](https://www.hycom.org/contact-info)\n- [Forum](http://groups.google.com/a/hycom.org/group/forum/)\n - [README](https://www.hycom.org/forum/readme)\n\n- [YouTube Videos](https://www.hycom.org/youtube)\n\n- [Data Server](https://www.hycom.org/dataserver)\n - [ESPC-D-V02](https://www.hycom.org/dataserver/espc-d-v02)\n - [Global Analysis](https://www.hycom.org/dataserver/espc-d-v02/global-analysis)\n - [GOFS 3.1](https://www.hycom.org/dataserver/gofs-3pt1/analysis)\n - [Global Analysis](https://www.hycom.org/dataserver/gofs-3pt1/analysis)\n - [Global Reanalysis](https://www.hycom.org/dataserver/gofs-3pt1/reanalysis)\n - [Ice Fields](https://www.hycom.org/dataserver/gofs-3pt1/reanalysis/ice)\n - [GOFS 3.0](https://www.hycom.org/dataserver/gofs-3pt0)\n - [Global Analysis](https://www.hycom.org/dataserver/gofs-3pt0/analysis)\n - [Global Reanalysis](https://www.hycom.org/dataserver/gofs-3pt0/reanalysis)\n - Gulf of Mexico\n - [GoM Analysis](https://www.hycom.org/data/gomu0pt04/expt-90pt1m000)\n - [GoM Reanalysis](https://www.hycom.org/dataserver/gom/gom-reanalysis)\n - [NAVGEM Forcing](https://www.hycom.org/dataserver/navgem)\n - [Legacy 1.3/1.4](https://www.hycom.org/dataserver/navgem/legacy-1pt3-1pt4)\n - [Legacy 1.1/1.2](https://www.hycom.org/dataserver/navgem/legacy-1pt1-1pt2)\n - [NOGAPS Forcing](https://www.hycom.org/dataserver/nogaps)\n - [NCEP CFSR](https://www.hycom.org/dataserver/ncep-cfsr)\n - [NCEP CFSv2](https://www.hycom.org/dataserver/ncep-cfsv2)\n - Access Methods\n - [http / https](https://www.hycom.org/dataserver/access-methods/http-https)\n - [ftp / ftps](https://www.hycom.org/dataserver/access-methods/ftp-ftps)\n - [rsync](https://www.hycom.org/dataserver/access-methods/rsync)\n - [THREDDS](https://www.hycom.org/dataserver/access-methods/thredds)\n - [NCSS](https://www.hycom.org/dataserver/access-methods/ncss)\n - [WCS](https://www.hycom.org/dataserver/access-methods/wcs)\n - [WMS](https://www.hycom.org/dataserver/access-methods/wms)\n\n- [Global](https://www.hycom.org/global)\n- [Basin](https://www.hycom.org/basin)\n- [Regional](https://www.hycom.org/regional)\n- [Coupled Simulations](https://www.hycom.org/coupled-simulations)\n- [Process Studies](https://www.hycom.org/process-studies)\n- [Data Assimilation](https://www.hycom.org/data-assimilation)\n- [Reanalysis](https://www.hycom.org/reanalysis)\n- [Miscellanea](https://www.hycom.org/miscellanea)\n\n- [Ocean Prediction](https://www.hycom.org/ocean-prediction)\n\n- [Outreach](https://www.hycom.org/outreach)\n- [Publications](https://www.hycom.org/publications)\n- [Meetings](https://www.hycom.org/meetings)\n- [Workshops](https://www.hycom.org/workshops)\n- [Web Links](https://www.hycom.org/weblinks)\n\n- [Request An Account](https://www.hycom.org/account-request)\n- [Contact Info](https://www.hycom.org/contact-info)\n- [Mailing Lists](https://www.hycom.org/mailing-lists)\n- [FAQs](https://www.hycom.org/faqs)\n\n[**Support HYCOM.org**\\\n**Donate Now!**](https://give.fsu.edu/coaps)\n\n| [Login](https://www.hycom.org/login) |\n| Data Assimilation\n\n| A hierarchy of data assimilation techniques are evaluated as a function of computational resources and prediction accuracy: 1. the Optimal Interpolation (OI) 2. the Parameter Matrix Objective Analysis algorithm (PMOA) 3. the Reduced Order Adaptive Filter (ROAF) 4. the Reduced Order Information Filter (ROIF) |\n| | |\n| --- |\n| Contributions |\n\n| | File | Year | Authors | Source | File size |\n| --- | --- | --- | --- | --- |\n| [An update on the comparison of sequential assimilation schemes](https://www.hycom.org/attachments/084_ashwanth_LOM_2009.pdf) | 2009 | Srinivasan, Chassignet, Thacker, Smedstad, Counillion, Bertino, Brankart, Brasseur, Chin, Cummings | RSMAS-CCS / University of Miami (LOM\u00a02009) | 2024 Kb |\n| [Improving the Parameterization of Errors Statistics for Data Assimilation](https://www.hycom.org/attachments/084_BrasseurOSorlandoSEEKHYCOM2.pdf) | 2008 | Brasseur, Broquet, Brankart, Castruccio, Lauvernet, Verron | LEGI, (Ocean\u00a0Sciences\u00a02008) | 3444 Kb |\n| [Demo and Comparison of Sequential Approaches for Altimeter Data Assimilation](https://www.hycom.org/attachments/084_Oceaan_Sciences_2008_Srinivasan.pdf) | 2008 | Srinivasan, Chassignet, Smedstad, Thacker, Bertino, Brasseur, Chin, Counillon, Cummings | FSU/COAPS, (Ocean\u00a0Sciences\u00a02008) | 3259 Kb |\n| [T/S relationships by local regression](https://www.hycom.org/attachments/084_6_Thacker.pdf) | 2007 | Thacker | NOAA/AOML, (10th\u00a0HYCOM) | 2213 Kb |\n| [Status and progress of NCODA assimilation in HYCOM](https://www.hycom.org/attachments/084_3_Cummings.pdf) | 2007 | Cummings | NRL, (11th\u00a0HYCOM) | 1192 Kb |\n| [HYCOM data assimilation at NCEP](https://www.hycom.org/attachments/084_5_Lozano.pdf) | 2007 | Lozano | MMAB/NCEP, (11th\u00a0HYCOM) | 2194 Kb |\n| [Gulf of Mexico data assimilation inter-comparison project](https://www.hycom.org/attachments/084_4_Ashwanth.pdf) | 2007 | Srinivasan | FSU, (11th\u00a0HYCOM) | 3608 Kb |\n| [Ensemble-based assimilation of HF-Radar surface currents in a West Florida Shelf](https://www.hycom.org/attachments/084_4_Barth.pdf) | 2007 | Barth | University of South Florida, (11th\u00a0HYCOM) | 2066 Kb |\n| [The Navy Coupled Ocean Data Assimilation (NCODA) in HYCOM](https://www.hycom.org/attachments/084_Smedstad.pdf) | 2006 | Smedstad | Planning Systems, (10th\u00a0HYCOM) | 3866 Kb |\n| [Multivariate properties of the Ensemble Optimal Interpolation in the GoM](https://www.hycom.org/attachments/084_Counillon.pdf) | 2006 | Counillon | NERSC, (10th\u00a0HYCOM) | 2210 Kb |\n| [On a 3D variational assimilation scheme in hybrid coordinates](https://www.hycom.org/attachments/084_Lozano-2.pdf) | 2006 | Lozano | NCEP/NOAA, (10th\u00a0HYCOM) | 979 Kb |\n| [Gulf of Mexico data assimilation comparison exercise](https://www.hycom.org/attachments/084_Lozano-1.pdf) | 2006 | Lozano | NCEP/NOAA, (10th\u00a0HYCOM) | 437 Kb |\n| [Gulf of Mexico data assimilation comparison exercise](https://www.hycom.org/attachments/084_Chassignet-2.pdf) | 2006 | Chassignet | FSU/COAPS, (10th\u00a0HYCOM) | 4756 Kb |\n| [Constrained Data Assimilation](https://www.hycom.org/attachments/084_Thacker.pdf) | 2006 | Thacker | NOAA/AOML, (10th\u00a0HYCOM) | 91 Kb |\n| [The NCODA implementation with re-layerization](https://www.hycom.org/attachments/084_H.Kang.pdf) | 2005 | Kang | NOAA/AOML, (9th\u00a0HYCOM) | 1795 Kb |\n| [Status of NRL coupled ocean data assimilation (NCODA) system](https://www.hycom.org/attachments/084_J.Cummings(2).pdf) | 2005 | Cummings | NRL, (9th HYCOM) | 470 Kb |\n| [Multi-processor implementation of ROIF assimilation scheme for HYCOM](https://www.hycom.org/attachments/084_A.Srinivasan_ROIF.pdf) | 2005 | Srinivasan | UM/RSMAS, (9th HYCOM) | 2200 Kb |\n| [Implementation of the NRL coupled ocean data assimiltion (NC",
          "original_query": "The HYCOM (Hybrid Coordinate Ocean Model) data assimilative system",
          "cleaned_query": "The HYCOM (Hybrid Coordinate Ocean Model) data assimilative system",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "Forecasting the eddying ocean with a deep neural network - Nature",
          "url": "https://www.nature.com/articles/s41467-025-57389-2",
          "content": "\n \n Introduction The ocean is turbulent, containing eddies with a wide range of sizes. Among them, the mesoscale eddies form the major fraction of the ocean kinetic energy reservoir 1, 2. These mesoscale eddies redistribute heat and materials in the ocean, contributing dominantly to the short-term (from daily to weekly time scales) variations of ocean thermohaline structures 3, 4, regulating the primary productivity and further marine ecosystem 5, 6, and acting as a major driver of extreme events like marine heatwaves 7. They also interact strongly with the overlaying atmosphere, influencing air temperature, humidity, winds, cloud fraction, and rainfall within local marine atmospheric boundary layer 8, 9 as well as atmospheric synoptic variability 10, 11 and large-scale circulations 12. Therefore, accurate eddy-resolving ocean forecasts are not only essential for supporting marine activities and managements but also necessary to improve weather forecast accuracy. So far, ocean forecasts rely primarily on ocean general circulation models (OGCMs) that make forecasts by numerically discretizing and integrating the governing partial differential equations of the ocean. However, constructing an accurate eddy-resolving global ocean forecast system (GOFS) based on the OGCMs (i.e., the numerical GOFS) remains a challenging issue both computationally and scientifically. On the one hand, resolving mesoscale eddies requires a grid size of O(10\u2009km) or even finer 13, 14, imposing a massive computational burden for operating global-scale OGCMs especially for implementing advanced data assimilation 15 and making ensemble forecast 16. In fact, the eddy-resolving numerical GOFSs did not emerge until the recent decade partially due to the enlarged computational resources. On the other hand, the chaotic nature of mesoscale eddies makes their forecast very sensitive to errors in initial conditions, boundary forcings, and OGCMs. In particular, despite development over about a half-century, there are still many uncertainties in OGCMs, including numerical errors caused by discretization, uncertainties in parameterizations of unresolved processes, and insufficient representation of interactions of the ocean with other components of the Earth system 17. Artificial intelligence (AI)-based methods provide a data-driven approach for making forecasts and have been successfully applied to the global medium-range weather forecasts 18, 19, 20, 21, 22, 23, 24, 25. Their success is achieved primarily from high-quality atmospheric reanalysis training datasets and customized deep neural network (DNN) designed to capture the hidden representations of atmospheric dynamics and alleviate accumulative error when rolling out multistep autoregressive forecasts 18, 19, 20, 21, 22, 23, 24, 25. These AI-based forecast systems show competitive forecast performance yet substantially reduce the computational burden compared to their numerical counterparts. The successful application of AI-based methods in medium-range weather forecasts has been inspiring their ocean-related usage, including the OGCM emulators, the short-term ocean forecast as well as the interannual-to-decadal ocean prediction 26, 27, 28, 29, although it has been well recognized that ocean reanalysis datasets are less robust compared to their atmospheric counterparts primarily due to the sparsity of ocean observations 30. It should be noted that the existing DNN architectures developed for the medium-range weather forecast may not be suitable for the eddy-resolving ocean forecast. Air-sea interactions play an important role in the short-term ocean variability, especially in the surface mixed layer 31. However, these interactions have not been explicitly incorporated into the AI-based medium-range weather forecast systems except for providing the SST as an input. Furthermore, the existing AI-based methods tend to smooth out mesoscale weather phenomena 32, 33 and are expected to dampen ocean mesoscale eddies in a similar way. Such a blurring effect is tolerable for the medium-range weather forecast as its variability is generally dominated by synoptic processes 34, but not so for short-term ocean forecast, at which time scale mesoscale eddies make a major contribution to the variability 35 (Supplementary Fig.\u00a0 S1). In this study, we present WenHai, an AI-based GOFS for short-term eddy-resolving forecast across the global upper ocean (0\u2013643\u2009m). WenHai explicitly incorporates atmospheric forcings into the DNN by exploiting the bulk formulae 36 on air-sea fluxes. Furthermore, the design of WenHai\u2019s architecture is guided by the characteristics of mesoscale eddies to better preserve their variabilities. As demonstrated below, these features make WenHai outperform state-of-the-art numerical and AI-based GOFSs in forecasting the eddying ocean. Results An AI-based eddy-resolving GOFS Trained on a state-of-the-art eddy-resolving (1/12\u00b0) global ocean reanalysis dataset 37 (See \u2018GLORYS reanalysis\u2019 in Methods), WenHai forecasts 1/12\u00b0 daily averaged sea surface height (SSH) and three-dimensional temperature, salinity, and horizontal current in the upper 643\u2009m across the global ocean in an autoregressive way. WenHai utilizes the Swin-Transformer 38 as its backbone. The training process is decomposed into two stages. WenHai is first pre-trained to minimize the loss for the one-day forecast. Then we adopt a finetune technique 20 to minimize the accumulative loss for a sequence of autoregressive forecasts over 5 days, which improves WenHai\u2019s performance at long forecast lead times. The architecture of WenHai\u00a0(Fig. 1) and its training details are elaborated in Methods (See \u2018WenHai model\u2019 in Methods). Fig. 1: A schematic of WenHai\u2019s architecture. The surface atmosphere and ocean variables on the day \\(t+l\\) are first combined to compute the air-sea momentum, heat, and freshwater fluxes based on the bulk formulae (green block), where t is an arbitrary date index and l is the forecast lead time (indexing in 1-day intervals). Then, the ocean variables and air-sea fluxes are sent to a deep neural network (red block) that forecasts temporal tendency between ocean variables on the days \\(t+l\\) and \\(t+l+1\\). The tendency field is added to the ocean variables on the day \\(t+l\\) to yield the ocean variable forecast on the day \\(t+l+1\\) \u00a0(blue block). Finally, the above processes are iterated to generate a sequence of forecasts. Maps created with Cartopy 65. Background land image provided by NASA Earth Observatory. Full size image We exploit domain knowledge in the air-sea interactions and ocean dynamics to guide WenHai\u2019s architecture design, which enhances its capacity to forecast the eddying ocean. First, to explicitly represent the atmospheric forcings, we implement a specialized block for computing air-sea momentum, heat and freshwater fluxes from the surface atmosphere variables (e.g., air temperature, winds, etc.) based on the bulk formulae (Supplementary Note\u00a0 1 and Supplementary Fig.\u00a0 S2). Second, the forecast output is chosen as the temporal tendency of an ocean variable between the two consecutive days rather than the ocean variable on the following day. This is essential to preserve the mesoscale eddy variabilities, as mesoscale eddies dominate the day-to-day variations not the absolute values of ocean variables (Supplementary Fig.\u00a0 S3). Similarly, we put more weight on the loss function in the upper part of the ocean, as mesoscale eddies are generally near-surface intensified 39. Finally, the land region is masked to make WenHai focus on forecasting the ocean variability. Supplementary Fig.\u00a0 S4 provides a visualization of sea surface kinetic energy (KE), sea surface height (SSH), and sea surface temperature (SST) forecast in the Kuroshio Extension region by WenHai. Here, the GLORYS reanalysis is approximated as the ground truth to test the capacity of WenHai to forecast the eddying ocean, with caution about the fidelity of the GLORYS reanalysis i",
          "original_query": "Forecasting the eddying ocean with a deep neural network",
          "cleaned_query": "Forecasting the eddying ocean with a deep neural network"
        },
        {
          "success": true,
          "title": "Mercator Ocean International Makes AI Ocean Forecasting ...",
          "url": "http://www.afp.com/en/infos/mercator-ocean-international-makes-ai-ocean-forecasting-operational-glonet-validated",
          "content": "Mercator Ocean International Makes AI Ocean Forecasting Operational with GLONET Validated by OceanBench at NeurIPS 2025 | AFP.com\n[Skip to main content](#main-content)\n[![](http://www.afp.com/themes/custom/afp9/assets/svg/logo-afp.svg)](http://www.afp.com/en)\n[Connexion](https://news.afp.com/#/u/login)\n[Contact us](http://www.afp.com/en/contact-us)\nen\n* [Fran\u00e7ais](http://www.afp.com/fr/node/3807251)\n* [English](http://www.afp.com/en/infos/mercator-ocean-international-makes-ai-ocean-forecasting-operational-glonet-validated)\n* [Espa\u00f1ol](http://www.afp.com/es/node/3807251)\n* [Portugu\u00eas](http://www.afp.com/pt/node/3807251)\n* [Deutsch](http://www.afp.com/de/node/3807251)\n* [\u0627\u0644\u0639\u0631\u0628\u064a\u0629](http://www.afp.com/ar/node/3807251)\n[Login](https://news.afp.com/#/u/login)\nPr\u00e9c\u00e9dent\n* Montreal (AFP)| 10/12/2025 - 16:06:30| Bank of Canada maintains key interest rate of 2.25 percent\n* Vienna (AFP)| 10/12/2025 - 15:24:17| Austrian court rejects Ukraine tycoon&#039;s US extradition: statement\n* Oslo (AFP)| 10/12/2025 - 14:22:29| Nobel peace laureate Machado says &#039;we must be willing to fight for freedom&#039;: speech delivered by daughter\n* Oslo (AFP)| 10/12/2025 - 14:16:24| Nobel peace laureate Machado blasts &#039;state terrorism&#039; in Venezuela: speech delivered by daughter\n* Oslo (AFP)| 10/12/2025 - 14:14:56| Nobel Peace Prize winner Machado will be back in Venezuela &#039;very soon&#039;: daughter\n* Oslo (AFP)| 10/12/2025 - 13:53:38| Daughter of absent Peace Prize laureate Machado accepts Nobel on her behalf\n* Oslo (AFP)| 10/12/2025 - 13:46:46| Nobel Peace Prize committee chair urges Venezuela&#039;s Maduro to &#039;step down&#039;\n* Kinshasa (AFP)| 10/12/2025 - 13:17:24| Burundi closes border with DR Congo after new M23 advance: security officials\n* Kinshasa (AFP)| 10/12/2025 - 13:11:53| Burundi closes border with DR Congo after new M23 advance: security officials\n* Paris (AFP)| 10/12/2025 - 12:26:31| Ukraine allies to hold Thursday video call: France\nSuivant\n[Previous](http://www.afp.com/en/infos/csg-xponent-sweeps-top-tier-analyst-industry-recognition-customer-journey-management)[Back to summary](http://www.afp.com/en/access-our-content/partners/partners/business-wire)[Next](http://www.afp.com/en/infos/netjets-bring-starlink-high-speed-connectivity-fleet)\n## Business Wire\n[![](https://www.afp.com/sites/default/files/afppartenaire/202003/logobusinesswire.png)](http://www.afp.com/en/access-our-content/partners/partners/business-wire)\n# Mercator Ocean International Makes AI Ocean Forecasting Operational with GLONET Validated by OceanBench at NeurIPS 2025\n10Dec 2025\n**SAN DIEGO &amp; TOULOUSE, France**\n**At NeurIPS 2025, one of the world\u2019s leading AI conferences, Mercator Ocean unveiled OceanBench, the first open benchmark designed to assess AI Ocean forecasting models. This milestone follows the publication of Mercator Ocean\u2019s AI model GLONET in September. Together, OceanBench and GLONET represent a decisive step toward integrating artificial intelligence into operational ocean forecasting, strengthening Europe\u2019s leadership in trustworthy, AI-driven predictions at global scale.**\nPresented by Anass El Aouni, Oceanographer and machine learning (ML) researcher, and Quentin Gaudel, ML Systems Architect, OceanBench establishes a unified standard to evaluate AI models against fundamental ocean dynamics. GLONET is the first forecasting system to be validated through OceanBench for operational use.\n**Anass El Aouni :**\u201c*OceanBench provides a transparent standard to evaluate AI-driven ocean forecasts*.*By comparing artificial intelligence with physics-based models, we can highlight strengths, identify areas for improvement, and guide the development of more reliable forecasting tools. There is no single \u2018best\u2019 model, but the benchmark ensures fair, science-based comparisons and helps users select the most suitable solution for each application.*\u201d\n**Quentin Gaudel:***\u201cArtificial intelligence is transforming the speed and scale of ocean prediction, but trust requires careful validation. OceanBench allows the community to rigorously measure performance while keeping AI models grounded in ocean physics. Combined with physics-based systems, AI can accelerate forecasts and broaden access to ocean information. This is critical for advancing operational forecasting and supporting Europe\u2019s Digital Twin Ocean vision.*\u201d\n**A new standard for AI Ocean prediction**\nWhile AI has revolutionized weather forecasts, its operational use in ocean forecasting has been limited by the lack of shared standards for quality, validation, and physical realism. OceanBench fills this gap by providing:\n* Open data and open-source tools for reproducible evaluation,\n* Three evaluation tracks: Model-to-Reanalysis, Model-to-Analysis, Model-to-Observations,\n* Physical diagnostics that verify forecasts against known ocean dynamics.\nThe first benchmark round includes four global forecasting systems: the high-resolution physics-based GLO12, and three ML ocean emulators \u2014GLONET, XiHe, and Wenhai. Early results indicate AI models deliver rapid, accurate forecasts in many areas, but high-resolution physics-based systems remain essential for capturing fine-scale and high-frequency ocean processes. This combined approach is key to building reliable, operational solutions.\n**GLONET, Mercator Ocean\u2019s new AI global forecasting system**\nPublished in September 2025, and following full validation through OceanBench, GLONET is ready for operational deployment**.**Its key features include:\n-**Daily 10-day global forecasts generated within seconds,\n**- Forecast variables including sea surface height, temperature, salinity, and currents across 21 depth levels,\n- Trained on Mercator Ocean\u2019s GLORYS12 reanalysis, ensuring physical consistency.\n**Marie Drevillon, Head of Operations at Mercator Ocean and co-author of OceanBench:**\u201c*With GLONET validated through OceanBench, AI becomes part of our operational forecasting toolkit. These capabilities enable faster simulations, broader access to ocean information, and ultimately more robust, science-based decision support.*\u201d\n**Supporting Europe\u2019s Digital Ocean Vision**\nThe launch of OceanBench and deployment of GLONET mark significant steps toward Europe\u2019s vision for a Digital Twin Ocean capable of providing fast, reliable, and physically robust predictions. By establishing a community-driven standard for AI evaluation, OceanBench strengthens Europe\u2019s leadership in trustworthy, operational AI ocean science.\nBy transitioning into an intergovernmental organization (IGO) representing its Member States, Mercator Ocean fosters long-term European collective action in support of this shared digital ocean vision.\n**Resources**\n* **OceanBench Paper**(NeurIPS 2025):[NeurIPS Poster OceanBench: A Benchmark for Data-Driven Global Ocean Forecasting systems](https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=https://neurips.cc/virtual/2025/loc/san-diego/poster/121394&amp;esheet=54372911&amp;newsitemid=20251210914358&amp;lan=en-US&amp;anchor=NeurIPS+Poster+OceanBench:+A+Benchmark+for+Data-Driven+Global+Ocean+Forecasting+systems&amp;index=1&amp;md5=a43adb211992ed0525f1efc4706f1de4)\n* **OceanBench Benchmark**:[https://oceanbench.lab.dive.edito.eu/](https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=https://oceanbench.lab.dive.edito.eu/&amp;esheet=54372911&amp;newsitemid=20251210914358&amp;lan=en-US&amp;anchor=https://oceanbench.lab.dive.edito.eu/&amp;index=2&amp;md5=b41baf90b63861eb500af5f26289df58)\n* **GLONET**:[GLONET: Mercator's End\u2010to\u2010End Neural Global Ocean Forecasting System - El Aouni - 2025 - Journal of Geophysical Research: Machine Learning and Computation - Wiley Online Library](https://cts.businesswire.com/ct/CT?id=smartlink&amp;url=https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2025JH000686&amp;esheet=54372911&amp;newsitemid=20251210914358&amp;lan=en-US&amp;anchor=GLONET:+Mercator's+End&#8208;to&#8208;End+Neural+Global+Ocean+Forecasting+System+-+El+Aouni+-+2025+-+Journal+of",
          "original_query": "Glonet: Mercator\u2019s end-to-end neural forecasting system",
          "cleaned_query": "Glonet: Mercator\u2019s end-to-end neural forecasting system",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "Accurate medium-range global weather forecasting with 3D ... - Nature",
          "url": "https://www.nature.com/articles/s41586-023-06185-3",
          "content": "Accurate medium-range global weather forecasting with 3D neural networks | Nature\n[Skip to main content](#content)\nThank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain\nthe best experience, we recommend you use a more up to date browser (or turn off compatibility mode in\nInternet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles\nand JavaScript.\nAdvertisement\n[![Nature](https://media.springernature.com/full/nature-cms/uploads/product/nature/header-86f1267ea01eccd46b530284be10585e.svg)](https://www.nature.com/)\n* [View all journals](https://www.nature.com/siteindex)\n* [Search](#search-menu)\n* [Log in](https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41586-023-06185-3?error=cookies_not_supported&code=c5cddab8-987b-4b7c-b53e-10f9daffb475)\n* [ContentExplore content](#explore)\n* [Aboutthe journal](#about-the-journal)\n* [Publishwith us](#publish-with-us)\n* [Sign up for alerts](https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41586)\n* [RSS feed](https://www.nature.com/nature.rss)\nAccurate medium-range global weather forecasting with 3D neural networks\n[Download PDF](https://www.nature.com/articles/s41586-023-06185-3.pdf)\n[Download PDF](https://www.nature.com/articles/s41586-023-06185-3.pdf)\n* Article\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:05 July 2023# Accurate medium-range global weather forecasting with 3D neural networks\n* [Kaifeng Bi](#auth-Kaifeng-Bi-Aff1)[ORCID:orcid.org/0009-0008-7872-1197](https://orcid.org/0009-0008-7872-1197)[1](#Aff1),\n* [Lingxi Xie](#auth-Lingxi-Xie-Aff1)[ORCID:orcid.org/0000-0003-4831-9451](https://orcid.org/0000-0003-4831-9451)[1](#Aff1),\n* [Hengheng Zhang](#auth-Hengheng-Zhang-Aff1)[ORCID:orcid.org/0009-0006-3241-6862](https://orcid.org/0009-0006-3241-6862)[1](#Aff1),\n* [Xin Chen](#auth-Xin-Chen-Aff1)[ORCID:orcid.org/0000-0003-3686-6212](https://orcid.org/0000-0003-3686-6212)[1](#Aff1),\n* [Xiaotao Gu](#auth-Xiaotao-Gu-Aff1)[ORCID:orcid.org/0009-0002-7896-4750](https://orcid.org/0009-0002-7896-4750)[1](#Aff1)&amp;\n* \u2026* [Qi Tian](#auth-Qi-Tian-Aff1)[ORCID:orcid.org/0000-0002-7252-5047](https://orcid.org/0000-0002-7252-5047)[1](#Aff1)Show authors\n[*Nature*](https://www.nature.com/)**volume619**,pages533\u2013538 (2023)[Cite this article](#citeas)\n* 313kAccesses\n* 1300Citations\n* 1905Altmetric\n* [Metricsdetails](https://www.nature.com/articles/s41586-023-06185-3/metrics)\n### Subjects\n* [Atmospheric dynamics](https://www.nature.com/subjects/atmospheric-dynamics)\n* [Computer science](https://www.nature.com/subjects/computer-science)\nAn[Author Correction](https://doi.org/10.1038/s41586-023-06545-z)to this article was published on 14 September 2023\nThis article has been[updated](#change-history)\n## Abstract\nWeather forecasting is important for science and society. At present, the most accurate forecast system is the numerical weather prediction (NWP) method, which represents atmospheric states as discretized grids and numerically solves partial differential equations that describe the transition between those states[1](https://www.nature.com/articles/s41586-023-06185-3#ref-CR1). However, this procedure is computationally expensive. Recently, artificial-intelligence-based methods[2](https://www.nature.com/articles/s41586-023-06185-3#ref-CR2)have shown potential in accelerating weather forecasting by orders of magnitude, but the forecast accuracy is still significantly lower than that of NWP methods. Here we introduce an artificial-intelligence-based method for accurate, medium-range global weather forecasting. We show that three-dimensional deep networks equipped with Earth-specific priors are effective at dealing with complex patterns in weather data, and that a hierarchical temporal aggregation strategy reduces accumulation errors in medium-range forecasting. Trained on 39\u2009years of global data, our program, Pangu-Weather, obtains stronger deterministic forecast results on reanalysis data in all tested variables when compared with the world\u2019s best NWP system, the operational integrated forecasting system of the European Centre for Medium-Range Weather Forecasts (ECMWF)[3](https://www.nature.com/articles/s41586-023-06185-3#ref-CR3). Our method also works well with extreme weather forecasts and ensemble forecasts. When initialized with reanalysis data, the accuracy of tracking tropical cyclones is also higher than that of ECMWF-HRES.\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41598-022-11936-9/MediaObjects/41598_2022_11936_Fig1_HTML.png)\n### [ECMWF short-term prediction accuracy improvement by deep learning](https://www.nature.com/articles/s41598-022-11936-9?fromPaywallRec=false)\nArticleOpen access12 May 2022\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs43247-025-02502-y/MediaObjects/43247_2025_2502_Fig1_HTML.png)\n### [The operational medium-range deterministic weather forecasting can be extended beyond a 10-day lead time](https://www.nature.com/articles/s43247-025-02502-y?fromPaywallRec=false)\nArticleOpen access03 July 2025\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs41467-025-62024-1/MediaObjects/41467_2025_62024_Fig1_HTML.png)\n### [A data-to-forecast machine learning system for global weather](https://www.nature.com/articles/s41467-025-62024-1?fromPaywallRec=false)\nArticleOpen access19 July 2025\n## Main\nWeather forecasting is an important application of scientific computing that aims to predict future weather changes, especially in regards to extreme weather events. In the past decade, high-performance computing systems have greatly accelerated research in the field of numerical weather prediction (NWP) methods[1](https://www.nature.com/articles/s41586-023-06185-3#ref-CR1). Conventional NWP methods are primarily concerned with describing the transitions between discretized grids of atmospheric states using partial differential equations (PDEs) and then solving them with numerical simulations[4](#ref-CR4),[5](#ref-CR5),[6](https://www.nature.com/articles/s41586-023-06185-3#ref-CR6). These methods are often slow; a single simulation for a ten-day forecast can take hours of computation in a supercomputer that has hundreds of nodes[7](https://www.nature.com/articles/s41586-023-06185-3#ref-CR7). In addition, conventional NWP algorithms largely rely on parameterization, which uses approximate functions to capture unresolved processes, where errors can be introduced by approximation[8](https://www.nature.com/articles/s41586-023-06185-3#ref-CR8),[9](https://www.nature.com/articles/s41586-023-06185-3#ref-CR9).\nThe rapid development of deep learning[10](https://www.nature.com/articles/s41586-023-06185-3#ref-CR10)has introduced a promising direction, which the scientific community refers to as artificial intelligence (AI)-based methods[2](https://www.nature.com/articles/s41586-023-06185-3#ref-CR2),[11](#ref-CR11),[12](#ref-CR12),[13](#ref-CR13),[14](#ref-CR14),[15](#ref-CR15),[16](https://www.nature.com/articles/s41586-023-06185-3#ref-CR16). Here, the methodology is to train a deep neural network to capture the relationship between the input (reanalysis weather data at a given point in time) and the output (reanalysis weather data at the target point in time). On specialized computational devices such as graphics processing units (GPUs), AI-based methods are extremely fast. To give a recent example, FourCastNet[2](https://www.nature.com/articles/s41586-023-06185-3#ref-CR2)takes only 7\u2009s to compute a 100-member, 24-hour forecast, which is orders of magnitudes faster than conventional NWP methods. However, the accuracy of FourCastNet is still below satisfactory; its root mean square error (RMSE) of a 5-day Z500\u00a0(500\u00a0hPa geopoten",
          "original_query": "Accurate medium-range global weather forecasting with 3D neural networks",
          "cleaned_query": "Accurate medium-range global weather forecasting with 3D neural networks"
        },
        {
          "success": true,
          "title": "Enhanced monitoring of life in the sea is a critical component of ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S0308597X21003109",
          "content": "[Skip to main content](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#screen-reader-main-content) [Skip to article](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#screen-reader-main-title)\n\n- [View\u00a0**PDF**](https://www.sciencedirect.com/science/article/pii/S0308597X21003109/pdfft?md5=7b8fe47edbd12d5cf06a76dd4e9573cf&pid=1-s2.0-S0308597X21003109-main.pdf)\n- Download full issue\n- [View Open Manuscript](https://www.sciencedirect.com/science/article/am/pii/S0308597X21003109)\n- Other access options\n\n\nSearch ScienceDirect\n\n## Outline\n\n01. [Highlights](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#ab0015)\n02. [Abstract](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#ab0010)\n03. [Keywords](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#keys0005)\n04. [1\\. The importance of life in the sea](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#sec0005)\n05. [2\\. Societal challenges addressed by ocean observations](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#sec0010)\n06. [3\\. Next steps](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#sec0035)\n07. [4\\. Criteria for success: ocean monitoring in national accounting](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#sec0040)\n08. [5\\. Conclusions](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#sec0045)\n09. [Acknowledgments](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#ack0005)\n10. [References](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#bibliog0005)\n\nShow full outline\n\n## [Cited by (25)](https://www.sciencedirect.com/science/article/pii/S0308597X21003109\\#section-cited-by)\n\n## Figures (2)\n\n1. [![Fig. 1. The value chain of ocean observing](https://ars.els-cdn.com/content/image/1-s2.0-S0308597X21003109-gr1.sml)](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#fig0005)\n2. [![Fig. 2. Proposed elements to implement a multidisciplinary ocean observing system that\u2026](https://ars.els-cdn.com/content/image/1-s2.0-S0308597X21003109-gr2.sml)](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#fig0010)\n\n## Tables (1)\n\n1. [Table 1](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#tbl0005)\n\n[![Elsevier](https://sdfestaticassets-us-east-1.sciencedirectassets.com/prod/0252b05f1c89f902c3409111ceb5a6d9843d1f5b/image/elsevier-non-solus.png)](https://www.sciencedirect.com/journal/marine-policy)\n\n## [Marine Policy](https://www.sciencedirect.com/journal/marine-policy)\n\n[Volume 132](https://www.sciencedirect.com/journal/marine-policy/vol/132/suppl/C), October 2021, 104699\n\n[![Marine Policy](https://ars.els-cdn.com/content/image/1-s2.0-S0308597X21X00085-cov150h.gif)](https://www.sciencedirect.com/journal/marine-policy/vol/132/suppl/C)\n\n# Enhanced monitoring of life in the sea is a critical component of conservation management and sustainable economic growth\n\nAuthor links open overlay panelMauriceEstesJr.a1, ClarissaAndersonb2, WardAppeltansc3, NicBaxd4, NinaBednar\u0161eke5, GabrielleCanonicof6, SamyDjavidniag7, ElvaEscobarh8, PeerFietzeki9, MarilaureGregoirej10, ElliottHazenk11, MariaKavanaughl12, FranckLejzerowiczm13, FabienLombardn14, PatriciaMiloslavicho15, Klas O.M\u00f6llerp16, JacquomoMonkq17, EnriqueMontesr18, HassanMoustahfids19, Monica M.C.Muelbertt20\u2026Lauren V.Weatherdonaa26\n\nShow more\n\nOutline\n\nAdd to Mendeley\n\nShare\n\nCite\n\n[https://doi.org/10.1016/j.marpol.2021.104699](https://doi.org/10.1016/j.marpol.2021.104699) [Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&contentID=S0308597X21003109&orderBeanReset=true)\n\nUnder a Creative Commons [license](http://creativecommons.org/licenses/by/4.0/)\n\nopen access\n\n## Highlights\n\n- \u2022\nDiversity of life is a fundamental characteristic of our planet, from its genes and cells to organisms and populations.\n\n- \u2022\nA growing human population will be even more dependent on marine organisms by 2030 and beyond.\n\n- \u2022\nMore biological data is needed in global observing systems to monitor and manage marine life and associated biodiversity.\n\n- \u2022\nBest practices are fundamental to consistent monitoring from place to place and accurate change detection over time.\n\n- \u2022\nThe Ocean Decade offers an opportunity to advance human capacity in developed/ underdeveloped regions to conserve marine life.\n\n\n## Abstract\n\nMarine biodiversity is a fundamental characteristic of our planet that depends on and influences climate, water quality, and many ocean state variables. It is also at the core of ecosystem services that can make or break economic development in any region. Our purpose is to highlight the need for marine biological observations to inform science and conservation management and to support the blue economy. We provide ten recommendations, applicable now, to measure and forecast biological Essential Ocean Variables (EOVs) as part of economic monitoring efforts. The UN Decade of Ocean Science for Sustainable Development (2021\u20132030) provides a timely opportunity to implement these recommendations to benefit humanity and enable the USD 3 trillion global ocean economy expected by 2030.\n\n- [Previous article in issue](https://www.sciencedirect.com/science/article/pii/S0308597X2100316X)\n- [Next article in issue](https://www.sciencedirect.com/science/article/pii/S0308597X21003134)\n\n## Keywords\n\nBiodiversity\n\nEssential ocean variables\n\nMarine biological data\n\nSustainable\n\nBlue economy\n\nEcological forecasting\n\n## 1\\. The importance of life in the sea\n\nDiversity of life is a fundamental characteristic of our planet, from its genes, cells, and organs, to organisms, populations, and communities. Feedback between life and the environment affects evolutionary and adaptive processes as well as global climate. In the sea, which represents about 95% of the biosphere, biodiversity depends on and influences water quality, other ocean state variables (temperature and nutrient, carbon, and oxygen concentration), and bottom structure (such as reefs). Our economies and well-being are linked to the diversity of life in the sea [\\[1\\]](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#bib1). The Organisation for Economic Co-operation and Development (OECD) conservatively valued the global ocean economy in 2010 at USD 1.5 trillion, and projects that this will double to over USD 3 trillion by 2030 [\\[2\\]](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#bib2). Tourism, recreation, aquaculture, fisheries, renewable energy, shipping, biotechnology, carbon capture and other industries depend on goods and services derived from natural capital assets [\\[3\\]](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#bib3). These industries lead in employment growth worldwide. Crucially, these industries depend on particular balances between different populations of organisms in ecological communities. Biological diversity is the foundation for the healthy production of food, novel energy resources, materials, and pharmaceuticals, as well as for tourism and cultural activities of many human societies around the world. It is in the best interest of societies everywhere to forecast abrupt or gradual changes to species and ecosystems we depend upon, and to detect and track organisms that can negatively alter ecosystem services (e.g., pathogenic, toxic, or invasive species), that pose a risk to food security or safety, and that affect the resilience of coastal biological and human communities. Monitoring changes in biodiversity is an important proxy for evaluating changes in ecosystem services [\\[4\\]](https://www.sciencedirect.com/science/article/pii/S0308597X21003109#bib4).\n\nA growing human population will be even more dependent on marine organisms by 2030 and beyond. Evaluating the proper uses of biodiversity for ecosystem health, economic performance, business operatio",
          "original_query": "Better informed marine operations and management: Multidisciplinary efforts in ocean forecasting research for socio-economic benefit",
          "cleaned_query": "Better informed marine operations and management: Multidisciplinary efforts in ocean forecasting research for socio-economic benefit"
        },
        {
          "success": true,
          "title": "[2402.02995] XiHe: A Data-Driven Model for Global Ocean ...",
          "url": "https://arxiv.org/abs/2402.02995",
          "content": "Authors: [Xiang Wang](https://arxiv.org/search/physics?searchtype=author&query=Wang,+X), [Renzhi Wang](https://arxiv.org/search/physics?searchtype=author&query=Wang,+R), [Ningzi Hu](https://arxiv.org/search/physics?searchtype=author&query=Hu,+N), [Pinqiang Wang](https://arxiv.org/search/physics?searchtype=author&query=Wang,+P), [Peng Huo](https://arxiv.org/search/physics?searchtype=author&query=Huo,+P), [Guihua Wang](https://arxiv.org/search/physics?searchtype=author&query=Wang,+G), [Huizan Wang](https://arxiv.org/search/physics?searchtype=author&query=Wang,+H), [Senzhang Wang](https://arxiv.org/search/physics?searchtype=author&query=Wang,+S), [Junxing Zhu](https://arxiv.org/search/physics?searchtype=author&query=Zhu,+J), [Jianbo Xu](https://arxiv.org/search/physics?searchtype=author&query=Xu,+J), [Jun Yin](https://arxiv.org/search/physics?searchtype=author&query=Yin,+J), [Senliang Bao](https://arxiv.org/search/physics?searchtype=author&query=Bao,+S), [Ciqiang Luo](https://arxiv.org/search/physics?searchtype=author&query=Luo,+C), [Ziqing Zu](https://arxiv.org/search/physics?searchtype=author&query=Zu,+Z), [Yi Han](https://arxiv.org/search/physics?searchtype=author&query=Han,+Y), [Weimin Zhang](https://arxiv.org/search/physics?searchtype=author&query=Zhang,+W), [Kaijun Ren](https://arxiv.org/search/physics?searchtype=author&query=Ren,+K), [Kefeng Deng](https://arxiv.org/search/physics?searchtype=author&query=Deng,+K), [Junqiang Song](https://arxiv.org/search/physics?searchtype=author&query=Song,+J)\n\n[View PDF](https://arxiv.org/pdf/2402.02995) [HTML (experimental)](https://arxiv.org/html/2402.02995v4)\n\n> Abstract:The leading operational Global Ocean Forecasting Systems (GOFSs) use physics-driven numerical forecasting models that solve the partial differential equations with expensive computation. Recently, specifically in atmosphere weather forecasting, data-driven models have demonstrated significant potential for speeding up environmental forecasting by orders of magnitude, but there is still no data-driven GOFS that matches the forecasting accuracy of the numerical GOFSs. In this paper, we propose the first data-driven 1/12\u00b0 resolution global ocean eddy-resolving forecasting model named XiHe, which is established from the 25-year France Mercator Ocean International's daily GLORYS12 reanalysis data. XiHe is a hierarchical transformer-based framework coupled with two special designs. One is the land-ocean mask mechanism for focusing exclusively on the global ocean circulation. The other is the ocean-specific block for effectively capturing both local ocean information and global teleconnection. Extensive experiments are conducted under satellite observations, in situ observations, and the IV-TT Class 4 evaluation framework of the world's leading operational GOFSs from January 2019 to December 2020. The results demonstrate that XiHe achieves stronger forecast performance in all testing variables than existing leading operational numerical GOFSs including Mercator Ocean Physical SYstem (PSY4), Global Ice Ocean Prediction System (GIOPS), BLUElinK OceanMAPS (BLK), and Forecast Ocean Assimilation Model (FOAM). Particularly, the accuracy of ocean current forecasting of XiHe out to 60 days is even better than that of PSY4 in just 10 days. Additionally, XiHe is able to forecast the large-scale circulation and the mesoscale eddies. Furthermore, it can make a 10-day forecast in only 0.35 seconds, which accelerates the forecast speed by thousands of times compared to the traditional numerical GOFSs.\n\n## Submission history\n\nFrom: Xiang Wang \\[ [view email](https://arxiv.org/show-email/dc761f38/2402.02995)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2402.02995v1)**\nMon, 5 Feb 2024 13:34:19 UTC (11,304 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2402.02995v2)**\nThu, 8 Feb 2024 14:13:56 UTC (11,300 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/2402.02995v3)**\nTue, 20 Aug 2024 12:27:13 UTC (18,708 KB)\n\n**\\[v4\\]**\nTue, 22 Oct 2024 09:29:56 UTC (18,762 KB)",
          "original_query": "Xihe: A data-driven model for global ocean eddy-resolving forecasting",
          "cleaned_query": "Xihe: A data-driven model for global ocean eddy-resolving forecasting"
        }
      ],
      "generated_ideas": [
        "Assimilation-Aware Training for AI GOFS (WenHai/XiHe/GLONET)\nTrain an ocean-forecast DNN with an explicit \u201canalysis step\u201d module that mimics sequential data assimilation (e.g., ROAF/ROIF-style reduced-order updates) so the network learns to optimally incorporate sparse observations at inference time. Evaluate on OceanBench\u2019s Model-to-Observations track by inserting synthetic Argo/altimetry updates during rollouts and measuring error growth versus purely free-running AI forecasts.",
        "Hybrid Reduced-Order Information Filter + Transformer for Uncertainty-Aware Ocean Forecasts\nBuild an operational ensemble system where a transformer provides a deterministic forecast and a learned reduced-order covariance model (inspired by ROIF/ROAF) propagates uncertainty in a low-dimensional subspace of dominant mesoscale modes. Output calibrated probabilistic products (e.g., SSH/current quantiles) and validate reliability/sharpness on OceanBench alongside deterministic skill.",
        "Multi-Forcing AI Ocean Forecasting with Learned Bulk-Flux Closure\nExtend WenHai\u2019s explicit forcing pathway by replacing fixed bulk formulae with a learnable, constrained air\u2013sea flux closure that ingests atmospheric variables (winds, humidity, radiation) and predicts surface momentum/heat/freshwater fluxes consistent with physical bounds. Compare fixed vs learned closures for mixed-layer temperature and surface current skill, especially during extreme wind/heat-flux events.",
        "Benchmark-Driven \u201cTrustworthiness Suite\u201d for Operational AI Ocean Forecasts\nBuild an evaluation layer on top of OceanBench that flags physically implausible forecasts (mass/heat content drift, spurious currents near coasts, unrealistic spectra) and links failures to interpretable model components (forcing module, attention blocks, mask handling). The key contribution is a standardized, automated set of operational acceptance tests\u2014analogous to QC in data assimilation\u2014supporting safe deployment of GLONET/WenHai/XiHe-class models."
      ],
      "generated_ideas_raw": "1. **Assimilation-Aware Training for AI GOFS (WenHai/XiHe/GLONET)**\n   Train an ocean-forecast DNN with an explicit \u201canalysis step\u201d module that mimics sequential data assimilation (e.g., ROAF/ROIF-style reduced-order updates) so the network learns to optimally incorporate sparse observations at inference time. Evaluate on OceanBench\u2019s Model-to-Observations track by inserting synthetic Argo/altimetry updates during rollouts and measuring error growth versus purely free-running AI forecasts.\n\n2. **Hybrid Reduced-Order Information Filter + Transformer for Uncertainty-Aware Ocean Forecasts**\n   Build an operational ensemble system where a transformer provides a deterministic forecast and a learned reduced-order covariance model (inspired by ROIF/ROAF) propagates uncertainty in a low-dimensional subspace of dominant mesoscale modes. Output calibrated probabilistic products (e.g., SSH/current quantiles) and validate reliability/sharpness on OceanBench alongside deterministic skill.\n\n3. **Physics-Constrained Eddy Preservation Losses Using OceanBench Diagnostics**\n   Design training objectives that directly penalize loss of mesoscale variability (the \u201cblurring\u201d problem) by matching kinetic-energy spectra, vorticity statistics, and eddy counts/lifetimes derived from SSH and currents. Implement these as differentiable spectral/structure-function losses and test whether WenHai/XiHe-like models retain realistic eddy energetics at 5\u201310 day and extended (30\u201360 day) leads.\n\n4. **Multi-Forcing AI Ocean Forecasting with Learned Bulk-Flux Closure**\n   Extend WenHai\u2019s explicit forcing pathway by replacing fixed bulk formulae with a learnable, constrained air\u2013sea flux closure that ingests atmospheric variables (winds, humidity, radiation) and predicts surface momentum/heat/freshwater fluxes consistent with physical bounds. Compare fixed vs learned closures for mixed-layer temperature and surface current skill, especially during extreme wind/heat-flux events.\n\n5. **Cross-Model Distillation: Numerical GOFS \u2192 AI with Process-Specific Fidelity**\n   Create a teacher\u2013student framework where high-resolution physics-based GOFS (e.g., GLO12-like) teaches an AI model not only state variables but also process tendencies (advection, mixing proxies) estimated from successive states. The contribution is an actionable recipe for distilling fine-scale/high-frequency dynamics into fast AI models and quantifying which processes remain irreducibly physics-dependent under OceanBench diagnostics.\n\n6. **Observation-Targeted Adaptive Sampling Guided by AI Forecast Sensitivities**\n   Use gradients/attention maps from transformer-based GOFS to identify where new observations (altimeter tracks, Argo float deployments, gliders) would most reduce forecast error for mesoscale eddies over the next 5\u201310 days. Couple this with a reduced-order filter (ROAF/ROIF) to translate sensitivity into expected analysis improvement, yielding a deployable adaptive observing strategy aligned with the \u201cvalue chain\u201d of ocean observing.\n\n7. **Biological EOV Nowcasting from AI Physical Forecasts via Coupled Learning**\n   Build a two-stage system: (i) AI GOFS forecasts physical fields (T/S/currents/SSH), (ii) a biological emulator predicts selected biological Essential Ocean Variables (e.g., chlorophyll-a, plankton functional types, habitat indices) using the physical forecasts plus available remote sensing. This directly operationalizes Paper 5\u2019s call for biological monitoring by producing short-term ecological forecasts with uncertainty, validated where in situ and satellite biology products exist.\n\n8. **Multi-Depth 3D Neural Architectures for Upper-Ocean Stratification Events**\n   Inspired by Pangu-Weather\u2019s 3D networks and WenHai\u2019s 0\u2013643 m targets, develop a true 3D ocean network that models vertical coupling explicitly (rather than depth-level token stacks) to better capture rapid restratification, barrier-layer formation, and mixed-layer deepening. Benchmark improvements on salinity/temperature vertical structure errors and event-based metrics (e.g., mixed-layer depth extremes) across basins.\n\n9. **Domain-Consistent Coastal Extension with Masked Tokens and Super-Resolution**\n   Extend XiHe\u2019s land\u2013ocean mask mechanism to a coastal-focused architecture that (a) learns consistent boundary behavior near complex coastlines and (b) super-resolves from 1/12\u00b0 open-ocean forecasts to finer coastal grids using shoreline-aware tokenization. The deliverable is a practical pathway to bring global AI GOFS skill into coastal management applications (navigation, hazards) where numerical models often struggle or are too costly.\n\n10. **Benchmark-Driven \u201cTrustworthiness Suite\u201d for Operational AI Ocean Forecasts**\n   Build an evaluation layer on top of OceanBench that flags physically implausible forecasts (mass/heat content drift, spurious currents near coasts, unrealistic spectra) and links failures to interpretable model components (forcing module, attention blocks, mask handling). The key contribution is a standardized, automated set of operational acceptance tests\u2014analogous to QC in data assimilation\u2014supporting safe deployment of GLONET/WenHai/XiHe-class models.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Assimilation-Aware Training for AI GOFS (WenHai/XiHe/GLONET)\nTrain an ocean-forecast DNN with an explicit \u201canalysis step\u201d module that mimics sequential data assimilation (e.g., ROAF/ROIF-style reduced",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Hybrid Reduced-Order Information Filter + Transformer for Uncertainty-Aware Ocean Forecasts\nBuild an operational ensemble system where a transformer provides a deterministic forecast and a learned red",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Multi-Forcing AI Ocean Forecasting with Learned Bulk-Flux Closure\nExtend WenHai\u2019s explicit forcing pathway by replacing fixed bulk formulae with a learnable, constrained air\u2013sea flux closure that inge",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Benchmark-Driven \u201cTrustworthiness Suite\u201d for Operational AI Ocean Forecasts\nBuild an evaluation layer on top of OceanBench that flags physically implausible forecasts (mass/heat content drift, spuriou",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 53,
      "paper_title": "Superposition Yields Robust Neural Scaling",
      "contribution": "The paper identifies representation superposition as a central driver of neural scaling laws, providing insights into the conditions under which these scaling laws can be enhanced or may break down.",
      "num_predecessors": 3,
      "predecessors_crawled": 3,
      "academic_sources": 3,
      "crawl_rate": 1.0,
      "ideas_generated": 4,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 6267,
      "output_tokens": 942,
      "predecessor_details": [
        {
          "success": true,
          "title": "[PDF] Revisiting Scaling Laws for Language Models: The Role of Data ...",
          "url": "https://aclanthology.org/2025.acl-long.1163.pdf",
          "content": "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 23881\u201323899\nJuly 27 - August 1, 2025 \u00a92025 Association for Computational Linguistics\nRevisiting Scaling Laws for Language Models: The Role of Data Quality\nand Training Strategies\nZhengyu Chen1*, Siqi Wang 2*, Teng Xiao3, Yudong Wang 4,\nShiqi Chen 5, Xunliang Cai1, Junxian He 5, Jingang Wang1\n1 Meituan Inc. 2 The University of Hong Kong 3 Pennsylvania State University\n4 Peking University 5 The Hong Kong University of Science and Technology\n{chenzhengyu04,wangjingang02}@meituan.com\nAbstract\nTraditional scaling laws in natural language\nprocessing suggest that increasing model size\nand training data enhances performance. How\u0002ever, recent studies reveal deviations, partic\u0002ularly in large language models, where per\u0002formance improvements decelerate\u2014a phe\u0002nomenon known as sub-scaling. This paper\nrevisits these scaling laws by examining the\nimpact of data quality and training strategies\non model performance. Through extensive em\u0002pirical analysis of over 400 models, we iden\u0002tify high data density and non-optimal resource\nallocation as key factors contributing to sub\u0002scaling. High data density leads to diminishing\nreturns due to redundant information, while\noptimal resource allocation is crucial for sus\u0002tained performance improvements. We propose\na sub-optimal scaling law that better predicts\nperformance in sub-scaling regimes, highlight\u0002ing the importance of data quality and diversity.\n1 Introduction\nThe rapid advancement in natural language pro\u0002cessing (NLP) has been significantly driven by the\ndevelopment of increasingly large language mod\u0002els. These models, such as LLaMA (Touvron et al.,\n2023), Chinchilla (70B) (Hoffmann et al., 2022),\nGopher (280B) (Rae et al., 2021), and Megatron\u0002Turing NLG (530B) (Smith et al., 2022), have\nset new benchmarks across a variety of linguistic\ntasks. There is also a growing body of research on\nscaling strategies (McCandlish et al., 2018; Yang\net al., 2022, 2023; Wang et al., 2024), which could\nbe beneficial for large language models (LLMs).\nThe conventional wisdom suggests that augmenting\nmodel size and corresponding training data gener\u0002ally results in enhanced performance. This trend\nhas led to the popularization of a \u2019bigger is better\u2019\nparadigm within the field. This scaling up has been\n*Equal Contribution.\ndriven by the empirical observation that larger mod\u0002els trained on vast amounts of data tend to perform\nbetter on various natural language processing tasks\n(Komatsuzaki, 2019; Hernandez et al., 2022a).\nHowever, recent empirical studies (Hernandez\net al., 2022a; Hu et al., 2023; Porian et al., 2024;\nMuennighoff et al., 2024) have observed devia\u0002tions from this expected trend, particularly in the\ncontext of exceptionally large language models.\nThese deviations manifest as sub-scaling growth,\nwhere the rate of performance improvement de\u0002celerates as the model or dataset size continues to\nincrease. Specifically, (Hernandez et al., 2022a;\nMuennighoff et al., 2024) observe that sub-scaling\noccurs in scenarios involving repeated training data,\nleading to diminishing returns in performance. (Hu\net al., 2023) highlight that sub-scaling is particu\u0002larly pronounced in tasks requiring complex reason\u0002ing or multi-step processes. Furthermore, (Porian\net al., 2024) find that sub-scaling exists under non\u0002optimal training strategies with sub-optimal hyper\u0002parameters. Figure 1 provides a visualization of the\ndiminishing returns, clearly showing that as train\u0002ing progresses, the actual training loss values tend\nto be higher than those extrapolated from earlier\nstages, indicating how traditional scaling laws fall\nshort when dealing with extensive datasets and sug\u0002gests the need for a modified approach. Moreover,\n(Hernandez et al., 2022a; Muennighoff et al., 2024)\nhave similar sub-scaling observations in repeated\ndata and non-optimal training strategy. However,\nthere is a lack of systematic research on the sub\u0002scaling behavior of large language models (LLMs).\nFurther extending this observation to model per\u0002formance, Figure 2 displays the results of our tests\non the performance scaling law (Yang et al., 2024;\nIsik et al., 2024; Wu and Tang, 2024) with LLaMA\n2 and LLaMA 3 models. Despite LLaMA 3 incor\u0002porating advanced training strategies and improved\ndata quality, the performance improvements from\nLLaMA 2 to LLaMA 3 decelerate as the training\n23881\nFigure 1: Sub-scaling phenomenon in loss. Scaling law fits well with 5B training tokens, but as tokens increase,\nloss curve shows greater curvature, and fitting accuracy decreases, especially for larger models.\nFigure 2: (left) LLaMA 2\u2019s scaling curve outperforms LLaMA 3\u2019s, despite LLaMA 3\u2019s advanced strategies. (right)\nHigher-density datasets lead to sub-scaling. We propose metric density to measure redundancy and diversity: higher\ndensity indicates more redundancy and less diversity, leading to sub-scaling (see Section 2.1).\n10\n7 10\n8 109 1010\nModel Size\n2.5\n3.0\n3.5\n4.0\nLoss\n1e+18\n3e+18\n6e+18\n1e+19\n3e+19\n6e+19\n1e+20\n3e+20\nOptimal Model/Data Allocation\nScaling Law\n10\n6 10\n7 10\n8 109 1010 1011 1012\nModel Size\n2.0\n2.5\n3.0\n3.5\n4.0\nLoss\nLlama3 80B-1.5T\nLlama3 8B-15T\n1e+18\n3e+18\n6e+18\n1e+19\n3e+19\n6e+19\n1e+20\n3e+20\nScaling Law\nFigure 3: (left) With a fixed total compute budget, we adjust the model-to-data allocation ratio and plot the training\nloss against model size. A black curve connects the minimum points of each curve, illustrating the optimal\nChinchilla law. (right) However, current large language models, such as Llama3 8B, are trained on 15T tokens, with\na model-to-data allocation strategy that significantly deviates from the optimal Chinchilla law.\nflops increase, LLaMA 2 with 70B parameters out\u0002performs LLaMA 3 with 8B parameters.This dis\u0002crepancy, depicted in Figure 2, underscores the\ninadequacies of traditional scaling laws. Addition\u0002ally, when the scale of training data surpasses an\noptimal threshold relative to the available computa\u0002tional resources, sub-scaling law happens with such\nover-training (Gadre et al., 2024), potentially lead\u0002ing to diminishing returns in model performance.\nMoreover, there is a lack of understanding of the\ntraining dynamics of large language models and\nthe sub-scaling laws governing the training strate\u0002gies of language models. This motivates the ques\u0002tion: Under what conditions do sub-scaling laws\ninfluence the performance and efficiency of large\nlanguage models?\nThis study aims to systematically investigate the\nsub-scaling law phenomenon through an extensive\nempirical analysis involving over 400 models, rang\u0002ing from 20 million to 7 billion parameters, with\nvarying datasets and training strategies. Our find\u0002ings indicate that sub-scaling laws arise primarily\n23882\nfrom high data density and non-optimal training\nresource allocations. Specifically, we observed that\nboth factors contribute more significantly to per\u0002formance deceleration than previously anticipated.\nWe examine the sub-scaling phenomenon from two\nperspectives: data density and training strategy.\nHigh data density leads to diminishing marginal\ngains in performance as shown in Figure 2, while\noptimal resource allocation is crucial for sustain\u0002ing performance improvements as shown in Figure\n3. Further, we propose a sub-optimal scaling law\nthat generalizes the Chinchilla scaling law (Hoff\u0002mann et al., 2022) to better predict performance\nand loss in sub-scaling regimes. Our analysis re\u0002veals that the quality and diversity of training data\nare paramount, often outweighing the benefits of\nmere scale in model size. Key findings from our\nstudy include:\n1. Sub-Scaling Law Phenomenon: Traditional\nscaling laws fail to predict performance improve\u0002ments for large models and datasets. The per\u0002formance gains decelerate, leading to sub-scaling\ngrowth, especially in high data density scenarios\nand with non-optimal resource allocation.\n2. Training Strategies under Over-Training:\nCompared to Gadre et a",
          "original_query": "Scaling laws for neural language models",
          "cleaned_query": "Scaling laws for neural language models"
        },
        {
          "success": true,
          "title": "Scaling laws from the data manifold dimension - ACM Digital Library",
          "url": "https://dl.acm.org/doi/abs/10.5555/3586589.3586598",
          "content": "Scaling laws from the data manifold dimension | The Journal of Machine Learning Research[skip to main content](#skip-to-main-content)\n[](#global-menu)\nSearch ACM Digital Library\nSearchSearch\n[Advanced Search](https://dl.acm.org/search/advanced)\n[The Journal of Machine Learning Research](#)\n**## Export Citations\nSelect Citation formatBibTeXEndNoteACM Ref**\n* Please download or close your previous search result export first before starting a new bulk export.\nPreview is not available.\nBy clicking download,**a status dialog**will open to start the export process. The process may take**a few minutes**but once it finishes a file will be downloadable from your browser. You may continue to browse the DL while the export process is in progress.\n* ```\n```\n* [Download citation**](javascript:void(0))\n* [Copy citation**](javascript:void(0))\nresearch-article\n**Free access\nShare on\n* **\n* **\n* **\n* **\n* **\n# Scaling laws from the data manifold dimension\nAUTHORs:[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)UtkarshSharma](#artseq-00001),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)JaredKaplan](#artseq-00002)[Authors Info &amp; Claims](#tab-contributors)\n[The Journal of Machine Learning Research,Volume23,Issue1](https://dl.acm.org/toc/jmlr/2022/23/1)\nArticle No.: 9, Pages343-376\nPublished:01 January 2022[Publication History](#core-history)\n**0citation**328Downloads\nMetrics\n[\nTotal Citations0\n](#tab-citations)[\nTotal Downloads328\n](#tab-metrics-inner)\nLast 12 Months113\nLast 6 weeks14\n**Get Citation Alerts\n**## New Citation Alert added!\nThis alert has been successfully added and will be sent to:\nYou will be notified whenever a record that you have chosen has been cited.\nTo manage your alert preferences, click on the button below.\n[Manage my Alerts](https://dl.acm.org/action/showPreferences?menuTab=Alerts)\n**## New Citation Alert!\nPlease[log in to your account](https://dl.acm.org/action/showLogin?redirectUri=/doi/abs/10.5555/3586589.3586598)\n**\n**\n[**PDF](https://dl.acm.org/doi/pdf/10.5555/3586589.3586598)[**eReader](https://dl.acm.org/doi/epdf/10.5555/3586589.3586598)[**Publisher Site](https://www.jmlr.org/papers/volume23/20-1111/20-1111.pdf)\n**Contents\n## Abstract\nWhen data is plentiful, the test loss achieved by well-trained neural networks scales as a power-law*L*\u221e*N*-\u03b1in the number of network parameters*N*. This empirical scaling law holds for a wide variety of data modalities, and may persist over many orders of magnitude. The scaling law can be explained if neural models are effectively just performing regression on a data manifold of intrinsic dimension d. This simple theory predicts that the scaling exponents \u03b1\u22484/*d*for cross-entropy and mean-squared error losses. We confirm the theory by independently measuring the intrinsic dimension and the scaling exponents in a teacher/student framework, where we can study a variety of*d*and \u03b1by dialing the properties of random teacher networks. We also test the theory with CNN image classifiers on several datasets and with GPT-type language models.\n## Formats available\nYou can view the full content in the following formats:\n[**PDF](https://dl.acm.org/doi/pdf/10.5555/3586589.3586598)\n## References\n[1]\nMart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Man\u00e9, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00e9gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software available from tensorflow.org.\n[Google Scholar](https://scholar.google.com/scholar?q=Mart\u00edn+Abadi,+Ashish+Agarwal,+Paul+Barham,+Eugene+Brevdo,+Zhifeng+Chen,+Craig+Citro,+Greg+S.+Corrado,+Andy+Davis,+Jeffrey+Dean,+Matthieu+Devin,+Sanjay+Ghemawat,+Ian+Goodfellow,+Andrew+Harp,+Geoffrey+Irving,+Michael+Isard,+Yangqing+Jia,+Rafal+Jozefowicz,+Lukasz+Kaiser,+Manjunath+Kudlur,+Josh+Levenberg,+Dandelion+Man\u00e9,+Rajat+Monga,+Sherry+Moore,+Derek+Murray,+Chris+Olah,+Mike+Schuster,+Jonathon+Shlens,+Benoit+Steiner,+Ilya+Sutskever,+Kunal+Talwar,+Paul+Tucker,+Vincent+Vanhoucke,+Vijay+Vasudevan,+Fernanda+Vi\u00e9gas,+Oriol+Vinyals,+Pete+Warden,+Martin+Wattenberg,+Martin+Wicke,+Yuan+Yu,+and+Xiaoqiang+Zheng.+TensorFlow:+Large-scale+machine+learning+on+heterogeneous+systems,+2015.+URL+https://www.tensorflow.org/.+Software+available+from+tensorflow.org.)\n[2]\nAlessio Ansuini, Alessandro Laio, Jakob H. Macke, and Davide Zoccolan. Intrinsic dimension of data representations in deep neural networks, 2019.\n[Google Scholar](https://scholar.google.com/scholar?q=Alessio+Ansuini,+Alessandro+Laio,+Jakob+H.+Macke,+and+Davide+Zoccolan.+Intrinsic+dimension+of+data+representations+in+deep+neural+networks,+2019.)\n[3]\nRonen Basri and David Jacobs. Efficient representation of low-dimensional manifolds using deep networks, 2016.\n[Google Scholar](https://scholar.google.com/scholar?q=Ronen+Basri+and+David+Jacobs.+Efficient+representation+of+low-dimensional+manifolds+using+deep+networks,+2016.)\n[4]\nG\u00c3\u0160rard Biau. Analysis of a random forests model.*Journal of Machine Learning Research*, 13(Apr):1063-1095, 2012.\n[Google Scholar](https://scholar.google.com/scholar?q=G\u00c3\u0160rard+Biau.+Analysis+of+a+random+forests+model.+Journal+of+Machine+Learning+Research,+13(Apr):1063-1095,+2012.)\n[5]\nPeter J Bickel, Bo Li, et al. Local polynomial regression on unknown manifolds. In*Complex datasets and inverse problems*, pages 177-186. Institute of Mathematical Statistics, 2007.\n[Google Scholar](https://scholar.google.com/scholar?q=Peter+J+Bickel,+Bo+Li,+et+al.+Local+polynomial+regression+on+unknown+manifolds.+In+Complex+datasets+and+inverse+problems,+pages+177-186.+Institute+of+Mathematical+Statistics,+2007.)\n[6]\nLars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and Ga\u00ebl Varoquaux. API design for machine learning software: experiences from the scikit-learn project. In*ECML PKDD Workshop: Languages for Data Mining and Machine Learning*, pages 108-122, 2013.\n[Google Scholar](https://scholar.google.com/scholar?q=Lars+Buitinck,+Gilles+Louppe,+Mathieu+Blondel,+Fabian+Pedregosa,+Andreas+Mueller,+Olivier+Grisel,+Vlad+Niculae,+Peter+Prettenhofer,+Alexandre+Gramfort,+Jaques+Grobler,+Robert+Layton,+Jake+VanderPlas,+Arnaud+Joly,+Brian+Holt,+and+Ga\u00ebl+Varoquaux.+API+design+for+machine+learning+software:+experiences+from+the+scikit-learn+project.+In+ECML+PKDD+Workshop:+Languages+for+Data+Mining+and+Machine+Learning,+pages+108-122,+2013.)\n[7]\nFrancesco Camastra and Antonino Staiano. Intrinsic dimension estimation: Advances and open problems.*Information Sciences*, 328:26-41, 2016.\n[Google Scholar](https://scholar.google.com/scholar?q=Francesco+Camastra+and+Antonino+Staiano.+Intrinsic+dimension+estimation:+Advances+and+open+problems.+Information+Sciences,+328:26-41,+2016.)\n[8]\nKarl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning, 2019.\n[Google Scholar](https://scholar.google.com/scholar?q=Karl+Cobbe,+Christopher+Hesse,+Jacob+Hilton,+and+John+Schulman.+Leveraging+procedural+generation+to+benchmark+reinforcement+learning,+2019.)\n[9]\nElena Facco, Maria d'Errico, Alex Rodriguez, and Alessandro Laio. Estimating the intrinsic dimension of datasets by a minimal neighborhood information.*Scientific Reports*, 7, 12 2017.\n[Crossref](https://doi.org/10.1038/s41598-017-11873-y)\n[Google Scholar](htt",
          "original_query": "Scaling laws from the data manifold dimension",
          "cleaned_query": "Scaling laws from the data manifold dimension"
        },
        {
          "success": true,
          "title": "[2209.10652] Toy Models of Superposition - arXiv",
          "url": "https://arxiv.org/abs/2209.10652",
          "content": "[2209.10652] Toy Models of Superposition\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2209.10652\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2209.10652**(cs)\n[Submitted on 21 Sep 2022]\n# Title:Toy Models of Superposition\nAuthors:[Nelson Elhage](https://arxiv.org/search/cs?searchtype=author&amp;query=Elhage,+N),[Tristan Hume](https://arxiv.org/search/cs?searchtype=author&amp;query=Hume,+T),[Catherine Olsson](https://arxiv.org/search/cs?searchtype=author&amp;query=Olsson,+C),[Nicholas Schiefer](https://arxiv.org/search/cs?searchtype=author&amp;query=Schiefer,+N),[Tom Henighan](https://arxiv.org/search/cs?searchtype=author&amp;query=Henighan,+T),[Shauna Kravec](https://arxiv.org/search/cs?searchtype=author&amp;query=Kravec,+S),[Zac Hatfield-Dodds](https://arxiv.org/search/cs?searchtype=author&amp;query=Hatfield-Dodds,+Z),[Robert Lasenby](https://arxiv.org/search/cs?searchtype=author&amp;query=Lasenby,+R),[Dawn Drain](https://arxiv.org/search/cs?searchtype=author&amp;query=Drain,+D),[Carol Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+C),[Roger Grosse](https://arxiv.org/search/cs?searchtype=author&amp;query=Grosse,+R),[Sam McCandlish](https://arxiv.org/search/cs?searchtype=author&amp;query=McCandlish,+S),[Jared Kaplan](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaplan,+J),[Dario Amodei](https://arxiv.org/search/cs?searchtype=author&amp;query=Amodei,+D),[Martin Wattenberg](https://arxiv.org/search/cs?searchtype=author&amp;query=Wattenberg,+M),[Christopher Olah](https://arxiv.org/search/cs?searchtype=author&amp;query=Olah,+C)\nView a PDF of the paper titled Toy Models of Superposition, by Nelson Elhage and 15 other authors\n[View PDF](https://arxiv.org/pdf/2209.10652)> > Abstract:\n> Neural networks often pack many unrelated concepts into a single neuron - a puzzling phenomenon known as &#39;polysemanticity&#39; which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in &#34;superposition.&#34; We demonstrate the existence of a phase change, a surprising connection to the geometry of uniform polytopes, and evidence of a link to adversarial examples. We also discuss potential implications for mechanistic interpretability. Comments:|Also available at[this https URL](https://transformer-circuits.pub/2022/toy_model/index.html)|\nSubjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:2209.10652](https://arxiv.org/abs/2209.10652)[cs.LG]|\n|(or[arXiv:2209.10652v1](https://arxiv.org/abs/2209.10652v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2209.10652](https://doi.org/10.48550/arXiv.2209.10652)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Samuel McCandlish [[view email](https://arxiv.org/show-email/df0fa1c4/2209.10652)]\n**[v1]**Wed, 21 Sep 2022 20:49:26 UTC (4,804 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Toy Models of Superposition, by Nelson Elhage and 15 other authors\n* [View PDF](https://arxiv.org/pdf/2209.10652)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2209.10652&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2209.10652&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2022-09](https://arxiv.org/list/cs.LG/2022-09)\nChange to browse by:\n[cs](https://arxiv.org/abs/2209.10652?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2209.10652)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2209.10652)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2209.10652)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2209.10652)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Toy models of superposition",
          "cleaned_query": "Toy models of superposition"
        }
      ],
      "generated_ideas": [
        "Compute-Optimal Token Budget with Redundancy-Penalized Allocation\nDevelop a compute allocation rule that replaces \u201ctokens seen\u201d with an \u201ceffective tokens\u201d count that down-weights near-duplicate or highly redundant samples (high density). Validate by training matched-compute model families where the baseline follows Chinchilla-style allocation, and the proposed rule reallocates compute to less-redundant data; measure whether the sub-scaling curvature in loss is reduced.",
        "Anti-Superposition Regularizers for High-Density Training\nDesign and evaluate regularizers that explicitly discourage feature superposition (e.g., encouraging sparse, disentangled feature representations or reducing neuron polysemanticity) and test if they restore scaling gains under repeated/redundant data. The key contribution is a causal intervention: keeping the dataset fixed (including high density) while modifying training to see if sub-scaling is mitigated.",
        "Manifold-Dimension-Weighted Data Deduplication for Better Scaling\nCreate a deduplication pipeline that removes samples not just by lexical similarity but by proximity in embedding space and local intrinsic dimension (removing points that do not increase manifold coverage). Compare against standard dedup on held-out perplexity and downstream reasoning, testing whether \u201cdimension-preserving dedup\u201d yields better scaling at large token counts.",
        "Effective Data Diversity Metrics that Predict Downstream Reasoning Gains\nGo beyond density by defining task-relevant diversity measures (e.g., coverage of latent reasoning \u201coperators\u201d inferred from model activations or from synthetic teacher tasks) and evaluate which metric best predicts improvements on multi-step reasoning benchmarks. The actionable output is a set of validated dataset diagnostics that decide whether to add more tokens, add new data sources, or change training strategy to avoid sub-scaling."
      ],
      "generated_ideas_raw": "1. **Density-Aware Scaling Laws via Intrinsic Dimension Estimation**  \nEstimate the intrinsic manifold dimension *d* of training corpora (globally and per-domain) using representation-based dimension estimators, then test whether the scaling exponent predicted by \u03b1\u22484/*d* explains the sub-scaling regimes attributed to high data density. Produce a unified empirical model that combines Paper 1\u2019s density metric with Paper 2\u2019s dimension theory to predict when adding tokens stops helping for a fixed model size.\n\n2. **Compute-Optimal Token Budget with Redundancy-Penalized Allocation**  \nDevelop a compute allocation rule that replaces \u201ctokens seen\u201d with an \u201ceffective tokens\u201d count that down-weights near-duplicate or highly redundant samples (high density). Validate by training matched-compute model families where the baseline follows Chinchilla-style allocation, and the proposed rule reallocates compute to less-redundant data; measure whether the sub-scaling curvature in loss is reduced.\n\n3. **Curriculum over Data Manifold Dimension to Delay Sub-Scaling**  \nConstruct a training curriculum that starts with lower intrinsic-dimension slices of data (easier, more templatic text) and progressively introduces higher-dimension slices (more diverse, compositional, multi-hop). Test whether controlling the *rate of increase* of estimated *d* during training improves scaling behavior, especially on complex reasoning tasks where Paper 1 notes stronger sub-scaling.\n\n4. **Superposition as a Mechanism for Data-Density-Induced Sub-Scaling**  \nProbe whether high-density corpora increase feature superposition (polysemanticity) by measuring sparsity, feature interference proxies, and activation pattern overlap across layers during training. Use controlled dataset mixtures with matched size but varying density/diversity to test if higher density leads to more superposition and earlier performance saturation, linking Paper 1\u2019s redundancy story to Paper 3\u2019s mechanism.\n\n5. **Anti-Superposition Regularizers for High-Density Training**  \nDesign and evaluate regularizers that explicitly discourage feature superposition (e.g., encouraging sparse, disentangled feature representations or reducing neuron polysemanticity) and test if they restore scaling gains under repeated/redundant data. The key contribution is a causal intervention: keeping the dataset fixed (including high density) while modifying training to see if sub-scaling is mitigated.\n\n6. **Phase-Transition Mapping: When Do LLMs Enter Superposition Under Fixed Compute?**  \nRun controlled sweeps over (model width, sparsity pressure, dataset density, token budget) to identify phase boundaries analogous to Paper 3\u2019s phase changes, but in small transformer LMs. Output a predictive \u201cphase diagram\u201d showing where polysemanticity sharply increases and correlate that boundary with the onset of loss/performance sub-scaling described in Paper 1.\n\n7. **Manifold-Dimension-Weighted Data Deduplication for Better Scaling**  \nCreate a deduplication pipeline that removes samples not just by lexical similarity but by proximity in embedding space and local intrinsic dimension (removing points that do not increase manifold coverage). Compare against standard dedup on held-out perplexity and downstream reasoning, testing whether \u201cdimension-preserving dedup\u201d yields better scaling at large token counts.\n\n8. **Resource Allocation that Tracks Representation Dimension Growth Over Training**  \nIntroduce an online controller that periodically estimates the intrinsic dimension of internal representations (per layer) and adapts learning rate, batch composition, and data sampling to keep representation dimension growth in a target range. The contribution is a training strategy grounded in Paper 2\u2019s dimension view and Paper 1\u2019s \u201cnon-optimal training strategies,\u201d aiming to avoid late-stage diminishing returns.\n\n9. **Adversarial Robustness as an Indicator of Superposition and Data Density**  \nTest the hypothesized link between superposition and adversarial vulnerability (Paper 3) under varying dataset density (Paper 1), measuring whether high-density training increases susceptibility to prompt attacks, jailbreak-like perturbations, or adversarial suffixes. Deliver a predictive relationship where robustness metrics act as early warning signals for impending sub-scaling.\n\n10. **Effective Data Diversity Metrics that Predict Downstream Reasoning Gains**  \nGo beyond density by defining task-relevant diversity measures (e.g., coverage of latent reasoning \u201coperators\u201d inferred from model activations or from synthetic teacher tasks) and evaluate which metric best predicts improvements on multi-step reasoning benchmarks. The actionable output is a set of validated dataset diagnostics that decide whether to add more tokens, add new data sources, or change training strategy to avoid sub-scaling.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Compute-Optimal Token Budget with Redundancy-Penalized Allocation\nDevelop a compute allocation rule that replaces \u201ctokens seen\u201d with an \u201ceffective tokens\u201d count that down-weights near-duplicate or hig",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Anti-Superposition Regularizers for High-Density Training\nDesign and evaluate regularizers that explicitly discourage feature superposition (e.g., encouraging sparse, disentangled feature representati",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Manifold-Dimension-Weighted Data Deduplication for Better Scaling\nCreate a deduplication pipeline that removes samples not just by lexical similarity but by proximity in embedding space and local intr",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Effective Data Diversity Metrics that Predict Downstream Reasoning Gains\nGo beyond density by defining task-relevant diversity measures (e.g., coverage of latent reasoning \u201coperators\u201d inferred from mo",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 54,
      "paper_title": "ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression",
      "contribution": "The paper establishes that CNNs predominantly rely on local shape features rather than being inherently biased towards texture, offering a new evaluation of feature reliance through controlled suppression.",
      "num_predecessors": 3,
      "predecessors_crawled": 3,
      "academic_sources": 1,
      "crawl_rate": 1.0,
      "ideas_generated": 6,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 6597,
      "output_tokens": 989,
      "predecessor_details": [
        {
          "success": true,
          "title": "[1811.12231] ImageNet-trained CNNs are biased towards texture",
          "url": "https://arxiv.org/abs/1811.12231",
          "content": "[1811.12231] ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1811.12231\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:1811.12231**(cs)\n[Submitted on 29 Nov 2018 ([v1](https://arxiv.org/abs/1811.12231v1)), last revised 9 Nov 2022 (this version, v3)]\n# Title:ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness\nAuthors:[Robert Geirhos](https://arxiv.org/search/cs?searchtype=author&amp;query=Geirhos,+R),[Patricia Rubisch](https://arxiv.org/search/cs?searchtype=author&amp;query=Rubisch,+P),[Claudio Michaelis](https://arxiv.org/search/cs?searchtype=author&amp;query=Michaelis,+C),[Matthias Bethge](https://arxiv.org/search/cs?searchtype=author&amp;query=Bethge,+M),[Felix A. Wichmann](https://arxiv.org/search/cs?searchtype=author&amp;query=Wichmann,+F+A),[Wieland Brendel](https://arxiv.org/search/cs?searchtype=author&amp;query=Brendel,+W)\nView a PDF of the paper titled ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness, by Robert Geirhos and 5 other authors\n[View PDF](https://arxiv.org/pdf/1811.12231)> > Abstract:\n> Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on &#34;Stylized-ImageNet&#34;, a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation. Comments:|Accepted at ICLR 2019 (oral)|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML)|\nCite as:|[arXiv:1811.12231](https://arxiv.org/abs/1811.12231)[cs.CV]|\n|(or[arXiv:1811.12231v3](https://arxiv.org/abs/1811.12231v3)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.1811.12231](https://doi.org/10.48550/arXiv.1811.12231)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Robert Geirhos [[view email](https://arxiv.org/show-email/5f3dace0/1811.12231)]\n**[[v1]](https://arxiv.org/abs/1811.12231v1)**Thu, 29 Nov 2018 15:04:05 UTC (7,714 KB)\n**[[v2]](https://arxiv.org/abs/1811.12231v2)**Mon, 14 Jan 2019 13:59:09 UTC (6,282 KB)\n**[v3]**Wed, 9 Nov 2022 23:15:15 UTC (4,790 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness, by Robert Geirhos and 5 other authors\n* [View PDF](https://arxiv.org/pdf/1811.12231)\n* [TeX Source](https://arxiv.org/src/1811.12231)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1811.12231&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1811.12231&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2018-11](https://arxiv.org/list/cs.CV/2018-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/1811.12231?context=cs)\n[cs.AI](https://arxiv.org/abs/1811.12231?context=cs.AI)\n[cs.LG](https://arxiv.org/abs/1811.12231?context=cs.LG)\n[q-bio](https://arxiv.org/abs/1811.12231?context=q-bio)\n[q-bio.NC](https://arxiv.org/abs/1811.12231?context=q-bio.NC)\n[stat](https://arxiv.org/abs/1811.12231?context=stat)\n[stat.ML](https://arxiv.org/abs/1811.12231?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1811.12231)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1811.12231)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1811.12231)\n### [1 blog link](https://arxiv.org/tb/1811.12231)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1811.html#abs-1811-12231)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1811-12231)\n[Robert Geirhos]()\n[Patricia Rubisch]()\n[Claudio Michaelis]()\n[Matthias Bethge]()\n[Felix A. Wichmann]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommender",
          "original_query": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
          "cleaned_query": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness"
        },
        {
          "success": true,
          "title": "[PDF] Image Style Transfer Using Convolutional Neural Networks",
          "url": "https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf",
          "content": "Image Style Transfer Using Convolutional Neural Networks\nLeon A. Gatys\nCentre for Integrative Neuroscience, University of Tubingen, Germany \u00a8\nBernstein Center for Computational Neuroscience, Tubingen, Germany \u00a8\nGraduate School of Neural Information Processing, University of Tubingen, Germany \u00a8\nleon.gatys@bethgelab.org\nAlexander S. Ecker\nCentre for Integrative Neuroscience, University of Tubingen, Germany \u00a8\nBernstein Center for Computational Neuroscience, Tubingen, Germany \u00a8\nMax Planck Institute for Biological Cybernetics, Tubingen, Germany \u00a8\nBaylor College of Medicine, Houston, TX, USA\nMatthias Bethge\nCentre for Integrative Neuroscience, University of Tubingen, Germany \u00a8\nBernstein Center for Computational Neuroscience, Tubingen, Germany \u00a8\nMax Planck Institute for Biological Cybernetics, Tubingen, Germany \u00a8\nAbstract\nRendering the semantic content of an image in different\nstyles is a difficult image processing task. Arguably, a major\nlimiting factor for previous approaches has been the lack of\nimage representations that explicitly represent semantic in\u0002formation and, thus, allow to separate image content from\nstyle. Here we use image representations derived from Con\u0002volutional Neural Networks optimised for object recogni\u0002tion, which make high level image information explicit. We\nintroduce A Neural Algorithm of Artistic Style that can sep\u0002arate and recombine the image content and style of natural\nimages. The algorithm allows us to produce new images of\nhigh perceptual quality that combine the content of an ar\u0002bitrary photograph with the appearance of numerous well\u0002known artworks. Our results provide new insights into the\ndeep image representations learned by Convolutional Neu\u0002ral Networks and demonstrate their potential for high level\nimage synthesis and manipulation.\n1. Introduction\nTransferring the style from one image onto another can\nbe considered a problem of texture transfer. In texture trans\u0002fer the goal is to synthesise a texture from a source image\nwhile constraining the texture synthesis in order to preserve\nthe semantic content of a target image. For texture synthesis\nthere exist a large range of powerful non-parametric algo\u0002rithms that can synthesise photorealistic natural textures by\nresampling the pixels of a given source texture [7, 30, 8, 20].\nMost previous texture transfer algorithms rely on these non\u0002parametric methods for texture synthesis while using differ\u0002ent ways to preserve the structure of the target image. For\ninstance, Efros and Freeman introduce a correspondence\nmap that includes features of the target image such as im\u0002age intensity to constrain the texture synthesis procedure\n[8]. Hertzman et al. use image analogies to transfer the tex\u0002ture from an already stylised image onto a target image[13].\nAshikhmin focuses on transferring the high-frequency tex\u0002ture information while preserving the coarse scale of the\ntarget image [1]. Lee et al. improve this algorithm by addi\u0002tionally informing the texture transfer with edge orientation\ninformation [22].\nAlthough these algorithms achieve remarkable results,\nthey all suffer from the same fundamental limitation: they\nuse only low-level image features of the target image to in\u0002form the texture transfer. Ideally, however, a style transfer\nalgorithm should be able to extract the semantic image con\u0002tent from the target image (e.g. the objects and the general\nscenery) and then inform a texture transfer procedure to ren\u0002der the semantic content of the target image in the style of\nthe source image. Therefore, a fundamental prerequisite is\nto find image representations that independently model vari\u0002ations in the semantic image content and the style in which\n12414\nInput image\nContent\nRepresentations\nStyle\nRepresentations\nConvolutional Neural Network\nStyle Reconstructions\nContent Reconstructions\na b c d e\na b c d e\nFigure 1. Image representations in a Convolutional Neural Network (CNN). A given input image is represented as a set of filtered images\nat each processing stage in the CNN. While the number of different filters increases along the processing hierarchy, the size of the filtered\nimages is reduced by some downsampling mechanism (e.g. max-pooling) leading to a decrease in the total number of units per layer of the\nnetwork. Content Reconstructions. We can visualise the information at different processing stages in the CNN by reconstructing the input\nimage from only knowing the network\u2019s responses in a particular layer. We reconstruct the input image from from layers \u2018conv1 2\u2019 (a),\n\u2018conv2 2\u2019 (b), \u2018conv3 2\u2019 (c), \u2018conv4 2\u2019 (d) and \u2018conv5 2\u2019 (e) of the original VGG-Network. We find that reconstruction from lower layers is\nalmost perfect (a\u2013c). In higher layers of the network, detailed pixel information is lost while the high-level content of the image is preserved\n(d,e). Style Reconstructions. On top of the original CNN activations we use a feature space that captures the texture information of an\ninput image. The style representation computes correlations between the different features in different layers of the CNN. We reconstruct\nthe style of the input image from a style representation built on different subsets of CNN layers ( \u2018conv1 1\u2019 (a), \u2018conv1 1\u2019 and \u2018conv2 1\u2019\n(b), \u2018conv1 1\u2019, \u2018conv2 1\u2019 and \u2018conv3 1\u2019 (c), \u2018conv1 1\u2019, \u2018conv2 1\u2019, \u2018conv3 1\u2019 and \u2018conv4 1\u2019 (d), \u2018conv1 1\u2019, \u2018conv2 1\u2019, \u2018conv3 1\u2019, \u2018conv4 1\u2019\nand \u2018conv5 1\u2019 (e). This creates images that match the style of a given image on an increasing scale while discarding information of the\nglobal arrangement of the scene.\nit is presented. Such factorised representations were pre\u0002viously achieved only for controlled subsets of natural im\u0002ages such as faces under different illumination conditions\nand characters in different font styles [29] or handwritten\ndigits and house numbers [17].\nTo generally separate content from style in natural im\u0002ages is still an extremely difficult problem. However, the re\u0002cent advance of Deep Convolutional Neural Networks [18]\nhas produced powerful computer vision systems that learn\nto extract high-level semantic information from natural im\u0002ages. It was shown that Convolutional Neural Networks\ntrained with sufficient labeled data on specific tasks such\nas object recognition learn to extract high-level image con\u0002tent in generic feature representations that generalise across\ndatasets [6] and even to other visual information processing\ntasks [19, 4, 2, 9, 23], including texture recognition [5] and\nartistic style classification [15].\nIn this work we show how the generic feature represen\u0002tations learned by high-performing Convolutional Neural\nNetworks can be used to independently process and ma\u0002nipulate the content and the style of natural images. We\nintroduce A Neural Algorithm of Artistic Style, a new algo\u00022415\nrithm to perform image style transfer. Conceptually, it is a\ntexture transfer algorithm that constrains a texture synthe\u0002sis method by feature representations from state-of-the-art\nConvolutional Neural Networks. Since the texture model is\nalso based on deep image representations, the style transfer\nmethod elegantly reduces to an optimisation problem within\na single neural network. New images are generated by per\u0002forming a pre-image search to match feature representations\nof example images. This general approach has been used\nbefore in the context of texture synthesis [12, 25, 10] and to\nimprove the understanding of deep image representations\n[27, 24]. In fact, our style transfer algorithm combines a\nparametric texture model based on Convolutional Neural\nNetworks [10] with a method to invert their image repre\u0002sentations [24].\n2. Deep image representations\nThe results presented below were generated on the ba\u0002sis of the VGG network [28], which was trained to perform\nobject recognition and localisation [26] and is described ex\u0002tensively in the original work [28]. We used the feature\nspace provided by a normalised version of the 16 convo\u0002lutional and 5 pooling layers of the 19-layer VGG network.\nWe normalized the net",
          "original_query": "Image style transfer using convolutional neural networks",
          "cleaned_query": "Image style transfer using convolutional neural networks",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "Deep convolutional networks do not classify based on global object ...",
          "url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006613",
          "content": "- [Article](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006613)\n- [Authors](https://journals.plos.org/ploscompbiol/article/authors?id=10.1371/journal.pcbi.1006613)\n- [Metrics](https://journals.plos.org/ploscompbiol/article/metrics?id=10.1371/journal.pcbi.1006613)\n- [Comments](https://journals.plos.org/ploscompbiol/article/comments?id=10.1371/journal.pcbi.1006613)\n- Media Coverage\n\n- [Reader Comments](https://journals.plos.org/ploscompbiol/article/comments?id=10.1371/journal.pcbi.1006613)\n- [Figures](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006613)\n\n## Figures\n\n![Fig 1](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g001)\n\n![Fig 2](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g002)\n\n![Fig 3](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g003)\n\n![Fig 4](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g004)\n\n![Fig 5](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g005)\n\n![Fig 6](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g006)\n\n![Fig 7](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g007)\n\n![Fig 8](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g008)\n\n![Fig 9](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g009)\n\n![Fig 10](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g010)\n\n![Fig 11](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g011)\n\n![Fig 12](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g012)\n\n![Fig 13](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g013)\n\n![Fig 14](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g014)\n\n![Fig 15](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g015)\n\n![Fig 16](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g016)\n\n![Fig 17](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g017)\n\n![Fig 18](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g018)\n\n![Fig 19](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g019)\n\n![Fig 20](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g020)\n\n![Fig 21](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g021)\n\n![Fig 22](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g022)\n\n![Fig 23](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g023)\n\n![Fig 24](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g024)\n\n![Fig 25](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g025)\n\n![Table 1](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.t001)\n\n![Fig 26](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g026)\n\n![Fig 27](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g027)\n\n![Fig 28](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.g028)\n\n![Table 2](https://journals.plos.org/ploscompbiol/article/figure/image?size=inline&id=10.1371/journal.pcbi.1006613.t002)\n\n## Abstract\n\nDeep convolutional networks (DCNNs) are achieving previously unseen performance in object classification, raising questions about whether DCNNs operate similarly to human vision. In biological vision, shape is arguably the most important cue for recognition. We tested the role of shape information in DCNNs trained to recognize objects. In Experiment 1, we presented a trained DCNN with object silhouettes that preserved overall shape but were filled with surface texture taken from other objects. Shape cues appeared to play some role in the classification of artifacts, but little or none for animals. In Experiments 2\u20134, DCNNs showed no ability to classify glass figurines or outlines but correctly classified some silhouettes. Aspects of these results led us to hypothesize that DCNNs do not distinguish object\u2019s bounding contours from other edges, and that DCNNs access some local shape features, but not global shape. In Experiment 5, we tested this hypothesis with displays that preserved local features but disrupted global shape, and vice versa. With disrupted global shape, which reduced human accuracy to 28%, DCNNs gave the same classification labels as with ordinary shapes. Conversely, local contour changes eliminated accurate DCNN classification but caused no difficulty for human observers. These results provide evidence that DCNNs have access to some local shape information in the form of local edge relations, but they have no access to global object shapes.\n\n## Author summary\n\n\u201cDeep learning\u201d systems\u2013specifically, deep convolutional neural networks (DCNNs)\u2013have recently achieved near human levels of performance in object recognition tasks. It has been suggested that the processing in these systems may model or explain object perception abilities in biological vision. For humans, shape is the most important cue for recognizing objects. We tested whether deep convolutional neural networks trained to recognize objects make use of object shape. Our findings indicate that other cues, such as surface texture, play a larger role in deep network classification than in human recognition. Most crucially, we show that deep learning systems have no sensitivity to the overall shape of an object. Whereas deep learning systems can access some local shape features, such as local orientation relations, they are not sensitive to the arrangement of these edge features or global shape in general, and they do not appear to distinguish bounding contours of objects from other edge information. These findings show a crucial divergence between artificial visual systems and biological visual processes.\n\n**Citation:** Baker N, Lu H, Erlikhman G, Kellman PJ (2018) Deep convolutional networks do not classify based on global object shape. PLoS Comput Biol 14(12):\ne1006613.\nhttps://doi.org/10.1371/journal.pcbi.1006613\n\n**Editor:** Wolfgang Einh\u00e4user, Technische Universitat Chemnitz, GERMANY\n\n**Received:** November 3, 2017; **Accepted:** October 31, 2018; **Published:** December 7, 2018\n\n**Copyright:** \u00a9 2018 Baker et al. This is an open access article distributed under the terms of the [Creative Commons Attribution License](http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\n\n**Data Availability:** All relevant data are within the manuscript and its Supporting Information files.\n\n**Funding:** This research was funded by the National Science Foundation Research Traineeship for ModEling and uNdersTanding human behaviOR (MENTOR) DGE-1829071 ( [http://www.math.ucla.edu/~bertozzi/NRT/index.html](http://www.math.ucla.edu/~bertozzi/NRT/index.html)) to NB and the Advancing Theory and A",
          "original_query": "Deep convolutional networks do not classify based on global object shape",
          "cleaned_query": "Deep convolutional networks do not classify based on global object shape",
          "note": "Non-academic source"
        }
      ],
      "generated_ideas": [
        "Curriculum Learning to Transition from Texture to Global Shape\nTrain ResNet-style models with a staged curriculum that progressively increases stylization strength (Stylized-ImageNet) while simultaneously introducing global-shape\u2013preserving, local-edge\u2013disrupting augmentations (e.g., contour jitter with preserved silhouette). Quantify when networks begin to rely on global shape by tracking performance on texture\u2013shape cue-conflict images and on stimuli that isolate local vs global contour cues (as in the PLOS experiments).",
        "Disentangling \u201cStyle\u201d (Texture) and \u201cShape\u201d via Dual-Branch Contrastive Learning\nBuild a two-encoder architecture where one branch is explicitly optimized to be invariant to Gatys-style texture statistics (Gram matrices) while the other is invariant to shape transformations (elastic deformations/silhouette warps). Use cross-branch contrastive objectives to enforce separable representations, then test whether the \u201cshape\u201d branch improves robustness and aligns better with human judgments on cue-conflict and outline-only recognition.",
        "A Global Contour Tokenizer for CNNs Using Differentiable Edge Grouping\nAdd a differentiable module that converts low-level edge maps into grouped contour \u201ctokens\u201d (closed boundaries, long curves) using learned affinity and spectral/graph clustering approximations. Feed these tokens to a recognition head alongside standard CNN features, and evaluate whether explicit boundary grouping fixes the failure mode \u201cnot distinguishing bounding contours from other edges\u201d reported in Paper",
        "Style-Statistics Regularization: Penalize Reliance on Gram-Matrix Cues\nDuring training, estimate how predictive Gram-matrix style features are of the class label (from selected CNN layers) and add a penalty that discourages class information in these style statistics. This directly operationalizes \u201ctexture bias\u201d via the same statistics used in neural style transfer, and tests whether reducing label-predictive style correlations increases shape bias and improves distortion/shift robustness.",
        "Training with Synthetic \u201cTexture-Confuser\u201d Data Generated by Fast Style Transfer\nGenerate a large dataset where each image\u2019s texture is replaced by textures sampled from *other* classes while preserving object boundaries (segmentation-assisted style transfer). Train classifiers on mixtures of original and texture-confuser images to force reliance away from texture cues, then evaluate on cue-conflict images, silhouettes, outlines, and common corruption benchmarks to measure generalization gains.",
        "Architectural Ablation: Replace Some Convolutions with Long-Range Contour Integrators\nCompare standard CNNs against hybrids where selected stages are replaced with modules designed for long-range integration (e.g., dilated contour attention over edge maps or transformer blocks operating on boundary points). Measure improvements specifically on tasks where global shape is necessary (outlines, glass figurines, disrupted-global-shape stimuli) to isolate which inductive biases best recover global shape."
      ],
      "generated_ideas_raw": "1. **Curriculum Learning to Transition from Texture to Global Shape**\n   Train ResNet-style models with a staged curriculum that progressively increases stylization strength (Stylized-ImageNet) while simultaneously introducing global-shape\u2013preserving, local-edge\u2013disrupting augmentations (e.g., contour jitter with preserved silhouette). Quantify when networks begin to rely on global shape by tracking performance on texture\u2013shape cue-conflict images and on stimuli that isolate local vs global contour cues (as in the PLOS experiments).\n\n2. **Disentangling \u201cStyle\u201d (Texture) and \u201cShape\u201d via Dual-Branch Contrastive Learning**\n   Build a two-encoder architecture where one branch is explicitly optimized to be invariant to Gatys-style texture statistics (Gram matrices) while the other is invariant to shape transformations (elastic deformations/silhouette warps). Use cross-branch contrastive objectives to enforce separable representations, then test whether the \u201cshape\u201d branch improves robustness and aligns better with human judgments on cue-conflict and outline-only recognition.\n\n3. **A Global Contour Tokenizer for CNNs Using Differentiable Edge Grouping**\n   Add a differentiable module that converts low-level edge maps into grouped contour \u201ctokens\u201d (closed boundaries, long curves) using learned affinity and spectral/graph clustering approximations. Feed these tokens to a recognition head alongside standard CNN features, and evaluate whether explicit boundary grouping fixes the failure mode \u201cnot distinguishing bounding contours from other edges\u201d reported in Paper 3.\n\n4. **Style-Statistics Regularization: Penalize Reliance on Gram-Matrix Cues**\n   During training, estimate how predictive Gram-matrix style features are of the class label (from selected CNN layers) and add a penalty that discourages class information in these style statistics. This directly operationalizes \u201ctexture bias\u201d via the same statistics used in neural style transfer, and tests whether reducing label-predictive style correlations increases shape bias and improves distortion/shift robustness.\n\n5. **Local-to-Global Shape Probing Benchmark with Controlled Contour Manipulations**\n   Create a public benchmark that systematically sweeps (a) local edge relation integrity and (b) global shape integrity independently, extending Paper 3\u2019s stimulus logic at scale. Include stylized variants (Paper 1) and texture-transferred variants (Paper 2) to measure how training data and augmentations alter reliance on local vs global shape across architectures.\n\n6. **Training with Synthetic \u201cTexture-Confuser\u201d Data Generated by Fast Style Transfer**\n   Generate a large dataset where each image\u2019s texture is replaced by textures sampled from *other* classes while preserving object boundaries (segmentation-assisted style transfer). Train classifiers on mixtures of original and texture-confuser images to force reliance away from texture cues, then evaluate on cue-conflict images, silhouettes, outlines, and common corruption benchmarks to measure generalization gains.\n\n7. **Human-in-the-Loop Shape Bias Tuning via Psychophysics-Inspired Losses**\n   Use small-scale human behavioral data on cue-conflict and contour-disruption stimuli to define a calibration loss: the model\u2019s class probabilities are penalized when they systematically deviate from human response distributions. Test whether this \u201cbehavioral alignment\u201d specifically increases global-shape sensitivity without sacrificing clean accuracy, extending Paper 1\u2019s human comparison from evaluation to training.\n\n8. **Architectural Ablation: Replace Some Convolutions with Long-Range Contour Integrators**\n   Compare standard CNNs against hybrids where selected stages are replaced with modules designed for long-range integration (e.g., dilated contour attention over edge maps or transformer blocks operating on boundary points). Measure improvements specifically on tasks where global shape is necessary (outlines, glass figurines, disrupted-global-shape stimuli) to isolate which inductive biases best recover global shape.\n\n9. **Quantifying Robustness Tradeoffs Between Texture and Shape Bias Under Domain Shift**\n   Run a controlled study across domains where texture cues invert or become unreliable (e.g., synthetic-to-real, seasonal changes, stylized domains), varying the degree of stylization training (Paper 1) and style-statistics regularization. Produce a predictive model of when shape bias helps vs hurts, yielding actionable guidance for deploying vision systems in texture-unstable environments.\n\n10. **Segment-Level Style Transfer to Preserve Object Boundaries While Randomizing Surface Cues**\n   Develop a segmentation-conditioned style transfer method that applies Gatys-like style changes *within* object masks while explicitly preserving boundary geometry (loss on edges/contours). Use it both as an augmentation tool and as a diagnostic: by controlling boundary preservation vs interior texture disruption, directly test the hypothesis from Paper 3 about boundary vs non-boundary edge usage and measure resulting changes in classification strategy.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Curriculum Learning to Transition from Texture to Global Shape\nTrain ResNet-style models with a staged curriculum that progressively increases stylization strength (Stylized-ImageNet) while simultaneo",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Disentangling \u201cStyle\u201d (Texture) and \u201cShape\u201d via Dual-Branch Contrastive Learning\nBuild a two-encoder architecture where one branch is explicitly optimized to be invariant to Gatys-style texture statis",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "A Global Contour Tokenizer for CNNs Using Differentiable Edge Grouping\nAdd a differentiable module that converts low-level edge maps into grouped contour \u201ctokens\u201d (closed boundaries, long curves) usin",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Style-Statistics Regularization: Penalize Reliance on Gram-Matrix Cues\nDuring training, estimate how predictive Gram-matrix style features are of the class label (from selected CNN layers) and add a p",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Training with Synthetic \u201cTexture-Confuser\u201d Data Generated by Fast Style Transfer\nGenerate a large dataset where each image\u2019s texture is replaced by textures sampled from *other* classes while preservi",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Architectural Ablation: Replace Some Convolutions with Long-Range Contour Integrators\nCompare standard CNNs against hybrids where selected stages are replaced with modules designed for long-range inte",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 55,
      "paper_title": "On Linear Mode Connectivity of Mixture-of-Experts Architectures",
      "contribution": "This paper investigates Linear Mode Connectivity (LMC) within Mixture-of-Experts (MoE) architectures, proposing a matching algorithm for aligning independently trained MoEs to discover low-loss paths in parameter space.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11730,
      "output_tokens": 1025,
      "predecessor_details": [
        {
          "success": true,
          "title": "Linear Mode Connectivity and the Lottery Ticket Hypothesis",
          "url": "https://arxiv.org/abs/1912.05671",
          "content": "[1912.05671] Linear Mode Connectivity and the Lottery Ticket Hypothesis\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1912.05671\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1912.05671**(cs)\n[Submitted on 11 Dec 2019 ([v1](https://arxiv.org/abs/1912.05671v1)), last revised 18 Jul 2020 (this version, v4)]\n# Title:Linear Mode Connectivity and the Lottery Ticket Hypothesis\nAuthors:[Jonathan Frankle](https://arxiv.org/search/cs?searchtype=author&amp;query=Frankle,+J),[Gintare Karolina Dziugaite](https://arxiv.org/search/cs?searchtype=author&amp;query=Dziugaite,+G+K),[Daniel M. Roy](https://arxiv.org/search/cs?searchtype=author&amp;query=Roy,+D+M),[Michael Carbin](https://arxiv.org/search/cs?searchtype=author&amp;query=Carbin,+M)\nView a PDF of the paper titled Linear Mode Connectivity and the Lottery Ticket Hypothesis, by Jonathan Frankle and 3 other authors\n[View PDF](https://arxiv.org/pdf/1912.05671)> > Abstract:\n> We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision models become stable to SGD noise in this way early in training. From then on, the outcome of optimization is determined to a linearly connected region. We use this technique to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained in isolation to full accuracy. We find that these subnetworks only reach full accuracy when they are stable to SGD noise, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (ResNet-50 and Inception-v3 on ImageNet). Comments:|Published in ICML 2020. This submission subsumes[arXiv:1903.01611](https://arxiv.org/abs/1903.01611)(&#34;Stabilizing the Lottery Ticket Hypothesis&#34; and &#34;The Lottery Ticket Hypothesis at Scale&#34;)|\nSubjects:|Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)|\nCite as:|[arXiv:1912.05671](https://arxiv.org/abs/1912.05671)[cs.LG]|\n|(or[arXiv:1912.05671v4](https://arxiv.org/abs/1912.05671v4)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1912.05671](https://doi.org/10.48550/arXiv.1912.05671)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jonathan Frankle [[view email](https://arxiv.org/show-email/e19c2dba/1912.05671)]\n**[[v1]](https://arxiv.org/abs/1912.05671v1)**Wed, 11 Dec 2019 22:22:21 UTC (1,980 KB)\n**[[v2]](https://arxiv.org/abs/1912.05671v2)**Thu, 20 Feb 2020 20:39:29 UTC (1,780 KB)\n**[[v3]](https://arxiv.org/abs/1912.05671v3)**Sat, 4 Jul 2020 19:36:46 UTC (2,117 KB)\n**[v4]**Sat, 18 Jul 2020 20:31:17 UTC (2,111 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Linear Mode Connectivity and the Lottery Ticket Hypothesis, by Jonathan Frankle and 3 other authors\n* [View PDF](https://arxiv.org/pdf/1912.05671)\n* [TeX Source](https://arxiv.org/src/1912.05671)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1912.05671&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1912.05671&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2019-12](https://arxiv.org/list/cs.LG/2019-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/1912.05671?context=cs)\n[cs.NE](https://arxiv.org/abs/1912.05671?context=cs.NE)\n[stat](https://arxiv.org/abs/1912.05671?context=stat)\n[stat.ML](https://arxiv.org/abs/1912.05671?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1912.05671)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1912.05671)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1912.05671)\n### [2 blog links](https://arxiv.org/tb/1912.05671)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1912.html#abs-1912-05671)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1912-05671)\n[Jonathan Frankle]()\n[Gintare Karolina Dziugaite]()\n[Daniel M. Roy]()\n[Michael Carbin]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value",
          "original_query": "Linear mode connectivity and the lottery ticket hypothesis",
          "cleaned_query": "Linear mode connectivity and the lottery ticket hypothesis"
        },
        {
          "success": true,
          "title": "The Role of Permutation Invariance in Linear Mode Connectivity of ...",
          "url": "https://arxiv.org/abs/2110.06296",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2110.06296** (cs)\n\n\\[Submitted on 12 Oct 2021 ( [v1](https://arxiv.org/abs/2110.06296v1)), last revised 5 Jul 2022 (this version, v2)\\]\n\n# Title:The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks\n\nAuthors: [Rahim Entezari](https://arxiv.org/search/cs?searchtype=author&query=Entezari,+R), [Hanie Sedghi](https://arxiv.org/search/cs?searchtype=author&query=Sedghi,+H), [Olga Saukh](https://arxiv.org/search/cs?searchtype=author&query=Saukh,+O), [Behnam Neyshabur](https://arxiv.org/search/cs?searchtype=author&query=Neyshabur,+B)\n\nView a PDF of the paper titled The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks, by Rahim Entezari and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2110.06296)\n\n> Abstract:In this paper, we conjecture that if the permutation invariance of neural networks is taken into account, SGD solutions will likely have no barrier in the linear interpolation between them. Although it is a bold conjecture, we show how extensive empirical attempts fall short of refuting it. We further provide a preliminary theoretical result to support our conjecture. Our conjecture has implications for lottery ticket hypothesis, distributed training, and ensemble methods.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2110.06296](https://arxiv.org/abs/2110.06296) \\[cs.LG\\] |\n| (or [arXiv:2110.06296v2](https://arxiv.org/abs/2110.06296v2) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2110.06296](https://doi.org/10.48550/arXiv.2110.06296) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Rahim Entezari \\[ [view email](https://arxiv.org/show-email/be9be6c8/2110.06296)\\] **[\\[v1\\]](https://arxiv.org/abs/2110.06296v1)**\nTue, 12 Oct 2021 19:28:48 UTC (4,511 KB)\n**\\[v2\\]**\nTue, 5 Jul 2022 11:40:49 UTC (19,747 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks, by Rahim Entezari and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2110.06296)\n- [TeX Source](https://arxiv.org/src/2110.06296)\n\n[view license](http://creativecommons.org/licenses/by-nc-sa/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2110.06296&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2110.06296&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-10](https://arxiv.org/list/cs.LG/2021-10)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2110.06296?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2110.06296)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2110.06296)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2110.06296)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2110.html#abs-2110-06296) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2110-06296)\n\n[Rahim Entezari](https://dblp.uni-trier.de/search/author?author=Rahim%20Entezari) [Hanie Sedghi](https://dblp.uni-trier.de/search/author?author=Hanie%20Sedghi) [Behnam Neyshabur](https://dblp.uni-trier.de/search/author?author=Behnam%20Neyshabur)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2110.06296) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "The role of permutation invariance in linear mode connectivity of neural networks",
          "cleaned_query": "The role of permutation invariance in linear mode connectivity of neural networks"
        },
        {
          "success": true,
          "title": "[1910.05653] Model Fusion via Optimal Transport - arXiv",
          "url": "https://arxiv.org/abs/1910.05653",
          "content": "[1910.05653] Model Fusion via Optimal Transport[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1910.05653\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1910.05653**(cs)\n[Submitted on 12 Oct 2019 ([v1](https://arxiv.org/abs/1910.05653v1)), last revised 16 May 2023 (this version, v6)]\n# Title:Model Fusion via Optimal Transport\nAuthors:[Sidak Pal Singh](https://arxiv.org/search/cs?searchtype=author&amp;query=Singh,+S+P),[Martin Jaggi](https://arxiv.org/search/cs?searchtype=author&amp;query=Jaggi,+M)\nView a PDF of the paper titled Model Fusion via Optimal Transport, by Sidak Pal Singh and Martin Jaggi\n[View PDF](https://arxiv.org/pdf/1910.05653)> > Abstract:\n> Combining different models is a widely used paradigm in machine learning applications. While the most common approach is to form an ensemble of models and average their individual predictions, this approach is often rendered infeasible by given resource constraints in terms of memory and computation, which grow linearly with the number of models. We present a layer-wise model fusion algorithm for neural networks that utilizes optimal transport to (soft-) align neurons across the models before averaging their associated parameters.\n> We show that this can successfully yield &#34;one-shot&#34; knowledge transfer (i.e, without requiring any retraining) between neural networks trained on heterogeneous non-i.i.d. data. In both i.i.d. and non-i.i.d. settings , we illustrate that our approach significantly outperforms vanilla averaging, as well as how it can serve as an efficient replacement for the ensemble with moderate fine-tuning, for standard convolutional networks (like VGG11), residual networks (like ResNet18), and multi-layer perceptrons on CIFAR10, CIFAR100, and MNIST. Finally, our approach also provides a principled way to combine the parameters of neural networks with different widths, and we explore its application for model compression. The code is available at the following link, [> this https URL\n](https://github.com/sidak/otfusion)> . Comments:|NeurIPS 2020 conference proceedings (early version featured in the Optimal Transport &amp; Machine Learning workshop, NeurIPS 2019)|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:1910.05653](https://arxiv.org/abs/1910.05653)[cs.LG]|\n|(or[arXiv:1910.05653v6](https://arxiv.org/abs/1910.05653v6)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1910.05653](https://doi.org/10.48550/arXiv.1910.05653)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Sidak Pal Singh [[view email](https://arxiv.org/show-email/f928ad4f/1910.05653)]\n**[[v1]](https://arxiv.org/abs/1910.05653v1)**Sat, 12 Oct 2019 22:07:15 UTC (778 KB)\n**[[v2]](https://arxiv.org/abs/1910.05653v2)**Thu, 19 Dec 2019 13:05:55 UTC (1,049 KB)\n**[[v3]](https://arxiv.org/abs/1910.05653v3)**Fri, 21 Feb 2020 15:16:06 UTC (1,928 KB)\n**[[v4]](https://arxiv.org/abs/1910.05653v4)**Fri, 19 Jun 2020 16:42:43 UTC (3,623 KB)\n**[[v5]](https://arxiv.org/abs/1910.05653v5)**Sun, 14 Feb 2021 21:50:57 UTC (2,124 KB)\n**[v6]**Tue, 16 May 2023 17:57:37 UTC (2,125 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Model Fusion via Optimal Transport, by Sidak Pal Singh and Martin Jaggi\n* [View PDF](https://arxiv.org/pdf/1910.05653)\n* [TeX Source](https://arxiv.org/src/1910.05653)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1910.05653&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1910.05653&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2019-10](https://arxiv.org/list/cs.LG/2019-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/1910.05653?context=cs)\n[stat](https://arxiv.org/abs/1910.05653?context=stat)\n[stat.ML](https://arxiv.org/abs/1910.05653?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1910.05653)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1910.05653)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1910.05653)\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1910.html#abs-1910-05653)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1910-05653)\n[Sidak Pal Singh]()\n[Martin Jaggi]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommen",
          "original_query": "Model fusion via optimal transport",
          "cleaned_query": "Model fusion via optimal transport"
        },
        {
          "success": true,
          "title": "Empirical Analysis of the Hessian of Over-Parametrized Neural ...",
          "url": "https://arxiv.org/abs/1706.04454",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1706.04454** (cs)\n\n\\[Submitted on 14 Jun 2017 ( [v1](https://arxiv.org/abs/1706.04454v1)), last revised 7 May 2018 (this version, v3)\\]\n\n# Title:Empirical Analysis of the Hessian of Over-Parametrized Neural Networks\n\nAuthors: [Levent Sagun](https://arxiv.org/search/cs?searchtype=author&query=Sagun,+L), [Utku Evci](https://arxiv.org/search/cs?searchtype=author&query=Evci,+U), [V. Ugur Guney](https://arxiv.org/search/cs?searchtype=author&query=Guney,+V+U), [Yann Dauphin](https://arxiv.org/search/cs?searchtype=author&query=Dauphin,+Y), [Leon Bottou](https://arxiv.org/search/cs?searchtype=author&query=Bottou,+L)\n\nView a PDF of the paper titled Empirical Analysis of the Hessian of Over-Parametrized Neural Networks, by Levent Sagun and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/1706.04454)\n\n> Abstract:We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et al. (2016): Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers. We believe that our observations have striking implications for non-convex optimization in high dimensions. First, the flatness of such landscapes (which can be measured by the singularity of the Hessian) implies that classical notions of basins of attraction may be quite misleading. And that the discussion of wide/narrow basins may be in need of a new perspective around over-parametrization and redundancy that are able to create large connected components at the bottom of the landscape. Second, the dependence of small number of large eigenvalues to the data distribution can be linked to the spectrum of the covariance matrix of gradients of model outputs. With this in mind, we may reevaluate the connections within the data-architecture-algorithm framework of a model, hoping that it would shed light into the geometry of high-dimensional and non-convex spaces in modern applications. In particular, we present a case that links the two observations: small and large batch gradient descent appear to converge to different basins of attraction but we show that they are in fact connected through their flat region and so belong to the same basin.\n\n| | |\n| --- | --- |\n| Comments: | Minor update for ICLR 2018 Workshop Track presentation |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:1706.04454](https://arxiv.org/abs/1706.04454) \\[cs.LG\\] |\n| (or [arXiv:1706.04454v3](https://arxiv.org/abs/1706.04454v3) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1706.04454](https://doi.org/10.48550/arXiv.1706.04454) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Levent Sagun \\[ [view email](https://arxiv.org/show-email/87d4f5bb/1706.04454)\\] **[\\[v1\\]](https://arxiv.org/abs/1706.04454v1)**\nWed, 14 Jun 2017 12:50:00 UTC (391 KB)\n**[\\[v2\\]](https://arxiv.org/abs/1706.04454v2)**\nWed, 8 Nov 2017 11:36:52 UTC (427 KB)\n**\\[v3\\]**\nMon, 7 May 2018 15:43:39 UTC (952 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Empirical Analysis of the Hessian of Over-Parametrized Neural Networks, by Levent Sagun and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/1706.04454)\n- [TeX Source](https://arxiv.org/src/1706.04454)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1706.04454&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1706.04454&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2017-06](https://arxiv.org/list/cs.LG/2017-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1706.04454?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1706.04454)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1706.04454)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1706.04454)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1706.html#SagunEGDB17) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/SagunEGDB17)\n\n[Levent Sagun](https://dblp.uni-trier.de/search/author?author=Levent%20Sagun) [Utku Evci](https://dblp.uni-trier.de/search/author?author=Utku%20Evci) [V. Ugur G\u00fcney](https://dblp.uni-trier.de/search/author?author=V.%20Ugur%20G%C3%BCney) [Yann Dauphin](https://dblp.uni-trier.de/search/author?author=Yann%20Dauphin) [Yann N. Dauphin](https://dblp.uni-trier.de/search/author?author=Yann%20N.%20Dauphin)\n\n\u2026\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1706.04454) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Empirical analysis of the hessian of over-parametrized neural networks",
          "cleaned_query": "Empirical analysis of the hessian of over-parametrized neural networks"
        },
        {
          "success": true,
          "title": "[2212.12042] Re-basin via implicit Sinkhorn differentiation - arXiv",
          "url": "https://arxiv.org/abs/2212.12042",
          "content": "[2212.12042] Re-basin via implicit Sinkhorn differentiation\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2212.12042\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2212.12042**(cs)\n[Submitted on 22 Dec 2022]\n# Title:Re-basin via implicit Sinkhorn differentiation\nAuthors:[Fidel A. Guerrero Pe\u00f1a](https://arxiv.org/search/cs?searchtype=author&amp;query=Pe\u00f1a,+F+A+G),[Heitor Rapela Medeiros](https://arxiv.org/search/cs?searchtype=author&amp;query=Medeiros,+H+R),[Thomas Dubail](https://arxiv.org/search/cs?searchtype=author&amp;query=Dubail,+T),[Masih Aminbeidokhti](https://arxiv.org/search/cs?searchtype=author&amp;query=Aminbeidokhti,+M),[Eric Granger](https://arxiv.org/search/cs?searchtype=author&amp;query=Granger,+E),[Marco Pedersoli](https://arxiv.org/search/cs?searchtype=author&amp;query=Pedersoli,+M)\nView a PDF of the paper titled Re-basin via implicit Sinkhorn differentiation, by Fidel A. Guerrero Pe\\\\\\~na and 5 other authors\n[View PDF](https://arxiv.org/pdf/2212.12042)> > Abstract:\n> The recent emergence of new algorithms for permuting models into functionally equivalent regions of the solution space has shed some light on the complexity of error surfaces, and some promising properties like mode connectivity. However, finding the right permutation is challenging, and current optimization techniques are not differentiable, which makes it difficult to integrate into a gradient-based optimization, and often leads to sub-optimal solutions. In this paper, we propose a Sinkhorn re-basin network with the ability to obtain the transportation plan that better suits a given objective. Unlike the current state-of-art, our method is differentiable and, therefore, easy to adapt to any task within the deep learning domain. Furthermore, we show the advantage of our re-basin method by proposing a new cost function that allows performing incremental learning by exploiting the linear mode connectivity property. The benefit of our method is compared against similar approaches from the literature, under several conditions for both optimal transport finding and linear mode connectivity. The effectiveness of our continual learning method based on re-basin is also shown for several common benchmark datasets, providing experimental results that are competitive with state-of-art results from the literature. Subjects:|Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:2212.12042](https://arxiv.org/abs/2212.12042)[cs.CV]|\n|(or[arXiv:2212.12042v1](https://arxiv.org/abs/2212.12042v1)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2212.12042](https://doi.org/10.48550/arXiv.2212.12042)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Fidel Alejandro Guerrero Pe\u00f1a [[view email](https://arxiv.org/show-email/1bdff0eb/2212.12042)]\n**[v1]**Thu, 22 Dec 2022 21:25:06 UTC (1,052 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Re-basin via implicit Sinkhorn differentiation, by Fidel A. Guerrero Pe\\\\\\~na and 5 other authors\n* [View PDF](https://arxiv.org/pdf/2212.12042)\n* [TeX Source](https://arxiv.org/src/2212.12042)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2212.12042&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2212.12042&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2022-12](https://arxiv.org/list/cs.CV/2022-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/2212.12042?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2212.12042)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2212.12042)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2212.12042)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2212.12042)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Re-basin via implicit Sinkhorn differentiation",
          "cleaned_query": "Re-basin via implicit Sinkhorn differentiation"
        }
      ],
      "generated_ideas": [
        "Hessian-Outlier\u2013Guided Prediction of SGD Noise Stability Onset\nBuild a diagnostic that predicts the \u201cstability to SGD noise\u201d point (when different SGD runs become linearly mode connected) by tracking only the top Hessian eigenvalues/outliers during training. Test whether changes in outlier magnitude/count reliably precede the connectivity transition across architectures (ResNets/VGG/MLPs) and datasets with controlled cluster/separability shifts.",
        "Permutation-Aware Linear Mode Connectivity for Transformers via Sinkhorn Re-basin\nExtend differentiable re-basin (implicit Sinkhorn) to Transformers by defining permutation groups for attention heads and MLP channels, then measuring whether post-alignment linear interpolation between independently trained models becomes barrier-free. Provide an actionable recipe for \u201cconnectivity after alignment\u201d and evaluate on language modeling and vision transformers, contrasting against naive head matching and weight averaging.",
        "Lottery Tickets \u201cAfter Stability\u201d: Pruning Schedules Triggered by Connectivity Metrics\nReplace fixed-epoch IMP rewinding with an adaptive trigger based on measured linear mode connectivity between two short SGD replicas (different data order/augmentations). The key contribution is a pruning pipeline that starts only once the model enters the linearly connected region, testing whether this yields higher-accuracy tickets and fewer pruning iterations on ImageNet-scale models.",
        "OT Fusion with Curvature-Aware Neuron Matching Using Hessian/Gradient Covariance Features\nImprove layer-wise optimal-transport fusion by augmenting the neuron cost matrix with local curvature descriptors (e.g., top-eigenspace projections or gradient-covariance embeddings) rather than only weights/activations. Evaluate whether curvature-aware matching yields better one-shot fusion across non-i.i.d. clients and reduces the need for fine-tuning compared to standard OTFusion.",
        "Connectivity-Preserving Federated Aggregation: Re-basin + OT Fusion + Linear Interpolation\nDesign a federated learning server step that (i) re-basins client models via differentiable Sinkhorn alignment, (ii) performs OT-based fusion, and (iii) selects an interpolation point along a low-loss linear path between fused and prior global models. The contribution is an aggregation rule explicitly optimized for mode connectivity, aiming to improve stability under heterogeneous data and reduce client drift without additional client compute.",
        "Functional-Equivalence Regularization: Training with a Differentiable Re-basin Consistency Loss\nDuring training, periodically sample a random within-layer permutation (or solve a soft Sinkhorn alignment) and add a loss that encourages invariance of predictions under these symmetry transformations. This directly operationalizes permutation invariance to steer optimization into wider, more connected basins, and can be evaluated by improved linear mode connectivity across random seeds and better post-hoc fuse-ability.",
        "Width-Mismatched Model Fusion Along Connected Paths for Compression Without Retraining\nCombine OTFusion\u2019s ability to fuse different widths with a mode-connectivity criterion: after aligning a wide teacher and a narrow student, choose a parameter-space interpolation (or piecewise-linear path) that minimizes loss on a small calibration set. The result is a concrete compression method that targets a low-loss connected region between models, attempting to match ensemble-level performance with near-zero additional training.",
        "Data-Geometry Controls on Connectivity: Synthetic Cluster Sweeps with Outlier-Eigenvalue Tracking\nCreate a benchmark where dataset difficulty is systematically varied (number of clusters, label noise, separability), then measure how (a) Hessian outliers evolve and (b) linear mode connectivity barriers emerge/disappear across seeds and batch sizes. The contribution is an empirical map linking data distribution properties to connectivity and curvature, testing Sagun et al.\u2019s conjecture in the specific context of permutation-aware connectivity.",
        "Multi-Model \u201cConnectivity Graphs\u201d for Ensemble Distillation via OT-Aligned Barycenters\nTrain many seeds/models, re-basin them into a shared alignment, and construct a graph where edges indicate low-loss linear connectivity. Use OT to compute barycentric fused models for connected components, then distill each component into a single representative without full retraining. This yields a practical alternative to ensembles: identify connected families and compress them into a few fused checkpoints with predictable performance.",
        "Continual Learning via Connectivity-Constrained Updates with Explicit Path Selection\nExtend incremental learning with re-basin by enforcing that each new-task solution remains linearly connected (after permutation alignment) to the previous task\u2019s solution, selecting updates that minimize the maximum loss along the interpolation path. The key contribution is a concrete objective/algorithm that treats \u201clow-loss path existence\u201d as the anti-forgetting constraint, evaluated against standard replay/regularization baselines."
      ],
      "generated_ideas_raw": "1. **Hessian-Outlier\u2013Guided Prediction of SGD Noise Stability Onset**  \n   Build a diagnostic that predicts the \u201cstability to SGD noise\u201d point (when different SGD runs become linearly mode connected) by tracking only the top Hessian eigenvalues/outliers during training. Test whether changes in outlier magnitude/count reliably precede the connectivity transition across architectures (ResNets/VGG/MLPs) and datasets with controlled cluster/separability shifts.\n\n2. **Permutation-Aware Linear Mode Connectivity for Transformers via Sinkhorn Re-basin**  \n   Extend differentiable re-basin (implicit Sinkhorn) to Transformers by defining permutation groups for attention heads and MLP channels, then measuring whether post-alignment linear interpolation between independently trained models becomes barrier-free. Provide an actionable recipe for \u201cconnectivity after alignment\u201d and evaluate on language modeling and vision transformers, contrasting against naive head matching and weight averaging.\n\n3. **Lottery Tickets \u201cAfter Stability\u201d: Pruning Schedules Triggered by Connectivity Metrics**  \n   Replace fixed-epoch IMP rewinding with an adaptive trigger based on measured linear mode connectivity between two short SGD replicas (different data order/augmentations). The key contribution is a pruning pipeline that starts only once the model enters the linearly connected region, testing whether this yields higher-accuracy tickets and fewer pruning iterations on ImageNet-scale models.\n\n4. **OT Fusion with Curvature-Aware Neuron Matching Using Hessian/Gradient Covariance Features**  \n   Improve layer-wise optimal-transport fusion by augmenting the neuron cost matrix with local curvature descriptors (e.g., top-eigenspace projections or gradient-covariance embeddings) rather than only weights/activations. Evaluate whether curvature-aware matching yields better one-shot fusion across non-i.i.d. clients and reduces the need for fine-tuning compared to standard OTFusion.\n\n5. **Connectivity-Preserving Federated Aggregation: Re-basin + OT Fusion + Linear Interpolation**  \n   Design a federated learning server step that (i) re-basins client models via differentiable Sinkhorn alignment, (ii) performs OT-based fusion, and (iii) selects an interpolation point along a low-loss linear path between fused and prior global models. The contribution is an aggregation rule explicitly optimized for mode connectivity, aiming to improve stability under heterogeneous data and reduce client drift without additional client compute.\n\n6. **Functional-Equivalence Regularization: Training with a Differentiable Re-basin Consistency Loss**  \n   During training, periodically sample a random within-layer permutation (or solve a soft Sinkhorn alignment) and add a loss that encourages invariance of predictions under these symmetry transformations. This directly operationalizes permutation invariance to steer optimization into wider, more connected basins, and can be evaluated by improved linear mode connectivity across random seeds and better post-hoc fuse-ability.\n\n7. **Width-Mismatched Model Fusion Along Connected Paths for Compression Without Retraining**  \n   Combine OTFusion\u2019s ability to fuse different widths with a mode-connectivity criterion: after aligning a wide teacher and a narrow student, choose a parameter-space interpolation (or piecewise-linear path) that minimizes loss on a small calibration set. The result is a concrete compression method that targets a low-loss connected region between models, attempting to match ensemble-level performance with near-zero additional training.\n\n8. **Data-Geometry Controls on Connectivity: Synthetic Cluster Sweeps with Outlier-Eigenvalue Tracking**  \n   Create a benchmark where dataset difficulty is systematically varied (number of clusters, label noise, separability), then measure how (a) Hessian outliers evolve and (b) linear mode connectivity barriers emerge/disappear across seeds and batch sizes. The contribution is an empirical map linking data distribution properties to connectivity and curvature, testing Sagun et al.\u2019s conjecture in the specific context of permutation-aware connectivity.\n\n9. **Multi-Model \u201cConnectivity Graphs\u201d for Ensemble Distillation via OT-Aligned Barycenters**  \n   Train many seeds/models, re-basin them into a shared alignment, and construct a graph where edges indicate low-loss linear connectivity. Use OT to compute barycentric fused models for connected components, then distill each component into a single representative without full retraining. This yields a practical alternative to ensembles: identify connected families and compress them into a few fused checkpoints with predictable performance.\n\n10. **Continual Learning via Connectivity-Constrained Updates with Explicit Path Selection**  \n   Extend incremental learning with re-basin by enforcing that each new-task solution remains linearly connected (after permutation alignment) to the previous task\u2019s solution, selecting updates that minimize the maximum loss along the interpolation path. The key contribution is a concrete objective/algorithm that treats \u201clow-loss path existence\u201d as the anti-forgetting constraint, evaluated against standard replay/regularization baselines.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Hessian-Outlier\u2013Guided Prediction of SGD Noise Stability Onset\nBuild a diagnostic that predicts the \u201cstability to SGD noise\u201d point (when different SGD runs become linearly mode connected) by tracking ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Permutation-Aware Linear Mode Connectivity for Transformers via Sinkhorn Re-basin\nExtend differentiable re-basin (implicit Sinkhorn) to Transformers by defining permutation groups for attention heads ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Lottery Tickets \u201cAfter Stability\u201d: Pruning Schedules Triggered by Connectivity Metrics\nReplace fixed-epoch IMP rewinding with an adaptive trigger based on measured linear mode connectivity between two",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "OT Fusion with Curvature-Aware Neuron Matching Using Hessian/Gradient Covariance Features\nImprove layer-wise optimal-transport fusion by augmenting the neuron cost matrix with local curvature descript",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Connectivity-Preserving Federated Aggregation: Re-basin + OT Fusion + Linear Interpolation\nDesign a federated learning server step that (i) re-basins client models via differentiable Sinkhorn alignmen",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Functional-Equivalence Regularization: Training with a Differentiable Re-basin Consistency Loss\nDuring training, periodically sample a random within-layer permutation (or solve a soft Sinkhorn alignme",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Width-Mismatched Model Fusion Along Connected Paths for Compression Without Retraining\nCombine OTFusion\u2019s ability to fuse different widths with a mode-connectivity criterion: after aligning a wide tea",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Data-Geometry Controls on Connectivity: Synthetic Cluster Sweeps with Outlier-Eigenvalue Tracking\nCreate a benchmark where dataset difficulty is systematically varied (number of clusters, label noise,",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Multi-Model \u201cConnectivity Graphs\u201d for Ensemble Distillation via OT-Aligned Barycenters\nTrain many seeds/models, re-basin them into a shared alignment, and construct a graph where edges indicate low-lo",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Continual Learning via Connectivity-Constrained Updates with Explicit Path Selection\nExtend incremental learning with re-basin by enforcing that each new-task solution remains linearly connected (afte",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 56,
      "paper_title": "OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model",
      "contribution": "OpenHOI introduces the first framework for synthesizing open-world hand-object interactions using multimodal large language models.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 5,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10340,
      "output_tokens": 1078,
      "predecessor_details": [
        {
          "success": true,
          "title": "Text-guided 3D Motion Generation for Hand-Object Interaction - arXiv",
          "url": "https://arxiv.org/abs/2404.00562",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2404.00562** (cs)\n\n\\[Submitted on 31 Mar 2024 ( [v1](https://arxiv.org/abs/2404.00562v1)), last revised 2 Apr 2024 (this version, v2)\\]\n\n# Title:Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction\n\nAuthors: [Junuk Cha](https://arxiv.org/search/cs?searchtype=author&query=Cha,+J), [Jihyeon Kim](https://arxiv.org/search/cs?searchtype=author&query=Kim,+J), [Jae Shin Yoon](https://arxiv.org/search/cs?searchtype=author&query=Yoon,+J+S), [Seungryul Baek](https://arxiv.org/search/cs?searchtype=author&query=Baek,+S)\n\nView a PDF of the paper titled Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction, by Junuk Cha and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2404.00562) [HTML (experimental)](https://arxiv.org/html/2404.00562v2)\n\n> Abstract:This paper introduces the first text-guided work for generating the sequence of hand-object interaction in 3D. The main challenge arises from the lack of labeled data where existing ground-truth datasets are nowhere near generalizable in interaction type and object category, which inhibits the modeling of diverse 3D hand-object interaction with the correct physical implication (e.g., contacts and semantics) from text prompts. To address this challenge, we propose to decompose the interaction generation task into two subtasks: hand-object contact generation; and hand-object motion generation. For contact generation, a VAE-based network takes as input a text and an object mesh, and generates the probability of contacts between the surfaces of hands and the object during the interaction. The network learns a variety of local geometry structure of diverse objects that is independent of the objects' category, and thus, it is applicable to general objects. For motion generation, a Transformer-based diffusion model utilizes this 3D contact map as a strong prior for generating physically plausible hand-object motion as a function of text prompts by learning from the augmented labeled dataset; where we annotate text labels from many existing 3D hand and object motion data. Finally, we further introduce a hand refiner module that minimizes the distance between the object surface and hand joints to improve the temporal stability of the object-hand contacts and to suppress the penetration artifacts. In the experiments, we demonstrate that our method can generate more realistic and diverse interactions compared to other baseline methods. We also show that our method is applicable to unseen objects. We will release our model and newly labeled data as a strong foundation for future research. Codes and data are available in: [this https URL](https://github.com/JunukCha/Text2HOI).\n\n| | |\n| --- | --- |\n| Comments: | Accepted to CVPR 2024 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2404.00562](https://arxiv.org/abs/2404.00562) \\[cs.CV\\] |\n| (or [arXiv:2404.00562v2](https://arxiv.org/abs/2404.00562v2) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2404.00562](https://doi.org/10.48550/arXiv.2404.00562) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Junuk Cha \\[ [view email](https://arxiv.org/show-email/14b1730a/2404.00562)\\] **[\\[v1\\]](https://arxiv.org/abs/2404.00562v1)**\nSun, 31 Mar 2024 04:56:30 UTC (40,787 KB)\n**\\[v2\\]**\nTue, 2 Apr 2024 02:08:55 UTC (40,787 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction, by Junuk Cha and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2404.00562)\n- [HTML (experimental)](https://arxiv.org/html/2404.00562v2)\n- [TeX Source](https://arxiv.org/src/2404.00562)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2404.00562&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2404.00562&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-04](https://arxiv.org/list/cs.CV/2024-04)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2404.00562?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2404.00562)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2404.00562)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2404.00562)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2404.00562) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction",
          "cleaned_query": "Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction"
        },
        {
          "success": true,
          "title": "DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from ...",
          "url": "https://arxiv.org/abs/2403.17827",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2403.17827** (cs)\n\n\\[Submitted on 26 Mar 2024 ( [v1](https://arxiv.org/abs/2403.17827v1)), last revised 23 Dec 2024 (this version, v2)\\]\n\n# Title:DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions\n\nAuthors: [Sammy Christen](https://arxiv.org/search/cs?searchtype=author&query=Christen,+S), [Shreyas Hampali](https://arxiv.org/search/cs?searchtype=author&query=Hampali,+S), [Fadime Sener](https://arxiv.org/search/cs?searchtype=author&query=Sener,+F), [Edoardo Remelli](https://arxiv.org/search/cs?searchtype=author&query=Remelli,+E), [Tomas Hodan](https://arxiv.org/search/cs?searchtype=author&query=Hodan,+T), [Eric Sauser](https://arxiv.org/search/cs?searchtype=author&query=Sauser,+E), [Shugao Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+S), [Bugra Tekin](https://arxiv.org/search/cs?searchtype=author&query=Tekin,+B)\n\nView a PDF of the paper titled DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions, by Sammy Christen and Shreyas Hampali and Fadime Sener and Edoardo Remelli and Tomas Hodan and Eric Sauser and Shugao Ma and Bugra Tekin\n\n[View PDF](https://arxiv.org/pdf/2403.17827) [HTML (experimental)](https://arxiv.org/html/2403.17827v2)\n\n> Abstract:Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful. Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets. In this paper, we propose a novel method, dubbed DiffH2O, which can synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object. The method introduces three techniques that enable effective learning from limited data. First, we decompose the task into a grasping stage and an text-based manipulation stage and use separate diffusion models for each. In the grasping stage, the model only generates hand motions, whereas in the manipulation phase both hand and object poses are synthesized. Second, we propose a compact representation that tightly couples hand and object poses and helps in generating realistic hand-object interactions. Third, we propose two different guidance schemes to allow more control of the generated motions: grasp guidance and detailed textual guidance. Grasp guidance takes a single target grasping pose and guides the diffusion model to reach this grasp at the end of the grasping stage, which provides control over the grasping pose. Given a grasping motion from this stage, multiple different actions can be prompted in the manipulation phase. For the textual guidance, we contribute comprehensive text descriptions to the GRAB dataset and show that they enable our method to have more fine-grained control over hand-object interactions. Our quantitative and qualitative evaluation demonstrates that the proposed method outperforms baseline methods and leads to natural hand-object motions.\n\n| | |\n| --- | --- |\n| Comments: | Project Page: [this https URL](https://diffh2o.github.io/) |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2403.17827](https://arxiv.org/abs/2403.17827) \\[cs.CV\\] |\n| | (or [arXiv:2403.17827v2](https://arxiv.org/abs/2403.17827v2) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2403.17827](https://doi.org/10.48550/arXiv.2403.17827) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | SIGGRAPH Asia Conference Papers, Article 145, 2024 |\n| Related DOI: | [https://doi.org/10.1145/3680528.3687563](https://doi.org/10.1145/3680528.3687563) Focus to learn more DOI(s) linking to related resources |\n\n## Submission history\n\nFrom: Sammy Christen \\[ [view email](https://arxiv.org/show-email/3fd16b00/2403.17827)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2403.17827v1)**\nTue, 26 Mar 2024 16:06:42 UTC (24,893 KB)\n\n**\\[v2\\]**\nMon, 23 Dec 2024 17:36:22 UTC (28,773 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions, by Sammy Christen and Shreyas Hampali and Fadime Sener and Edoardo Remelli and Tomas Hodan and Eric Sauser and Shugao Ma and Bugra Tekin\n\n- [View PDF](https://arxiv.org/pdf/2403.17827)\n- [HTML (experimental)](https://arxiv.org/html/2403.17827v2)\n- [TeX Source](https://arxiv.org/src/2403.17827)\n- [Other Formats](https://arxiv.org/format/2403.17827)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2403.17827&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2403.17827&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-03](https://arxiv.org/list/cs.CV/2024-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2403.17827?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2403.17827?context=cs.AI)\n\n[cs.GR](https://arxiv.org/abs/2403.17827?context=cs.GR)\n\n[cs.LG](https://arxiv.org/abs/2403.17827?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2403.17827)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2403.17827)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2403.17827)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2403.17827&description=DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2403.17827&title=DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institu",
          "original_query": "DiffH2O: Diffusion-based synthesis of hand-object interactions from textual descriptions",
          "cleaned_query": "DiffH2O: Diffusion-based synthesis of hand-object interactions from textual descriptions"
        },
        {
          "success": true,
          "title": "HOIGPT: Learning Long Sequence Hand-Object Interaction ... - arXiv",
          "url": "https://arxiv.org/abs/2503.19157",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2503.19157** (cs)\n\n\\[Submitted on 24 Mar 2025\\]\n\n# Title:HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models\n\nAuthors: [Mingzhen Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+M), [Fu-Jen Chu](https://arxiv.org/search/cs?searchtype=author&query=Chu,+F), [Bugra Tekin](https://arxiv.org/search/cs?searchtype=author&query=Tekin,+B), [Kevin J Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang,+K+J), [Haoyu Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+H), [Weiyao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+W), [Xingyu Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+X), [Pierre Gleize](https://arxiv.org/search/cs?searchtype=author&query=Gleize,+P), [Hongfei Xue](https://arxiv.org/search/cs?searchtype=author&query=Xue,+H), [Siwei Lyu](https://arxiv.org/search/cs?searchtype=author&query=Lyu,+S), [Kris Kitani](https://arxiv.org/search/cs?searchtype=author&query=Kitani,+K), [Matt Feiszli](https://arxiv.org/search/cs?searchtype=author&query=Feiszli,+M), [Hao Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+H)\n\nView a PDF of the paper titled HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models, by Mingzhen Huang and 12 other authors\n\n[View PDF](https://arxiv.org/pdf/2503.19157) [HTML (experimental)](https://arxiv.org/html/2503.19157v1)\n\n> Abstract:We introduce HOIGPT, a token-based generative method that unifies 3D hand-object interactions (HOI) perception and generation, offering the first comprehensive solution for captioning and generating high-quality 3D HOI sequences from a diverse range of conditional signals (\\\\eg text, objects, partial sequences). At its core, HOIGPT utilizes a large language model to predict the bidrectional transformation between HOI sequences and natural language descriptions. Given text inputs, HOIGPT generates a sequence of hand and object meshes; given (partial) HOI sequences, HOIGPT generates text descriptions and completes the sequences. To facilitate HOI understanding with a large language model, this paper introduces two key innovations: (1) a novel physically grounded HOI tokenizer, the hand-object decomposed VQ-VAE, for discretizing HOI sequences, and (2) a motion-aware language model trained to process and generate both text and HOI tokens. Extensive experiments demonstrate that HOIGPT sets new state-of-the-art performance on both text generation (+2.01% R Precision) and HOI generation (-2.56 FID) across multiple tasks and benchmarks.\n\n| | |\n| --- | --- |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2503.19157](https://arxiv.org/abs/2503.19157) \\[cs.CV\\] |\n| | (or [arXiv:2503.19157v1](https://arxiv.org/abs/2503.19157v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2503.19157](https://doi.org/10.48550/arXiv.2503.19157) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Mingzhen Huang \\[ [view email](https://arxiv.org/show-email/fa7f401d/2503.19157)\\]\n\n**\\[v1\\]**\nMon, 24 Mar 2025 21:25:29 UTC (2,367 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models, by Mingzhen Huang and 12 other authors\n\n- [View PDF](https://arxiv.org/pdf/2503.19157)\n- [HTML (experimental)](https://arxiv.org/html/2503.19157v1)\n- [TeX Source](https://arxiv.org/src/2503.19157)\n- [Other Formats](https://arxiv.org/format/2503.19157)\n\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2503.19157&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2503.19157&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2025-03](https://arxiv.org/list/cs.CV/2025-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2503.19157?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2503.19157)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2503.19157)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2503.19157)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2503.19157&description=HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2503.19157&title=HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2503.19157) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models",
          "cleaned_query": "HOIGPT: Learning Long Sequence Hand-Object Interaction with Language Models"
        },
        {
          "success": true,
          "title": "Universal 3D Object Understanding for Embodied Interaction - arXiv",
          "url": "https://arxiv.org/abs/2402.17766",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2402.17766** (cs)\n\n\\[Submitted on 27 Feb 2024 ( [v1](https://arxiv.org/abs/2402.17766v1)), last revised 12 Jul 2024 (this version, v3)\\]\n\n# Title:ShapeLLM: Universal 3D Object Understanding for Embodied Interaction\n\nAuthors: [Zekun Qi](https://arxiv.org/search/cs?searchtype=author&query=Qi,+Z), [Runpei Dong](https://arxiv.org/search/cs?searchtype=author&query=Dong,+R), [Shaochen Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+S), [Haoran Geng](https://arxiv.org/search/cs?searchtype=author&query=Geng,+H), [Chunrui Han](https://arxiv.org/search/cs?searchtype=author&query=Han,+C), [Zheng Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge,+Z), [Li Yi](https://arxiv.org/search/cs?searchtype=author&query=Yi,+L), [Kaisheng Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+K)\n\nView a PDF of the paper titled ShapeLLM: Universal 3D Object Understanding for Embodied Interaction, by Zekun Qi and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2402.17766) [HTML (experimental)](https://arxiv.org/html/2402.17766v3)\n\n> Abstract:This paper presents ShapeLLM, the first 3D Multimodal Large Language Model (LLM) designed for embodied interaction, exploring a universal 3D object understanding with 3D point clouds and languages. ShapeLLM is built upon an improved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view image distillation for enhanced geometry understanding. By utilizing ReCon++ as the 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructed instruction-following data and tested on our newly human-curated benchmark, 3D MM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance in 3D geometry understanding and language-unified 3D interaction tasks, such as embodied visual grounding. Project page: [this https URL](https://qizekun.github.io/shapellm/)\n\n| | |\n| --- | --- |\n| Comments: | Accepted at ECCV 2024 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2402.17766](https://arxiv.org/abs/2402.17766) \\[cs.CV\\] |\n| (or [arXiv:2402.17766v3](https://arxiv.org/abs/2402.17766v3) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2402.17766](https://doi.org/10.48550/arXiv.2402.17766) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Runpei Dong \\[ [view email](https://arxiv.org/show-email/3f14c175/2402.17766)\\] **[\\[v1\\]](https://arxiv.org/abs/2402.17766v1)**\nTue, 27 Feb 2024 18:57:12 UTC (7,432 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2402.17766v2)**\nWed, 6 Mar 2024 15:11:37 UTC (7,420 KB)\n**\\[v3\\]**\nFri, 12 Jul 2024 15:36:15 UTC (9,266 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled ShapeLLM: Universal 3D Object Understanding for Embodied Interaction, by Zekun Qi and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2402.17766)\n- [HTML (experimental)](https://arxiv.org/html/2402.17766v3)\n- [TeX Source](https://arxiv.org/src/2402.17766)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2402.17766&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2402.17766&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-02](https://arxiv.org/list/cs.CV/2024-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2402.17766?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2402.17766)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2402.17766)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2402.17766)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2402.17766) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Shapellm: Universal 3d object understanding for embodied interaction",
          "cleaned_query": "Shapellm: Universal 3d object understanding for embodied interaction"
        },
        {
          "success": true,
          "title": "3D-AffordanceLLM: Harnessing Large Language Models for ... - arXiv",
          "url": "https://arxiv.org/abs/2502.20041",
          "content": "[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\n[We gratefully acknowledge support from\\\n\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds",
          "cleaned_query": "3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds"
        },
        {
          "success": true,
          "title": "Gaze-guided Hand-Object Interaction Synthesis: Dataset and Method",
          "url": "https://arxiv.org/abs/2403.16169",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2403.16169** (cs)\n\n\\[Submitted on 24 Mar 2024 ( [v1](https://arxiv.org/abs/2403.16169v1)), last revised 7 Jan 2025 (this version, v5)\\]\n\n# Title:Gaze-guided Hand-Object Interaction Synthesis: Dataset and Method\n\nAuthors: [Jie Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian,+J), [Ran Ji](https://arxiv.org/search/cs?searchtype=author&query=Ji,+R), [Lingxiao Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+L), [Suting Ni](https://arxiv.org/search/cs?searchtype=author&query=Ni,+S), [Yuexin Ma](https://arxiv.org/search/cs?searchtype=author&query=Ma,+Y), [Lan Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+L), [Jingyi Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+J), [Ye Shi](https://arxiv.org/search/cs?searchtype=author&query=Shi,+Y), [Jingya Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+J)\n\nView a PDF of the paper titled Gaze-guided Hand-Object Interaction Synthesis: Dataset and Method, by Jie Tian and 8 other authors\n\n[View PDF](https://arxiv.org/pdf/2403.16169) [HTML (experimental)](https://arxiv.org/html/2403.16169v5)\n\n> Abstract:Gaze plays a crucial role in revealing human attention and intention, particularly in hand-object interaction scenarios, where it guides and synchronizes complex tasks that require precise coordination between the brain, hand, and object. Motivated by this, we introduce a novel task: Gaze-Guided Hand-Object Interaction Synthesis, with potential applications in augmented reality, virtual reality, and assistive technologies. To support this task, we present GazeHOI, the first dataset to capture simultaneous 3D modeling of gaze, hand, and object interactions. This task poses significant challenges due to the inherent sparsity and noise in gaze data, as well as the need for high consistency and physical plausibility in generating hand and object motions. To tackle these issues, we propose a stacked gaze-guided hand-object interaction diffusion model, named GHO-Diffusion. The stacked design effectively reduces the complexity of motion generation. We also introduce HOI-Manifold Guidance during the sampling stage of GHO-Diffusion, enabling fine-grained control over generated motions while maintaining the data manifold. Additionally, we propose a spatial-temporal gaze feature encoding for the diffusion condition and select diffusion results based on consistency scores between gaze-contact maps and gaze-interaction trajectories. Extensive experiments highlight the effectiveness of our method and the unique contributions of our dataset. More details in [this https URL](https://takiee.github.io/gaze-hoi/).\n\n| | |\n| --- | --- |\n| Comments: | Project Page: [this https URL](https://takiee.github.io/gaze-hoi/) |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2403.16169](https://arxiv.org/abs/2403.16169) \\[cs.CV\\] |\n| (or [arXiv:2403.16169v5](https://arxiv.org/abs/2403.16169v5) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2403.16169](https://doi.org/10.48550/arXiv.2403.16169) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jie Tian \\[ [view email](https://arxiv.org/show-email/173c4b3e/2403.16169)\\] **[\\[v1\\]](https://arxiv.org/abs/2403.16169v1)**\nSun, 24 Mar 2024 14:24:13 UTC (2,356 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2403.16169v2)**\nTue, 26 Mar 2024 06:39:30 UTC (2,807 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2403.16169v3)**\nThu, 28 Mar 2024 06:56:45 UTC (2,803 KB)\n**[\\[v4\\]](https://arxiv.org/abs/2403.16169v4)**\nThu, 22 Aug 2024 13:51:34 UTC (3,677 KB)\n**\\[v5\\]**\nTue, 7 Jan 2025 14:39:31 UTC (3,753 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Gaze-guided Hand-Object Interaction Synthesis: Dataset and Method, by Jie Tian and 8 other authors\n\n- [View PDF](https://arxiv.org/pdf/2403.16169)\n- [HTML (experimental)](https://arxiv.org/html/2403.16169v5)\n- [TeX Source](https://arxiv.org/src/2403.16169)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2403.16169&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2403.16169&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2024-03](https://arxiv.org/list/cs.CV/2024-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2403.16169?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2403.16169)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2403.16169)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2403.16169)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2403.16169) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Gaze-guided Hand-Object Interaction Synthesis: Dataset and Method",
          "cleaned_query": "Gaze-guided Hand-Object Interaction Synthesis: Dataset and Method"
        }
      ],
      "generated_ideas": [
        "Bidirectional \u201cExplainable HOI\u201d: Generate Contact-and-Text Rationales from Motion Tokens\nBuild on HOIGPT\u2019s HOI tokenization by adding an auxiliary decoder that produces (i) time-resolved contact events and (ii) structured textual rationales (\u201cthumb stabilizes rim while index pulls tab\u201d). This turns HOI generation into an interpretable system where generated sequences are accompanied by verifiable intermediate explanations, and where mismatches between rationale/contact/motion can be used as self-correction signals.",
        "Long-Horizon Task Graphs for HOI: From Single Prompt to Multi-Step Manipulation Programs\nExtend HOIGPT from sequence generation to *programmed* generation: parse text into a task graph (e.g., \u201cpick up mug \u2192 bring to mouth \u2192 tilt \u2192 put down\u201d) and generate a consistent long sequence with state continuity. Concretely, train a planner LLM over HOI tokens that outputs subgoal tokens, then refine each segment with diffusion while enforcing inter-segment contact/state constraints.",
        "Contact-Aware Tokenizer: Discretize HOI into Motion + Contact Event Codes for Better Control\nAugment HOIGPT\u2019s VQ-VAE tokenizer with explicit discrete contact-event codes (onset/offset, contact region IDs, bimanual coordination) learned jointly with geometry tokens. This yields a controllable representation where prompting can specify \u201cmaintain palm contact while rotating\u201d or \u201cswitch grip at t=",
        "2s,\u201d improving fine-grained editability and sequence completion.\n",
        "Counterfactual HOI Generation for Safety: \u201cWhat If the Object Were Slippery/Hot/Fragile?\u201d\nCreate a counterfactual conditioning mechanism that perturbs interaction constraints (reduced friction, avoidance zones, gentle force proxy) while keeping the same text goal, generating alternative safe strategies. Build this by (i) extending contact priors to include \u201cno-contact\u201d regions and (ii) using guidance terms during diffusion sampling to encourage compliant grasps/manipulations, enabling data generation for safety-aware robot learning."
      ],
      "generated_ideas_raw": "1. **Part-Level Contact Priors for Text-to-HOI on Articulated Objects**  \n   Extend Text2HOI-style contact-map generation to articulated objects (e.g., scissors, staplers, drawers) by predicting *part-conditioned* contact distributions on a kinematic graph rather than a single rigid mesh. Train with synthetic articulation perturbations and enforce consistency between predicted contacts and feasible joint states, enabling text prompts like \u201copen the scissors halfway\u201d to produce correct hand placement and object articulation.\n\n2. **Uncertainty-Aware Contact Maps for Robust HOI Diffusion on Unseen Shapes**  \n   Replace deterministic contact probabilities with calibrated uncertainty (e.g., Dirichlet or ensemble-based) in the contact VAE of Text2HOI, and propagate this uncertainty as a sampling-time prior in the diffusion motion generator (Text2HOI/DiffH2O). This would allow the model to explicitly hedge between multiple plausible grasps/manipulations for novel objects and choose trajectories that minimize penetration/contact violations under uncertainty.\n\n3. **Bidirectional \u201cExplainable HOI\u201d: Generate Contact-and-Text Rationales from Motion Tokens**  \n   Build on HOIGPT\u2019s HOI tokenization by adding an auxiliary decoder that produces (i) time-resolved contact events and (ii) structured textual rationales (\u201cthumb stabilizes rim while index pulls tab\u201d). This turns HOI generation into an interpretable system where generated sequences are accompanied by verifiable intermediate explanations, and where mismatches between rationale/contact/motion can be used as self-correction signals.\n\n4. **Multi-Objective Guidance: Joint Text + Gaze + Target Grasp Control in One Sampler**  \n   Combine DiffH2O\u2019s grasp guidance and GazeHOI\u2019s manifold guidance into a unified sampling framework that supports simultaneously: a target grasp pose, a text manipulation instruction, and a gaze trajectory. Implement guidance as multiple energy terms (grasp endpoint, gaze-contact alignment, language-action alignment) with adaptive weighting to avoid over-guidance, enabling controllable synthesis for AR/VR teleoperation scenarios.\n\n5. **Affordance-to-Motion Bridge: ShapeLLM-Guided HOI Generation via Functional Regions**  \n   Use ShapeLLM-style 3D understanding to extract functional regions (handle, button, rim) and convert them into a sparse \u201caffordance graph\u201d that conditions Text2HOI/DiffH2O motion generation. The key contribution is a modular pipeline where an object-only 3D-LLM proposes interaction hotspots and constraints, reducing reliance on HOI labels and improving generalization to unseen categories.\n\n6. **Long-Horizon Task Graphs for HOI: From Single Prompt to Multi-Step Manipulation Programs**  \n   Extend HOIGPT from sequence generation to *programmed* generation: parse text into a task graph (e.g., \u201cpick up mug \u2192 bring to mouth \u2192 tilt \u2192 put down\u201d) and generate a consistent long sequence with state continuity. Concretely, train a planner LLM over HOI tokens that outputs subgoal tokens, then refine each segment with diffusion while enforcing inter-segment contact/state constraints.\n\n7. **Physics-Verified HOI Diffusion with Differentiable Contact and Friction Constraints**  \n   Integrate a lightweight differentiable contact/friction model into the sampling/refinement stage (analogous to Text2HOI\u2019s hand refiner) to verify and correct diffusion samples. The actionable contribution is a \u201cphysics verifier\u201d that penalizes slipping (insufficient normal force proxy), unstable grasps, and interpenetration, producing more physically plausible results without requiring full rigid-body simulation in the training loop.\n\n8. **Personalized Hand-Object Interaction Generation via Few-Shot User Calibration**  \n   Adapt Text2HOI/DiffH2O/HOIGPT to individual users by learning a low-rank personalization module (e.g., LoRA on token embeddings or diffusion conditioning) from a few seconds of captured hand motion. The model would preserve the global interaction prior while matching user-specific hand shape, preferred grasps, and motion style\u2014useful for avatar animation and assistive interfaces.\n\n9. **Contact-Aware Tokenizer: Discretize HOI into Motion + Contact Event Codes for Better Control**  \n   Augment HOIGPT\u2019s VQ-VAE tokenizer with explicit discrete contact-event codes (onset/offset, contact region IDs, bimanual coordination) learned jointly with geometry tokens. This yields a controllable representation where prompting can specify \u201cmaintain palm contact while rotating\u201d or \u201cswitch grip at t=1.2s,\u201d improving fine-grained editability and sequence completion.\n\n10. **Counterfactual HOI Generation for Safety: \u201cWhat If the Object Were Slippery/Hot/Fragile?\u201d**  \n    Create a counterfactual conditioning mechanism that perturbs interaction constraints (reduced friction, avoidance zones, gentle force proxy) while keeping the same text goal, generating alternative safe strategies. Build this by (i) extending contact priors to include \u201cno-contact\u201d regions and (ii) using guidance terms during diffusion sampling to encourage compliant grasps/manipulations, enabling data generation for safety-aware robot learning.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Bidirectional \u201cExplainable HOI\u201d: Generate Contact-and-Text Rationales from Motion Tokens\nBuild on HOIGPT\u2019s HOI tokenization by adding an auxiliary decoder that produces (i) time-resolved contact event",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Long-Horizon Task Graphs for HOI: From Single Prompt to Multi-Step Manipulation Programs\nExtend HOIGPT from sequence generation to *programmed* generation: parse text into a task graph (e.g., \u201cpick up",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Contact-Aware Tokenizer: Discretize HOI into Motion + Contact Event Codes for Better Control\nAugment HOIGPT\u2019s VQ-VAE tokenizer with explicit discrete contact-event codes (onset/offset, contact region ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "2s,\u201d improving fine-grained editability and sequence completion.\n",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Counterfactual HOI Generation for Safety: \u201cWhat If the Object Were Slippery/Hot/Fragile?\u201d\nCreate a counterfactual conditioning mechanism that perturbs interaction constraints (reduced friction, avoida",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 57,
      "paper_title": "Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think",
      "contribution": "Representation Entanglement for Generation (REG) enhances the training efficiency and quality of image generation in diffusion models by entangling class tokens from pretrained models with low-level image latents.",
      "num_predecessors": 4,
      "predecessors_crawled": 4,
      "academic_sources": 4,
      "crawl_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 8903,
      "output_tokens": 961,
      "predecessor_details": [
        {
          "success": true,
          "title": "sihyun-yu/REPA: [ICLR'25 Oral] Representation Alignment ... - GitHub",
          "url": "https://github.com/sihyun-yu/REPA",
          "content": "GitHub - sihyun-yu/REPA: [ICLR&#39;25 Oral] Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think\n[Skip to content](#start-of-content)\n## Navigation Menu\nToggle navigation\n[](https://github.com/)\n[Sign in](https://github.com/login?return_to=https://github.com/sihyun-yu/REPA)\nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n \nSearch\nClear\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\n# Provide feedback\n \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n \nName\nQuery\nTo see all available qualifiers, see our[documentation](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax).\nCancelCreate saved search\n[Sign in](https://github.com/login?return_to=https://github.com/sihyun-yu/REPA)\n[Sign up](https://github.com/signup?ref_cta=Sign+up&amp;ref_loc=header+logged+out&amp;ref_page=/%3Cuser-name%3E/%3Crepo-name%3E&amp;source=header-repo&amp;source_repo=sihyun-yu/REPA)\nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload]()to refresh your session.You signed out in another tab or window.[Reload]()to refresh your session.You switched accounts on another tab or window.[Reload]()to refresh your session.Dismiss alert\n{{ message }}\n[sihyun-yu](https://github.com/sihyun-yu)/**[REPA](https://github.com/sihyun-yu/REPA)**Public\n* [Notifications](https://github.com/login?return_to=/sihyun-yu/REPA)You must be signed in to change notification settings\n* [Fork70](https://github.com/login?return_to=/sihyun-yu/REPA)\n* [Star1.5k](https://github.com/login?return_to=/sihyun-yu/REPA)\n[ICLR'25 Oral] Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think\n[sihyun.me/REPA/](http://sihyun.me/REPA/)\n### License\n[MIT license](https://github.com/sihyun-yu/REPA/blob/main/LICENSE)\n[1.5kstars](https://github.com/sihyun-yu/REPA/stargazers)[70forks](https://github.com/sihyun-yu/REPA/forks)[Branches](https://github.com/sihyun-yu/REPA/branches)[Tags](https://github.com/sihyun-yu/REPA/tags)[Activity](https://github.com/sihyun-yu/REPA/activity)\n[Star](https://github.com/login?return_to=/sihyun-yu/REPA)\n[Notifications](https://github.com/login?return_to=/sihyun-yu/REPA)You must be signed in to change notification settings\n# sihyun-yu/REPA\nmain\n[Branches](https://github.com/sihyun-yu/REPA/branches)[Tags](https://github.com/sihyun-yu/REPA/tags)\n[](https://github.com/sihyun-yu/REPA/branches)[](https://github.com/sihyun-yu/REPA/tags)\nGo to file\nCode\nOpen more actions menu\n## Folders and files\n|Name|Name|\nLast commit message\n|\nLast commit date\n|\n## Latest commit\n## History\n[20 Commits](https://github.com/sihyun-yu/REPA/commits/main/)\n[](https://github.com/sihyun-yu/REPA/commits/main/)\n|\n[models](https://github.com/sihyun-yu/REPA/tree/main/models)\n|\n[models](https://github.com/sihyun-yu/REPA/tree/main/models)\n|\n|\n|\n[preprocessing](https://github.com/sihyun-yu/REPA/tree/main/preprocessing)\n|\n[preprocessing](https://github.com/sihyun-yu/REPA/tree/main/preprocessing)\n|\n|\n|\n[.gitignore](https://github.com/sihyun-yu/REPA/blob/main/.gitignore)\n|\n[.gitignore](https://github.com/sihyun-yu/REPA/blob/main/.gitignore)\n|\n|\n|\n[LICENSE](https://github.com/sihyun-yu/REPA/blob/main/LICENSE)\n|\n[LICENSE](https://github.com/sihyun-yu/REPA/blob/main/LICENSE)\n|\n|\n|\n[README.md](https://github.com/sihyun-yu/REPA/blob/main/README.md)\n|\n[README.md](https://github.com/sihyun-yu/REPA/blob/main/README.md)\n|\n|\n|\n[dataset.py](https://github.com/sihyun-yu/REPA/blob/main/dataset.py)\n|\n[dataset.py](https://github.com/sihyun-yu/REPA/blob/main/dataset.py)\n|\n|\n|\n[generate.py](https://github.com/sihyun-yu/REPA/blob/main/generate.py)\n|\n[generate.py](https://github.com/sihyun-yu/REPA/blob/main/generate.py)\n|\n|\n|\n[generate\\_t2i.py](https://github.com/sihyun-yu/REPA/blob/main/generate_t2i.py)\n|\n[generate\\_t2i.py](https://github.com/sihyun-yu/REPA/blob/main/generate_t2i.py)\n|\n|\n|\n[loss.py](https://github.com/sihyun-yu/REPA/blob/main/loss.py)\n|\n[loss.py](https://github.com/sihyun-yu/REPA/blob/main/loss.py)\n|\n|\n|\n[requirements.txt](https://github.com/sihyun-yu/REPA/blob/main/requirements.txt)\n|\n[requirements.txt](https://github.com/sihyun-yu/REPA/blob/main/requirements.txt)\n|\n|\n|\n[samplers.py](https://github.com/sihyun-yu/REPA/blob/main/samplers.py)\n|\n[samplers.py](https://github.com/sihyun-yu/REPA/blob/main/samplers.py)\n|\n|\n|\n[samplers\\_t2i.py](https://github.com/sihyun-yu/REPA/blob/main/samplers_t2i.py)\n|\n[samplers\\_t2i.py](https://github.com/sihyun-yu/REPA/blob/main/samplers_t2i.py)\n|\n|\n|\n[train.py](https://github.com/sihyun-yu/REPA/blob/main/train.py)\n|\n[train.py](https://github.com/sihyun-yu/REPA/blob/main/train.py)\n|\n|\n|\n[train\\_t2i.py](https://github.com/sihyun-yu/REPA/blob/main/train_t2i.py)\n|\n[train\\_t2i.py](https://github.com/sihyun-yu/REPA/blob/main/train_t2i.py)\n|\n|\n|\n[utils.py](https://github.com/sihyun-yu/REPA/blob/main/utils.py)\n|\n[utils.py](https://github.com/sihyun-yu/REPA/blob/main/utils.py)\n|\n|\n|\nView all files\n|\n## Repository files navigation\n# Representation Alignment for Generation:\nTraining Diffusion Transformers Is Easier Than You Think\n[](#-representation-alignment-for-generation-training-diffusion-transformers-is-easier-than-you-think)\n[![arXiv](https://camo.githubusercontent.com/6c07d72b64b0db5486ebae513c9a43700473c6c2fda6b3657c1cfc0cf6c32e42/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f617258697625323070617065722d323431302e30363934302d6233316231622e737667)](https://arxiv.org/abs/2410.06940)[![PWC](https://camo.githubusercontent.com/17e2ef4e8930d284567c9c7c6beee28524ad3aa766c13db1413ed08bc807b3af/68747470733a2f2f696d672e736869656c64732e696f2f656e64706f696e742e7376673f75726c3d68747470733a2f2f70617065727377697468636f64652e636f6d2f62616467652f726570726573656e746174696f6e2d616c69676e6d656e742d666f722d67656e65726174696f6e2f696d6167652d67656e65726174696f6e2d6f6e2d696d6167656e65742d32353678323536)](https://paperswithcode.com/sota/image-generation-on-imagenet-256x256?p=representation-alignment-for-generation)\n[Sihyun\u00a0Yu](https://sihyun.me/)1**\u00b7**[Sangkyung\u00a0Kwak](https://www.linkedin.com/in/SangkyungKwak/)1**\u00b7**[Huiwon\u00a0Jang](https://huiwon-jang.github.io/)1**\u00b7**[Jongheon\u00a0Jeong](https://jh-jeong.github.io/)2\n[Jonathan\u00a0Huang](http://jonathan-huang.org/)3**\u00b7**[Jinwoo\u00a0Shin](https://alinlab.kaist.ac.kr/shin.html)1\\***\u00b7**[Saining\u00a0Xie](https://www.sainingxie.com/)4\\*\n1KAIST2Korea University3Scaled Foundations4New York University\n\\*Equal Advising\n### [[project page](https://sihyun.me/REPA)]\u2003[[arXiv](http://arxiv.org/abs/2410.06940)]\n[](#project-pagearxiv)\n**Summary**: We propose REPresentation Alignment (REPA), a method that aligns noisy input states in diffusion models with representations from pretrained visual encoders. This significantly improves training efficiency and generation quality. REPA speeds up SiT training by 17.5x and achieves state-of-the-art FID=1.42.\n### 1. Environment setup\n[](#1-environment-setup)\n```\nconda create -n repa python=3.9 -y\nconda activate repa\npip install -r requirements.txt\n```\n### 2. Dataset\n[](#2-dataset)\n#### Dataset download\n[](#dataset-download)\nCurrently, we provide experiments for[ImageNet](https://www.kaggle.com/competitions/imagenet-object-localization-challenge/data). You can place the data that you want and can specifiy it via`--data-dir`arguments in training scripts. Please refer to our[preprocessing guide](https://github.com/sihyun-yu/REPA/tree/master/preprocessing).\n### 3. Training\n[](#3-training)\n```\naccelerate launch train.py \\\\\n--report-to=\"wandb\"\\\\\n--allow-tf32 \\\\\n--mixed-precision=\"fp16\"\\\\\n--seed=0 \\\\\n--path-type=\"linear\"\\\\\n--prediction=\"v\"\\\\\n--weighting=\"uniform\"\\\\\n--model=\"SiT-XL/2\"\\\\\n--enc-type=\"dinov2-vit-b\"\\\\\n--proj-coeff=0",
          "original_query": "REPA: Robust Efficient Pretraining Alignment for Generative Models",
          "cleaned_query": "REPA: Robust Efficient Pretraining Alignment for Generative Models"
        },
        {
          "success": true,
          "title": "SiT: Exploring Flow and Diffusion-based Generative ...",
          "url": "https://arxiv.org/abs/2401.08740",
          "content": "[2401.08740] SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2401.08740\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2401.08740**(cs)\n[Submitted on 16 Jan 2024 ([v1](https://arxiv.org/abs/2401.08740v1)), last revised 23 Sep 2024 (this version, v2)]\n# Title:SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers\nAuthors:[Nanye Ma](https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+N),[Mark Goldstein](https://arxiv.org/search/cs?searchtype=author&amp;query=Goldstein,+M),[Michael S. Albergo](https://arxiv.org/search/cs?searchtype=author&amp;query=Albergo,+M+S),[Nicholas M. Boffi](https://arxiv.org/search/cs?searchtype=author&amp;query=Boffi,+N+M),[Eric Vanden-Eijnden](https://arxiv.org/search/cs?searchtype=author&amp;query=Vanden-Eijnden,+E),[Saining Xie](https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+S)\nView a PDF of the paper titled SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers, by Nanye Ma and 5 other authors\n[View PDF](https://arxiv.org/pdf/2401.08740)[HTML (experimental)](https://arxiv.org/html/2401.08740v2)> > Abstract:\n> We present Scalable Interpolant Transformers (SiT), a family of generative models built on the backbone of Diffusion Transformers (DiT). The interpolant framework, which allows for connecting two distributions in a more flexible way than standard diffusion models, makes possible a modular study of various design choices impacting generative models built on dynamical transport: learning in discrete or continuous time, the objective function, the interpolant that connects the distributions, and deterministic or stochastic sampling. By carefully introducing the above ingredients, SiT surpasses DiT uniformly across model sizes on the conditional ImageNet 256x256 and 512x512 benchmark using the exact same model structure, number of parameters, and GFLOPs. By exploring various diffusion coefficients, which can be tuned separately from learning, SiT achieves an FID-50K score of 2.06 and 2.62, respectively. Comments:|ECCV 2024; Code available:[this https URL](https://github.com/willisma/SiT)|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)|\nCite as:|[arXiv:2401.08740](https://arxiv.org/abs/2401.08740)[cs.CV]|\n|(or[arXiv:2401.08740v2](https://arxiv.org/abs/2401.08740v2)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2401.08740](https://doi.org/10.48550/arXiv.2401.08740)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Saining Xie [[view email](https://arxiv.org/show-email/5abdea4a/2401.08740)]\n**[[v1]](https://arxiv.org/abs/2401.08740v1)**Tue, 16 Jan 2024 18:55:25 UTC (16,564 KB)\n**[v2]**Mon, 23 Sep 2024 15:59:41 UTC (16,962 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers, by Nanye Ma and 5 other authors\n* [View PDF](https://arxiv.org/pdf/2401.08740)\n* [HTML (experimental)](https://arxiv.org/html/2401.08740v2)\n* [TeX Source](https://arxiv.org/src/2401.08740)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2401.08740&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2401.08740&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2024-01](https://arxiv.org/list/cs.CV/2024-01)\nChange to browse by:\n[cs](https://arxiv.org/abs/2401.08740?context=cs)\n[cs.LG](https://arxiv.org/abs/2401.08740?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2401.08740)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2401.08740)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2401.08740)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2401.08740)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Scalable Interpolant Transformers (SiT)",
          "cleaned_query": "Scalable Interpolant Transformers (SiT)"
        },
        {
          "success": true,
          "title": "Latent Denoising Diffusion GAN: Faster Sampling, Higher Image ...",
          "url": "https://www.researchgate.net/publication/380948198_Latent_Denoising_Diffusion_GAN_Faster_sampling_Higher_image_quality",
          "content": "Article PDF Available Abstract and Figures Diffusion models are emerging as a powerful solution for generating high-fidelity and diverse images, often surpassing GANs under many circumstances. However, their slow inference speeds hinder their potential for real-time applications. To address this, DiffusionGAN leveraged a conditional GAN to drastically reduce denoising steps and speed up inference. Its advancement, Wavelet Diffusion, further accelerated the process by converting data into wavelet space, thus enhancing efficiency. Nonetheless, these models still fall short of GANs in terms of speed and image quality. To bridge these gaps, this paper introduces the Latent Denoising Diffusion GAN, which employs pre-trained autoencoders to compress images into a compact latent space, significantly improving inference speed and image quality. Furthermore, we propose a Weighted Learning strategy to enhance diversity and image quality. Experimental results on the CIFAR-10, CelebA-HQ, and LSUN-Church datasets prove that our model achieves a state-of-the-art running speed among diffusion models. Compared to its predecessors, DiffusionGAN and Wavelet Diffusion, our model shows remarkable improvements on all evaluation metrics. Code and pre-trained checkpoints: https://github.com/thanhluantrinh/LDDGAN.git. Content may be subject to copyright. Discover the world's research 25+ million members 160+ million publication pages 2.3+ billion citations Join for free Content may be subject to copyright. \n \n \n \n \n \n \n Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. Digital Object Identifier 10.1 109/ACCESS.20 23.0322000 Latent Denoising Diffusion GAN: Faster sampling, Higher image quality LUAN THANH TRINH 1, TOMOKI HAMAGAMI 2 1 Department of Mathematics, Physics, Electrical Engineering and Computer Science, Y okohama National University (e-mail: tt.luan.ynu@gmail.com) 2 Department of Mathematics, Physics, Electrical Engineering and Computer Science, Y okohama National University (e-mail: hamagami@ynu.ac.jp) ABSTRACT Diffusion models are emerging as a powerful solution for generating high-\ufb01delity and di verse images, often surpassing GANs under many circumstances. Howev er, their slow inference speeds hinder their potential for real-time applications. T o address this, DiffusionGAN leveraged a conditional GAN to drastically reduce denoising steps and speed up inference. Its advancement, W avelet Diffusion, further accelerated the process by converting data into wav elet space, thus enhancing ef\ufb01ciency. Nonetheless, these models still fall short of GANs in terms of speed and image quality. T o bridge these gaps, this paper introduces the Latent Denoising Diffusion GAN, which employs pre-trained autoencoders to compress images into a compact latent space, signi\ufb01cantly improving inference speed and image quality. Furthermore, we propose a W eighted Learning strategy to enhance diversity and image quality. Experimental results on the CIFAR-10, CelebA-HQ, and LSUN-Church datasets pro ve that our model achieves a state-of-the-art running speed among diffusion models. Compared to its predecessors, DiffusionGAN and W avelet Diffusion, our model shows remarkable improvements on all e valuation metrics. Code and pre-trained checkpoints: https://github.com/thanhluantrinh/LDDGAN.git. INDEX TERMS Diffusion models, Image generation, Generative adv ersarial networks I. INTRODUCTION Despite being a recent introduction, diffusion models have quickly established themselves as a pivotal paradigm for image-generation tasks. At their core, diffusion models hinge on two crucial processes: the forward process (or diffusion process) and the reverse process (denoising process). In the forward process, Gaussian noise is incrementally infused into the input image until it transforms into an isotropic Gaussian. Conversely, in the reverse process, a model is meticulously trained to invert the forward process and faithfully reproduce the original input image. After training, the power of diffusion models shines as we can generate high quality images by nav- igating randomly sampled noise through the adeptly learned denoising process. In comparison to other prominent deep generative models, diffusion models distinguish themselves through the excel- lence of the generated images in terms of quality and diver- sity, coupled with their inherent training stability. Particularly noteworthy is the observation that dif fusion models have surpassed Generative Adversarial Networks (GANs), which have dominated the image generation task in recent years, excelling in both image quality ( [1], [2]) and diversity ( [3], [4], [49]). One notable aspect heightening expectations for diffusion models is their growing capacity to effecti vely in- corporate various conditional inputs, such as semantic maps, text, representations, and images. This versatility expands the potential applications of diffusion models into areas like text- to-image generation ( [2], [5], [30]), video generation ( [8], [10]), image-to-image translation ( [6], [7], [9]), text-to-3D generation [11], and beyond. Despite their considerable potential, the hindrance of slow inference speed poses a signi\ufb01cant obstacle for diffusion models in becoming fully-\ufb02edged image generation models that can meet the diverse expectations across v arious do- mains, particularly in real-time applications. The fundamen- tal cause of slow sampling in these models lies in the Gaussian assumption made during the denoising step, a assumption that is valid only for small step sizes. Consequently, diffusion models often necessitate a substantial number of denoising steps, typically ranging in the hundreds or thousands. By modeling complex and multimodal distributions through con- ditional GANs, DiffusionGAN [28] enables larger denois- ing steps, reducing the number of denoising steps to just a few, thereby signi\ufb01cantly accelerating the inference time of diffusion models. W avelet Diffusion [29], an enhancement of DiffusionGAN, achieves a further increase in inference speed by transferring data from pixel space to wavelet space, VOLUME 11, 2023 1 This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3406535 This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ \n \n \n Trinh et al.: Latent Denoising Diffusion GAN: Faster sampling, Higher image quality reducing the input data size by a factor of 4 and becoming the fastest existing diffusion model. Howe ver, Wav elet Diffusion still lags considerably behind StyleGAN [60]. Additionally, the acceleration of inference speed shows signs of compro- mising the output image quality, as the output quality of both DiffusionGAN and W avelet Diffusion is lower than that of StyleGAN and recent diffusion models. This paper aims to bridge both the gap in image quality and the gap in speed by introducing Latent Denoising Diffusion GAN (LDDGAN). Firstly, instead of residing in a high- dimensional pixel space, input images are compressed as much as possible into a low-dimensional latent space through pre-trained autoencoders. This compression signi\ufb01cantly re- duces computational costs during both training and inference, facilitating faster sampling. Given that the latent space is more suitable for diffusion models than the high-dimensional pixel space [30], our approach aims to enhance both image quality and sample diversity by utilizing this space for both the diffusion and denoising processes. Following the principles of DiffusionGAN, a conditional GAN is employed to model complex and multimodal distributions and enable a large denoising step. Additionally, to enhance di versity through adversarial loss while leveraging the ef fect of",
          "original_query": "Latent Denoising Diffusion Models",
          "cleaned_query": "Latent Denoising Diffusion Models"
        },
        {
          "success": true,
          "title": "Denoising Diffusion Autoencoders are Unified Self-supervised ...",
          "url": "https://arxiv.org/abs/2303.09769",
          "content": "[2303.09769] Denoising Diffusion Autoencoders are Unified Self-supervised Learners\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2303.09769\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2303.09769**(cs)\n[Submitted on 17 Mar 2023 ([v1](https://arxiv.org/abs/2303.09769v1)), last revised 19 Aug 2023 (this version, v2)]\n# Title:Denoising Diffusion Autoencoders are Unified Self-supervised Learners\nAuthors:[Weilai Xiang](https://arxiv.org/search/cs?searchtype=author&amp;query=Xiang,+W),[Hongyu Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+H),[Di Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+D),[Yunhong Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y)\nView a PDF of the paper titled Denoising Diffusion Autoencoders are Unified Self-supervised Learners, by Weilai Xiang and 3 other authors\n[View PDF](https://arxiv.org/pdf/2303.09769)> > Abstract:\n> Inspired by recent advances in diffusion models, which are reminiscent of denoising autoencoders, we investigate whether they can acquire discriminative representations for classification via generative pre-training. This paper shows that the networks in diffusion models, namely denoising diffusion autoencoders (DDAE), are unified self-supervised learners: by pre-training on unconditional image generation, DDAE has already learned strongly linear-separable representations within its intermediate layers without auxiliary encoders, thus making diffusion pre-training emerge as a general approach for generative-and-discriminative dual learning. To validate this, we conduct linear probe and fine-tuning evaluations. Our diffusion-based approach achieves 95.9% and 50.0% linear evaluation accuracies on CIFAR-10 and Tiny-ImageNet, respectively, and is comparable to contrastive learning and masked autoencoders for the first time. Transfer learning from ImageNet also confirms the suitability of DDAE for Vision Transformers, suggesting the potential to scale DDAEs as unified foundation models. Code is available at [> this http URL\n](http://github.com/FutureXiang/ddae)> . Comments:|ICCV 2023 Oral|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)|\nCite as:|[arXiv:2303.09769](https://arxiv.org/abs/2303.09769)[cs.CV]|\n|(or[arXiv:2303.09769v2](https://arxiv.org/abs/2303.09769v2)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2303.09769](https://doi.org/10.48550/arXiv.2303.09769)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Weilai Xiang [[view email](https://arxiv.org/show-email/9737f2b1/2303.09769)]\n**[[v1]](https://arxiv.org/abs/2303.09769v1)**Fri, 17 Mar 2023 04:20:47 UTC (3,185 KB)\n**[v2]**Sat, 19 Aug 2023 11:12:29 UTC (3,818 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Denoising Diffusion Autoencoders are Unified Self-supervised Learners, by Weilai Xiang and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2303.09769)\n* [TeX Source](https://arxiv.org/src/2303.09769)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2303.09769&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2303.09769&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2023-03](https://arxiv.org/list/cs.CV/2023-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2303.09769?context=cs)\n[cs.LG](https://arxiv.org/abs/2303.09769?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2303.09769)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2303.09769)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2303.09769)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2303.09769)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Denoising Diffusion Autoencoders are Unified Self-Supervised Learners",
          "cleaned_query": "Denoising Diffusion Autoencoders are Unified Self-Supervised Learners"
        }
      ],
      "generated_ideas": [
        "REPA-Interpolant: Representation-aligned SiT for more stable transport training\nCombine REPA\u2019s representation alignment loss with SiT\u2019s interpolant training by aligning intermediate DiT/SiT features to a fixed teacher (e.g., a pretrained self-supervised ViT) across interpolant time. Evaluate whether alignment reduces sensitivity to interpolant choice and diffusion coefficient schedules while improving FID/IS and training stability at matched compute.",
        "Time-Consistent Feature Alignment for Diffusion Transformers\nExtend REPA by enforcing *temporal consistency* of representations: features from the same underlying image at different noise/interpolant times should map to a shared manifold (e.g., via contrastive or regression losses across time). Test whether this improves both generation (fewer artifacts at low steps) and downstream linear-probe accuracy as in DDAE.",
        "Latent SiT: Scalable interpolant transformers operating in pretrained autoencoder latent space\nPort SiT\u2019s interpolant framework into the latent space used by LDDGAN (pretrained autoencoder latents) to reduce sampling cost while keeping the flexible transport benefits of interpolants. Compare against latent diffusion baselines using identical backbone capacity and report speed\u2013quality tradeoffs (FID vs. latency) at multiple resolutions.",
        "Adversarially-Accelerated Interpolant Sampling (GAN-corrected transport steps)\nInspired by LDDGAN\u2019s GAN-assisted large steps, train a lightweight discriminator on intermediate samples along SiT trajectories and use it to correct the sampler (e.g., one or two \u201cadversarial refinement\u201d steps). Quantify how many transport steps can be removed before quality collapses, and whether adversarial correction preserves diversity better than purely deterministic solvers.",
        "Weighted Learning Revisited: Step-adaptive reweighting for diffusion/interpolant objectives\nGeneralize LDDGAN\u2019s weighted learning strategy into a principled, step-adaptive weighting for diffusion/SiT objectives (e.g., weight by estimated score error, SNR, or feature misalignment). Implement an online rule that reweights time steps (or interpolant regions) to target perceptual failure modes, then benchmark improvements in both FID and worst-case class performance on ImageNet.",
        "Unified Generative-and-Discriminative Pretraining with REPA as the bridge\nBuild a single DiT/SiT model trained for generation while explicitly optimizing for linear-separable intermediate features (DDAE finding) using REPA-style alignment to a discriminative teacher. Measure whether this closes the gap to MAE/contrastive pretraining on downstream tasks while keeping (or improving) generative quality and sample efficiency.",
        "Interpolant-aware Distillation: Distill high-step SiT into a 2\u20138 step student in latent space\nTrain a fast student model that matches a strong SiT teacher\u2019s predicted velocity/score along interpolant time, but only at a small set of strategically chosen times (co-design the time grid with the interpolant). Run distillation in autoencoder latent space to maximize speed, and evaluate whether the student surpasses LDDGAN-like models on the same latency budget.",
        "Representation-Guided Sampler Selection: choosing deterministic vs. stochastic steps on the fly\nUse intermediate representation statistics (e.g., feature alignment error from REPA or feature confidence from a DDAE probe) to adaptively switch between deterministic and stochastic updates during SiT sampling. Hypothesis: allocate stochasticity only when representations indicate mode collapse risk, improving diversity at near-deterministic runtime."
      ],
      "generated_ideas_raw": "1. **REPA-Interpolant: Representation-aligned SiT for more stable transport training**  \n   Combine REPA\u2019s representation alignment loss with SiT\u2019s interpolant training by aligning intermediate DiT/SiT features to a fixed teacher (e.g., a pretrained self-supervised ViT) across interpolant time. Evaluate whether alignment reduces sensitivity to interpolant choice and diffusion coefficient schedules while improving FID/IS and training stability at matched compute.\n\n2. **Time-Consistent Feature Alignment for Diffusion Transformers**  \n   Extend REPA by enforcing *temporal consistency* of representations: features from the same underlying image at different noise/interpolant times should map to a shared manifold (e.g., via contrastive or regression losses across time). Test whether this improves both generation (fewer artifacts at low steps) and downstream linear-probe accuracy as in DDAE.\n\n3. **Latent SiT: Scalable interpolant transformers operating in pretrained autoencoder latent space**  \n   Port SiT\u2019s interpolant framework into the latent space used by LDDGAN (pretrained autoencoder latents) to reduce sampling cost while keeping the flexible transport benefits of interpolants. Compare against latent diffusion baselines using identical backbone capacity and report speed\u2013quality tradeoffs (FID vs. latency) at multiple resolutions.\n\n4. **Adversarially-Accelerated Interpolant Sampling (GAN-corrected transport steps)**  \n   Inspired by LDDGAN\u2019s GAN-assisted large steps, train a lightweight discriminator on intermediate samples along SiT trajectories and use it to correct the sampler (e.g., one or two \u201cadversarial refinement\u201d steps). Quantify how many transport steps can be removed before quality collapses, and whether adversarial correction preserves diversity better than purely deterministic solvers.\n\n5. **Weighted Learning Revisited: Step-adaptive reweighting for diffusion/interpolant objectives**  \n   Generalize LDDGAN\u2019s weighted learning strategy into a principled, step-adaptive weighting for diffusion/SiT objectives (e.g., weight by estimated score error, SNR, or feature misalignment). Implement an online rule that reweights time steps (or interpolant regions) to target perceptual failure modes, then benchmark improvements in both FID and worst-case class performance on ImageNet.\n\n6. **Unified Generative-and-Discriminative Pretraining with REPA as the bridge**  \n   Build a single DiT/SiT model trained for generation while explicitly optimizing for linear-separable intermediate features (DDAE finding) using REPA-style alignment to a discriminative teacher. Measure whether this closes the gap to MAE/contrastive pretraining on downstream tasks while keeping (or improving) generative quality and sample efficiency.\n\n7. **Multi-Teacher Representation Alignment for controllable generation**  \n   Extend REPA from one teacher to multiple specialized teachers (e.g., CLIP for semantics, DINOv2 for structure, a depth/normal estimator for geometry) and align different layers to different teachers. Test if layer-wise, teacher-specific alignment yields better controllability (prompt adherence, layout fidelity) without requiring explicit additional conditioning channels.\n\n8. **Interpolant-aware Distillation: Distill high-step SiT into a 2\u20138 step student in latent space**  \n   Train a fast student model that matches a strong SiT teacher\u2019s predicted velocity/score along interpolant time, but only at a small set of strategically chosen times (co-design the time grid with the interpolant). Run distillation in autoencoder latent space to maximize speed, and evaluate whether the student surpasses LDDGAN-like models on the same latency budget.\n\n9. **Representation-Guided Sampler Selection: choosing deterministic vs. stochastic steps on the fly**  \n   Use intermediate representation statistics (e.g., feature alignment error from REPA or feature confidence from a DDAE probe) to adaptively switch between deterministic and stochastic updates during SiT sampling. Hypothesis: allocate stochasticity only when representations indicate mode collapse risk, improving diversity at near-deterministic runtime.\n\n10. **Cross-Resolution Feature Alignment to stabilize 256\u2192512 scaling in DiT/SiT**  \n   Introduce a training scheme where a low-resolution branch (256) and high-resolution branch (512) share backbone weights, and REPA aligns their intermediate representations after appropriate pooling/patch matching. This targets the common instability and quality regressions when scaling resolution, and can be evaluated by measuring high-res FID gains at fixed parameters and training tokens.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "REPA-Interpolant: Representation-aligned SiT for more stable transport training\nCombine REPA\u2019s representation alignment loss with SiT\u2019s interpolant training by aligning intermediate DiT/SiT features t",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Time-Consistent Feature Alignment for Diffusion Transformers\nExtend REPA by enforcing *temporal consistency* of representations: features from the same underlying image at different noise/interpolant ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Latent SiT: Scalable interpolant transformers operating in pretrained autoencoder latent space\nPort SiT\u2019s interpolant framework into the latent space used by LDDGAN (pretrained autoencoder latents) to",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Adversarially-Accelerated Interpolant Sampling (GAN-corrected transport steps)\nInspired by LDDGAN\u2019s GAN-assisted large steps, train a lightweight discriminator on intermediate samples along SiT trajec",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Weighted Learning Revisited: Step-adaptive reweighting for diffusion/interpolant objectives\nGeneralize LDDGAN\u2019s weighted learning strategy into a principled, step-adaptive weighting for diffusion/SiT ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Unified Generative-and-Discriminative Pretraining with REPA as the bridge\nBuild a single DiT/SiT model trained for generation while explicitly optimizing for linear-separable intermediate features (DD",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Interpolant-aware Distillation: Distill high-step SiT into a 2\u20138 step student in latent space\nTrain a fast student model that matches a strong SiT teacher\u2019s predicted velocity/score along interpolant ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Representation-Guided Sampler Selection: choosing deterministic vs. stochastic steps on the fly\nUse intermediate representation statistics (e.g., feature alignment error from REPA or feature confidenc",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 58,
      "paper_title": "Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation",
      "contribution": "Dynam3D presents a dynamic layered 3D representation model that enhances vision-and-language navigation by improving spatial understanding and flexibility in changing environments.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11307,
      "output_tokens": 1128,
      "predecessor_details": [
        {
          "success": true,
          "title": "Learning Transferable Visual Models From Natural Language ...",
          "url": "https://arxiv.org/abs/2103.00020",
          "content": "[2103.00020] Learning Transferable Visual Models From Natural Language Supervision\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2103.00020\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2103.00020**(cs)\n[Submitted on 26 Feb 2021]\n# Title:Learning Transferable Visual Models From Natural Language Supervision\nAuthors:[Alec Radford](https://arxiv.org/search/cs?searchtype=author&amp;query=Radford,+A),[Jong Wook Kim](https://arxiv.org/search/cs?searchtype=author&amp;query=Kim,+J+W),[Chris Hallacy](https://arxiv.org/search/cs?searchtype=author&amp;query=Hallacy,+C),[Aditya Ramesh](https://arxiv.org/search/cs?searchtype=author&amp;query=Ramesh,+A),[Gabriel Goh](https://arxiv.org/search/cs?searchtype=author&amp;query=Goh,+G),[Sandhini Agarwal](https://arxiv.org/search/cs?searchtype=author&amp;query=Agarwal,+S),[Girish Sastry](https://arxiv.org/search/cs?searchtype=author&amp;query=Sastry,+G),[Amanda Askell](https://arxiv.org/search/cs?searchtype=author&amp;query=Askell,+A),[Pamela Mishkin](https://arxiv.org/search/cs?searchtype=author&amp;query=Mishkin,+P),[Jack Clark](https://arxiv.org/search/cs?searchtype=author&amp;query=Clark,+J),[Gretchen Krueger](https://arxiv.org/search/cs?searchtype=author&amp;query=Krueger,+G),[Ilya Sutskever](https://arxiv.org/search/cs?searchtype=author&amp;query=Sutskever,+I)\nView a PDF of the paper titled Learning Transferable Visual Models From Natural Language Supervision, by Alec Radford and 11 other authors\n[View PDF](https://arxiv.org/pdf/2103.00020)> > Abstract:\n> State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at [> this https URL\n](https://github.com/OpenAI/CLIP)> . Subjects:|Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)|\nCite as:|[arXiv:2103.00020](https://arxiv.org/abs/2103.00020)[cs.CV]|\n|(or[arXiv:2103.00020v1](https://arxiv.org/abs/2103.00020v1)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2103.00020](https://doi.org/10.48550/arXiv.2103.00020)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jong Wook Kim [[view email](https://arxiv.org/show-email/6c157f7d/2103.00020)]\n**[v1]**Fri, 26 Feb 2021 19:04:58 UTC (6,174 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Learning Transferable Visual Models From Natural Language Supervision, by Alec Radford and 11 other authors\n* [View PDF](https://arxiv.org/pdf/2103.00020)\n* [TeX Source](https://arxiv.org/src/2103.00020)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2103.00020&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2103.00020&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2021-03](https://arxiv.org/list/cs.CV/2021-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2103.00020?context=cs)\n[cs.LG](https://arxiv.org/abs/2103.00020?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2103.00020)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2103.00020)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2103.00020)\n### [16 blog links](https://arxiv.org/tb/2103.00020)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2103.html#abs-2103-00020)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2103-00020)\n[Alec Radford]()\n[Jong Wook Kim]()\n[Aditya Ramesh]()\n[Gabriel Goh]()\n[Girish Sastry]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommende",
          "original_query": "Learning transferable visual models from natural language supervision",
          "cleaned_query": "Learning transferable visual models from natural language supervision"
        },
        {
          "success": true,
          "title": "Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments",
          "url": "https://ieeexplore.ieee.org/document/8578485",
          "content": "Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments \\| IEEE Conference Publication \\| IEEE Xplore\n\n### IEEE Account\n\n- [Change Username/Password](https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Update Address](https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n\n### Purchase Details\n\n- [Payment Options](https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Order History](https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [View Purchased Documents](https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp)\n\n### Profile Information\n\n- [Communications Preferences](https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Profession and Education](https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Technical Interests](https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n\n### Need Help?\n\n- **US & Canada:** +1 800 678 4333\n- **Worldwide:** +1 732 981 0060\n\n- [Contact & Support](https://ieeexplore.ieee.org/xpl/contact)\n\n- [About IEEE _Xplore_](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-ieee-xplore)\n- [Contact Us](https://ieeexplore.ieee.org/xpl/contact)\n- [Help](https://ieeexplore.ieee.org/Xplorehelp)\n- [Accessibility](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/accessibility-statement)\n- [Terms of Use](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/terms-of-use)\n- [Nondiscrimination Policy](http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html)\n- [Sitemap](https://ieeexplore.ieee.org/xpl/sitemap.jsp)\n- [Privacy & Opting Out of Cookies](http://www.ieee.org/about/help/security_privacy.html)\n\nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.\n\n\u00a9 Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.",
          "original_query": "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments",
          "cleaned_query": "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments"
        },
        {
          "success": true,
          "title": "[2306.12156] Fast Segment Anything",
          "url": "https://arxiv.org/abs/2306.12156",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2306.12156** (cs)\n\n\\[Submitted on 21 Jun 2023\\]\n\n# Title:Fast Segment Anything\n\nAuthors: [Xu Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao,+X), [Wenchao Ding](https://arxiv.org/search/cs?searchtype=author&query=Ding,+W), [Yongqi An](https://arxiv.org/search/cs?searchtype=author&query=An,+Y), [Yinglong Du](https://arxiv.org/search/cs?searchtype=author&query=Du,+Y), [Tao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+T), [Min Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+M), [Ming Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+M), [Jinqiao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+J)\n\nView a PDF of the paper titled Fast Segment Anything, by Xu Zhao and 7 other authors\n\n[View PDF](https://arxiv.org/pdf/2306.12156)\n\n> Abstract:The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at high-resolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance. By reformulating the task as segments-generation and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM method at 50 times higher run-time speed. We give sufficient experimental results to demonstrate its effectiveness. The codes and demos will be released at [this https URL](https://github.com/CASIA-IVA-Lab/FastSAM).\n\n| | |\n| --- | --- |\n| Comments: | Technical Report. The code is released at [this https URL](https://github.com/CASIA-IVA-Lab/FastSAM) |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2306.12156](https://arxiv.org/abs/2306.12156) \\[cs.CV\\] |\n| | (or [arXiv:2306.12156v1](https://arxiv.org/abs/2306.12156v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2306.12156](https://doi.org/10.48550/arXiv.2306.12156) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Yongqi An \\[ [view email](https://arxiv.org/show-email/6d2d92ac/2306.12156)\\]\n\n**\\[v1\\]**\nWed, 21 Jun 2023 10:08:29 UTC (6,125 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Fast Segment Anything, by Xu Zhao and 7 other authors\n\n- [View PDF](https://arxiv.org/pdf/2306.12156)\n- [TeX Source](https://arxiv.org/src/2306.12156)\n- [Other Formats](https://arxiv.org/format/2306.12156)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2306.12156&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2306.12156&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2023-06](https://arxiv.org/list/cs.CV/2023-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2306.12156?context=cs)\n\n[cs.AI](https://arxiv.org/abs/2306.12156?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2306.12156)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2306.12156)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2306.12156)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2306.12156&description=Fast Segment Anything) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2306.12156&title=Fast Segment Anything)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2306.12156) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Fast segment anything",
          "cleaned_query": "Fast segment anything"
        },
        {
          "success": true,
          "title": "g3D-LF: Generalizable 3D-Language Feature Fields for Embodied ...",
          "url": "https://arxiv.org/abs/2411.17030",
          "content": "[2411.17030] g3D-LF: Generalizable 3D-Language Feature Fields for Embodied Tasks\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2411.17030\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2411.17030**(cs)\n[Submitted on 26 Nov 2024]\n# Title:g3D-LF: Generalizable 3D-Language Feature Fields for Embodied Tasks\nAuthors:[Zihan Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z),[Gim Hee Lee](https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+G+H)\nView a PDF of the paper titled g3D-LF: Generalizable 3D-Language Feature Fields for Embodied Tasks, by Zihan Wang and 1 other authors\n[View PDF](https://arxiv.org/pdf/2411.17030)[HTML (experimental)](https://arxiv.org/html/2411.17030v1)> > Abstract:\n> We introduce Generalizable 3D-Language Feature Fields (g3D-LF), a 3D representation model pre-trained on large-scale 3D-language dataset for embodied tasks. Our g3D-LF processes posed RGB-D images from agents to encode feature fields for: 1) Novel view representation predictions from any position in the 3D scene; 2) Generations of BEV maps centered on the agent; 3) Querying targets using multi-granularity language within the above-mentioned representations. Our representation can be generalized to unseen environments, enabling real-time construction and dynamic updates. By volume rendering latent features along sampled rays and integrating semantic and spatial relationships through multiscale encoders, our g3D-LF produces representations at different scales and perspectives, aligned with multi-granularity language, via multi-level contrastive learning. Furthermore, we prepare a large-scale 3D-language dataset to align the representations of the feature fields with language. Extensive experiments on Vision-and-Language Navigation under both Panorama and Monocular settings, Zero-shot Object Navigation, and Situated Question Answering tasks highlight the significant advantages and effectiveness of our g3D-LF for embodied tasks. Subjects:|Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Robotics (cs.RO)|\nCite as:|[arXiv:2411.17030](https://arxiv.org/abs/2411.17030)[cs.CV]|\n|(or[arXiv:2411.17030v1](https://arxiv.org/abs/2411.17030v1)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2411.17030](https://doi.org/10.48550/arXiv.2411.17030)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Zihan Wang [[view email](https://arxiv.org/show-email/d8e19cbf/2411.17030)]\n**[v1]**Tue, 26 Nov 2024 01:54:52 UTC (4,930 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled g3D-LF: Generalizable 3D-Language Feature Fields for Embodied Tasks, by Zihan Wang and 1 other authors\n* [View PDF](https://arxiv.org/pdf/2411.17030)\n* [HTML (experimental)](https://arxiv.org/html/2411.17030v1)\n* [TeX Source](https://arxiv.org/src/2411.17030)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2411.17030&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2411.17030&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2024-11](https://arxiv.org/list/cs.CV/2024-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/2411.17030?context=cs)\n[cs.AI](https://arxiv.org/abs/2411.17030?context=cs.AI)\n[cs.RO](https://arxiv.org/abs/2411.17030?context=cs.RO)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2411.17030)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2411.17030)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2411.17030)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2411.17030)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Generalizable 3D-language feature fields for embodied tasks",
          "cleaned_query": "Generalizable 3D-language feature fields for embodied tasks"
        },
        {
          "success": true,
          "title": "Matterport3D: Learning from RGB-D Data in Indoor Environments",
          "url": "https://arxiv.org/abs/1709.06158",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:1709.06158** (cs)\n\n\\[Submitted on 18 Sep 2017\\]\n\n# Title:Matterport3D: Learning from RGB-D Data in Indoor Environments\n\nAuthors: [Angel Chang](https://arxiv.org/search/cs?searchtype=author&query=Chang,+A), [Angela Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai,+A), [Thomas Funkhouser](https://arxiv.org/search/cs?searchtype=author&query=Funkhouser,+T), [Maciej Halber](https://arxiv.org/search/cs?searchtype=author&query=Halber,+M), [Matthias Nie\u00dfner](https://arxiv.org/search/cs?searchtype=author&query=Nie%C3%9Fner,+M), [Manolis Savva](https://arxiv.org/search/cs?searchtype=author&query=Savva,+M), [Shuran Song](https://arxiv.org/search/cs?searchtype=author&query=Song,+S), [Andy Zeng](https://arxiv.org/search/cs?searchtype=author&query=Zeng,+A), [Yinda Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+Y)\n\nView a PDF of the paper titled Matterport3D: Learning from RGB-D Data in Indoor Environments, by Angel Chang and 8 other authors\n\n[View PDF](https://arxiv.org/pdf/1709.06158)\n\n> Abstract:Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.\n\n| | |\n| --- | --- |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:1709.06158](https://arxiv.org/abs/1709.06158) \\[cs.CV\\] |\n| (or [arXiv:1709.06158v1](https://arxiv.org/abs/1709.06158v1) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1709.06158](https://doi.org/10.48550/arXiv.1709.06158) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Matthias Nie\u00dfner \\[ [view email](https://arxiv.org/show-email/77fff39b/1709.06158)\\] **\\[v1\\]**\nMon, 18 Sep 2017 20:34:48 UTC (18,615 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Matterport3D: Learning from RGB-D Data in Indoor Environments, by Angel Chang and 8 other authors\n\n- [View PDF](https://arxiv.org/pdf/1709.06158)\n- [TeX Source](https://arxiv.org/src/1709.06158)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1709.06158&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1709.06158&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2017-09](https://arxiv.org/list/cs.CV/2017-09)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1709.06158?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1709.06158)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1709.06158)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1709.06158)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1709.html#abs-1709-06158) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1709-06158)\n\n[Angel X. Chang](https://dblp.uni-trier.de/search/author?author=Angel%20X.%20Chang) [Angela Dai](https://dblp.uni-trier.de/search/author?author=Angela%20Dai) [Thomas A. Funkhouser](https://dblp.uni-trier.de/search/author?author=Thomas%20A.%20Funkhouser) [Maciej Halber](https://dblp.uni-trier.de/search/author?author=Maciej%20Halber) [Matthias Nie\u00dfner](https://dblp.uni-trier.de/search/author?author=Matthias%20Nie%C3%9Fner)\n\n\u2026\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1709.06158) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Matterport3D: Learning from rgb-d data in indoor environments",
          "cleaned_query": "Matterport3D: Learning from rgb-d data in indoor environments"
        },
        {
          "success": true,
          "title": "Bridging 3D Scene and Large Language Models with Object Identifiers",
          "url": "https://openreview.net/forum?id=t3BhmwAzhv&noteId=fqVOOHvTCf",
          "content": "[Go to **NeurIPS 2024 Conference** homepage](https://openreview.net/group?id=NeurIPS.cc/2024/Conference)\n\n## Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers\n\n### [Haifeng Huang](https://openreview.net/profile?id=~Haifeng_Huang3), [Yilun Chen](https://openreview.net/profile?id=~Yilun_Chen1), [Zehan Wang](https://openreview.net/profile?id=~Zehan_Wang2), [Rongjie Huang](https://openreview.net/profile?id=~Rongjie_Huang1), [Runsen Xu](https://openreview.net/profile?id=~Runsen_Xu1), [Tai Wang](https://openreview.net/profile?id=~Tai_Wang2), [Luping Liu](https://openreview.net/profile?id=~Luping_Liu2), [Xize Cheng](https://openreview.net/profile?id=~Xize_Cheng1), [Yang Zhao](https://openreview.net/profile?id=~Yang_Zhao14), [Jiangmiao Pang](https://openreview.net/profile?id=~Jiangmiao_Pang1), [Zhou Zhao](https://openreview.net/profile?id=~Zhou_Zhao3)\n\nPublished: 25 Sept 2024, Last Modified: 06 Nov 2024NeurIPS 2024 posterEveryone[Revisions](https://openreview.net/revisions?id=t3BhmwAzhv)[BibTeX](https://openreview.net/openreview.net)[CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/)\n\n**Keywords:** 3D Scene Understanding, Multi-modal Large Language Model\n\n**TL;DR:** A 3D large language model unifying referencing and grounding capabilities.\n\n**Abstract:** Recent advancements in 3D Large Language Models (LLMs) have demonstrated promising capabilities for 3D scene understanding. However, previous methods exhibit deficiencies in general referencing and grounding capabilities for intricate scene comprehension. In this paper, we introduce the use of object identifiers and object-centric representations to interact with scenes at the object level. Specifically, we decompose the input 3D scene into a set of object proposals, each assigned a unique identifier token, which enables efficient object referencing and grounding during user-assistant interactions. Given the scarcity of scene-language data, we model the scene embeddings as a sequence of explicit object-level embeddings, derived from semantic-rich 2D or 3D representations. By employing object identifiers, we transform diverse 3D scene-language tasks into a unified question-answering format, facilitating joint training without the need for additional task-specific heads. With minimal fine-tuning on all downstream tasks, our model significantly outperforms existing methods on benchmarks including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D.\n\n**Supplementary Material:** [zip](https://openreview.net/attachment?id=t3BhmwAzhv&name=supplementary_material)\n\n**Primary Area:** Machine vision\n\n**Submission Number:** 1498\n\nLoading\n\n[OpenReview](https://openreview.net/about) is a long-term project to advance science through improved peer review with legal nonprofit status. We gratefully acknowledge the support of the [OpenReview Sponsors](https://openreview.net/sponsors). \u00a9 2025 OpenReview",
          "original_query": "Chat-scene: Bridging 3D scene and large language models with object identifiers",
          "cleaned_query": "Chat-scene: Bridging 3D scene and large language models with object identifiers"
        }
      ],
      "generated_ideas": [
        "1. **CLIP-Guided FastSAM for Open-Vocabulary Instance Segmentation**\n   - Combine CLIP\u2019s text-image alignment with FastSAM\u2019s CNN instance segmentation to enable open-vocabulary segmentation via text prompts without relying on heavy transformer backbones. Train with pseudo-labels created by ranking FastSAM proposals using CLIP similarity to caption phrases, then distill into a lightweight promptable model. Evaluate zero-shot segmentation on indoor scenes (Matterport3D) and general internet imagery.",
        "**Panorama-Aware 3D-Language Feature Fields for VLN in Building-Scale Scenes**\n   - Extend g3D-LF to explicitly model panoramic observation geometry (equirectangular projection) as used in VLN and Matterport3D. Introduce a spherical ray-sampling and multiscale panoramic encoder that preserves long-range boundary continuity, then align with multi-granularity language via contrastive losses. Benchmark on panorama vs. monocular VLN to quantify gains in instruction following and viewpoint transfer.",
        "**Object-Identifier Feature Fields: A Unified 3D Tokenization for LLM Navigation and QA**\n   - Fuse Chat-Scene\u2019s object identifier tokens with g3D-LF by generating persistent object IDs directly from the 3D feature field (via clustering/proposal tracking) and exposing them as discrete tokens to an LLM. This enables consistent multi-turn dialogue (\u201cgo to object #12 again\u201d) and cross-task transfer between VLN, situated QA, and object navigation. Contribute a training recipe that jointly optimizes ID stability, grounding accuracy, and language generation.",
        "**Language-Conditioned BEV Map Editing for Embodied Planning**\n   - Build on g3D-LF\u2019s BEV generation by enabling text-driven edits to the BEV latent map (e.g., \u201cavoid the kitchen and go through the hallway\u201d). Learn an edit operator that transforms BEV features conditioned on instruction constraints, then feed the edited BEV to a planner/policy for action selection. Evaluate improvements in constraint satisfaction and recovery from ambiguous instructions in VLN.",
        "**Real-Time Incremental 3D-Language Mapping with Confidence-Calibrated Updates**\n   - Develop an online update mechanism for g3D-LF that maintains uncertainty over feature-field regions as new RGB-D frames arrive. Use calibrated confidence (e.g., temperature-scaled contrastive scores) to decide when to overwrite vs. fuse features, reducing catastrophic drift in long trajectories. Validate on long-horizon indoor runs with revisits (Matterport3D-style) measuring map consistency and downstream navigation success.",
        "**Fast Proposal-to-Field Distillation: Training g3D-LF from Lightweight 2D Segmentation**\n   - Use FastSAM to produce fast instance masks and object proposals on incoming frames, then distill these into g3D-LF\u2019s 3D feature field as auxiliary supervision (objectness, boundaries, instance separability). This reduces reliance on expensive 3D annotations while improving object-centric structure in the field. Show that better instance-structured fields improve object navigation and referring expression grounding.",
        "**Instruction Decomposition with 3D Grounded Subgoals via Contrastive Retrieval**\n   - Create a pipeline that decomposes VLN instructions into subgoals (landmarks/actions) and retrieves corresponding 3D field regions using g3D-LF\u2019s multi-granularity language querying. Train a contrastive objective linking sub-instruction spans to 3D waypoints/BEV cells derived from expert trajectories. Demonstrate gains in interpretability (explicit subgoal grounding) and robustness to instruction paraphrases.",
        "**Cross-Modal Hard Negative Mining Using 3D Spatial Confusability**\n   - Improve CLIP-style contrastive learning for embodied scenes by constructing \u201chard negatives\u201d from spatially confusable regions/objects in 3D (e.g., identical chairs in different rooms) using Matterport3D reconstructions. Apply this to pre-train a 3D-language representation (or fine-tune g3D-LF) that better separates semantically similar but spatially distinct entities. Measure improvements on ScanRefer/Multi3DRefer-style grounding and VLN landmark disambiguation.",
        "**Multi-Turn Embodied Dialogue Benchmark with Persistent Object IDs and Panoramic Observations**\n   - Build a dataset/benchmark where an agent in Matterport3D-like scenes must handle multi-turn user dialogue that references previously mentioned objects using persistent identifiers (Chat-Scene) and natural phrases (\u201cthe second door on the left\u201d). Provide aligned panoramic RGB-D, object proposal tracks, and dialogue annotations spanning navigation + QA. The contribution is a standardized evaluation that tests memory, grounding consistency, and instruction-following in one loop.",
        "**Energy-Efficient On-Device Embodied Perception Stack: CLIP Retrieval + FastSAM + Lightweight 3D Field**\n   - Design a deployable stack where CLIP performs low-cost semantic retrieval (\u201cfind a microwave\u201d), FastSAM produces candidate masks, and a compact g3D-LF variant maintains a local 3D-language map for planning. Introduce system-level optimizations (caching text embeddings, selective ray sampling, adaptive resolution BEV) and report latency/energy alongside task success. This bridges foundation-model capabilities with real-time robotics constraints."
      ],
      "generated_ideas_raw": "1. **CLIP-Guided FastSAM for Open-Vocabulary Instance Segmentation**\n   - Combine CLIP\u2019s text-image alignment with FastSAM\u2019s CNN instance segmentation to enable open-vocabulary segmentation via text prompts without relying on heavy transformer backbones. Train with pseudo-labels created by ranking FastSAM proposals using CLIP similarity to caption phrases, then distill into a lightweight promptable model. Evaluate zero-shot segmentation on indoor scenes (Matterport3D) and general internet imagery.\n\n2. **Panorama-Aware 3D-Language Feature Fields for VLN in Building-Scale Scenes**\n   - Extend g3D-LF to explicitly model panoramic observation geometry (equirectangular projection) as used in VLN and Matterport3D. Introduce a spherical ray-sampling and multiscale panoramic encoder that preserves long-range boundary continuity, then align with multi-granularity language via contrastive losses. Benchmark on panorama vs. monocular VLN to quantify gains in instruction following and viewpoint transfer.\n\n3. **Object-Identifier Feature Fields: A Unified 3D Tokenization for LLM Navigation and QA**\n   - Fuse Chat-Scene\u2019s object identifier tokens with g3D-LF by generating persistent object IDs directly from the 3D feature field (via clustering/proposal tracking) and exposing them as discrete tokens to an LLM. This enables consistent multi-turn dialogue (\u201cgo to object #12 again\u201d) and cross-task transfer between VLN, situated QA, and object navigation. Contribute a training recipe that jointly optimizes ID stability, grounding accuracy, and language generation.\n\n4. **Language-Conditioned BEV Map Editing for Embodied Planning**\n   - Build on g3D-LF\u2019s BEV generation by enabling text-driven edits to the BEV latent map (e.g., \u201cavoid the kitchen and go through the hallway\u201d). Learn an edit operator that transforms BEV features conditioned on instruction constraints, then feed the edited BEV to a planner/policy for action selection. Evaluate improvements in constraint satisfaction and recovery from ambiguous instructions in VLN.\n\n5. **Real-Time Incremental 3D-Language Mapping with Confidence-Calibrated Updates**\n   - Develop an online update mechanism for g3D-LF that maintains uncertainty over feature-field regions as new RGB-D frames arrive. Use calibrated confidence (e.g., temperature-scaled contrastive scores) to decide when to overwrite vs. fuse features, reducing catastrophic drift in long trajectories. Validate on long-horizon indoor runs with revisits (Matterport3D-style) measuring map consistency and downstream navigation success.\n\n6. **Fast Proposal-to-Field Distillation: Training g3D-LF from Lightweight 2D Segmentation**\n   - Use FastSAM to produce fast instance masks and object proposals on incoming frames, then distill these into g3D-LF\u2019s 3D feature field as auxiliary supervision (objectness, boundaries, instance separability). This reduces reliance on expensive 3D annotations while improving object-centric structure in the field. Show that better instance-structured fields improve object navigation and referring expression grounding.\n\n7. **Instruction Decomposition with 3D Grounded Subgoals via Contrastive Retrieval**\n   - Create a pipeline that decomposes VLN instructions into subgoals (landmarks/actions) and retrieves corresponding 3D field regions using g3D-LF\u2019s multi-granularity language querying. Train a contrastive objective linking sub-instruction spans to 3D waypoints/BEV cells derived from expert trajectories. Demonstrate gains in interpretability (explicit subgoal grounding) and robustness to instruction paraphrases.\n\n8. **Cross-Modal Hard Negative Mining Using 3D Spatial Confusability**\n   - Improve CLIP-style contrastive learning for embodied scenes by constructing \u201chard negatives\u201d from spatially confusable regions/objects in 3D (e.g., identical chairs in different rooms) using Matterport3D reconstructions. Apply this to pre-train a 3D-language representation (or fine-tune g3D-LF) that better separates semantically similar but spatially distinct entities. Measure improvements on ScanRefer/Multi3DRefer-style grounding and VLN landmark disambiguation.\n\n9. **Multi-Turn Embodied Dialogue Benchmark with Persistent Object IDs and Panoramic Observations**\n   - Build a dataset/benchmark where an agent in Matterport3D-like scenes must handle multi-turn user dialogue that references previously mentioned objects using persistent identifiers (Chat-Scene) and natural phrases (\u201cthe second door on the left\u201d). Provide aligned panoramic RGB-D, object proposal tracks, and dialogue annotations spanning navigation + QA. The contribution is a standardized evaluation that tests memory, grounding consistency, and instruction-following in one loop.\n\n10. **Energy-Efficient On-Device Embodied Perception Stack: CLIP Retrieval + FastSAM + Lightweight 3D Field**\n   - Design a deployable stack where CLIP performs low-cost semantic retrieval (\u201cfind a microwave\u201d), FastSAM produces candidate masks, and a compact g3D-LF variant maintains a local 3D-language map for planning. Introduce system-level optimizations (caching text embeddings, selective ray sampling, adaptive resolution BEV) and report latency/energy alongside task success. This bridges foundation-model capabilities with real-time robotics constraints.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "1. **CLIP-Guided FastSAM for Open-Vocabulary Instance Segmentation**\n   - Combine CLIP\u2019s text-image alignment with FastSAM\u2019s CNN instance segmentation to enable open-vocabulary segmentation via text p",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "**Panorama-Aware 3D-Language Feature Fields for VLN in Building-Scale Scenes**\n   - Extend g3D-LF to explicitly model panoramic observation geometry (equirectangular projection) as used in VLN and Mat",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "**Object-Identifier Feature Fields: A Unified 3D Tokenization for LLM Navigation and QA**\n   - Fuse Chat-Scene\u2019s object identifier tokens with g3D-LF by generating persistent object IDs directly from ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "**Language-Conditioned BEV Map Editing for Embodied Planning**\n   - Build on g3D-LF\u2019s BEV generation by enabling text-driven edits to the BEV latent map (e.g., \u201cavoid the kitchen and go through the ha",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "**Real-Time Incremental 3D-Language Mapping with Confidence-Calibrated Updates**\n   - Develop an online update mechanism for g3D-LF that maintains uncertainty over feature-field regions as new RGB-D f",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "**Fast Proposal-to-Field Distillation: Training g3D-LF from Lightweight 2D Segmentation**\n   - Use FastSAM to produce fast instance masks and object proposals on incoming frames, then distill these in",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "**Instruction Decomposition with 3D Grounded Subgoals via Contrastive Retrieval**\n   - Create a pipeline that decomposes VLN instructions into subgoals (landmarks/actions) and retrieves corresponding ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "**Cross-Modal Hard Negative Mining Using 3D Spatial Confusability**\n   - Improve CLIP-style contrastive learning for embodied scenes by constructing \u201chard negatives\u201d from spatially confusable regions/",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "**Multi-Turn Embodied Dialogue Benchmark with Persistent Object IDs and Panoramic Observations**\n   - Build a dataset/benchmark where an agent in Matterport3D-like scenes must handle multi-turn user d",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "**Energy-Efficient On-Device Embodied Perception Stack: CLIP Retrieval + FastSAM + Lightweight 3D Field**\n   - Design a deployable stack where CLIP performs low-cost semantic retrieval (\u201cfind a microw",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 59,
      "paper_title": "Learning (Approximately) Equivariant Networks via Constrained Optimization",
      "contribution": "The paper introduces Adaptive Constrained Equivariance (ACE), a framework that systematically relaxes equivariance constraints during training to improve performance in equivariant neural networks.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 2,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 8978,
      "output_tokens": 1130,
      "predecessor_details": [
        {
          "success": true,
          "title": "[1602.07576] Group Equivariant Convolutional Networks - arXiv",
          "url": "https://arxiv.org/abs/1602.07576",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:1602.07576** (cs)\n\n\\[Submitted on 24 Feb 2016 ( [v1](https://arxiv.org/abs/1602.07576v1)), last revised 3 Jun 2016 (this version, v3)\\]\n\n# Title:Group Equivariant Convolutional Networks\n\nAuthors: [Taco S. Cohen](https://arxiv.org/search/cs?searchtype=author&query=Cohen,+T+S), [Max Welling](https://arxiv.org/search/cs?searchtype=author&query=Welling,+M)\n\nView a PDF of the paper titled Group Equivariant Convolutional Networks, by Taco S. Cohen and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/1602.07576)\n\n> Abstract:We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:1602.07576](https://arxiv.org/abs/1602.07576) \\[cs.LG\\] |\n| (or [arXiv:1602.07576v3](https://arxiv.org/abs/1602.07576v3) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.1602.07576](https://doi.org/10.48550/arXiv.1602.07576) Focus to learn more arXiv-issued DOI via DataCite |\n| Journal\u00a0reference: | Proceedings of the International Conference on Machine Learning (ICML), 2016 |\n\n## Submission history\n\nFrom: Taco Cohen \\[ [view email](https://arxiv.org/show-email/38ca562a/1602.07576)\\] **[\\[v1\\]](https://arxiv.org/abs/1602.07576v1)**\nWed, 24 Feb 2016 16:17:15 UTC (105 KB)\n**[\\[v2\\]](https://arxiv.org/abs/1602.07576v2)**\nFri, 11 Mar 2016 18:26:26 UTC (106 KB)\n**\\[v3\\]**\nFri, 3 Jun 2016 10:54:16 UTC (454 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Group Equivariant Convolutional Networks, by Taco S. Cohen and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/1602.07576)\n- [TeX Source](https://arxiv.org/src/1602.07576)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1602.07576&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1602.07576&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2016-02](https://arxiv.org/list/cs.LG/2016-02)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1602.07576?context=cs) [stat](https://arxiv.org/abs/1602.07576?context=stat) [stat.ML](https://arxiv.org/abs/1602.07576?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1602.07576)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1602.07576)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1602.07576)\n\n### [2 blog links](https://arxiv.org/tb/1602.07576)\n\n( [what is this?](https://info.arxiv.org/help/trackback.html))\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1602.html#CohenW16) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/CohenW16)\n\n[Taco S. Cohen](https://dblp.uni-trier.de/search/author?author=Taco%20S.%20Cohen) [Max Welling](https://dblp.uni-trier.de/search/author?author=Max%20Welling)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1602.07576) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Group equivariant convolutional networks",
          "cleaned_query": "Group equivariant convolutional networks"
        },
        {
          "success": true,
          "title": "Energy functions for early vision and analog networks",
          "url": "https://link.springer.com/article/10.1007/BF00204595",
          "content": "Energy functions for early vision and analog networks | Biological Cybernetics\n[Skip to main content](#main)\nAdvertisement\n[![Springer Nature Link](https://link.springer.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1007/BF00204595?)\n# Energy functions for early vision and analog networks\n* Published:June 1989\n* Volume\u00a061,\u00a0pages 115\u2013123, (1989)\n* [Cite this article](#citeas)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/422?as=webp)Biological Cybernetics](https://link.springer.com/journal/422)[Aims and scope](https://link.springer.com/journal/422/aims-and-scope)[Submit manuscript](https://submission.nature.com/new-submission/422/3)\n* [A. L. Yuille](#auth-A__L_-Yuille-Aff1)[1](#Aff1)\n* 87Accesses\n* 50Citations\n* [Explore all metrics](https://link.springer.com/article/10.1007/BF00204595/metrics)\n## Abstract\nThis paper describes attempts to model the modules of early vision in terms of minimizing energy functions, in particular energy functions allowing discontinuities in the solution. It examines the success of using Hopfield-style analog networks for solving such problems. Finally it discusses the limitations of the energy function approach.\nThis is a preview of subscription content,[log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/article/10.1007/BF00204595?error=cookies_not_supported&code=e32c1e4d-d26d-431f-828f-3a30d06a8c00)to check access.\n## Access this article\n[Log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/article/10.1007/BF00204595?error=cookies_not_supported&code=e32c1e4d-d26d-431f-828f-3a30d06a8c00)\n## Subscribe and save\nSpringer+\nfrom $39.99 /Month\n* Starting from 10 chapters or articles per month\n* Access and download chapters and articles from more than 300k books and 2,500 journals\n* Cancel anytime[View plans](https://link.springer.com/product/springer-plus)\n## Buy Now\nBuy article PDF USD 39.95\nPrice excludes VAT (USA)\nTax calculation will be finalised during checkout.\nInstant access to the full article PDF.\n[Institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/journals)\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs00521-016-2694-9/MediaObjects/521_2016_2694_Fig1_HTML.gif)\n### [Fast-forward solver for inhomogeneous media using machine learning methods: artificial neural network, support vector machine and fuzzy logic](https://link.springer.com/10.1007/s00521-016-2694-9?fromPaywallRec=true)\nArticle15 November 2016\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-3-030-96772-7?as&#x3D;webp)\n### [Photonic Computing and Communication for Neural Network Accelerators](https://link.springer.com/10.1007/978-3-030-96772-7_12?fromPaywallRec=true)\nChapter\u00a9 2022\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42005-025-01972-y/MediaObjects/42005_2025_1972_Fig1_HTML.png)\n### [Annealing-inspired training of an optical neural network with ternary weights](https://link.springer.com/10.1038/s42005-025-01972-y?fromPaywallRec=true)\nArticleOpen access13 February 2025\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Computer Vision](https://link.springer.com/subjects/computer-vision)\n* [Intervision](https://link.springer.com/subjects/intervision)\n* [Object vision](https://link.springer.com/subjects/object-vision)\n* [Pattern vision](https://link.springer.com/subjects/pattern-vision)\n* [Visual Perception](https://link.springer.com/subjects/visual-perception)\n* [Computer Imaging, Vision, Pattern Recognition and Graphics](https://link.springer.com/subjects/computer-imaging-vision-pattern-recognition-and-graphics)\n## References\n* Barnard S (1986) Proceedings of the Image Understanding Workshop, Los Angeles\n* Barrow HG, Tennenbaum JM (1981) Computational vision. Proc. IEEE 69:572\u2013595\n[Google Scholar]()\n* Blake A (1985) The least-disturbance principle and weak constraints. Patt Recogn Lett 1:393\u2013399\n[Google Scholar]()\n* Brandt A (1977) Multi-level adaptive solutions to boundaryvalue problems. Math Comp 31:333\u2013390\n[Google Scholar]()\n* Courant R, Hilbert D (1953) Methods of mathematical physics, vol 1. Interscience, New York\n[Google Scholar]()\n* Dev P (1975) Perception of depth surfaces in random-dot stereograms. Int. J. Man-Mach Stud 7:511\u2013528\n[Google Scholar]()\n* Feynman R (1963) The Feynman lectures on Physics. Addison-Wesley, Reading, Mass\n[Google Scholar]()\n* Gamble E, Poggio T (1987) Visual integration and detection of discontinuities: the key role of intensity edges. Artif Intell Memo 970. MIT, Cambridge\n[Google Scholar]()\n* Geman S, Geman D (1984) Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. IEEE Trans Pattern Anal Mach Intell 6:721\u2013741\n[Google Scholar]()\n* Gennert M (1987) A computational framework for understanding problems in stereo vision. MIT Artificial Intelligence Laboratory PhD. Thesis\n* Grimson WEL (1981) From images to surfaces: a computational study of the human early visual system. MIT Press, Cambridge, Mass\n[Google Scholar]()\n* Grzywacz NM, Yuille AL (1986) Motion correspondence and analog networks. Proc Am Inst Phys Conf Neur Netw Comput 151:200\u2013205\n[Google Scholar]()\n* Grzywacz NM, Yuille AL (1988) Massively parallel implementation of theories for apparent motion. Spatl Vision 1:15\u201344\n[Google Scholar]()\n* Hildreth EC (1984) The measurement of visual motion. MIT Press, Cambridge, Mass\n[Google Scholar]()\n* Hinton, G (1979) Relaxation and its role in vision. Ph. D. Thesis. University of Edinburgh\n* Hopfield JJ (1982) Neural networks and physical systems with emergent collective computational abilities. Proc Natl Acad Sci USA 79:2554\u20132558\n[Google Scholar]()\n* Hopfield JJ (1984) Neurons with graded response have collective computational properties like those of two-state neurons. Proc Natl Acad Sci USA 81:3088\u20133092\n[Google Scholar]()\n* Hopfield JJ, Tank DW (1985) Neural computation in optimization problems. Biol Cybern 52:141\u2013152\n[Google Scholar]()\n* Horn BKP (1974) Determining lightness from an image. Comput Graph Image Process 3:277\u2013299\n[Google Scholar]()\n* Horn BKP (1986) Robot vision. MIT Press, Cambridge, Mass\n[Google Scholar]()\n* Horn BKP, Schunk BG (1981) Determining optical flow. Artif Intell 17:185\u2013203\n[Google Scholar]()\n* Hutchinson JM, Koch C (1986) Simple analog and hibrid networks for surface interpolation. Proc Am Inst Phys Conf Neur Netw Comput 151:235\u2013240\n[Google Scholar]()\n* Hutchinson JM, Koch C, Luo J, Mead C (1988) Computing motion using analog and binary resistive networks. Computer 21:52\u201363\n[Google Scholar]()\n* Ikeuchi K (1980a) Numerical shape from shading and occluding contours in a single view. A.I. Memo 566. AI Lab MIT\n* Ikeuchi K (1980b) Shape from regular patterns. A.I. Memo 566, AI Lab MIT\n* Ikeuchi K, Horn BKP (1981) Numerical shape from shading and occluding boundaries. Artif Intell 17:141\u2013184\n[Google Scholar]()\n* Julesz B (1971) Foundations of cyclopean perception. The University of Chicago Press, Chicago\n[Google Scholar]()\n* Karplus WJ (1958) Analog circuits: solutions of field problems. McGraw-Hill, New York\n[Google Scholar]()\n* Kirkpatrick S, Gellatt C Jr, Vecchi MP (1983) Optimization by simulated annealing. Science 220:671\u2013680\n[Google Scholar]()\n* Koch C, Marroquin J, Yuille AL (1986) Analog \u201cneuronal\u201d networks in early vision. Proc Natl Acad Sci USA 83:4263\u20134267\n[Google Scholar]()\n* Marr D (1982) \u201cVision\u201d. Freeman, San Francisco\n[Google Scholar](http://scholar.google.com/scholar_lookup?&amp;title=\u201cVision\u201d&amp;publication_year=1982&amp;author=Marr,D)\n* Marr D, Poggio T (1977) Cooperative com",
          "original_query": "Energy functions for early vision and analog networks",
          "cleaned_query": "Energy functions for early vision and analog networks"
        },
        {
          "success": true,
          "title": "[2305.17592] Approximation-Generalization Trade-offs under ... - arXiv",
          "url": "https://arxiv.org/abs/2305.17592",
          "content": "\n Download PDF \nThe explicit incorporation of task-specific inductive biases through symmetry has emerged as a general design precept in the development of high-performance machine learning models. For example, group equivariant neural networks have demonstrated impressive performance across various domains and applications such as protein and drug design. A prevalent intuition about such models is that the integration of relevant symmetry results in enhanced generalization. Moreover, it is posited that when the data and/or the model may only exhibit $\\textit{approximate}$ or $\\textit{partial}$ symmetry, the optimal or best-performing model is one where the model symmetry aligns with the data symmetry. In this paper, we conduct a formal unified investigation of these intuitions. To begin, we present general quantitative bounds that demonstrate how models capturing task-specific symmetries lead to improved generalization. In fact, our results do not require the transformations to be finite or even form a group and can work with partial or approximate equivariance. Utilizing this quantification, we examine the more general question of model mis-specification i.e. when the model symmetries don't align with the data symmetries. We establish, for a given symmetry group, a quantitative comparison between the approximate/partial equivariance of the model and that of the data distribution, precisely connecting model equivariance error and data equivariance error. Our result delineates conditions under which the model equivariance error is optimal, thereby yielding the best-performing model for the given task and data.\n \n \n Submission history From: Shubhendu Trivedi [ view email] [v1] \n Sat, 27 May 2023 22:53:37 UTC (145 KB) \n ||||I|||| Skip to main content\n We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate\n > cs > arXiv:2305.17592\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Machine Learning\n\n arXiv:2305.17592 (cs)\n [Submitted on 27 May 2023]\n\n Title: Approximation-Generalization Trade-offs under (Approximate) Group Equivariance\n\n Authors: Mircea Petrache, Shubhendu Trivedi\n Download a PDF of the paper titled Approximation-Generalization Trade-offs under (Approximate) Group Equivariance, by Mircea Petrache and 1 other authors\n Download PDF\n Abstract: The explicit incorporation of task-specific inductive biases through symmetry has emerged as a general design precept in the development of high-performance machine learning models. For example, group equivariant neural networks have demonstrated impressive performance across various domains and applications such as protein and drug design. A prevalent intuition about such models is that the integration of relevant symmetry results in enhanced generalization. Moreover, it is posited that when the data and/or the model may only exhibit $\\textit{approximate}$ or $\\textit{partial}$ symmetry, the optimal or best-performing model is one where the model symmetry aligns with the data symmetry. In this paper, we conduct a formal unified investigation of these intuitions. To begin, we present general quantitative bounds that demonstrate how models capturing task-specific symmetries lead to improved generalization. In fact, our results do not require the transformations to be finite or even form a group and can work with partial or approximate equivariance. Utilizing this quantification, we examine the more general question of model mis-specification i.e. when the model symmetries don't align with the data symmetries. We establish, for a given symmetry group, a quantitative comparison between the approximate/partial equivariance of the model and that of the data distribution, precisely connecting model equivariance error and data equivariance error. Our result delineates conditions under which the model equivariance error is optimal, thereby yielding the best-performing model for the given task and data.\n Subjects: Machine Learning (cs.LG) ; Machine Learning (stat.ML)\n Cite as: arXiv:2305.17592 [cs.LG] \n (or arXiv:2305.17592v1 [cs.LG] for this version) \n https://doi.org/10.48550/arXiv.2305.17592 \n Focus to learn more \n arXiv-issued DOI via DataCite \n \n\n Submission history\n\n From: Shubhendu Trivedi [view email]\n [v1] Sat, 27 May 2023 22:53:37 UTC (145 KB)\n Full-text links:\n\n Access Paper:\n\n Download a PDF of the paper titled Approximation-Generalization Trade-offs under (Approximate) Group Equivariance, by Mircea Petrache and 1 other authors\n * Download PDF\n * PostScript\n * Other Formats\n (view license)\n Current browse context:\n cs.LG\n < prev | next >\n new | recent | 2305\n Change to browse by:\n cs\n stat\n stat.ML\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export BibTeX citation Loading...\n\n BibTeX formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n DagsHub Toggle\n DagsHub (What is DagsHub?)\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Link to Influence Flower\n Influence Flower (What are Influence Flowers?)\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n IArxiv recommender toggle\n IArxiv Recommender (What is IArxiv?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Approximation-generalization trade-offs under (approximate) group equivariance",
          "cleaned_query": "Approximation-generalization trade-offs under (approximate) group equivariance"
        },
        {
          "success": true,
          "title": "[2306.02426] Resilient Constrained Learning - arXiv",
          "url": "https://arxiv.org/abs/2306.02426",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2306.02426** (cs)\n\n\\[Submitted on 4 Jun 2023 ( [v1](https://arxiv.org/abs/2306.02426v1)), last revised 11 Jan 2024 (this version, v4)\\]\n\n# Title:Resilient Constrained Learning\n\nAuthors: [Ignacio Hounie](https://arxiv.org/search/cs?searchtype=author&query=Hounie,+I), [Alejandro Ribeiro](https://arxiv.org/search/cs?searchtype=author&query=Ribeiro,+A), [Luiz F. O. Chamon](https://arxiv.org/search/cs?searchtype=author&query=Chamon,+L+F+O)\n\nView a PDF of the paper titled Resilient Constrained Learning, by Ignacio Hounie and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2306.02426)\n\n> Abstract:When deploying machine learning solutions, they must satisfy multiple requirements beyond accuracy, such as fairness, robustness, or safety. These requirements are imposed during training either implicitly, using penalties, or explicitly, using constrained optimization methods based on Lagrangian duality. Either way, specifying requirements is hindered by the presence of compromises and limited prior knowledge about the data. Furthermore, their impact on performance can often only be evaluated by actually solving the learning problem. This paper presents a constrained learning approach that adapts the requirements while simultaneously solving the learning task. To do so, it relaxes the learning constraints in a way that contemplates how much they affect the task at hand by balancing the performance gains obtained from the relaxation against a user-defined cost of that relaxation. We call this approach resilient constrained learning after the term used to describe ecological systems that adapt to disruptions by modifying their operation. We show conditions under which this balance can be achieved and introduce a practical algorithm to compute it, for which we derive approximation and generalization guarantees. We showcase the advantages of this resilient learning method in image classification tasks involving multiple potential invariances and in heterogeneous federated learning.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2306.02426](https://arxiv.org/abs/2306.02426) \\[cs.LG\\] |\n| | (or [arXiv:2306.02426v4](https://arxiv.org/abs/2306.02426v4) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2306.02426](https://doi.org/10.48550/arXiv.2306.02426) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Ignacio Hounie \\[ [view email](https://arxiv.org/show-email/76b6a382/2306.02426)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2306.02426v1)**\nSun, 4 Jun 2023 18:14:18 UTC (371 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2306.02426v2)**\nMon, 25 Sep 2023 19:26:06 UTC (378 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/2306.02426v3)**\nTue, 31 Oct 2023 18:22:09 UTC (397 KB)\n\n**\\[v4\\]**\nThu, 11 Jan 2024 15:30:24 UTC (397 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Resilient Constrained Learning, by Ignacio Hounie and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2306.02426)\n- [TeX Source](https://arxiv.org/src/2306.02426)\n- [Other Formats](https://arxiv.org/format/2306.02426)\n\n[![license icon](https://arxiv.org/icons/licenses/by-nc-sa-4.0.png)view license](http://creativecommons.org/licenses/by-nc-sa/4.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2306.02426&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2306.02426&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2023-06](https://arxiv.org/list/cs.LG/2023-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2306.02426?context=cs)\n\n[stat](https://arxiv.org/abs/2306.02426?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2306.02426?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2306.02426)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2306.02426)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2306.02426)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2306.02426&description=Resilient Constrained Learning) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2306.02426&title=Resilient Constrained Learning)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2306.02426) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Resilient constrained learning",
          "cleaned_query": "Resilient constrained learning"
        },
        {
          "success": true,
          "title": "SchNet: A continuous-filter convolutional neural network for ...",
          "url": "https://arxiv.org/abs/1706.08566",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "Schnet: A continuous-filter convolutional neural network for modeling quantum interactions",
          "cleaned_query": "Schnet: A continuous-filter convolutional neural network for modeling quantum interactions"
        },
        {
          "success": true,
          "title": "Residual Pathway Priors for Soft Equivariance Constraints - arXiv",
          "url": "https://arxiv.org/abs/2112.01388",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2112.01388** (cs)\n\n\\[Submitted on 2 Dec 2021\\]\n\n# Title:Residual Pathway Priors for Soft Equivariance Constraints\n\nAuthors: [Marc Finzi](https://arxiv.org/search/cs?searchtype=author&query=Finzi,+M), [Gregory Benton](https://arxiv.org/search/cs?searchtype=author&query=Benton,+G), [Andrew Gordon Wilson](https://arxiv.org/search/cs?searchtype=author&query=Wilson,+A+G)\n\nView a PDF of the paper titled Residual Pathway Priors for Soft Equivariance Constraints, by Marc Finzi and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2112.01388)\n\n> Abstract:There is often a trade-off between building deep learning systems that are expressive enough to capture the nuances of the reality, and having the right inductive biases for efficient learning. We introduce Residual Pathway Priors (RPPs) as a method for converting hard architectural constraints into soft priors, guiding models towards structured solutions, while retaining the ability to capture additional complexity. Using RPPs, we construct neural network priors with inductive biases for equivariances, but without limiting flexibility. We show that RPPs are resilient to approximate or misspecified symmetries, and are as effective as fully constrained models even when symmetries are exact. We showcase the broad applicability of RPPs with dynamical systems, tabular data, and reinforcement learning. In Mujoco locomotion tasks, where contact forces and directional rewards violate strict equivariance assumptions, the RPP outperforms baseline model-free RL agents, and also improves the learned transition models for model-based RL.\n\n| | |\n| --- | --- |\n| Comments: | NeurIPS 2021. Code available at [this https URL](https://github.com/mfinzi/residual-pathway-priors) |\n| Subjects: | Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2112.01388](https://arxiv.org/abs/2112.01388) \\[cs.LG\\] |\n| | (or [arXiv:2112.01388v1](https://arxiv.org/abs/2112.01388v1) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2112.01388](https://doi.org/10.48550/arXiv.2112.01388) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Andrew Wilson \\[ [view email](https://arxiv.org/show-email/03e2bf8a/2112.01388)\\]\n\n**\\[v1\\]**\nThu, 2 Dec 2021 16:18:17 UTC (1,160 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Residual Pathway Priors for Soft Equivariance Constraints, by Marc Finzi and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2112.01388)\n- [TeX Source](https://arxiv.org/src/2112.01388)\n- [Other Formats](https://arxiv.org/format/2112.01388)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2112.01388&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2112.01388&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2021-12](https://arxiv.org/list/cs.LG/2021-12)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2112.01388?context=cs)\n\n[stat](https://arxiv.org/abs/2112.01388?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2112.01388?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2112.01388)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2112.01388)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2112.01388)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2112.html#abs-2112-01388) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2112-01388)\n\n[Marc Finzi](https://dblp.uni-trier.de/search/author?author=Marc%20Finzi)\n\n[Andrew Gordon Wilson](https://dblp.uni-trier.de/search/author?author=Andrew%20Gordon%20Wilson)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2112.01388&description=Residual Pathway Priors for Soft Equivariance Constraints) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2112.01388&title=Residual Pathway Priors for Soft Equivariance Constraints)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2112.01388) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Residual pathway priors for soft equivariance constraints",
          "cleaned_query": "Residual pathway priors for soft equivariance constraints"
        }
      ],
      "generated_ideas": [
        "Data-Driven Symmetry Selection via Equivariance-Error Estimation\nBuild a pipeline that estimates *data equivariance error* for a candidate set of transformations (e.g., rotations, reflections, permutations) by measuring distributional consistency under augmentation, then selects the model\u2019s symmetry (G-CNN, partial equivariance, or none) to minimize the bound-implied trade-off from Paper",
        "Implement as an automated \u201csymmetry model selection\u201d module that outputs the best group/subgroup and degree of approximate equivariance for a task.\n"
      ],
      "generated_ideas_raw": "1. **Data-Driven Symmetry Selection via Equivariance-Error Estimation**\n   Build a pipeline that estimates *data equivariance error* for a candidate set of transformations (e.g., rotations, reflections, permutations) by measuring distributional consistency under augmentation, then selects the model\u2019s symmetry (G-CNN, partial equivariance, or none) to minimize the bound-implied trade-off from Paper 3. Implement as an automated \u201csymmetry model selection\u201d module that outputs the best group/subgroup and degree of approximate equivariance for a task.\n\n2. **Resilient Soft-Equivariance Training with Dual-Controlled RPP Strength**\n   Combine Resilient Constrained Learning (Paper 4) with Residual Pathway Priors (Paper 6) by treating \u201cequivariance error \u2264 \u03b5\u201d as a constraint whose slack is adaptively chosen during training. The key contribution is an algorithm that dynamically adjusts the RPP mixing coefficient (hard-to-soft bias strength) based on measured performance gains vs. a user-defined cost of symmetry violation, yielding models that automatically \u201cback off\u201d from mis-specified symmetries.\n\n3. **Equivariant Energy-Based Early Vision Modules with Discontinuity-Preserving Priors**\n   Revisit Yuille-style discontinuity-friendly energy functions (Paper 2) and implement them as modern energy-based models whose inference is performed by unrolled optimization, but with *group-equivariant* parameterization (Paper 1). Concretely, design energies for edge-aware smoothing/segmentation/optical flow that are translation/rotation equivariant while still allowing sharp discontinuities, and compare against G-CNN baselines on robustness to rotations and occlusions.\n\n4. **Approximate-Equivariance Regularizers for Continuous-Filter Molecular Networks**\n   Extend SchNet-style continuous-filter convolutions (Paper 5) with an explicit, differentiable penalty for *approximate* E(3) equivariance error (Paper 3) measured across random rigid motions of 3D conformations. The contribution is a training objective that can interpolate between strict invariance/equivariance and task-driven symmetry breaking (e.g., external fields, binding pockets), improving generalization when symmetry is only partially present.\n\n5. **Federated Symmetry Alignment under Heterogeneous Client Invariances**\n   Build a federated learning method where each client learns its own optimal symmetry bias (group choice and strength) using resilient constraint relaxation (Paper 4), while the server aggregates in a way that preserves shared equivariant structure (Paper 1/6). Evaluate on vision (some clients have rotated data, others not) and on molecular/property prediction (clients with different measurement conditions), quantifying when personalized symmetry improves global and local generalization.\n\n6. **Equivariance-Aware Robustness Certification via Energy Upper Bounds**\n   Use energy-function formulations (Paper 2) to derive computable upper bounds on prediction change under transformation sets, then connect these to approximate equivariance generalization bounds (Paper 3). The actionable result is a \u201ccertificate\u201d that a model is stable to rotations/reflections up to a specified tolerance, and a training-time objective that tightens the certificate while preserving accuracy.\n\n7. **Task-Conditional Symmetry Gating: Mixture of G-CNN and Residual Paths**\n   Create an architecture that routes each input through (i) a group-equivariant pathway (Paper 1) and (ii) a non-equivariant residual pathway (Paper 6), with a learned gating network deciding the mix per sample. This makes symmetry *input-adaptive*\u2014useful when only subsets of the data obey the symmetry\u2014and can be trained with an auxiliary loss encouraging minimal reliance on the non-equivariant path unless needed.\n\n8. **Learning Subgroup Equivariance for Partial Symmetries in Real Images**\n   Develop a method to learn *which subgroup* of a candidate group best matches the data (Paper 3), e.g., prefer 90\u00b0 rotations over continuous rotations, or prefer reflections only for specific classes. Implement by training a G-CNN family with shared weights across subgroups and a selection mechanism (e.g., differentiable architecture search or resilient constraints) that outputs the subgroup yielding the best approximation\u2013generalization trade-off.\n\n9. **Equivariant Unrolled Analog Optimization Networks for Inference-Time Adaptation**\n   Translate Hopfield/analog-network minimization ideas (Paper 2) into a differentiable unrolled optimizer whose update rules are group-equivariant (Paper 1), enabling fast inference-time adaptation to nuisances like unknown rotations. The contribution is a hybrid model that (a) predicts an initial solution with a G-CNN, then (b) refines it by minimizing an equivariant energy for a few learned iterations, improving performance in low-data or distribution-shift settings.\n\n10. **Benchmarking Mis-Specified Symmetries: Controlled Datasets and Metrics**\n   Create a suite of controlled synthetic-to-real benchmarks where symmetry is gradually violated (e.g., rotated MNIST with class-dependent rotation bias; CIFAR with anisotropic textures; molecules under oriented external fields), and evaluate G-CNNs (Paper 1), RPP (Paper 6), and resilient constraint relaxation (Paper 4) against the theory of approximate equivariance (Paper 3). The key contribution is standardized metrics for *data equivariance error*, *model equivariance error*, and downstream generalization, enabling principled comparison of \u201chard vs soft vs adaptive\u201d symmetry methods.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Data-Driven Symmetry Selection via Equivariance-Error Estimation\nBuild a pipeline that estimates *data equivariance error* for a candidate set of transformations (e.g., rotations, reflections, permuta",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Implement as an automated \u201csymmetry model selection\u201d module that outputs the best group/subgroup and degree of approximate equivariance for a task.\n",
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 60,
      "paper_title": "SAGE: A Unified Framework for Generalizable Object State Recognition with State-Action Graph Embedding",
      "contribution": "SAGE introduces a unified framework for recognizing object physical states and their temporal evolutions using State-Action Graph Embedding, enhancing generalization to unseen objects and actions.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 9984,
      "output_tokens": 1111,
      "predecessor_details": [
        {
          "success": true,
          "title": "Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos",
          "url": "https://arxiv.org/abs/2203.11637",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2203.11637** (cs)\n\n\\[Submitted on 22 Mar 2022\\]\n\n# Title:Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos\n\nAuthors: [Tom\u00e1\u0161 Sou\u010dek](https://arxiv.org/search/cs?searchtype=author&query=Sou%C4%8Dek,+T), [Jean-Baptiste Alayrac](https://arxiv.org/search/cs?searchtype=author&query=Alayrac,+J), [Antoine Miech](https://arxiv.org/search/cs?searchtype=author&query=Miech,+A), [Ivan Laptev](https://arxiv.org/search/cs?searchtype=author&query=Laptev,+I), [Josef Sivic](https://arxiv.org/search/cs?searchtype=author&query=Sivic,+J)\n\nView a PDF of the paper titled Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos, by Tom\\\\'a\\\\v{s} Sou\\\\v{c}ek and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2203.11637)\n\n> Abstract:Human actions often induce changes of object states such as \"cutting an apple\", \"cleaning shoes\" or \"pouring coffee\". In this paper, we seek to temporally localize object states (e.g. \"empty\" and \"full\" cup) together with the corresponding state-modifying actions (\"pouring coffee\") in long uncurated videos with minimal supervision. The contributions of this work are threefold. First, we develop a self-supervised model for jointly learning state-modifying actions together with the corresponding object states from an uncurated set of videos from the Internet. The model is self-supervised by the causal ordering signal, i.e. initial object state $\\\\rightarrow$ manipulating action $\\\\rightarrow$ end state. Second, to cope with noisy uncurated training data, our model incorporates a noise adaptive weighting module supervised by a small number of annotated still images, that allows to efficiently filter out irrelevant videos during training. Third, we collect a new dataset with more than 2600 hours of video and 34 thousand changes of object states, and manually annotate a part of this data to validate our approach. Our results demonstrate substantial improvements over prior work in both action and object state-recognition in video.\n\n| | |\n| --- | --- |\n| Comments: | To be published in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2203.11637](https://arxiv.org/abs/2203.11637) \\[cs.CV\\] |\n| | (or [arXiv:2203.11637v1](https://arxiv.org/abs/2203.11637v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2203.11637](https://doi.org/10.48550/arXiv.2203.11637) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Tom\u00e1\u0161 Sou\u010dek \\[ [view email](https://arxiv.org/show-email/bbc7a25e/2203.11637)\\]\n\n**\\[v1\\]**\nTue, 22 Mar 2022 11:45:10 UTC (11,495 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos, by Tom\\\\'a\\\\v{s} Sou\\\\v{c}ek and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2203.11637)\n- [TeX Source](https://arxiv.org/src/2203.11637)\n- [Other Formats](https://arxiv.org/format/2203.11637)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2203.11637&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2203.11637&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2022-03](https://arxiv.org/list/cs.CV/2022-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2203.11637?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2203.11637)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2203.11637)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2203.11637)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2203.11637&description=Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2203.11637&title=Look for the Change: Learning Object States and State-Modifying Actions from Untrimmed Web Videos)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2203.11637) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Learning Object States and Actions from Instructional Videos",
          "cleaned_query": "Learning Object States and Actions from Instructional Videos"
        },
        {
          "success": true,
          "title": "Vidi2: Large Multimodal Models for Video Understanding and Creation",
          "url": "https://arxiv.org/abs/2511.19529",
          "content": "[2511.19529] Vidi2: Large Multimodal Models for Video Understanding and Creation\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2511.19529\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2511.19529**(cs)\n[Submitted on 24 Nov 2025]\n# Title:Vidi2: Large Multimodal Models for Video Understanding and Creation\nAuthors:[Vidi Team](https://arxiv.org/search/cs?searchtype=author&amp;query=Vidi+Team),[Celong Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+C),[Chia-Wen Kuo](https://arxiv.org/search/cs?searchtype=author&amp;query=Kuo,+C),[Chuang Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+C),[Dawei Du](https://arxiv.org/search/cs?searchtype=author&amp;query=Du,+D),[Fan Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+F),[Guang Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+G),[Haoji Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+H),[Haojun Zhao](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+H),[Lingxi Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+L),[Lu Guo](https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+L),[Lusha Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+L),[Longyin Wen](https://arxiv.org/search/cs?searchtype=author&amp;query=Wen,+L),[Qihang Fan](https://arxiv.org/search/cs?searchtype=author&amp;query=Fan,+Q),[Qingyu Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Q),[Rachel Deng](https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+R),[Sijie Zhu](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+S),[Stuart Siew](https://arxiv.org/search/cs?searchtype=author&amp;query=Siew,+S),[Tong Jin](https://arxiv.org/search/cs?searchtype=author&amp;query=Jin,+T),[Weiyan Tao](https://arxiv.org/search/cs?searchtype=author&amp;query=Tao,+W),[Wen Zhong](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhong,+W),[Xiaohui Shen](https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+X),[Xin Gu](https://arxiv.org/search/cs?searchtype=author&amp;query=Gu,+X),[Zhenfang Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Z),[Zuhua Lin](https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+Z)\nView a PDF of the paper titled Vidi2: Large Multimodal Models for Video Understanding and Creation, by Vidi Team and 24 other authors\n[View PDF](https://arxiv.org/pdf/2511.19529)[HTML (experimental)](https://arxiv.org/html/2511.19529v1)> > Abstract:\n> Video has emerged as the primary medium for communication and creativity on the Internet, driving strong demand for scalable, high-quality video production. Vidi models continue to evolve toward next-generation video creation and have achieved state-of-the-art performance in multimodal temporal retrieval (TR). In its second release, Vidi2 advances video understanding with fine-grained spatio-temporal grounding (STG) and extends its capability to video question answering (Video QA), enabling comprehensive multimodal reasoning. Given a text query, Vidi2 can identify not only the corresponding timestamps but also the bounding boxes of target objects within the output time ranges. This end-to-end spatio-temporal grounding capability enables potential applications in complex editing scenarios, such as plot or character understanding, automatic multi-view switching, and intelligent, composition-aware reframing and cropping. To enable comprehensive evaluation of STG in practical settings, we introduce a new benchmark, VUE-STG, which offers four key improvements over existing STG datasets: 1) Video duration: spans from roughly 10s to 30 mins, enabling long-context reasoning; 2) Query format: queries are mostly converted into noun phrases while preserving sentence-level expressiveness; 3) Annotation quality: all ground-truth time ranges and bounding boxes are manually annotated with high accuracy; 4) Evaluation metric: a refined vIoU/tIoU/vIoU-Intersection scheme. In addition, we upgrade the previous VUE-TR benchmark to VUE-TR-V2, achieving a more balanced video-length distribution and more user-style queries. Remarkably, the Vidi2 model substantially outperforms leading proprietary systems, such as Gemini 3 Pro (Preview) and GPT-5, on both VUE-TR-V2 and VUE-STG, while achieving competitive results with popular open-source models with similar scale on video QA benchmarks. Subjects:|Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:2511.19529](https://arxiv.org/abs/2511.19529)[cs.CV]|\n|(or[arXiv:2511.19529v1](https://arxiv.org/abs/2511.19529v1)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2511.19529](https://doi.org/10.48550/arXiv.2511.19529)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Sijie Zhu [[view email](https://arxiv.org/show-email/f6560f65/2511.19529)]\n**[v1]**Mon, 24 Nov 2025 07:58:29 UTC (6,931 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Vidi2: Large Multimodal Models for Video Understanding and Creation, by Vidi Team and 24 other authors\n* [View PDF](https://arxiv.org/pdf/2511.19529)\n* [HTML (experimental)](https://arxiv.org/html/2511.19529v1)\n* [TeX Source](https://arxiv.org/src/2511.19529)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2511.19529&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2511.19529&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2025-11](https://arxiv.org/list/cs.CV/2025-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/2511.19529?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2511.19529)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2511.19529)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2511.19529)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\n",
          "original_query": "Large Language Models for Video Understanding",
          "cleaned_query": "Large Language Models for Video Understanding"
        },
        {
          "success": true,
          "title": "Do Pre-trained Vision-Language Models Encode Object States?",
          "url": "https://arxiv.org/abs/2409.10488",
          "content": "\n View PDF \n HTML (experimental) \nFor a vision-language model (VLM) to understand the physical world, such as cause and effect, a first step is to capture the temporal dynamics of the visual world, for example how the physical states of objects evolve over time (e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs pre-trained on web-scale data learn to encode object states, which can be extracted with zero-shot text prompts. We curate an object state recognition dataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models trained with contrastive and generative objectives. We observe that while these state-of-the-art vision-language models can reliably perform object recognition, they consistently fail to accurately distinguish the objects' physical states. Through extensive experiments, we identify three areas for improvements for VLMs to better encode object states, namely the quality of object localization, the architecture to bind concepts to objects, and the objective to learn discriminative visual and language encoders on object states. Data and code are released.\n \n \n Submission history From: Kaleb Newman [ view email] [v1] \nMon, 16 Sep 2024 17:22:18 UTC (1,847 KB) \n",
          "original_query": "Object State Recognition using Vision-Language Models",
          "cleaned_query": "Object State Recognition using Vision-Language Models"
        },
        {
          "success": true,
          "title": "Multimodal Fusion with Vision-Language-Action Models for Robotic ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S1566253525011248",
          "content": "Multimodal fusion with vision-language-action models for robotic manipulation: A systematic review - ScienceDirect\n[Skip to main content](#screen-reader-main-content)[Skip to article](#screen-reader-main-title)\n[![Elsevier logo](https://www.sciencedirect.com/shared-assets/24/images/elsevier-non-solus-new-grey.svg)ScienceDirect](https://www.sciencedirect.com/)\n[My account](https://www.sciencedirect.com/user/login?targetURL=/science/article/pii/S1566253525011248&amp;from=globalheader)\n[Sign in](https://www.sciencedirect.com/user/institution/login?targetURL=/science/article/pii/S1566253525011248)\n* [View**PDF**](https://www.sciencedirect.com/science/article/pii/S1566253525011248/pdfft?md5=0d1f8d7a0100d1a7f4c453e3ed1f1593&amp;pid=1-s2.0-S1566253525011248-main.pdf)\n* Download full issue\nSearch ScienceDirect\n## Outline\n1. [Highlights](#absh001)\n2. [Abstract](#abs0001)\n3. [Keywords](#keys0001)\n4. [1. Introduction](#sec0001)\n5. [2. Literature search and selection criteria](#sec0002)\n6. [3. Fundamentals of multi-modal fusion for VLA](#sec0003)\n7. [4. Vision language action models](#sec0012)\n8. [5. Datasets for multi-modal fusion](#sec0018)\n9. [6. Simulation tools](#sec0023)\n10. [7. Cross-Domain evaluation and analysis of VLA models](#sec0026)\n11. [8. Progress, challenges and future directions](#sec0031)\n12. [9. Conclusion](#sec0043)\n13. [Data and Code Availability](#sec0063)\n14. [Declaration](#sec0064)\n15. [CRediT authorship contribution statement](#sec0064a)\n16. [Declaration of competing interest](#sec0065)\n17. [Appendix A. Supplementary materials](#sec0044)\n18. [Appendix B. Key terminologies](#sec0045)\n19. [Appendix C. Statistical methodologies and key concepts](#sec0047)\n20. [Appendix D. Theoretical entropy and fusion energy computation](#sec0053)\n21. [Appendix E. Domain and component analysis methodology](#sec0058)\n22. [Data availability](#da01)\n23. [References](#bib001)Show full outline\n## Figures (22)\n1. [![Fig. 1. Timeline of Vision-Language-Action (VLA) developments between 2022 and 2025](https://ars.els-cdn.com/content/image/1-s2.0-S1566253525011248-gr1.sml)](#fig0001)\n2. [![Fig. 2. Overview of the skeleton of the paper, highlighting the main sections and their\u2026](https://ars.els-cdn.com/content/image/1-s2.0-S1566253525011248-gr2.sml)](#fig0002)\n3. [![Fig. 3. Annual VLA models and foundational VLA datasets count from 2022 to 2025](https://ars.els-cdn.com/content/image/1-s2.0-S1566253525011248-gr3.sml)](#fig0003)\n4. [![Fig. 4. An overview of the Transformer architecture highlighting the encoder-decoder\u2026](https://ars.els-cdn.com/content/image/1-s2.0-S1566253525011248-gr4.sml)](#fig0004)\n5. [![Fig. 5. Architecture of the ViT](https://ars.els-cdn.com/content/image/1-s2.0-S1566253525011248-gr5.sml)](#fig0005)\n6. [![Fig. 6. Architecture of a VLM for image captioning and semantic understanding](https://ars.els-cdn.com/content/image/1-s2.0-S1566253525011248-gr6.sml)](#fig0006)Show 16 more figures\n## Tables (13)\n1. [Table 1](#tbl0001)\n2. [Table 1](#tbl0002)\n3. [Table 1](#tbl0003)\n4. [Table 1](#tbl0004)\n5. [Table 1](#tbl0005)\n6. [Table 1](#tbl0006)Show all tables\n## Extras (1)\n1. [Supplementary Data S1](#ecom0001)\n[![Elsevier](https://www.sciencedirect.com/us-east-1/prod/bd96d51d266808527bf1018bd38b59c0b4bc6286/image/elsevier-non-solus.svg)](https://www.sciencedirect.com/journal/information-fusion)\n## [Information Fusion](https://www.sciencedirect.com/journal/information-fusion)\n[Volume 129](https://www.sciencedirect.com/journal/information-fusion/vol/129/suppl/C),May 2026, 104062\n[![Information Fusion](https://ars.els-cdn.com/content/image/1-s2.0-S1566253525X00154-cov150h.gif)](https://www.sciencedirect.com/journal/information-fusion/vol/129/suppl/C)\n# Full Length Article\nMultimodal fusion with vision-language-action models for robotic manipulation: A systematic review\nAuthor links open overlay panelMuhayyUd Dina,WaseemAkrama,LyesSaad Saouda,JanRosellb,IrfanHussaina\nShow more\nOutline\nAdd to Mendeley\nShare\nCite\n[https://doi.org/10.1016/j.inffus.2025.104062](https://doi.org/10.1016/j.inffus.2025.104062)[Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S1566253525011248&amp;orderBeanReset=true)\nUnder a Creative Commons[license](http://creativecommons.org/licenses/by/4.0/)\nOpen access\n## Highlights\n* \u2022Provides a unified taxonomy that organizes more than 100 VLA architectures.\n* \u2022Maps 26 major VLA datasets using a framework based on task difficulty and modality richness.\n* \u2022Presents a large-scale quantitative analysis linking model design choices to normalized performance.\n* \u2022Demonstrates that diffusion-based decoders and hierarchical fusion significantly improve manipulation success.\n* \u2022Introduces the VLA-FEB benchmark with new metrics for measuring multimodal fusion quality and alignment.\n* \u2022Proposes an agentic VLA framework where LLM planners verify and re-plan actions using uncertainty-driven feedback for self-improving robotic autonomy.\n## Abstract\nVision Language Action (VLA) models represent a new frontier in robotics by unifying perception, reasoning, and control within a single multimodal learning framework. By integrating visual, linguistic, and action modalities, they enable multimodal fusion systems designed for instruction-driven manipulation and generalist autonomy. This systematic review synthesizes the state of the art in VLA research with an emphasis on architectures, algorithms, and applications relevant to robotic manipulation. We examine 102 models, 26 foundational datasets, and 12 simulation platforms, categorizing them according to their fusion strategies and integration mechanisms. Foundational datasets are evaluated using a novel criterion based on task complexity, modality richness, and dataset scale, allowing a comparative analysis of their suitability for generalist policy learning. We further introduce a structured taxonomy of fusion hierarchies and encoder-decoder families, together with a two-dimensional dataset characterization framework and a meta-analytic benchmarking protocol that quantitatively links design variables to empirical performance across benchmarks. Our analysis shows that hierarchical and late fusion architectures achieve the highest manipulation success and generalization, confirming the benefit of multi-level cross-modal integration. Diffusion-based decoders demonstrate superior cross-domain transfer and robustness compared to autoregressive heads. Dataset analysis highlights a persistent lack of benchmarks that combine high-complexity, multimodal, and long-horizon tasks, while existing simulators offer limited multimodal synchronization and real-to-sim consistency. To address these gaps, we propose the VLA Fusion Evaluation Benchmark to quantify fusion efficiency and alignment. Drawing on both academic and industrial advances, the review outlines future research directions in adaptive and modular fusion architectures, computational resource optimization, and the deployment of interpretable, resource-efficient robotic systems. We further propose a forward-looking agentic VLA paradigm where LLM planners integrate VLA skills as verifiable tools within a closed feedback loop for adaptive and self-improving robotic control. This work provides both a conceptual foundation and a quantitative roadmap for advancing embodied intelligence through multimodal information fusion across robotic domains. A public repository summarizing models, datasets, and simulators is available at:[https://muhayyuddin.github.io/VLAs/](https://muhayyuddin.github.io/VLAs/).\n* [Previousarticlein issue](https://www.sciencedirect.com/science/article/pii/S1566253525011236)\n* [Nextarticlein issue](https://www.sciencedirect.com/science/article/pii/S1566253525011285)\n## Keywords\nInformation fusion\nVision-Language-Action models\nRobotic and embodied control\nFoundation models\nLanguage-conditioned manipulation\nRobotic manipulation\nMultimodal learning\nCross-modal alignment\nEmbodied AI\nAgentic AI\n## 1. Int",
          "original_query": "Action Conditioned Models for Robotic Manipulation",
          "cleaned_query": "Action Conditioned Models for Robotic Manipulation"
        },
        {
          "success": true,
          "title": "[2507.15569] DynImg: Key Frames with Visual Prompts are Good ...",
          "url": "https://arxiv.org/abs/2507.15569",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2507.15569** (cs)\n\n\\[Submitted on 21 Jul 2025\\]\n\n# Title:DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding\n\nAuthors: [Xiaoyi Bao](https://arxiv.org/search/cs?searchtype=author&query=Bao,+X), [Chenwei Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie,+C), [Hao Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+H), [Tingyu Weng](https://arxiv.org/search/cs?searchtype=author&query=Weng,+T), [Xiaofeng Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+X), [Yun Zheng](https://arxiv.org/search/cs?searchtype=author&query=Zheng,+Y), [Xingang Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+X)\n\nView a PDF of the paper titled DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding, by Xiaoyi Bao and 5 other authors\n\n[View PDF](https://arxiv.org/pdf/2507.15569) [HTML (experimental)](https://arxiv.org/html/2507.15569v1)\n\n> Abstract:In recent years, the introduction of Multi-modal Large Language Models (MLLMs) into video understanding tasks has become increasingly prevalent. However, how to effectively integrate temporal information remains a critical research focus. Traditional approaches treat spatial and temporal information separately. Due to issues like motion blur, it is challenging to accurately represent the spatial information of rapidly moving objects. This can lead to temporally important regions being underemphasized during spatial feature extraction, which in turn hinders accurate spatio-temporal interaction and video understanding. To address this limitation, we propose an innovative video representation method called Dynamic-Image (DynImg). Specifically, we introduce a set of non-key frames as temporal prompts to highlight the spatial areas containing fast-moving objects. During the process of visual feature extraction, these prompts guide the model to pay additional attention to the fine-grained spatial features corresponding to these regions. Moreover, to maintain the correct sequence for DynImg, we employ a corresponding 4D video Rotary Position Embedding. This retains both the temporal and spatial adjacency of DynImg, helping MLLM understand the spatio-temporal order within this combined format. Experimental evaluations reveal that DynImg surpasses the state-of-the-art methods by approximately 2% across multiple video understanding benchmarks, proving the effectiveness of our temporal prompts in enhancing video comprehension.\n\n| | |\n| --- | --- |\n| Comments: | Accepted by ICCV 2025 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2507.15569](https://arxiv.org/abs/2507.15569) \\[cs.CV\\] |\n| (or [arXiv:2507.15569v1](https://arxiv.org/abs/2507.15569v1) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2507.15569](https://doi.org/10.48550/arXiv.2507.15569) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Xiaoyi Bao \\[ [view email](https://arxiv.org/show-email/68a71786/2507.15569)\\] **\\[v1\\]**\nMon, 21 Jul 2025 12:50:49 UTC (9,948 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding, by Xiaoyi Bao and 5 other authors\n\n- [View PDF](https://arxiv.org/pdf/2507.15569)\n- [HTML (experimental)](https://arxiv.org/html/2507.15569v1)\n- [TeX Source](https://arxiv.org/src/2507.15569)\n- [Other Formats](https://arxiv.org/format/2507.15569)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2507.15569&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2507.15569&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2025-07](https://arxiv.org/list/cs.CV/2025-07)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2507.15569?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2507.15569)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2507.15569)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2507.15569)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2507.15569) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Dynamic Representations in Video Understanding",
          "cleaned_query": "Dynamic Representations in Video Understanding"
        },
        {
          "success": true,
          "title": "Few-Shot Object Detection: Research Advances and Challenges",
          "url": "https://arxiv.org/abs/2404.04799",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "Few-Shot Learning in Object Recognition",
          "cleaned_query": "Few-Shot Learning in Object Recognition"
        }
      ],
      "generated_ideas": [
        "1. **Causal State-Change Pretraining for Video-Language Models**\n   Build a pretraining objective that explicitly enforces the causal chain \u201cpre-state \u2192 action \u2192 post-state\u201d from Paper 1, but implemented inside a Vidi2-style multimodal transformer (Paper 2). Train on untrimmed web videos by mining candidate change segments, then optimize a triplet/ordering loss that makes the model predict (a) which state comes first and (b) which action best explains the transition, improving downstream state recognition where VLMs currently fail (Paper 3).",
        "**State-Aware Spatio-Temporal Grounding (STG) with Pre/Post Boxes**\n   Extend VUE-STG-style annotations (Paper 2) to include *paired* bounding boxes and timestamps for the same object in two states (e.g., \u201ccup empty\u201d vs \u201ccup full\u201d), plus the transition interval. Train a grounding model to output two boxes and an interval, and evaluate with a new \u201cpaired vIoU + temporal consistency\u201d metric to measure whether the model truly tracks state changes rather than static object presence.",
        "**DynImg-for-Change: Temporal Prompting Targeted to State Transitions**\n   Adapt DynImg (Paper 5) to create \u201ctransition-focused\u201d dynamic images by selecting non-key frames specifically around detected change points (Paper 1) and using them as visual prompts. Compare against vanilla DynImg on ChangeIt-Frames (Paper 3) and long-video STG tasks (Paper 2), testing whether prompting around *state boundaries* yields larger gains than generic motion-based prompting.",
        "**Noise-Adaptive Web Video Curriculum for Learning Object States**\n   Generalize Paper 1\u2019s noise-adaptive weighting into a curriculum that gradually expands from high-confidence state-change videos to noisier ones, using a Vidi2-like retrieval+grounding model to re-score data each epoch (Paper 2). The key contribution is a closed-loop data engine where the model\u2019s uncertainty filters training samples, aiming to reduce the \u201cstate confusion\u201d observed in pretrained VLMs (Paper 3) without requiring many labeled frames.",
        "**Object-Centric State Binding via Box-Conditioned Language Tokens**\n   Address Paper 3\u2019s \u201cconcept-to-object binding\u201d issue by introducing box-conditioned state tokens: the model predicts state text embeddings conditioned on ROI features from grounded boxes (Paper 2) rather than whole-frame features. Train with a contrastive loss over *object crops* and state prompts (e.g., \u201ca sliced apple\u201d), and evaluate whether correct localization + binding reduces spurious correlations with background context.",
        "**Few-Shot State Detection with Change Prototypes**\n   Create a few-shot benchmark and method (Paper 6 theme) where the task is to detect *state-specific instances* (e.g., \u201copen jar\u201d, \u201cclosed jar\u201d) with only K labeled exemplars per state. Use Paper 1\u2019s mined transitions to generate prototype features for each state from pre/post segments, then perform prototype-based detection on new objects/categories, measuring cross-object generalization of \u201cstate\u201d concepts.",
        "**Counterfactual Video Generation for State Robustness Testing**\n   Use a Vidi2 creation pipeline (Paper 2) to generate controlled counterfactual clips where the object identity and scene remain constant but the state is swapped (e.g., generate \u201cempty cup\u201d vs \u201cfull cup\u201d) while keeping action unchanged or removed. Evaluate VLMs (Paper 3) on these pairs to quantify sensitivity to state, and fine-tune with a consistency loss that forces embeddings to change with state but remain stable to non-state nuisances.",
        "**Long-Horizon State Graph Memory for Video QA**\n   Extend Video QA in Vidi2 (Paper 2) with an explicit \u201cstate graph memory\u201d module that stores object nodes and timestamped state attributes inferred from Paper 1-style change localization. Train it to answer queries like \u201cWas the pan heated before the eggs were added?\u201d or \u201cAfter pouring, was the cup ever emptied again?\u201d, testing long-context reasoning and temporal causality beyond single-clip classification.",
        "**Fusion Quality Metric for State-Driven Robotic Manipulation**\n   Inspired by the VLA fusion review and metrics (Paper 4), design a manipulation benchmark where success depends on correctly inferring object state (e.g., \u201cgrasp only if lid is open\u201d, \u201cpour until cup is full\u201d). Introduce a \u201cState-Action Alignment Score\u201d that measures whether the robot\u2019s action distribution changes appropriately with perceived state, enabling standardized evaluation of state grounding as a first-class component of VLA fusion.",
        "**Untrimmed-to-Robotics Transfer: Learning State Preconditions for Action Policies**\n   Train a VLA policy that uses state-change priors learned from untrimmed web videos (Paper 1) as *preconditions* and *postconditions* for actions (e.g., \u201cto cut: object must be whole; after cut: object becomes sliced\u201d). Implement a planner that queries the learned model to verify whether the current state satisfies action preconditions and triggers re-planning using uncertainty feedback (Paper 4), then evaluate transfer to real manipulation tasks with minimal additional supervision."
      ],
      "generated_ideas_raw": "1. **Causal State-Change Pretraining for Video-Language Models**\n   Build a pretraining objective that explicitly enforces the causal chain \u201cpre-state \u2192 action \u2192 post-state\u201d from Paper 1, but implemented inside a Vidi2-style multimodal transformer (Paper 2). Train on untrimmed web videos by mining candidate change segments, then optimize a triplet/ordering loss that makes the model predict (a) which state comes first and (b) which action best explains the transition, improving downstream state recognition where VLMs currently fail (Paper 3).\n\n2. **State-Aware Spatio-Temporal Grounding (STG) with Pre/Post Boxes**\n   Extend VUE-STG-style annotations (Paper 2) to include *paired* bounding boxes and timestamps for the same object in two states (e.g., \u201ccup empty\u201d vs \u201ccup full\u201d), plus the transition interval. Train a grounding model to output two boxes and an interval, and evaluate with a new \u201cpaired vIoU + temporal consistency\u201d metric to measure whether the model truly tracks state changes rather than static object presence.\n\n3. **DynImg-for-Change: Temporal Prompting Targeted to State Transitions**\n   Adapt DynImg (Paper 5) to create \u201ctransition-focused\u201d dynamic images by selecting non-key frames specifically around detected change points (Paper 1) and using them as visual prompts. Compare against vanilla DynImg on ChangeIt-Frames (Paper 3) and long-video STG tasks (Paper 2), testing whether prompting around *state boundaries* yields larger gains than generic motion-based prompting.\n\n4. **Noise-Adaptive Web Video Curriculum for Learning Object States**\n   Generalize Paper 1\u2019s noise-adaptive weighting into a curriculum that gradually expands from high-confidence state-change videos to noisier ones, using a Vidi2-like retrieval+grounding model to re-score data each epoch (Paper 2). The key contribution is a closed-loop data engine where the model\u2019s uncertainty filters training samples, aiming to reduce the \u201cstate confusion\u201d observed in pretrained VLMs (Paper 3) without requiring many labeled frames.\n\n5. **Object-Centric State Binding via Box-Conditioned Language Tokens**\n   Address Paper 3\u2019s \u201cconcept-to-object binding\u201d issue by introducing box-conditioned state tokens: the model predicts state text embeddings conditioned on ROI features from grounded boxes (Paper 2) rather than whole-frame features. Train with a contrastive loss over *object crops* and state prompts (e.g., \u201ca sliced apple\u201d), and evaluate whether correct localization + binding reduces spurious correlations with background context.\n\n6. **Few-Shot State Detection with Change Prototypes**\n   Create a few-shot benchmark and method (Paper 6 theme) where the task is to detect *state-specific instances* (e.g., \u201copen jar\u201d, \u201cclosed jar\u201d) with only K labeled exemplars per state. Use Paper 1\u2019s mined transitions to generate prototype features for each state from pre/post segments, then perform prototype-based detection on new objects/categories, measuring cross-object generalization of \u201cstate\u201d concepts.\n\n7. **Counterfactual Video Generation for State Robustness Testing**\n   Use a Vidi2 creation pipeline (Paper 2) to generate controlled counterfactual clips where the object identity and scene remain constant but the state is swapped (e.g., generate \u201cempty cup\u201d vs \u201cfull cup\u201d) while keeping action unchanged or removed. Evaluate VLMs (Paper 3) on these pairs to quantify sensitivity to state, and fine-tune with a consistency loss that forces embeddings to change with state but remain stable to non-state nuisances.\n\n8. **Long-Horizon State Graph Memory for Video QA**\n   Extend Video QA in Vidi2 (Paper 2) with an explicit \u201cstate graph memory\u201d module that stores object nodes and timestamped state attributes inferred from Paper 1-style change localization. Train it to answer queries like \u201cWas the pan heated before the eggs were added?\u201d or \u201cAfter pouring, was the cup ever emptied again?\u201d, testing long-context reasoning and temporal causality beyond single-clip classification.\n\n9. **Fusion Quality Metric for State-Driven Robotic Manipulation**\n   Inspired by the VLA fusion review and metrics (Paper 4), design a manipulation benchmark where success depends on correctly inferring object state (e.g., \u201cgrasp only if lid is open\u201d, \u201cpour until cup is full\u201d). Introduce a \u201cState-Action Alignment Score\u201d that measures whether the robot\u2019s action distribution changes appropriately with perceived state, enabling standardized evaluation of state grounding as a first-class component of VLA fusion.\n\n10. **Untrimmed-to-Robotics Transfer: Learning State Preconditions for Action Policies**\n   Train a VLA policy that uses state-change priors learned from untrimmed web videos (Paper 1) as *preconditions* and *postconditions* for actions (e.g., \u201cto cut: object must be whole; after cut: object becomes sliced\u201d). Implement a planner that queries the learned model to verify whether the current state satisfies action preconditions and triggers re-planning using uncertainty feedback (Paper 4), then evaluate transfer to real manipulation tasks with minimal additional supervision.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "1. **Causal State-Change Pretraining for Video-Language Models**\n   Build a pretraining objective that explicitly enforces the causal chain \u201cpre-state \u2192 action \u2192 post-state\u201d from Paper 1, but implemen",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "**State-Aware Spatio-Temporal Grounding (STG) with Pre/Post Boxes**\n   Extend VUE-STG-style annotations (Paper 2) to include *paired* bounding boxes and timestamps for the same object in two states (e",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "**DynImg-for-Change: Temporal Prompting Targeted to State Transitions**\n   Adapt DynImg (Paper 5) to create \u201ctransition-focused\u201d dynamic images by selecting non-key frames specifically around detected",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "**Noise-Adaptive Web Video Curriculum for Learning Object States**\n   Generalize Paper 1\u2019s noise-adaptive weighting into a curriculum that gradually expands from high-confidence state-change videos to",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "**Object-Centric State Binding via Box-Conditioned Language Tokens**\n   Address Paper 3\u2019s \u201cconcept-to-object binding\u201d issue by introducing box-conditioned state tokens: the model predicts state text e",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "**Few-Shot State Detection with Change Prototypes**\n   Create a few-shot benchmark and method (Paper 6 theme) where the task is to detect *state-specific instances* (e.g., \u201copen jar\u201d, \u201cclosed jar\u201d) wi",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "**Counterfactual Video Generation for State Robustness Testing**\n   Use a Vidi2 creation pipeline (Paper 2) to generate controlled counterfactual clips where the object identity and scene remain const",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "**Long-Horizon State Graph Memory for Video QA**\n   Extend Video QA in Vidi2 (Paper 2) with an explicit \u201cstate graph memory\u201d module that stores object nodes and timestamped state attributes inferred f",
          "is_match": true
        },
        {
          "idea_idx": 8,
          "idea_text": "**Fusion Quality Metric for State-Driven Robotic Manipulation**\n   Inspired by the VLA fusion review and metrics (Paper 4), design a manipulation benchmark where success depends on correctly inferring",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "**Untrimmed-to-Robotics Transfer: Learning State Preconditions for Action Policies**\n   Train a VLA policy that uses state-change priors learned from untrimmed web videos (Paper 1) as *preconditions* ",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 61,
      "paper_title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?",
      "contribution": "The paper critically evaluates the effectiveness of Reinforcement Learning with Verifiable Rewards (RLVR) in enhancing the reasoning capabilities of large language models (LLMs) and reveals that the improvements are primarily superficial, as they do not generate fundamentally new reasoning patterns beyond those already established by base models.",
      "num_predecessors": 3,
      "predecessors_crawled": 3,
      "academic_sources": 3,
      "crawl_rate": 1.0,
      "ideas_generated": 4,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 6344,
      "output_tokens": 883,
      "predecessor_details": [
        {
          "success": true,
          "title": "Proximal policy optimization with reward-based prioritization",
          "url": "https://www.sciencedirect.com/science/article/abs/pii/S0957417425012813",
          "content": "Proximal policy optimization with reward-based prioritization - ScienceDirect\nTypesetting math: 100%\n[Skip to main content](#screen-reader-main-content)[Skip to article](#screen-reader-main-title)\n[![Elsevier logo](https://www.sciencedirect.com/shared-assets/24/images/elsevier-non-solus-new-grey.svg)ScienceDirect](https://www.sciencedirect.com/)\n[My account](https://www.sciencedirect.com/user/login?targetURL=/science/article/pii/S0957417425012813&amp;from=globalheader)\n[Sign in](https://www.sciencedirect.com/user/institution/login?targetURL=/science/article/pii/S0957417425012813)\n* [Access through**your organization**](https://www.sciencedirect.com/user/institution/login?targetUrl=/science/article/pii/S0957417425012813)\n* [Purchase PDF](https://www.sciencedirect.com/getaccess/pii/S0957417425012813/purchase)\nSearch ScienceDirect\n## Article preview\n* [Abstract](#preview-section-abstract)\n* [Introduction](#preview-section-introduction)\n* [Section snippets](#preview-section-snippets)\n* [References (27)](#preview-section-references)\n* [Cited by (6)](#preview-section-cited-by)\n[![Elsevier](https://www.sciencedirect.com/us-east-1/prod/372ce7cbeb39f7604e4fea79a56707dae91cbf4a/image/elsevier-non-solus.svg)](https://www.sciencedirect.com/journal/expert-systems-with-applications)\n## [Expert Systems with Applications](https://www.sciencedirect.com/journal/expert-systems-with-applications)\n[Volume 283](https://www.sciencedirect.com/journal/expert-systems-with-applications/vol/283/suppl/C),15 July 2025, 127659\n[![Expert Systems with Applications](https://ars.els-cdn.com/content/image/1-s2.0-S0957417425X00154-cov150h.gif)](https://www.sciencedirect.com/journal/expert-systems-with-applications/vol/283/suppl/C)\n# Proximal policy optimization with reward-based prioritization\nAuthor links open overlay panelMingshengZhenga,JunweiZhangab,ChangshuaiZhana,XinyuRena,[ShuaiL\u00fcabc](https://www.sciencedirect.com/author/55017142700/shuai-lu)\nShow more\nAdd to Mendeley\nShare\nCite\n[https://doi.org/10.1016/j.eswa.2025.127659](https://doi.org/10.1016/j.eswa.2025.127659)[Get rights and content](https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S0957417425012813&amp;orderBeanReset=true)\n## Abstract\nThe PPO (Proximal Policy Optimization) algorithm is a policy optimization-based[deep reinforcement learning](https://www.sciencedirect.com/topics/computer-science/deep-reinforcement-learning)algorithm that has achieved outstanding results and[widespread applications](https://www.sciencedirect.com/topics/computer-science/widespread-application). Despite the popularity of the PPO algorithm, it has several notable drawbacks, including its sensitivity to hyperparameters, slow convergence, and limited exploration. In recent research, code-level optimization has been proposed to improve training effectiveness and achieved[good performance](https://www.sciencedirect.com/topics/computer-science/good-performance). In this paper, we propose a Proximal Policy Optimization with Reward-based Prioritization (RP-PPO) algorithm that gives different experiences different priorities to update policy based on reward and find the model that gets the highest average reward. We also apply some minor techniques including Normalized Reward and Dual[Learning Rate](https://www.sciencedirect.com/topics/computer-science/learning-rate)Decay to optimize our algorithm. Finally, we conduct a series of experiments and tests in four problem domains in the Gym environment to demonstrate the superiority of our algorithm.\n## Introduction\nIn recent years, deep reinforcement learning algorithms based on policy gradients have achieving significant improvements in many tasks, including the Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017). The PPO algorithm applies policy gradients to optimize a surrogate objective, thereby effectively addressing optimization challenges inherent in deep reinforcement learning. Therefore, it has good stability and convergence properties.\nThe PPO algorithm uses a two-part learning strategy, which includes sampling the policy distribution and iteratively optimizing the policy based on collected data. Specifically, the algorithm conducts multiple rounds or \u201cepochs\u201d of comprehensive policy updates utilizing small data batches via stochastic gradient descent. This approach, which employs multiple epochs, promotes the efficient utilization of samples in practice. Additionally, there are two main versions of PPO: Kullback Leibler Regularized Proximal Policy Optimization (KL-PPO) (Schulman et al., 2015) and Ratio Clipping Proximal Policy Optimization (RC-PPO) (Schulman et al., 2017).\nRegularization is essential in PPO as it introduces a distinct trust region constraint in every optimization epoch. This point is underscored by Schulman et al. (2017) and underlined by Neu et al. (2017). As demonstrated in previous work (Liu et al., 2019), KL-PPO has a vital role in guaranteeing PPO\u2019s convergence.\nContrary to KL-PPO, RC-PPO does not directly employ regularization in its optimization procedure. Instead, it clips the ratios between the policy to be optimized and the policy used to collect samples in each epoch. The objective function of RC-PPO forms a pessimistic estimate of the surrogate objective proposed in Conservative Policy Iteration (CPI) (Liu et al., 2002). Compared to KL-PPO and TRPO, ratio clipping eliminates the need for expensive computation of the KL divergence, making it a scalable approach for large-scale problems (Ye et al., 2020). Furthermore, empirical evidence suggests that RC-PPO outperforms KL-PPO in terms of practical performance (Schulman et al., 2017).\nNumerous studies, inclusive of the original paper by Schulman et al. (2017), interpret ratio clipping as a method for limiting ratios. However, recent research has shown that ratio clip is controversial: ratios may become unbounded through ratio clipping (Tomar et al., 2022, Wang et al., 2020), and even with bounded ratios, trust region constraints based on Kullback Leibler may still be violated (Wang et al., 2020). Unlike studies on ratio clip, some research suggests that RC-PPO may benefit from code-level optimization rather than ratio clipping itself (Engstrom et al., 2020). The Early Stopping Policy Optimization (EPSO) algorithm, proposed in Sun et al. (2022) and optimized at the code level, identifies the optimal point to terminate training across multiple rounds, thereby circumventing significant discrepancies in ratios. This approach can significantly improve the training efficiency of RC-PPO.\nIn addition to optimizing ratio, optimizing reward is a common approach in reinforcement learning:\nThe initial technique proposed by van Hasselt et al. (2016) standardizes the reward to accelerate deep reinforcement learning training and improve performance. Based on the maximum entropy theory, Haarnoja et al. (2018) introduce a Soft Actor-Critic (SAC) algorithm. This method uses reward scaling to tackle the problem of sparse rewards and to enhance performance. To address the issue of function approximation errors, Fujimoto et al. (2018) present a method for normalizing rewards, converting them into a normal distribution that meets certain criteria. In the field of network intrusion detection systems, a novel application of several deep reinforcement learning (DRL) is proposed, which generates rewards based on detection errors found during training in a pseudo-environment (Lopez-Martin et al., 2020). The IEM-PPO algorithm, introduced by Zhang et al. (2022), implements an N-network to transform the reward, thus encouraging the agent to explore unfamiliar behaviors. A novel multi-agent modeling approach is adopted to support adaptive learning, with each machine modeled as a cooperative agent. The reward function is constructed based on the evaluation of system-level production loss (Su et al., 2022).\nWhile the realm of reward optimization has witnessed considerable advancements, their deployment wi",
          "original_query": "Proximal policy optimization algorithms",
          "cleaned_query": "Proximal policy optimization algorithms"
        },
        {
          "success": true,
          "title": "Tulu 3: Pushing Frontiers in Open Language Model Post-Training",
          "url": "https://arxiv.org/abs/2411.15124",
          "content": "[2411.15124] Tulu 3: Pushing Frontiers in Open Language Model Post-Training\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2411.15124\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2411.15124**(cs)\n[Submitted on 22 Nov 2024 ([v1](https://arxiv.org/abs/2411.15124v1)), last revised 14 Apr 2025 (this version, v5)]\n# Title:Tulu 3: Pushing Frontiers in Open Language Model Post-Training\nAuthors:[Nathan Lambert](https://arxiv.org/search/cs?searchtype=author&amp;query=Lambert,+N),[Jacob Morrison](https://arxiv.org/search/cs?searchtype=author&amp;query=Morrison,+J),[Valentina Pyatkin](https://arxiv.org/search/cs?searchtype=author&amp;query=Pyatkin,+V),[Shengyi Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+S),[Hamish Ivison](https://arxiv.org/search/cs?searchtype=author&amp;query=Ivison,+H),[Faeze Brahman](https://arxiv.org/search/cs?searchtype=author&amp;query=Brahman,+F),[Lester James V. Miranda](https://arxiv.org/search/cs?searchtype=author&amp;query=Miranda,+L+J+V),[Alisa Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+A),[Nouha Dziri](https://arxiv.org/search/cs?searchtype=author&amp;query=Dziri,+N),[Shane Lyu](https://arxiv.org/search/cs?searchtype=author&amp;query=Lyu,+S),[Yuling Gu](https://arxiv.org/search/cs?searchtype=author&amp;query=Gu,+Y),[Saumya Malik](https://arxiv.org/search/cs?searchtype=author&amp;query=Malik,+S),[Victoria Graf](https://arxiv.org/search/cs?searchtype=author&amp;query=Graf,+V),[Jena D. Hwang](https://arxiv.org/search/cs?searchtype=author&amp;query=Hwang,+J+D),[Jiangjiang Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+J),[Ronan Le Bras](https://arxiv.org/search/cs?searchtype=author&amp;query=Bras,+R+L),[Oyvind Tafjord](https://arxiv.org/search/cs?searchtype=author&amp;query=Tafjord,+O),[Chris Wilhelm](https://arxiv.org/search/cs?searchtype=author&amp;query=Wilhelm,+C),[Luca Soldaini](https://arxiv.org/search/cs?searchtype=author&amp;query=Soldaini,+L),[Noah A. Smith](https://arxiv.org/search/cs?searchtype=author&amp;query=Smith,+N+A),[Yizhong Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y),[Pradeep Dasigi](https://arxiv.org/search/cs?searchtype=author&amp;query=Dasigi,+P),[Hannaneh Hajishirzi](https://arxiv.org/search/cs?searchtype=author&amp;query=Hajishirzi,+H)\nView a PDF of the paper titled Tulu 3: Pushing Frontiers in Open Language Model Post-Training, by Nathan Lambert and 21 other authors\n[View PDF](https://arxiv.org/pdf/2411.15124)[HTML (experimental)](https://arxiv.org/html/2411.15124v5)> > Abstract:\n> Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce Tulu 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. Tulu 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.\n> In addition to the Tulu 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the Tulu 3 approach to more domains. Comments:|Added Tulu 3 405B results and additional analyses|\nSubjects:|Computation and Language (cs.CL)|\nCite as:|[arXiv:2411.15124](https://arxiv.org/abs/2411.15124)[cs.CL]|\n|(or[arXiv:2411.15124v5](https://arxiv.org/abs/2411.15124v5)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2411.15124](https://doi.org/10.48550/arXiv.2411.15124)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Nathan Lambert [[view email](https://arxiv.org/show-email/1fc22626/2411.15124)]\n**[[v1]](https://arxiv.org/abs/2411.15124v1)**Fri, 22 Nov 2024 18:44:04 UTC (3,039 KB)\n**[[v2]](https://arxiv.org/abs/2411.15124v2)**Fri, 6 Dec 2024 01:01:20 UTC (3,039 KB)\n**[[v3]](https://arxiv.org/abs/2411.15124v3)**Wed, 29 Jan 2025 18:46:59 UTC (6,596 KB)\n**[[v4]](https://arxiv.org/abs/2411.15124v4)**Thu, 13 Feb 2025 21:18:55 UTC (6,532 KB)\n**[v5]**Mon, 14 Apr 2025 22:39:09 UTC (6,532 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Tulu 3: Pushing Frontiers in Open Language Model Post-Training, by Nathan Lambert and 21 other authors\n* [View PDF](https://arxiv.org/pdf/2411.15124)\n* [HTML (experimental)](https://arxiv.org/html/2411.15124v5)\n* [TeX Source](https://arxiv.org/src/2411.15124)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2411.15124&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2411.15124&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2024-11](https://arxiv.org/list/cs.CL/2024-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/2411.15124?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2411.15124)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2411.15124)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2411.15124)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\n",
          "original_query": "Gpts: Pushing frontiers in open language model post-training",
          "cleaned_query": "Gpts: Pushing frontiers in open language model post-training"
        },
        {
          "success": true,
          "title": "[2303.08774] GPT-4 Technical Report",
          "url": "https://arxiv.org/abs/2303.08774",
          "content": "[2303.08774] GPT-4 Technical Report\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2303.08774\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2303.08774**(cs)\n[Submitted on 15 Mar 2023 ([v1](https://arxiv.org/abs/2303.08774v1)), last revised 4 Mar 2024 (this version, v6)]\n# Title:GPT-4 Technical Report\nAuthors:[OpenAI](https://arxiv.org/search/cs?searchtype=author&amp;query=OpenAI),[Josh Achiam](https://arxiv.org/search/cs?searchtype=author&amp;query=Achiam,+J),[Steven Adler](https://arxiv.org/search/cs?searchtype=author&amp;query=Adler,+S),[Sandhini Agarwal](https://arxiv.org/search/cs?searchtype=author&amp;query=Agarwal,+S),[Lama Ahmad](https://arxiv.org/search/cs?searchtype=author&amp;query=Ahmad,+L),[Ilge Akkaya](https://arxiv.org/search/cs?searchtype=author&amp;query=Akkaya,+I),[Florencia Leoni Aleman](https://arxiv.org/search/cs?searchtype=author&amp;query=Aleman,+F+L),[Diogo Almeida](https://arxiv.org/search/cs?searchtype=author&amp;query=Almeida,+D),[Janko Altenschmidt](https://arxiv.org/search/cs?searchtype=author&amp;query=Altenschmidt,+J),[Sam Altman](https://arxiv.org/search/cs?searchtype=author&amp;query=Altman,+S),[Shyamal Anadkat](https://arxiv.org/search/cs?searchtype=author&amp;query=Anadkat,+S),[Red Avila](https://arxiv.org/search/cs?searchtype=author&amp;query=Avila,+R),[Igor Babuschkin](https://arxiv.org/search/cs?searchtype=author&amp;query=Babuschkin,+I),[Suchir Balaji](https://arxiv.org/search/cs?searchtype=author&amp;query=Balaji,+S),[Valerie Balcom](https://arxiv.org/search/cs?searchtype=author&amp;query=Balcom,+V),[Paul Baltescu](https://arxiv.org/search/cs?searchtype=author&amp;query=Baltescu,+P),[Haiming Bao](https://arxiv.org/search/cs?searchtype=author&amp;query=Bao,+H),[Mohammad Bavarian](https://arxiv.org/search/cs?searchtype=author&amp;query=Bavarian,+M),[Jeff Belgum](https://arxiv.org/search/cs?searchtype=author&amp;query=Belgum,+J),[Irwan Bello](https://arxiv.org/search/cs?searchtype=author&amp;query=Bello,+I),[Jake Berdine](https://arxiv.org/search/cs?searchtype=author&amp;query=Berdine,+J),[Gabriel Bernadett-Shapiro](https://arxiv.org/search/cs?searchtype=author&amp;query=Bernadett-Shapiro,+G),[Christopher Berner](https://arxiv.org/search/cs?searchtype=author&amp;query=Berner,+C),[Lenny Bogdonoff](https://arxiv.org/search/cs?searchtype=author&amp;query=Bogdonoff,+L),[Oleg Boiko](https://arxiv.org/search/cs?searchtype=author&amp;query=Boiko,+O),[Madelaine Boyd](https://arxiv.org/search/cs?searchtype=author&amp;query=Boyd,+M),[Anna-Luisa Brakman](https://arxiv.org/search/cs?searchtype=author&amp;query=Brakman,+A),[Greg Brockman](https://arxiv.org/search/cs?searchtype=author&amp;query=Brockman,+G),[Tim Brooks](https://arxiv.org/search/cs?searchtype=author&amp;query=Brooks,+T),[Miles Brundage](https://arxiv.org/search/cs?searchtype=author&amp;query=Brundage,+M),[Kevin Button](https://arxiv.org/search/cs?searchtype=author&amp;query=Button,+K),[Trevor Cai](https://arxiv.org/search/cs?searchtype=author&amp;query=Cai,+T),[Rosie Campbell](https://arxiv.org/search/cs?searchtype=author&amp;query=Campbell,+R),[Andrew Cann](https://arxiv.org/search/cs?searchtype=author&amp;query=Cann,+A),[Brittany Carey](https://arxiv.org/search/cs?searchtype=author&amp;query=Carey,+B),[Chelsea Carlson](https://arxiv.org/search/cs?searchtype=author&amp;query=Carlson,+C),[Rory Carmichael](https://arxiv.org/search/cs?searchtype=author&amp;query=Carmichael,+R),[Brooke Chan](https://arxiv.org/search/cs?searchtype=author&amp;query=Chan,+B),[Che Chang](https://arxiv.org/search/cs?searchtype=author&amp;query=Chang,+C),[Fotis Chantzis](https://arxiv.org/search/cs?searchtype=author&amp;query=Chantzis,+F),[Derek Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+D),[Sully Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+S),[Ruby Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+R),[Jason Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+J),[Mark Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+M),[Ben Chess](https://arxiv.org/search/cs?searchtype=author&amp;query=Chess,+B),[Chester Cho](https://arxiv.org/search/cs?searchtype=author&amp;query=Cho,+C),[Casey Chu](https://arxiv.org/search/cs?searchtype=author&amp;query=Chu,+C),[Hyung Won Chung](https://arxiv.org/search/cs?searchtype=author&amp;query=Chung,+H+W),[Dave Cummings](https://arxiv.org/search/cs?searchtype=author&amp;query=Cummings,+D),[Jeremiah Currier](https://arxiv.org/search/cs?searchtype=author&amp;query=Currier,+J),[Yunxing Dai](https://arxiv.org/search/cs?searchtype=author&amp;query=Dai,+Y),[Cory Decareaux](https://arxiv.org/search/cs?searchtype=author&amp;query=Decareaux,+C),[Thomas Degry](https://arxiv.org/search/cs?searchtype=author&amp;query=Degry,+T),[Noah Deutsch](https://arxiv.org/search/cs?searchtype=author&amp;query=Deutsch,+N),[Damien Deville](https://arxiv.org/search/cs?searchtype=author&amp;query=Deville,+D),[Arka Dhar](https://arxiv.org/search/cs?searchtype=author&amp;query=Dhar,+A),[David Dohan](https://arxiv.org/search/cs?searchtype=author&amp;query=Dohan,+D),[Steve Dowling](https://arxiv.org/search/cs?searchtype=author&amp;query=Dowling,+S),[Sheila Dunning](https://arxiv.org/search/cs?searchtype=author&amp;query=Dunning,+S),[Adrien Ecoffet](https://arxiv.org/search/cs?searchtype=author&amp;query=Ecoffet,+A),[Atty Eleti](https://arxiv.org/search/cs?searchtype=author&amp;query=Eleti,+A),[Tyna Eloundou](https://arxiv.org/search/cs?searchtype=author&amp;query=Eloundou,+T),[David Farhi](https://arxiv.org/search/cs?searchtype=author&amp;query=Farhi,+D),[Liam Fedus](https://arxiv.org/search/cs?searchtype=author&amp;query=Fedus,+L),[Niko Felix](https://arxiv.org/search/cs?searchtype=author&amp;query=Felix,+N),[Sim\u00f3n Posada Fishman](https://arxiv.org/search/cs?searchtype=author&amp;query=Fishman,+S+P),[Juston Forte](https://arxiv.org/search/cs?searchtype=author&amp;query=Forte,+J),[Isabella Fulford](https://arxiv.org/search/cs?searchtype=author&amp;query=Fulford,+I),[Leo Gao](https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+L),[Elie Georges](https://arxiv.org/search/cs?searchtype=author&amp;query=Georges,+E),[Christian Gibson](https://arxiv.org/search/cs?searchtype=author&amp;query=Gibson,+C),[Vik Goel](https://arxiv.org/search/cs?searchtype=author&amp;query=Goel,+V),[Tarun Gogineni](https://arxiv.org/search/cs?searchtype=author&amp;query=Gogineni,+T),[Gabriel Goh](https://arxiv.org/search/cs?searchtype=author&amp;query=Goh,+G),[Rapha Gontijo-Lopes](https://arxiv.org/search/cs?searchtype=author&amp;query=Gontijo-Lopes,+R),[Jonathan Gordon](https://arxiv.org/search/cs?searchtype=author&amp;query=Gordon,+J),[Morgan Grafstein](https://arxiv.org/search/cs?searchtype=author&amp;query=Grafstein,+M),[Scott Gray](https://arxiv.org/search/cs?searchtype=author&amp;query=Gray,+S),[Ryan Greene](https://arxiv.org/search/cs?searchtype=auth",
          "original_query": "GPT-4 technical report",
          "cleaned_query": "GPT-4 technical report"
        }
      ],
      "generated_ideas": [
        "Normalized-Reward Calibration for Cross-Task RLVR\nIntroduce reward normalization across heterogeneous verifiers (math, code, QA, safety) so that one task\u2019s reward scale doesn\u2019t dominate RL updates. Implement per-task running normalization plus a global target distribution, and test whether it improves multitask generalization and prevents \u201cmode collapse\u201d toward tasks with easier-to-satisfy verifiers.",
        "Priority-by-Disagreement: Targeting Verifier/Preference Uncertainty\nReplace reward-only prioritization with a hybrid priority score combining reward magnitude and verifier uncertainty (e.g., ensemble variance, self-consistency disagreement, or preference model entropy). This explicitly focuses updates on ambiguous but impactful samples, and can be benchmarked for sample-efficiency improvements over classic RP-PPO prioritization in RLVR.",
        "Verifier-Gated Exploration Bonuses for Limited-Exploration PPO in Language\nAddress PPO\u2019s limited exploration by adding a verifier-gated exploration bonus: encourage diverse generations only when verifiers can reliably judge them (to avoid drifting into unscorable space). Implement as an auxiliary reward term in RLVR and test whether it improves discovery of novel correct solution strategies in math/coding tasks.",
        "Safety-Constrained Prioritized RL: Joint Optimization of Helpfulness and Risk\nDevelop a constrained variant of reward-based prioritization where high-helpfulness-reward samples are downweighted if they increase a learned risk metric (e.g., jailbreak susceptibility or policy-violation score). Evaluate Pareto frontiers (helpfulness vs. safety) and compare to unconstrained RLVR/DPO baselines to produce a more controllable post-training recipe."
      ],
      "generated_ideas_raw": "1. **Reward-Prioritized RLVR for Verifiable LLM Post-Training**\n   Build an RLVR variant that uses RP-PPO-style reward-based prioritization to upweight trajectories where verifiers provide high-confidence signals (e.g., proof checks, unit tests, deterministic graders). Compare against uniform sampling RLVR on Tulu 3\u2019s multitask suite to quantify gains in convergence speed and robustness to hyperparameters.\n\n2. **Dual Learning-Rate Decay Schedules for DPO and RLVR Stability**\n   Adapt the \u201cdual learning rate decay\u201d idea from RP-PPO into LLM post-training by separately scheduling learning rates for (a) the policy network and (b) the preference/critic or verifier head. Run controlled ablations on Tulu 3-style SFT\u2192DPO\u2192RLVR pipelines to determine which schedule reduces reward hacking and improves out-of-distribution (unseen) evaluation.\n\n3. **Normalized-Reward Calibration for Cross-Task RLVR**\n   Introduce reward normalization across heterogeneous verifiers (math, code, QA, safety) so that one task\u2019s reward scale doesn\u2019t dominate RL updates. Implement per-task running normalization plus a global target distribution, and test whether it improves multitask generalization and prevents \u201cmode collapse\u201d toward tasks with easier-to-satisfy verifiers.\n\n4. **Trust-Region Diagnostics for Ratio Clipping Failures in LLM RL**\n   Leverage concerns about unbounded ratios in PPO-style clipping to design diagnostics specific to LLM RL (e.g., monitoring per-token likelihood ratio tails and KL spikes on high-reward samples). Produce an actionable toolkit that flags when clipping stops functioning as a trust region, and evaluate whether flagged runs correlate with regressions on Tulu 3 unseen benchmarks.\n\n5. **Priority-by-Disagreement: Targeting Verifier/Preference Uncertainty**\n   Replace reward-only prioritization with a hybrid priority score combining reward magnitude and verifier uncertainty (e.g., ensemble variance, self-consistency disagreement, or preference model entropy). This explicitly focuses updates on ambiguous but impactful samples, and can be benchmarked for sample-efficiency improvements over classic RP-PPO prioritization in RLVR.\n\n6. **Decontamination-Aware Prioritized Sampling for Post-Training Data Curation**\n   Extend Tulu 3\u2019s decontamination pipeline by integrating a \u201ccontamination risk\u201d score into the prioritization strategy: prioritize high-reward, low-contamination-risk experiences and downweight borderline samples. Quantify tradeoffs between benchmark performance, memorization indicators, and generalization to truly unseen tasks.\n\n7. **Verifier-Gated Exploration Bonuses for Limited-Exploration PPO in Language**\n   Address PPO\u2019s limited exploration by adding a verifier-gated exploration bonus: encourage diverse generations only when verifiers can reliably judge them (to avoid drifting into unscorable space). Implement as an auxiliary reward term in RLVR and test whether it improves discovery of novel correct solution strategies in math/coding tasks.\n\n8. **Reward-Prioritized Curriculum over Reasoning \u201cDifficulty Bands\u201d**\n   Construct a curriculum where tasks are partitioned into difficulty bands using verifier outcomes (pass rate, minimal counterexample length, proof depth), then apply RP-PPO prioritization within and across bands. Measure whether this yields more consistent gains than static mixture training on Tulu 3\u2019s multitask evaluation, especially for long-context and multi-step reasoning.\n\n9. **Safety-Constrained Prioritized RL: Joint Optimization of Helpfulness and Risk**\n   Develop a constrained variant of reward-based prioritization where high-helpfulness-reward samples are downweighted if they increase a learned risk metric (e.g., jailbreak susceptibility or policy-violation score). Evaluate Pareto frontiers (helpfulness vs. safety) and compare to unconstrained RLVR/DPO baselines to produce a more controllable post-training recipe.\n\n10. **Generalization-Prioritized Updates Using Unseen Dev Sets as a Control Signal**\n   Introduce \u201cgeneralization priority\u201d by assigning higher weight to training samples whose updates most improve a small unseen-dev proxy (meta-gradient or influence-function approximation). This directly operationalizes Tulu 3\u2019s emphasis on unseen evaluation and can reveal whether prioritizing for generalization beats prioritizing for immediate reward in RLVR-style training.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Normalized-Reward Calibration for Cross-Task RLVR\nIntroduce reward normalization across heterogeneous verifiers (math, code, QA, safety) so that one task\u2019s reward scale doesn\u2019t dominate RL updates. Im",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Priority-by-Disagreement: Targeting Verifier/Preference Uncertainty\nReplace reward-only prioritization with a hybrid priority score combining reward magnitude and verifier uncertainty (e.g., ensemble ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Verifier-Gated Exploration Bonuses for Limited-Exploration PPO in Language\nAddress PPO\u2019s limited exploration by adding a verifier-gated exploration bonus: encourage diverse generations only when verif",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Safety-Constrained Prioritized RL: Joint Optimization of Helpfulness and Risk\nDevelop a constrained variant of reward-based prioritization where high-helpfulness-reward samples are downweighted if the",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 62,
      "paper_title": "Learning to Learn with Contrastive Meta-Objective",
      "contribution": "The paper introduces ConML, a meta-learning framework that utilizes task identity as additional supervision through contrastive learning to enhance generalizability.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 10037,
      "output_tokens": 970,
      "predecessor_details": [
        {
          "success": true,
          "title": "Learning to Learn: Introduction and Overview - Springer Link",
          "url": "https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_1",
          "content": "Learning to Learn: Introduction and Overview | Springer Nature Link (formerly SpringerLink)\n[Skip to main content](#main-content)\nAdvertisement\n[![Springer Nature Link](https://link.springer.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_1?)\n# Learning to Learn: Introduction and Overview\n* Chapter\n* pp 3\u201317\n* [Cite this chapter](#citeas)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/book/978-1-4615-5529-2?as=webp)Learning to Learn](https://link.springer.com/book/10.1007/978-1-4615-5529-2)\n* [Sebastian Thrun](#auth-Sebastian-Thrun)&amp;\n* [Lorien Pratt](#auth-Lorien-Pratt)\n* 3282Accesses\n* 444Citations\n* 7[Altmetric](https://link.altmetric.com/details/36897609)\n## Abstract\nOver the past three decades or so, research on machine learning and data mining has led to a wide variety of algorithms that learn general functions from experience. As machine learning is maturing, it has begun to make the successful transition from academic research to various practical applications. Generic techniques such as decision trees and artificial neural networks, for example, are now being used in various commercial and industrial applications (see e.g., [Langley, 1992; Widrow et al., 1994]).\nThis is a preview of subscription content,[log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_1?error=cookies_not_supported&code=988ab554-48a8-4f69-be38-722cea986301)to check access.\n## Access this chapter\n[Log in via an institution](https://wayf.springernature.com/?redirect_uri&#x3D;https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_1?error=cookies_not_supported&code=988ab554-48a8-4f69-be38-722cea986301)\n[Institutional subscriptions](https://www.springernature.com/gp/librarians/licensing/agc/ebooks)\n## Preview\nUnable to display preview.[Download preview\nPDF.](https://page-one.springer.com/pdf/preview/10.1007/978-1-4615-5529-2_1)\nUnable to display preview.[Download preview\nPDF.](https://page-one.springer.com/pdf/preview/10.1007/978-1-4615-5529-2_1)\n### Similar content being viewed by others\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-981-19-2879-6?as&#x3D;webp)\n### [Machine Learning](https://link.springer.com/10.1007/978-981-19-2879-6_2?fromPaywallRec=true)\nChapter\u00a9 2023\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-981-97-7426-5?as&#x3D;webp)\n### [A Study of Machine Learning Dynamics: Algorithms, Applications, and Fundamental Frameworks](https://link.springer.com/10.1007/978-981-97-7426-5_4?fromPaywallRec=true)\nChapter\u00a9 2025\n![](https://media.springernature.com/w92h120/springer-static/cover-hires/book/978-981-97-1260-1?as&#x3D;webp)\n### [Machine Learning: Future Prospectus and Research Direction](https://link.springer.com/10.1007/978-981-97-1260-1_14?fromPaywallRec=true)\nChapter\u00a9 2024\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Learning algorithms](https://link.springer.com/subjects/learning-algorithms)\n* [Learning Process](https://link.springer.com/subjects/learning-process)\n* [Learning Theory](https://link.springer.com/subjects/learning-theory)\n* [Machine Learning](https://link.springer.com/subjects/machine-learning)\n* [Organizational Learning](https://link.springer.com/subjects/organizational-learning)\n* [Statistical Learning](https://link.springer.com/subjects/statistical-learning)\n## References\n* Y. S. Abu-Mostafa. A method for learning from hints. In S. J. Hanson, J. Cowan, and C. L. Giles, editors,*Advances in Neural Information Processing Systems 5*, pages 73\u201380, San Mateo, CA, 1993. Morgan Kaufmann.\n[Google Scholar]()\n* W.-K. Ahn and W. F. Brewer. Psychological studies of explanation-based learning. In G. DeJong, editor,*Investigating Explanation-Based Learning*. Kluwer Academic Publishers, Boston/ Dordrecht/London, 1993.\n[Google Scholar]()\n* W.-K. Ahn, R. Mooney, W. F. Brewer, and G. F. DeJong. Schema acquisition from one example: Psychological evidence for explanation-based learning. In*Proceedings of the Ninth Annual Conference of the Cognitive Science Society*, Seattle, WA, July 1987.\n[Google Scholar]()\n* C. A. Atkeson. Using locally weighted regression for robot learning. In*Proceedings of the 1991 IEEE International Conference on Robotics and Automation*, pages 958\u2013962, Sacramento, CA, April 1991.\n[Google Scholar]()\n* A. G. Barto, S. J. Bradtke, and S. P. Singh. Learning to act using real-time dynamic programming.*Artificial Intelligence*, 72:81\u2013138, 1995.\n[Article](https://doi.org/10.1016/0004-3702(94)00011-O)[Google Scholar]()\n* J. Baxter. The Canonical Distortion Measure for Vector Quantization and Function Approximation. Chapter 7 in this book.\n[Google Scholar]()\n* J. Baxter.*Learning Internal Representations*. PhD thesis, Flinders University, Australia, 1995.\n[Google Scholar]()\n* D. Beymer and T. Poggio. Face recognition from one model view. In*Proceedings of the International Conference on Computer Vision*, 1995.\n[Google Scholar]()\n* A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Occams razor.*Information Processing Letters*, 24:377\u2013380, 1987.\n[Article](https://doi.org/10.1016/0020-0190(87)90114-1)[MathSciNet](http://www.ams.org/mathscinet-getitem?mr=896392)[MATH](http://www.emis.de/MATH-item?0653.68084)[Google Scholar]()\n* C.E. Brodley.*Recursive Automatic Algorithm Selection for Inductive Learning*. PhD thesis, University of Massachusetts, Amherst, MA 01003, August 1994. also available as COINS Technical Report 94-61.\n[Google Scholar]()\n* R. Caruana. Multitask learning: A knowledge-based of source of inductive bias. In P. E. Utgoff, editor,*Proceedings of the Tenth International Conference on Machine Learning*, pages 41\u201348, San Mateo, CA, 1993. Morgan Kaufmann.\n[Google Scholar]()\n* R. Caruana. Algorithms and applications for multitask learning. In L. Saitta, editor,*Proceedings of the Thirteenth International Conference on Machine Learning*, San Mateo, CA, July 1996. Morgan Kaufmann.\n[Google Scholar]()\n* R. Caruana and S. Baluja. Using the future to\u2019 sort out\u2019 the present: Rankprop and multitask learning for medical risk evaluation. In D. Touretzky, M. Mozer, and M.E. Hasselmo, editors,*Advances in Neural Information Processing Systems 8*, Cambridge, MA, 1996. MIT Press. to appear.\n[Google Scholar]()\n* R. Caruana, D.L. Silver, J. Baxter, T.M. Mitchell, L.Y. Pratt, and Thrun. S. Workshop on \u201cLearning to learn: Knowledge consolidation and transfer in inductive systems\u201d. Workshop, held at NIPS-95, Vail, CO, see World Wide Web at[http://www.cs.cmu](http://www.cs.cmu.edu/afscs.cmu), December 1995.\n* N.L. Cramer. A representation for the adaptive generation of simple sequential programs. In J.J. Grefenstette, editor,*Proceedings of First International Conference on Genetic Algorithms and their Applications*, pages 183\u2013187, Pittsburgh, PA, 1985.\n[Google Scholar]()\n* P. Dayan and G. E. Hinton. Feudal reinforcement learning. In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors,*Advances in Neural Information Processing Systems 5*, San Mateo, CA, 1993. Morgan Kaufmann.\n[Google Scholar]()\n* L. DeRaedt, N. Lavra\u010d, and S. D\u017eeroski. Multiple predicate learning. In*Proceedings of IJCAI-93*, pages 1037\u20131042, Chamberry, France, July 1993. IJCAI, Inc.\n[Google Scholar]()\n* A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. A general lower bound on the number of examples needed for learning.*Information and Computation*, 82:247\u2013261, 1989.\n[Article](https://doi.org/10.1016/0890-5401(89)90002-3)[MathSciNet](http://www.ams.org/mathscinet-getitem?mr=1016683)[MATH](http://www.emis.de/MATH-item?0679.68158)[Google Scholar]()\n* R. Franke. Scattered data interpolation: Tests of some methods",
          "original_query": "Learning to learn: Introduction and overview",
          "cleaned_query": "Learning to learn: Introduction and overview"
        },
        {
          "success": true,
          "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep ...",
          "url": "https://arxiv.org/abs/1703.03400",
          "content": "[1703.03400] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1703.03400\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1703.03400**(cs)\n[Submitted on 9 Mar 2017 ([v1](https://arxiv.org/abs/1703.03400v1)), last revised 18 Jul 2017 (this version, v3)]\n# Title:Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nAuthors:[Chelsea Finn](https://arxiv.org/search/cs?searchtype=author&amp;query=Finn,+C),[Pieter Abbeel](https://arxiv.org/search/cs?searchtype=author&amp;query=Abbeel,+P),[Sergey Levine](https://arxiv.org/search/cs?searchtype=author&amp;query=Levine,+S)\nView a PDF of the paper titled Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks, by Chelsea Finn and 2 other authors\n[View PDF](https://arxiv.org/pdf/1703.03400)> > Abstract:\n> We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies. Comments:|ICML 2017. Code at[this https URL](https://github.com/cbfinn/maml), Videos of RL results at[this https URL](https://sites.google.com/view/maml), Blog post at[this http URL](http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/)|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE)|\nCite as:|[arXiv:1703.03400](https://arxiv.org/abs/1703.03400)[cs.LG]|\n|(or[arXiv:1703.03400v3](https://arxiv.org/abs/1703.03400v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1703.03400](https://doi.org/10.48550/arXiv.1703.03400)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Chelsea Finn [[view email](https://arxiv.org/show-email/578acec3/1703.03400)]\n**[[v1]](https://arxiv.org/abs/1703.03400v1)**Thu, 9 Mar 2017 18:58:03 UTC (5,061 KB)\n**[[v2]](https://arxiv.org/abs/1703.03400v2)**Tue, 9 May 2017 17:14:08 UTC (5,065 KB)\n**[v3]**Tue, 18 Jul 2017 16:45:29 UTC (5,063 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks, by Chelsea Finn and 2 other authors\n* [View PDF](https://arxiv.org/pdf/1703.03400)\n* [TeX Source](https://arxiv.org/src/1703.03400)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1703.03400&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1703.03400&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2017-03](https://arxiv.org/list/cs.LG/2017-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/1703.03400?context=cs)\n[cs.AI](https://arxiv.org/abs/1703.03400?context=cs.AI)\n[cs.CV](https://arxiv.org/abs/1703.03400?context=cs.CV)\n[cs.NE](https://arxiv.org/abs/1703.03400?context=cs.NE)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1703.03400)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1703.03400)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1703.03400)\n### [10 blog links](https://arxiv.org/tb/1703.03400)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1703.html#FinnAL17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/FinnAL17)\n[Chelsea Finn]()\n[Pieter Abbeel]()\n[Sergey Levine]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHav",
          "original_query": "Model-agnostic meta-learning for fast adaptation of deep networks",
          "cleaned_query": "Model-agnostic meta-learning for fast adaptation of deep networks"
        },
        {
          "success": true,
          "title": "A Simple Framework for Contrastive Learning of Visual ... - arXiv",
          "url": "https://arxiv.org/abs/2002.05709",
          "content": "[2002.05709] A Simple Framework for Contrastive Learning of Visual Representations\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2002.05709\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2002.05709**(cs)\n[Submitted on 13 Feb 2020 ([v1](https://arxiv.org/abs/2002.05709v1)), last revised 1 Jul 2020 (this version, v3)]\n# Title:A Simple Framework for Contrastive Learning of Visual Representations\nAuthors:[Ting Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+T),[Simon Kornblith](https://arxiv.org/search/cs?searchtype=author&amp;query=Kornblith,+S),[Mohammad Norouzi](https://arxiv.org/search/cs?searchtype=author&amp;query=Norouzi,+M),[Geoffrey Hinton](https://arxiv.org/search/cs?searchtype=author&amp;query=Hinton,+G)\nView a PDF of the paper titled A Simple Framework for Contrastive Learning of Visual Representations, by Ting Chen and 3 other authors\n[View PDF](https://arxiv.org/pdf/2002.05709)> > Abstract:\n> This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels. Comments:|ICML&#39;2020. Code and pretrained models at[this https URL](https://github.com/google-research/simclr)|\nSubjects:|Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)|\nCite as:|[arXiv:2002.05709](https://arxiv.org/abs/2002.05709)[cs.LG]|\n|(or[arXiv:2002.05709v3](https://arxiv.org/abs/2002.05709v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2002.05709](https://doi.org/10.48550/arXiv.2002.05709)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Ting Chen [[view email](https://arxiv.org/show-email/2290131d/2002.05709)]\n**[[v1]](https://arxiv.org/abs/2002.05709v1)**Thu, 13 Feb 2020 18:50:45 UTC (5,093 KB)\n**[[v2]](https://arxiv.org/abs/2002.05709v2)**Mon, 30 Mar 2020 15:32:51 UTC (5,047 KB)\n**[v3]**Wed, 1 Jul 2020 00:09:08 UTC (5,829 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled A Simple Framework for Contrastive Learning of Visual Representations, by Ting Chen and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2002.05709)\n* [TeX Source](https://arxiv.org/src/2002.05709)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2002.05709&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2002.05709&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2020-02](https://arxiv.org/list/cs.LG/2020-02)\nChange to browse by:\n[cs](https://arxiv.org/abs/2002.05709?context=cs)\n[cs.CV](https://arxiv.org/abs/2002.05709?context=cs.CV)\n[stat](https://arxiv.org/abs/2002.05709?context=stat)\n[stat.ML](https://arxiv.org/abs/2002.05709?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2002.05709)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2002.05709)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2002.05709)\n### [15 blog links](https://arxiv.org/tb/2002.05709)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2002.html#abs-2002-05709)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2002-05709)\n[Ting Chen]()\n[Simon Kornblith]()\n[Mohammad Norouzi]()\n[Geoffrey E. Hinton]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv featur",
          "original_query": "A simple framework for contrastive learning of visual representations",
          "cleaned_query": "A simple framework for contrastive learning of visual representations"
        },
        {
          "success": true,
          "title": "Function Contrastive Learning of Transferable Meta-Representations",
          "url": "https://arxiv.org/abs/2010.07093",
          "content": "[2010.07093] Function Contrastive Learning of Transferable Meta-Representations\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2010.07093\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2010.07093**(cs)\n[Submitted on 14 Oct 2020 ([v1](https://arxiv.org/abs/2010.07093v1)), last revised 22 Jul 2021 (this version, v3)]\n# Title:Function Contrastive Learning of Transferable Meta-Representations\nAuthors:[Muhammad Waleed Gondal](https://arxiv.org/search/cs?searchtype=author&amp;query=Gondal,+M+W),[Shruti Joshi](https://arxiv.org/search/cs?searchtype=author&amp;query=Joshi,+S),[Nasim Rahaman](https://arxiv.org/search/cs?searchtype=author&amp;query=Rahaman,+N),[Stefan Bauer](https://arxiv.org/search/cs?searchtype=author&amp;query=Bauer,+S),[Manuel W\u00fcthrich](https://arxiv.org/search/cs?searchtype=author&amp;query=W\u00fcthrich,+M),[Bernhard Sch\u00f6lkopf](https://arxiv.org/search/cs?searchtype=author&amp;query=Sch\u00f6lkopf,+B)\nView a PDF of the paper titled Function Contrastive Learning of Transferable Meta-Representations, by Muhammad Waleed Gondal and 5 other authors\n[View PDF](https://arxiv.org/pdf/2010.07093)> > Abstract:\n> Meta-learning algorithms adapt quickly to new tasks that are drawn from the same task distribution as the training tasks. The mechanism leading to fast adaptation is the conditioning of a downstream predictive model on the inferred representation of the task&#39;s underlying data generative process, or \\emph{function}. This \\emph{meta-representation}, which is computed from a few observed examples of the underlying function, is learned jointly with the predictive model. In this work, we study the implications of this joint training on the transferability of the meta-representations. Our goal is to learn meta-representations that are robust to noise in the data and facilitate solving a wide range of downstream tasks that share the same underlying functions. To this end, we propose a decoupled encoder-decoder approach to supervised meta-learning, where the encoder is trained with a contrastive objective to find a good representation of the underlying function. In particular, our training scheme is driven by the self-supervision signal indicating whether two sets of examples stem from the same function. Our experiments on a number of synthetic and real-world datasets show that the representations we obtain outperform strong baselines in terms of downstream performance and noise robustness, even when these baselines are trained in an end-to-end manner. Comments:|ICML 2021|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2010.07093](https://arxiv.org/abs/2010.07093)[cs.LG]|\n|(or[arXiv:2010.07093v3](https://arxiv.org/abs/2010.07093v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2010.07093](https://doi.org/10.48550/arXiv.2010.07093)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Muhammad Waleed Gondal [[view email](https://arxiv.org/show-email/88c113cc/2010.07093)]\n**[[v1]](https://arxiv.org/abs/2010.07093v1)**Wed, 14 Oct 2020 13:50:22 UTC (7,827 KB)\n**[[v2]](https://arxiv.org/abs/2010.07093v2)**Wed, 3 Mar 2021 19:35:16 UTC (4,838 KB)\n**[v3]**Thu, 22 Jul 2021 11:45:09 UTC (6,424 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Function Contrastive Learning of Transferable Meta-Representations, by Muhammad Waleed Gondal and 5 other authors\n* [View PDF](https://arxiv.org/pdf/2010.07093)\n* [TeX Source](https://arxiv.org/src/2010.07093)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2010.07093&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2010.07093&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2020-10](https://arxiv.org/list/cs.LG/2020-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/2010.07093?context=cs)\n[stat](https://arxiv.org/abs/2010.07093?context=stat)\n[stat.ML](https://arxiv.org/abs/2010.07093?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2010.07093)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2010.07093)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2010.07093)\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2010.html#abs-2010-07093)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2010-07093)\n[Muhammad Waleed Gondal]()\n[Shruti Joshi]()\n[Nasim Rahaman]()\n[Stefan Bauer]()\n[Manuel W\u00fcthrich]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly",
          "original_query": "Function contrastive learning of transferable meta-representations",
          "cleaned_query": "Function contrastive learning of transferable meta-representations"
        },
        {
          "success": true,
          "title": "Learning to Model the Tail - NIPS papers",
          "url": "https://papers.nips.cc/paper/7278-learning-to-model-the-tail",
          "content": "\n Learning to Model the Tail \n \nPart of\n Advances in Neural Information Processing Systems 30 (NIPS 2017)\n \n \n \n Authors \n Yu-Xiong Wang, Deva Ramanan, Martial Hebert \n Abstract \n We describe an approach to learning from long-tailed, imbalanced datasets that are prevalent in real-world settings. Here, the challenge is to learn accurate \"few-shot'' models for classes in the tail of the class distribution, for which little data is available. We cast this problem as transfer learning, where knowledge from the data-rich classes in the head of the distribution is transferred to the data-poor classes in the tail. Our key insights are as follows. First, we propose to transfer meta-knowledge about learning-to-learn from the head classes. This knowledge is encoded with a meta-network that operates on the space of model parameters, that is trained to predict many-shot model parameters from few-shot model parameters. Second, we transfer this meta-knowledge in a progressive manner, from classes in the head to the \"body'', and from the \"body'' to the tail. That is, we transfer knowledge in a gradual fashion, regularizing meta-networks for few-shot regression with those trained with more training data. This allows our final network to capture a notion of model dynamics, that predicts how model parameters are likely to change as more training data is gradually added. We demonstrate results on image classification datasets (SUN, Places, and ImageNet) tuned for the long-tailed setting, that significantly outperform common heuristics, such as data resampling or reweighting. \n \n Requests for name changes in the electronic proceedings will be accepted with no questions asked. However name changes may cause bibliographic tracking issues. Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings. \n Use the \"Report an Issue\" link to request a name change. \n Do not remove: This comment is monitored to verify that the site is working properly",
          "original_query": "Learning to model the tail",
          "cleaned_query": "Learning to model the tail"
        }
      ],
      "generated_ideas": [
        "Meta-Augmentation Policies for Few-Shot Task Adaptation\nTrain a MAML-style meta-learner where the \u201cinner-loop data augmentation composition\u201d (as in SimCLR) is itself meta-optimized per task to maximize post-adaptation generalization. Concretely, learn a small augmentation-policy network that outputs augmentation parameters for support examples, and update it through meta-gradients computed on the query loss.",
        "Function-Level Contrastive Pretraining for Gradient-Based Meta-Learning\nCombine Function Contrastive Learning (FCL) with MAML by first learning a task/function encoder via contrastive \u201csame-function vs different-function\u201d episodes, then conditioning the MAML initialization on this encoder (e.g., via FiLM or hypernetwork). Evaluate whether the resulting task-conditioned initialization reduces the number of inner-loop steps and improves robustness under noisy support sets.",
        "Long-Tail Meta-Networks with Self-Supervised Prototypes\nExtend \u201cLearning to Model the Tail\u201d by replacing few-shot class parameter estimates with SimCLR-pretrained embeddings aggregated into class prototypes, then meta-learn a parameter-prediction network from prototype statistics (mean, covariance, sample count). This makes the head-to-tail transfer less sensitive to label noise and improves tail performance when only a handful of labeled examples exist.",
        "Progressive Head\u2192Body\u2192Tail Transfer with Task-Contrastive Regularization\nModify the progressive meta-network transfer scheme for long-tailed learning by adding an FCL-style contrastive objective across classes: encourage predicted parameters for classes with similar \u201cdata-generating functions\u201d to be close, and dissimilar ones to be separated. Implement similarity using unlabeled feature distributions (from SimCLR embeddings) and test whether it reduces overfitting for extreme-tail classes.",
        "Meta-Learned Inner-Loop Optimizers Guided by Contrastive Geometry\nBuild a meta-optimizer (learning-to-learn) that adjusts inner-loop learning rates and update directions based on the local contrastive representation geometry (e.g., alignment/uniformity metrics from SimCLR embeddings). The contribution is an actionable rule: use representation-space statistics computed on the support set to modulate gradient steps, improving stability in few-shot and long-tail regimes.",
        "Noise-Aware Meta-Representation Learning via Augmentation Consistency\nExtend FCL to explicitly handle corrupted labels or input noise by generating multiple augmented \u201cviews\u201d of each support set and enforcing that the inferred function embedding is invariant across these views. Train the encoder with a joint objective: function-level contrastive loss + invariance regularizer, then plug into a downstream meta-learner to quantify robustness gains under controlled noise.",
        "Task Retrieval for Fast Adaptation Using Function Embeddings\nUse FCL-learned function embeddings as keys in a memory of past tasks; at meta-test time, retrieve nearest tasks and initialize adaptation from a weighted combination of their parameters (or hypernetwork states). This yields a concrete hybrid of metric-based transfer and MAML-style fine-tuning, aimed at reducing compute and improving adaptation when tasks are heterogeneous.",
        "Parameter-Trajectory Prediction for Few-Shot to Many-Shot Scaling\nGeneralize \u201cLearning to Model the Tail\u201d by predicting not just final many-shot parameters, but an entire parameter trajectory as a function of sample count (a learned dynamics model over parameters). Train on head classes with varying subsample sizes; at tail time, use the predicted trajectory to decide how to allocate additional labeling budget (active acquisition) for maximal expected improvement.",
        "Meta-Learning Contrastive Temperature and Projection Heads per Task\nInspired by SimCLR\u2019s learnable projection head and sensitivity to temperature, meta-learn these contrastive components across tasks so that each new task can quickly calibrate its contrastive objective using a few examples. The actionable outcome is a task-adaptive self-supervised module that improves representation quality for low-label adaptation, especially when the target domain differs from meta-training.",
        "Unified Benchmark and Protocol for Function-Shifted Long-Tail Meta-Learning\nCreate a benchmark where long-tailed class frequency imbalance is coupled with controlled \u201cfunction shifts\u201d (e.g., style/physics/causal mechanism changes) to test whether meta-representations truly capture underlying functions as claimed by FCL. Provide standardized episodes that evaluate (i) few-shot adaptation (MAML), (ii) parameter-prediction for tail classes, and (iii) robustness to noise\u2014enabling principled ablations across these methods."
      ],
      "generated_ideas_raw": "1. **Meta-Augmentation Policies for Few-Shot Task Adaptation**\n   Train a MAML-style meta-learner where the \u201cinner-loop data augmentation composition\u201d (as in SimCLR) is itself meta-optimized per task to maximize post-adaptation generalization. Concretely, learn a small augmentation-policy network that outputs augmentation parameters for support examples, and update it through meta-gradients computed on the query loss.\n\n2. **Function-Level Contrastive Pretraining for Gradient-Based Meta-Learning**\n   Combine Function Contrastive Learning (FCL) with MAML by first learning a task/function encoder via contrastive \u201csame-function vs different-function\u201d episodes, then conditioning the MAML initialization on this encoder (e.g., via FiLM or hypernetwork). Evaluate whether the resulting task-conditioned initialization reduces the number of inner-loop steps and improves robustness under noisy support sets.\n\n3. **Long-Tail Meta-Networks with Self-Supervised Prototypes**\n   Extend \u201cLearning to Model the Tail\u201d by replacing few-shot class parameter estimates with SimCLR-pretrained embeddings aggregated into class prototypes, then meta-learn a parameter-prediction network from prototype statistics (mean, covariance, sample count). This makes the head-to-tail transfer less sensitive to label noise and improves tail performance when only a handful of labeled examples exist.\n\n4. **Progressive Head\u2192Body\u2192Tail Transfer with Task-Contrastive Regularization**\n   Modify the progressive meta-network transfer scheme for long-tailed learning by adding an FCL-style contrastive objective across classes: encourage predicted parameters for classes with similar \u201cdata-generating functions\u201d to be close, and dissimilar ones to be separated. Implement similarity using unlabeled feature distributions (from SimCLR embeddings) and test whether it reduces overfitting for extreme-tail classes.\n\n5. **Meta-Learned Inner-Loop Optimizers Guided by Contrastive Geometry**\n   Build a meta-optimizer (learning-to-learn) that adjusts inner-loop learning rates and update directions based on the local contrastive representation geometry (e.g., alignment/uniformity metrics from SimCLR embeddings). The contribution is an actionable rule: use representation-space statistics computed on the support set to modulate gradient steps, improving stability in few-shot and long-tail regimes.\n\n6. **Noise-Aware Meta-Representation Learning via Augmentation Consistency**\n   Extend FCL to explicitly handle corrupted labels or input noise by generating multiple augmented \u201cviews\u201d of each support set and enforcing that the inferred function embedding is invariant across these views. Train the encoder with a joint objective: function-level contrastive loss + invariance regularizer, then plug into a downstream meta-learner to quantify robustness gains under controlled noise.\n\n7. **Task Retrieval for Fast Adaptation Using Function Embeddings**\n   Use FCL-learned function embeddings as keys in a memory of past tasks; at meta-test time, retrieve nearest tasks and initialize adaptation from a weighted combination of their parameters (or hypernetwork states). This yields a concrete hybrid of metric-based transfer and MAML-style fine-tuning, aimed at reducing compute and improving adaptation when tasks are heterogeneous.\n\n8. **Parameter-Trajectory Prediction for Few-Shot to Many-Shot Scaling**\n   Generalize \u201cLearning to Model the Tail\u201d by predicting not just final many-shot parameters, but an entire parameter trajectory as a function of sample count (a learned dynamics model over parameters). Train on head classes with varying subsample sizes; at tail time, use the predicted trajectory to decide how to allocate additional labeling budget (active acquisition) for maximal expected improvement.\n\n9. **Meta-Learning Contrastive Temperature and Projection Heads per Task**\n   Inspired by SimCLR\u2019s learnable projection head and sensitivity to temperature, meta-learn these contrastive components across tasks so that each new task can quickly calibrate its contrastive objective using a few examples. The actionable outcome is a task-adaptive self-supervised module that improves representation quality for low-label adaptation, especially when the target domain differs from meta-training.\n\n10. **Unified Benchmark and Protocol for Function-Shifted Long-Tail Meta-Learning**\n   Create a benchmark where long-tailed class frequency imbalance is coupled with controlled \u201cfunction shifts\u201d (e.g., style/physics/causal mechanism changes) to test whether meta-representations truly capture underlying functions as claimed by FCL. Provide standardized episodes that evaluate (i) few-shot adaptation (MAML), (ii) parameter-prediction for tail classes, and (iii) robustness to noise\u2014enabling principled ablations across these methods.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Meta-Augmentation Policies for Few-Shot Task Adaptation\nTrain a MAML-style meta-learner where the \u201cinner-loop data augmentation composition\u201d (as in SimCLR) is itself meta-optimized per task to maximiz",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Function-Level Contrastive Pretraining for Gradient-Based Meta-Learning\nCombine Function Contrastive Learning (FCL) with MAML by first learning a task/function encoder via contrastive \u201csame-function v",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "Long-Tail Meta-Networks with Self-Supervised Prototypes\nExtend \u201cLearning to Model the Tail\u201d by replacing few-shot class parameter estimates with SimCLR-pretrained embeddings aggregated into class prot",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Progressive Head\u2192Body\u2192Tail Transfer with Task-Contrastive Regularization\nModify the progressive meta-network transfer scheme for long-tailed learning by adding an FCL-style contrastive objective acros",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Meta-Learned Inner-Loop Optimizers Guided by Contrastive Geometry\nBuild a meta-optimizer (learning-to-learn) that adjusts inner-loop learning rates and update directions based on the local contrastive",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Noise-Aware Meta-Representation Learning via Augmentation Consistency\nExtend FCL to explicitly handle corrupted labels or input noise by generating multiple augmented \u201cviews\u201d of each support set and e",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Task Retrieval for Fast Adaptation Using Function Embeddings\nUse FCL-learned function embeddings as keys in a memory of past tasks; at meta-test time, retrieve nearest tasks and initialize adaptation ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Parameter-Trajectory Prediction for Few-Shot to Many-Shot Scaling\nGeneralize \u201cLearning to Model the Tail\u201d by predicting not just final many-shot parameters, but an entire parameter trajectory as a fun",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Meta-Learning Contrastive Temperature and Projection Heads per Task\nInspired by SimCLR\u2019s learnable projection head and sensitivity to temperature, meta-learn these contrastive components across tasks ",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "Unified Benchmark and Protocol for Function-Shifted Long-Tail Meta-Learning\nCreate a benchmark where long-tailed class frequency imbalance is coupled with controlled \u201cfunction shifts\u201d (e.g., style/phy",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 63,
      "paper_title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
      "contribution": "KVzip introduces a query-agnostic KV cache eviction method that enables the reuse of compressed KV caches across diverse queries, significantly reducing memory overhead and attention latency.",
      "num_predecessors": 3,
      "predecessors_crawled": 3,
      "academic_sources": 3,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 6084,
      "output_tokens": 969,
      "predecessor_details": [
        {
          "success": true,
          "title": "[1706.03762] Attention Is All You Need - arXiv",
          "url": "https://arxiv.org/abs/1706.03762",
          "content": "[1706.03762] Attention Is All You Need[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1706.03762\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:1706.03762**(cs)\n[Submitted on 12 Jun 2017 ([v1](https://arxiv.org/abs/1706.03762v1)), last revised 2 Aug 2023 (this version, v7)]\n# Title:Attention Is All You Need\nAuthors:[Ashish Vaswani](https://arxiv.org/search/cs?searchtype=author&amp;query=Vaswani,+A),[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&amp;query=Shazeer,+N),[Niki Parmar](https://arxiv.org/search/cs?searchtype=author&amp;query=Parmar,+N),[Jakob Uszkoreit](https://arxiv.org/search/cs?searchtype=author&amp;query=Uszkoreit,+J),[Llion Jones](https://arxiv.org/search/cs?searchtype=author&amp;query=Jones,+L),[Aidan N. Gomez](https://arxiv.org/search/cs?searchtype=author&amp;query=Gomez,+A+N),[Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaiser,+L),[Illia Polosukhin](https://arxiv.org/search/cs?searchtype=author&amp;query=Polosukhin,+I)\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n[View PDF](https://arxiv.org/pdf/1706.03762)[HTML (experimental)](https://arxiv.org/html/1706.03762v7)> > Abstract:\n> The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Comments:|15 pages, 5 figures|\nSubjects:|Computation and Language (cs.CL); Machine Learning (cs.LG)|\nCite as:|[arXiv:1706.03762](https://arxiv.org/abs/1706.03762)[cs.CL]|\n|(or[arXiv:1706.03762v7](https://arxiv.org/abs/1706.03762v7)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Llion Jones [[view email](https://arxiv.org/show-email/f53b7360/1706.03762)]\n**[[v1]](https://arxiv.org/abs/1706.03762v1)**Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB)\n**[[v2]](https://arxiv.org/abs/1706.03762v2)**Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB)\n**[[v3]](https://arxiv.org/abs/1706.03762v3)**Tue, 20 Jun 2017 05:20:02 UTC (1,125 KB)\n**[[v4]](https://arxiv.org/abs/1706.03762v4)**Fri, 30 Jun 2017 17:29:30 UTC (1,124 KB)\n**[[v5]](https://arxiv.org/abs/1706.03762v5)**Wed, 6 Dec 2017 03:30:32 UTC (1,124 KB)\n**[[v6]](https://arxiv.org/abs/1706.03762v6)**Mon, 24 Jul 2023 00:48:54 UTC (1,124 KB)\n**[v7]**Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n* [View PDF](https://arxiv.org/pdf/1706.03762)\n* [HTML (experimental)](https://arxiv.org/html/1706.03762v7)\n* [TeX Source](https://arxiv.org/src/1706.03762)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1706.03762&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1706.03762&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2017-06](https://arxiv.org/list/cs.CL/2017-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1706.03762?context=cs)\n[cs.LG](https://arxiv.org/abs/1706.03762?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1706.03762)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1706.03762)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1706.03762)\n### [123 blog links](https://arxiv.org/tb/1706.03762)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1706.html#VaswaniSPUJGKP17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/VaswaniSPUJGKP17)\n[Ashish Vaswani]()\n[Noam Shazeer]()\n[Niki Parmar]()\n[Jakob Uszkoreit]()\n[Llion Jones]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces",
          "original_query": "Attention is all you need",
          "cleaned_query": "Attention is all you need"
        },
        {
          "success": true,
          "title": "Dynamic Context Pruning for Efficient and Interpretable ... - arXiv",
          "url": "https://arxiv.org/abs/2305.15805",
          "content": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
          "original_query": "Dynamic context pruning for efficient and interpretable autoregressive transformers",
          "cleaned_query": "Dynamic context pruning for efficient and interpretable autoregressive transformers"
        },
        {
          "success": true,
          "title": "SnapKV: LLM Knows What You are Looking for Before Generation",
          "url": "https://arxiv.org/abs/2404.14469",
          "content": "[2404.14469] SnapKV: LLM Knows What You are Looking for Before Generation\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2404.14469\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:2404.14469**(cs)\n[Submitted on 22 Apr 2024 ([v1](https://arxiv.org/abs/2404.14469v1)), last revised 17 Jun 2024 (this version, v2)]\n# Title:SnapKV: LLM Knows What You are Looking for Before Generation\nAuthors:[Yuhong Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y),[Yingbing Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+Y),[Bowen Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+B),[Bharat Venkitesh](https://arxiv.org/search/cs?searchtype=author&amp;query=Venkitesh,+B),[Acyr Locatelli](https://arxiv.org/search/cs?searchtype=author&amp;query=Locatelli,+A),[Hanchen Ye](https://arxiv.org/search/cs?searchtype=author&amp;query=Ye,+H),[Tianle Cai](https://arxiv.org/search/cs?searchtype=author&amp;query=Cai,+T),[Patrick Lewis](https://arxiv.org/search/cs?searchtype=author&amp;query=Lewis,+P),[Deming Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+D)\nView a PDF of the paper titled SnapKV: LLM Knows What You are Looking for Before Generation, by Yuhong Li and 8 other authors\n[View PDF](https://arxiv.org/pdf/2404.14469)[HTML (experimental)](https://arxiv.org/html/2404.14469v2)> > Abstract:\n> Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications.\n> We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an &#39;observation&#39; window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV&#39;s potential for practical applications. Subjects:|Computation and Language (cs.CL); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2404.14469](https://arxiv.org/abs/2404.14469)[cs.CL]|\n|(or[arXiv:2404.14469v2](https://arxiv.org/abs/2404.14469v2)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.2404.14469](https://doi.org/10.48550/arXiv.2404.14469)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Yuhong Li [[view email](https://arxiv.org/show-email/3a574cea/2404.14469)]\n**[[v1]](https://arxiv.org/abs/2404.14469v1)**Mon, 22 Apr 2024 17:42:58 UTC (512 KB)\n**[v2]**Mon, 17 Jun 2024 03:01:58 UTC (633 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled SnapKV: LLM Knows What You are Looking for Before Generation, by Yuhong Li and 8 other authors\n* [View PDF](https://arxiv.org/pdf/2404.14469)\n* [HTML (experimental)](https://arxiv.org/html/2404.14469v2)\n* [TeX Source](https://arxiv.org/src/2404.14469)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2404.14469&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2404.14469&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2024-04](https://arxiv.org/list/cs.CL/2024-04)\nChange to browse by:\n[cs](https://arxiv.org/abs/2404.14469?context=cs)\n[cs.AI](https://arxiv.org/abs/2404.14469?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2404.14469)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2404.14469)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2404.14469)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and sha",
          "original_query": "SnapKV: LLM knows what you are looking for before generation",
          "cleaned_query": "SnapKV: LLM knows what you are looking for before generation"
        }
      ],
      "generated_ideas": [
        "Query-Conditioned SnapKV for Task-Adaptive KV Compression\nExtend SnapKV by conditioning the per-head KV selection on an embedding of the user query/task (e.g., instruction type, expected output format) rather than relying only on an \u201cobservation window\u201d at the prompt end. Implement a lightweight gating network that predicts which KV clusters to retain per head given the query, and evaluate on heterogeneous long-context suites where relevance patterns shift across tasks.",
        "Streaming SnapKV: Online KV Retention for Infinite-Scroll Contexts\nAdapt SnapKV to a streaming setting where tokens arrive continuously and the model must decide in real time whether to keep or evict KV entries. Introduce an online clustering/update rule for head-specific \u201cimportant KV prototypes\u201d that can evolve as the topic drifts, and benchmark on streaming QA/summarization with hard memory budgets.",
        "Layer-Wise Importance Routing: Assigning Long-Range vs. Local Memory to Specific Transformer Layers\nBuild on the Transformer\u2019s multi-layer attention to explicitly allocate different retention policies by layer (e.g., early layers keep local syntax, later layers keep sparse global facts) using SnapKV-style head patterns. The key contribution is a layer-wise routing policy that reduces KV growth more aggressively in layers empirically less useful for long-range retrieval, while preserving layers critical for \u201cneedle\u201d recovery.",
        "Robust SnapKV Under Adversarial and Distractor-Rich Prompts\nStress-test and harden KV compression by constructing adversarial long-context prompts where distractor segments mimic attention features in the observation window. Propose a defense that uses consistency checks across multiple observation windows (e.g., end + mid-prompt) or uncertainty-aware selection to avoid dropping truly relevant keys, and report robustness/latency trade-offs.",
        "Cross-Head Redundancy Elimination via Shared KV Dictionaries\nSnapKV selects important positions per head; extend this by learning (or inferring) a shared dictionary of KV \u201cmemory slots\u201d across heads to eliminate redundant retention. Implement a merge-and-deduplicate step that identifies heads attending to equivalent KV clusters and stores a single shared representation, reducing memory without requiring fine-tuning.",
        "Explainable Long-Context Attribution from SnapKV Cluster Selections\nTurn SnapKV\u2019s clustered important KV positions into an explanation interface: output the minimal set of prompt spans \u201ckept\u201d per head/layer that most influenced generation. Contribute evaluation protocols that compare these explanations against human-annotated rationales (where available) and against faithfulness metrics (e.g., deletion tests) while maintaining the same runtime gains.",
        "Training-Time Regularization to Encourage Compressible Attention Patterns\nAlthough SnapKV is fine-tuning-free, develop a training regularizer that nudges attention heads to form stable, clusterable patterns so SnapKV-like compression becomes even more effective. For example, add a penalty encouraging low-entropy distributions over a small set of KV clusters per head during long-context pretraining, then measure improved compression ratios at equal accuracy.",
        "Hybrid Retrieval-and-Cache Management: Let RAG Fill What Pruning Drops\nIntegrate dynamic context pruning with an external retrieval mechanism that can re-inject dropped information on demand during decoding. The contribution is a controller that decides whether to (a) keep tokens in-context, (b) keep only compressed KV, or (c) drop and rely on retrieval later\u2014optimized for latency/memory under long-context knowledge-intensive tasks.",
        "Budget-Aware Multi-Objective Scheduler for Prefill vs. Decode Optimization\nCreate a scheduler that chooses pruning aggressiveness (prompt token removal) and SnapKV compression rate per head based on a target budget (GPU memory cap, latency SLA) and predicted difficulty (e.g., expected number of \u201cneedle\u201d queries). Contribute a practical algorithm that estimates marginal utility of additional KV slots vs. additional prompt tokens and automatically tunes policies per input, demonstrating stable QoS across variable context lengths."
      ],
      "generated_ideas_raw": "1. **Query-Conditioned SnapKV for Task-Adaptive KV Compression**  \n   Extend SnapKV by conditioning the per-head KV selection on an embedding of the user query/task (e.g., instruction type, expected output format) rather than relying only on an \u201cobservation window\u201d at the prompt end. Implement a lightweight gating network that predicts which KV clusters to retain per head given the query, and evaluate on heterogeneous long-context suites where relevance patterns shift across tasks.\n\n2. **Dynamic Context Pruning + SnapKV Co-Design for End-to-End Long-Context Efficiency**  \n   Combine token-level context pruning (dropping irrelevant prompt tokens prefill) with head-wise KV cache compression (during decode) into a single pipeline that optimizes both prefill FLOPs and decode memory. Contribute a unified objective that trades off retention vs. latency (prefill+decode) and a calibration procedure that keeps accuracy stable across context lengths (16K\u2192256K).\n\n3. **Streaming SnapKV: Online KV Retention for Infinite-Scroll Contexts**  \n   Adapt SnapKV to a streaming setting where tokens arrive continuously and the model must decide in real time whether to keep or evict KV entries. Introduce an online clustering/update rule for head-specific \u201cimportant KV prototypes\u201d that can evolve as the topic drifts, and benchmark on streaming QA/summarization with hard memory budgets.\n\n4. **Layer-Wise Importance Routing: Assigning Long-Range vs. Local Memory to Specific Transformer Layers**  \n   Build on the Transformer\u2019s multi-layer attention to explicitly allocate different retention policies by layer (e.g., early layers keep local syntax, later layers keep sparse global facts) using SnapKV-style head patterns. The key contribution is a layer-wise routing policy that reduces KV growth more aggressively in layers empirically less useful for long-range retrieval, while preserving layers critical for \u201cneedle\u201d recovery.\n\n5. **Robust SnapKV Under Adversarial and Distractor-Rich Prompts**  \n   Stress-test and harden KV compression by constructing adversarial long-context prompts where distractor segments mimic attention features in the observation window. Propose a defense that uses consistency checks across multiple observation windows (e.g., end + mid-prompt) or uncertainty-aware selection to avoid dropping truly relevant keys, and report robustness/latency trade-offs.\n\n6. **Cross-Head Redundancy Elimination via Shared KV Dictionaries**  \n   SnapKV selects important positions per head; extend this by learning (or inferring) a shared dictionary of KV \u201cmemory slots\u201d across heads to eliminate redundant retention. Implement a merge-and-deduplicate step that identifies heads attending to equivalent KV clusters and stores a single shared representation, reducing memory without requiring fine-tuning.\n\n7. **Explainable Long-Context Attribution from SnapKV Cluster Selections**  \n   Turn SnapKV\u2019s clustered important KV positions into an explanation interface: output the minimal set of prompt spans \u201ckept\u201d per head/layer that most influenced generation. Contribute evaluation protocols that compare these explanations against human-annotated rationales (where available) and against faithfulness metrics (e.g., deletion tests) while maintaining the same runtime gains.\n\n8. **Training-Time Regularization to Encourage Compressible Attention Patterns**  \n   Although SnapKV is fine-tuning-free, develop a training regularizer that nudges attention heads to form stable, clusterable patterns so SnapKV-like compression becomes even more effective. For example, add a penalty encouraging low-entropy distributions over a small set of KV clusters per head during long-context pretraining, then measure improved compression ratios at equal accuracy.\n\n9. **Hybrid Retrieval-and-Cache Management: Let RAG Fill What Pruning Drops**  \n   Integrate dynamic context pruning with an external retrieval mechanism that can re-inject dropped information on demand during decoding. The contribution is a controller that decides whether to (a) keep tokens in-context, (b) keep only compressed KV, or (c) drop and rely on retrieval later\u2014optimized for latency/memory under long-context knowledge-intensive tasks.\n\n10. **Budget-Aware Multi-Objective Scheduler for Prefill vs. Decode Optimization**  \n   Create a scheduler that chooses pruning aggressiveness (prompt token removal) and SnapKV compression rate per head based on a target budget (GPU memory cap, latency SLA) and predicted difficulty (e.g., expected number of \u201cneedle\u201d queries). Contribute a practical algorithm that estimates marginal utility of additional KV slots vs. additional prompt tokens and automatically tunes policies per input, demonstrating stable QoS across variable context lengths.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Query-Conditioned SnapKV for Task-Adaptive KV Compression\nExtend SnapKV by conditioning the per-head KV selection on an embedding of the user query/task (e.g., instruction type, expected output format",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Streaming SnapKV: Online KV Retention for Infinite-Scroll Contexts\nAdapt SnapKV to a streaming setting where tokens arrive continuously and the model must decide in real time whether to keep or evict ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Layer-Wise Importance Routing: Assigning Long-Range vs. Local Memory to Specific Transformer Layers\nBuild on the Transformer\u2019s multi-layer attention to explicitly allocate different retention policies",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Robust SnapKV Under Adversarial and Distractor-Rich Prompts\nStress-test and harden KV compression by constructing adversarial long-context prompts where distractor segments mimic attention features in",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Cross-Head Redundancy Elimination via Shared KV Dictionaries\nSnapKV selects important positions per head; extend this by learning (or inferring) a shared dictionary of KV \u201cmemory slots\u201d across heads t",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Explainable Long-Context Attribution from SnapKV Cluster Selections\nTurn SnapKV\u2019s clustered important KV positions into an explanation interface: output the minimal set of prompt spans \u201ckept\u201d per head",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Training-Time Regularization to Encourage Compressible Attention Patterns\nAlthough SnapKV is fine-tuning-free, develop a training regularizer that nudges attention heads to form stable, clusterable pa",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Hybrid Retrieval-and-Cache Management: Let RAG Fill What Pruning Drops\nIntegrate dynamic context pruning with an external retrieval mechanism that can re-inject dropped information on demand during de",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Budget-Aware Multi-Objective Scheduler for Prefill vs. Decode Optimization\nCreate a scheduler that chooses pruning aggressiveness (prompt token removal) and SnapKV compression rate per head based on a",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 64,
      "paper_title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models",
      "contribution": "HyperET introduces a paradigm for effectively training multi-modal large language models in hyperbolic space to align visual and textual representations across varying levels of granularity with improved efficiency.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 5,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 8369,
      "output_tokens": 1015,
      "predecessor_details": [
        {
          "success": true,
          "title": "Learning Transferable Visual Models From Natural ...",
          "url": "https://arxiv.org/abs/2103.00020",
          "content": "[2103.00020] Learning Transferable Visual Models From Natural Language Supervision\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2103.00020\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2103.00020**(cs)\n[Submitted on 26 Feb 2021]\n# Title:Learning Transferable Visual Models From Natural Language Supervision\nAuthors:[Alec Radford](https://arxiv.org/search/cs?searchtype=author&amp;query=Radford,+A),[Jong Wook Kim](https://arxiv.org/search/cs?searchtype=author&amp;query=Kim,+J+W),[Chris Hallacy](https://arxiv.org/search/cs?searchtype=author&amp;query=Hallacy,+C),[Aditya Ramesh](https://arxiv.org/search/cs?searchtype=author&amp;query=Ramesh,+A),[Gabriel Goh](https://arxiv.org/search/cs?searchtype=author&amp;query=Goh,+G),[Sandhini Agarwal](https://arxiv.org/search/cs?searchtype=author&amp;query=Agarwal,+S),[Girish Sastry](https://arxiv.org/search/cs?searchtype=author&amp;query=Sastry,+G),[Amanda Askell](https://arxiv.org/search/cs?searchtype=author&amp;query=Askell,+A),[Pamela Mishkin](https://arxiv.org/search/cs?searchtype=author&amp;query=Mishkin,+P),[Jack Clark](https://arxiv.org/search/cs?searchtype=author&amp;query=Clark,+J),[Gretchen Krueger](https://arxiv.org/search/cs?searchtype=author&amp;query=Krueger,+G),[Ilya Sutskever](https://arxiv.org/search/cs?searchtype=author&amp;query=Sutskever,+I)\nView a PDF of the paper titled Learning Transferable Visual Models From Natural Language Supervision, by Alec Radford and 11 other authors\n[View PDF](https://arxiv.org/pdf/2103.00020)> > Abstract:\n> State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at [> this https URL\n](https://github.com/OpenAI/CLIP)> . Subjects:|Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)|\nCite as:|[arXiv:2103.00020](https://arxiv.org/abs/2103.00020)[cs.CV]|\n|(or[arXiv:2103.00020v1](https://arxiv.org/abs/2103.00020v1)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2103.00020](https://doi.org/10.48550/arXiv.2103.00020)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jong Wook Kim [[view email](https://arxiv.org/show-email/6c157f7d/2103.00020)]\n**[v1]**Fri, 26 Feb 2021 19:04:58 UTC (6,174 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Learning Transferable Visual Models From Natural Language Supervision, by Alec Radford and 11 other authors\n* [View PDF](https://arxiv.org/pdf/2103.00020)\n* [TeX Source](https://arxiv.org/src/2103.00020)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2103.00020&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2103.00020&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2021-03](https://arxiv.org/list/cs.CV/2021-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/2103.00020?context=cs)\n[cs.LG](https://arxiv.org/abs/2103.00020?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2103.00020)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2103.00020)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2103.00020)\n### [16 blog links](https://arxiv.org/tb/2103.00020)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2103.html#abs-2103-00020)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2103-00020)\n[Alec Radford]()\n[Jong Wook Kim]()\n[Aditya Ramesh]()\n[Gabriel Goh]()\n[Girish Sastry]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommende",
          "original_query": "Learning transferable visual models from natural language supervision",
          "cleaned_query": "Learning transferable visual models from natural language supervision"
        },
        {
          "success": true,
          "title": "[2408.00714] SAM 2: Segment Anything in Images and Videos - arXiv",
          "url": "https://arxiv.org/abs/2408.00714",
          "content": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
          "original_query": "Segment anything",
          "cleaned_query": "Segment anything"
        },
        {
          "success": true,
          "title": "Hyperbolic Contrastive Learning for Visual Representations beyond ...",
          "url": "https://arxiv.org/abs/2212.00653",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2212.00653** (cs)\n\n\\[Submitted on 1 Dec 2022\\]\n\n# Title:Hyperbolic Contrastive Learning for Visual Representations beyond Objects\n\nAuthors: [Songwei Ge](https://arxiv.org/search/cs?searchtype=author&query=Ge,+S), [Shlok Mishra](https://arxiv.org/search/cs?searchtype=author&query=Mishra,+S), [Simon Kornblith](https://arxiv.org/search/cs?searchtype=author&query=Kornblith,+S), [Chun-Liang Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+C), [David Jacobs](https://arxiv.org/search/cs?searchtype=author&query=Jacobs,+D)\n\nView a PDF of the paper titled Hyperbolic Contrastive Learning for Visual Representations beyond Objects, by Songwei Ge and 4 other authors\n\n[View PDF](https://arxiv.org/pdf/2212.00653)\n\n> Abstract:Although self-/un-supervised methods have led to rapid progress in visual representation learning, these methods generally treat objects and scenes using the same lens. In this paper, we focus on learning representations for objects and scenes that preserve the structure among them.\n>\n> Motivated by the observation that visually similar objects are close in the representation space, we argue that the scenes and objects should instead follow a hierarchical structure based on their compositionality. To exploit such a structure, we propose a contrastive learning framework where a Euclidean loss is used to learn object representations and a hyperbolic loss is used to encourage representations of scenes to lie close to representations of their constituent objects in a hyperbolic space. This novel hyperbolic objective encourages the scene-object hypernymy among the representations by optimizing the magnitude of their norms. We show that when pretraining on the COCO and OpenImages datasets, the hyperbolic loss improves downstream performance of several baselines across multiple datasets and tasks, including image classification, object detection, and semantic segmentation. We also show that the properties of the learned representations allow us to solve various vision tasks that involve the interaction between scenes and objects in a zero-shot fashion. Our code can be found at \\\\url{ [this https URL](https://github.com/shlokk/HCL/tree/main/HCL)}.\n\n| | |\n| --- | --- |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2212.00653](https://arxiv.org/abs/2212.00653) \\[cs.CV\\] |\n| | (or [arXiv:2212.00653v1](https://arxiv.org/abs/2212.00653v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2212.00653](https://doi.org/10.48550/arXiv.2212.00653) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Shlok Mishra \\[ [view email](https://arxiv.org/show-email/ad5e6d46/2212.00653)\\]\n\n**\\[v1\\]**\nThu, 1 Dec 2022 16:58:57 UTC (3,818 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Hyperbolic Contrastive Learning for Visual Representations beyond Objects, by Songwei Ge and 4 other authors\n\n- [View PDF](https://arxiv.org/pdf/2212.00653)\n- [TeX Source](https://arxiv.org/src/2212.00653)\n- [Other Formats](https://arxiv.org/format/2212.00653)\n\n[![license icon](https://arxiv.org/icons/licenses/zero-1.0.png)view license](http://creativecommons.org/publicdomain/zero/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2212.00653&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2212.00653&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2022-12](https://arxiv.org/list/cs.CV/2022-12)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2212.00653?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2212.00653?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2212.00653)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2212.00653)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2212.00653)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2212.00653&description=Hyperbolic Contrastive Learning for Visual Representations beyond Objects) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2212.00653&title=Hyperbolic Contrastive Learning for Visual Representations beyond Objects)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2212.00653) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Hyperbolic contrastive learning for visual representations beyond objects",
          "cleaned_query": "Hyperbolic contrastive learning for visual representations beyond objects"
        },
        {
          "success": true,
          "title": "Hyperbolic trigonometry and its application in the Poincar\u00e9 ...",
          "url": "https://www.academia.edu/53029883/Hyperbolic_trigonometry_and_its_application_in_the_Poincar%C3%A9_ball_model_of_hyperbolic_geometry",
          "content": "(PDF) Hyperbolic trigonometry and its application in the Poincar\u00e9 ball model of hyperbolic geometry\nAcademia.edu no longer supports Internet Explorer.\nTo browse Academia.edu and the wider internet faster and more securely, please take a few seconds to[upgrade your browser](https://www.academia.edu/upgrade-browser).\n[![Academia.edu](https://a.academia-assets.com/images/academia-logo-redesign-2015-A.svg)![Academia.edu](https://a.academia-assets.com/images/academia-logo-redesign-2015.svg)](https://www.academia.edu/)\n* [Log In](https://www.academia.edu/login)\n* [Sign Up](https://www.academia.edu/signup)\n* more\n* * [About](https://www.academia.edu/about)\n* [Press](https://www.academia.edu/press)\n* [Papers](https://www.academia.edu/documents)\n* [Terms](https://www.academia.edu/terms)\n* [Privacy](https://www.academia.edu/privacy)\n* [Copyright](https://www.academia.edu/copyright)\n* [We&#39;re Hiring!](https://www.academia.edu/hiring)\n* [Help Center](https://support.academia.edu/hc/en-us)\n* less\nOutline\nkeyboard\\_arrow\\_down\n[Title](#title)\n[Abstract](#abstract)\n[Key Takeaways](#key-takeaways)\n[Introduction](#outer_page_1)\n[References](#references)\n[FAQs](#faq)\n![First page of \u201cHyperbolic trigonometry and its application in the Poincar\u00e9 ball model of hyperbolic geometry\u201d](https://0.academia-photos.com/attachment_thumbnails/70005914/mini_magick20210920-19594-16q6pvs.png?1632164953)![PDF Icon](https://a.academia-assets.com/images/single_work_splash/adobe_icon.svg)\ndownload\nDownload Free PDF\nDownload Free PDF\n# Hyperbolic trigonometry and its application in the Poincar\u00e9 ball model of hyperbolic geometry\n[![Profile image of Abraham Ungar](https://0.academia-photos.com/104358935/41535423/33631079/s65_abraham.ungar.jpg)Abraham Ungar](https://independent.academia.edu/UngarAbraham)\n2001, Computers &amp; Mathematics with Applications\n[https://doi.org/10.1016/S0898-1221(01)85012-4](https://doi.org/10.1016/S0898-1221(01)85012-4)\nvisibility\n\u2026description\n13 pages\ndescriptionSee full PDFdownloadDownload PDF\nbookmarkSave to LibraryshareShare\nclose\n![](https://a.academia-assets.com/images/academia-logo-capital-white.svg)#### Sign up for access to the world's latest research\nSign up for freearrow\\_forward\ncheckGet notified about relevant papers\ncheckSave papers to use in your research\ncheckJoin the discussion with peers\ncheckTrack your impact\n## Abstract\nHyperbolic trigonometry is developed and illustrated in this article along lines pa+lel to Euclidean trigonometry by exposing the hyperbolic trigonometric law of cosines and of sines in the Poincarb ball model of n-dimensional hyperbolic geometry, as well as their application. The Poincarb ball model of three-dimensional hyperbolic geometry is becoming increasingly important in the construction of hyperbolic browsers in computer graphics. These allow in computer graphics the exploitation of hyperbolic geometry in the development of visualization techniques. It is, therefore, clear that hyperbolic trigonometry in the Poincare ball model of hyperbolic geometry, as presented here, will prove useful in the development of efficient hyperbolic browsers in computer graphics. Hyperbolic trigonometry is governed by gyrovector spaces in the same way that Euclidean trigonometry is governed by vector spaces. The capability of gyrovector space theory to capture analogies and its powerful elegance is thus demonstrated once more.\n...Read more\n## Key takeaways\n![sparkles](https://a.academia-assets.com/images/icons/sparkle.svg)\nAI\n1. Hyperbolic trigonometry parallels Euclidean trigonometry through laws of cosines and sines in the Poincar\u00e9 model.\n2. The article illustrates hyperbolic trigonometric laws using a numerical example in the Poincar\u00e9 ball model.\n3. Mobius gyrovector spaces provide a foundation for hyperbolic geometry analogous to vector spaces for Euclidean geometry.\n4. Three-dimensional hyperbolic geometry enhances computer graphics via hyperbolic browsers, optimizing information visualization.\n5. Hyperbolic trigonometry can be applied to solve hyperbolic triangle problems, similar to Euclidean methods.\n## Related papers\n[Hyperbolic trigonometry in the Einstein relativistic velocity model of hyperbolic geometry](https://www.academia.edu/53029889/Hyperbolic_trigonometry_in_the_Einstein_relativistic_velocity_model_of_hyperbolic_geometry)\n[Abraham Ungar](https://independent.academia.edu/UngarAbraham)\nComputers &amp; Mathematics with Applications, 2000\nHyperbolic geometry is a fundamental aspect of modern physics. We explore in this paper the use of Einstein&#39;&#39;s velocity addition as a model of vector addition in hyperbolic geometry. Guided by analogies with ordinary vector addition, we develop hyperbolic vector spaces, called gyrovector spaces, which provide the setting for hyperbolic geometry in the same way that vector spaces provide the setting for Euclidean geometry. The resulting gyrovector spaces enable Euclidean trigonometry to be extended to hyperbolic trigonometry. In particular, we present the hyperbolic law of cosines and sines and the Hyperbolic Pythagorean Theorem emerges when the common vector addition is replaced by the Einstein velocity addition. (\\~\ndownloadDownload free PDF[View PDFchevron\\_right](https://www.academia.edu/53029889/Hyperbolic_trigonometry_in_the_Einstein_relativistic_velocity_model_of_hyperbolic_geometry)\n[Introduction to Hyperbolic Geometry Al &#39; a](https://www.academia.edu/76075639/Introduction_to_Hyperbolic_Geometry_Al_a)\n[Khawla Muhtaseb](https://independent.academia.edu/KhawlaMuhtaseb)\n2012\nHyperbolic Geometry is a particular type of Non-Euclidean Geometry. We can see Hyperbolic Geometry and its application in our life . In this seminar we studied a brief history of Euclidean and Hyperbolic geometries, and discussed postulates of Euclidean Geometry . After that we introduced some definitions, axioms of Hyperbolic Geometry and main theorems in Neutral Geometry . Finally we talked about two models of Hyperbolic Geometry; Klein Disk Model and Poincar\u00e9 Disk Model.\ndownloadDownload free PDF[View PDFchevron\\_right](https://www.academia.edu/76075639/Introduction_to_Hyperbolic_Geometry_Al_a)\n[Hyperbolic Geometry](https://www.academia.edu/111914929/Hyperbolic_Geometry)\n[Abraham A Ungar](https://independent.academia.edu/AbrahamUngar)\n2013\nRelativistic hyperbolic geometry is a model of the hyperbolic geometry of Lobachevsky and Bolyai in which Einstein addition of relativistically admissible velocities plays the role of vector addition. The adaptation of barycentric coordinates for use in relativistic hyperbolic geometry results in the relativistic barycentric coordinates. The latter are covariant with respect to the Lorentz transformation group just as the former are covariant with respect to the Galilei transformation group. Furthermore, the latter give rise to hyperbolically convex sets just as the former give rise to convex sets in Euclidean geometry. Convexity considerations are important in non-relativistic quantum mechanics where mixed states are positive barycentric combinations of pure states and where barycentric coordinates are interpreted as probabilities. In order to set the stage for its application in the geometry of relativistic quantum states, the notion of the relativistic barycentric coordinates that relativistic hyperbolic geometry admits is studied.\ndownloadDownload free PDF[View PDFchevron\\_right](https://www.academia.edu/111914929/Hyperbolic_Geometry)\n[Introduction to Hyperbolic Geometry](https://www.academia.edu/127566590/Introduction_to_Hyperbolic_Geometry)\n[John Ratcliffe](https://independent.academia.edu/JohnRatcliffe1)\nThe American Mathematical Monthly, 1996\ndownloadDownload free PDF[View PDFchevron\\_right](https://www.academia.edu/127566590/Introduction_to_Hyperbolic_Geometry)\n[Analytic Hyperbolic Geometry\u2014Mathematical Foundations and Applications by Abraham A. Ungar](https://www.academia.edu/114804404/Analytic_Hyperbolic_Geometry_Mathematical_Foundations_and_Applicati",
          "original_query": "Hyperbolic trigonometry and its application in the Poincar\u00e9 ball model of hyperbolic geometry",
          "cleaned_query": "Hyperbolic trigonometry and its application in the Poincar\u00e9 ball model of hyperbolic geometry",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "Compositional Entailment Learning for Hyperbolic Vision-Language ...",
          "url": "https://arxiv.org/abs/2410.06912",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "Compositional entailment learning for hyperbolic vision-language models",
          "cleaned_query": "Compositional entailment learning for hyperbolic vision-language models"
        },
        {
          "success": true,
          "title": "[2101.01600] Learning the Predictability of the Future - arXiv",
          "url": "https://arxiv.org/abs/2101.01600",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2101.01600** (cs)\n\n\\[Submitted on 1 Jan 2021\\]\n\n# Title:Learning the Predictability of the Future\n\nAuthors: [D\u00eddac Sur\u00eds](https://arxiv.org/search/cs?searchtype=author&query=Sur%C3%ADs,+D), [Ruoshi Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+R), [Carl Vondrick](https://arxiv.org/search/cs?searchtype=author&query=Vondrick,+C)\n\nView a PDF of the paper titled Learning the Predictability of the Future, by D\\\\'idac Sur\\\\'is and 2 other authors\n\n[View PDF](https://arxiv.org/pdf/2101.01600)\n\n> Abstract:We introduce a framework for learning from unlabeled video what is predictable in the future. Instead of committing up front to features to predict, our approach learns from data which features are predictable. Based on the observation that hyperbolic geometry naturally and compactly encodes hierarchical structure, we propose a predictive model in hyperbolic space. When the model is most confident, it will predict at a concrete level of the hierarchy, but when the model is not confident, it learns to automatically select a higher level of abstraction. Experiments on two established datasets show the key role of hierarchical representations for action prediction. Although our representation is trained with unlabeled video, visualizations show that action hierarchies emerge in the representation.\n\n| | |\n| --- | --- |\n| Comments: | Website: [this https URL](https://hyperfuture.cs.columbia.edu) |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV) |\n| Cite as: | [arXiv:2101.01600](https://arxiv.org/abs/2101.01600) \\[cs.CV\\] |\n| | (or [arXiv:2101.01600v1](https://arxiv.org/abs/2101.01600v1) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2101.01600](https://doi.org/10.48550/arXiv.2101.01600) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Didac Sur\u00eds Coll-Vinent \\[ [view email](https://arxiv.org/show-email/b20cd588/2101.01600)\\]\n\n**\\[v1\\]**\nFri, 1 Jan 2021 18:58:36 UTC (12,893 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Learning the Predictability of the Future, by D\\\\'idac Sur\\\\'is and 2 other authors\n\n- [View PDF](https://arxiv.org/pdf/2101.01600)\n- [TeX Source](https://arxiv.org/src/2101.01600)\n- [Other Formats](https://arxiv.org/format/2101.01600)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2101.01600&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2101.01600&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2021-01](https://arxiv.org/list/cs.CV/2021-01)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2101.01600?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2101.01600?context=cs.LG)\n\n[eess](https://arxiv.org/abs/2101.01600?context=eess)\n\n[eess.IV](https://arxiv.org/abs/2101.01600?context=eess.IV)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2101.01600)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2101.01600)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2101.01600)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2101.html#abs-2101-01600) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2101-01600)\n\n[D\u00eddac Sur\u00eds](https://dblp.uni-trier.de/search/author?author=D%C3%ADdac%20Sur%C3%ADs)\n\n[Carl Vondrick](https://dblp.uni-trier.de/search/author?author=Carl%20Vondrick)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2101.01600&description=Learning the Predictability of the Future) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2101.01600&title=Learning the Predictability of the Future)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2101.01600) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Learning the predictability of the future",
          "cleaned_query": "Learning the predictability of the future"
        }
      ],
      "generated_ideas": [
        "Uncertainty-Calibrated Hyperbolic Action Forecasting with Language Prompts\nCombine \u201cpredictability of the future\u201d hyperbolic forecasting with language supervision by prompting a vision\u2013language encoder with candidate future action descriptions and predicting at the appropriate abstraction level when uncertain. Implement a hierarchical action lattice in hyperbolic space (fine actions near boundary, coarse near origin) and learn a calibrated abstention mechanism that chooses coarse predictions when visual evidence is weak.",
        "Gyrovector (M\u00f6bius) Cross-Attention for Multimodal Transformers\nReplace Euclidean attention updates with gyrovector-space operations (M\u00f6bius addition/scalar multiplication) inspired by hyperbolic trigonometry/gyrovector theory to better preserve hierarchical relations during fusion. Implement and benchmark a \u201cHyperbolic Fusion Transformer\u201d where token updates stay on the Poincar\u00e9 ball, targeting gains on scene understanding, compositional VQA, and long-tail classification.",
        "Hierarchy-Aware Prompt Learning for Zero-Shot Transfer\nDevelop a prompt-learning method that learns *sets* of prompts positioned at different radii in hyperbolic space, corresponding to different abstraction levels (\u201cdog\u201d vs \u201canimal\u201d vs \u201cmammal\u201d). At inference, select or blend prompts based on image evidence (or entropy), aiming to reduce catastrophic errors in zero-shot classification by backing off to correct coarse categories when fine-grained discrimination is unreliable.",
        "Temporal Entailment Learning: Events as Hyperbolic Paths\nModel video events as trajectories (geodesic paths) in hyperbolic space where progression corresponds to increasing specificity (e.g., \u201cpreparing food\u201d \u2192 \u201ccutting\u201d \u2192 \u201cslicing cucumber\u201d). Train with weak narration/sentence alignment plus predictive losses so that future embeddings must lie on plausible entailment-consistent paths, enabling zero-shot temporal localization and forecasting with structured uncertainty.",
        "Diagnostic Benchmark for Hyperbolic vs Euclidean Representations in Vision\u2013Language\nCreate a targeted evaluation suite where success requires hierarchical reasoning: scene\u2192object inclusion, compositional entailment, and abstraction-aware forecasting (e.g., predict \u201csport\u201d when unsure of \u201ctennis\u201d). Compare Euclidean CLIP-style models, hyperbolic contrastive models, and hybrid models; include controlled long-tail splits and prompt-based tests to quantify when hyperbolic geometry yields measurable benefits."
      ],
      "generated_ideas_raw": "1. **Hyperbolic CLIP for Scene\u2013Object\u2013Part Hierarchies**  \n   Extend CLIP-style image\u2013text contrastive pretraining by mapping captions into hyperbolic space and enforcing entailment constraints (scene \u2192 objects \u2192 parts) using hyperbolic distances/norms. Use SAM 2 masks to extract object/part regions and create region\u2013caption pairs, then train a joint embedding where regions sit \u201cbelow\u201d scenes in the hierarchy for improved zero-shot compositional recognition and retrieval.\n\n2. **Mask-Conditioned Vision\u2013Language Pretraining with SAM 2 Pseudo-Regions**  \n   Build a large-scale pretraining pipeline that uses SAM 2 to generate stable object tracks/masks in videos and couples them with nearby narration/subtitles or auto-generated captions. Train a CLIP-like objective at the region-track level (track \u2194 text) plus an image-level objective, enabling stronger grounding and better transfer to referring segmentation and video understanding without manual masks.\n\n3. **Uncertainty-Calibrated Hyperbolic Action Forecasting with Language Prompts**  \n   Combine \u201cpredictability of the future\u201d hyperbolic forecasting with language supervision by prompting a vision\u2013language encoder with candidate future action descriptions and predicting at the appropriate abstraction level when uncertain. Implement a hierarchical action lattice in hyperbolic space (fine actions near boundary, coarse near origin) and learn a calibrated abstention mechanism that chooses coarse predictions when visual evidence is weak.\n\n4. **Compositional Referring Video Segmentation via Hyperbolic Entailment**  \n   Use SAM 2 to propose candidate masks/tracks and rank them with a hyperbolic vision\u2013language scoring model trained to respect compositional entailment (e.g., \u201cthe person holding a red cup\u201d entails \u201cperson\u201d). The key contribution is a training scheme that generates positive/negative compositional queries from captions and enforces entailment ordering in hyperbolic space, improving robustness to complex referring expressions.\n\n5. **Gyrovector (M\u00f6bius) Cross-Attention for Multimodal Transformers**  \n   Replace Euclidean attention updates with gyrovector-space operations (M\u00f6bius addition/scalar multiplication) inspired by hyperbolic trigonometry/gyrovector theory to better preserve hierarchical relations during fusion. Implement and benchmark a \u201cHyperbolic Fusion Transformer\u201d where token updates stay on the Poincar\u00e9 ball, targeting gains on scene understanding, compositional VQA, and long-tail classification.\n\n6. **Hierarchy-Aware Prompt Learning for Zero-Shot Transfer**  \n   Develop a prompt-learning method that learns *sets* of prompts positioned at different radii in hyperbolic space, corresponding to different abstraction levels (\u201cdog\u201d vs \u201canimal\u201d vs \u201cmammal\u201d). At inference, select or blend prompts based on image evidence (or entropy), aiming to reduce catastrophic errors in zero-shot classification by backing off to correct coarse categories when fine-grained discrimination is unreliable.\n\n7. **Hyperbolic Region Graph Contrastive Learning Using SAM 2**  \n   Construct a region adjacency/containment graph from SAM 2 masks (objects, parts, and background regions) and train contrastive objectives that place parent regions closer to the origin and children nearer the boundary in hyperbolic space. This yields an explicit geometric encoding of compositional structure that can be plugged into detectors/segmenters for improved generalization under distribution shift (new scenes, new object combinations).\n\n8. **Temporal Entailment Learning: Events as Hyperbolic Paths**  \n   Model video events as trajectories (geodesic paths) in hyperbolic space where progression corresponds to increasing specificity (e.g., \u201cpreparing food\u201d \u2192 \u201ccutting\u201d \u2192 \u201cslicing cucumber\u201d). Train with weak narration/sentence alignment plus predictive losses so that future embeddings must lie on plausible entailment-consistent paths, enabling zero-shot temporal localization and forecasting with structured uncertainty.\n\n9. **Open-Vocabulary Amodal Segmentation via Language-Guided Mask Completion**  \n   Use SAM 2 for visible masks/tracks, then train a model to predict amodal (occluded) masks conditioned on text describing the object (\u201cfull bicycle behind fence\u201d). Leverage CLIP embeddings to condition completion and enforce consistency across frames; the contribution is an open-vocabulary amodal benchmark and a scalable pseudo-labeling strategy that uses multi-view temporal cues to supervise occlusion completion.\n\n10. **Diagnostic Benchmark for Hyperbolic vs Euclidean Representations in Vision\u2013Language**  \n   Create a targeted evaluation suite where success requires hierarchical reasoning: scene\u2192object inclusion, compositional entailment, and abstraction-aware forecasting (e.g., predict \u201csport\u201d when unsure of \u201ctennis\u201d). Compare Euclidean CLIP-style models, hyperbolic contrastive models, and hybrid models; include controlled long-tail splits and prompt-based tests to quantify when hyperbolic geometry yields measurable benefits.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Uncertainty-Calibrated Hyperbolic Action Forecasting with Language Prompts\nCombine \u201cpredictability of the future\u201d hyperbolic forecasting with language supervision by prompting a vision\u2013language encode",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Gyrovector (M\u00f6bius) Cross-Attention for Multimodal Transformers\nReplace Euclidean attention updates with gyrovector-space operations (M\u00f6bius addition/scalar multiplication) inspired by hyperbolic trig",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Hierarchy-Aware Prompt Learning for Zero-Shot Transfer\nDevelop a prompt-learning method that learns *sets* of prompts positioned at different radii in hyperbolic space, corresponding to different abst",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Temporal Entailment Learning: Events as Hyperbolic Paths\nModel video events as trajectories (geodesic paths) in hyperbolic space where progression corresponds to increasing specificity (e.g., \u201cprepari",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Diagnostic Benchmark for Hyperbolic vs Euclidean Representations in Vision\u2013Language\nCreate a targeted evaluation suite where success requires hierarchical reasoning: scene\u2192object inclusion, compositio",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 65,
      "paper_title": "SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing",
      "contribution": "Introduction of SAVVY-Bench, the first benchmark for 3D spatial reasoning in dynamic audio-visual scenes, and a novel training-free reasoning pipeline that enhances AV-LLMs' performance in understanding such environments.",
      "num_predecessors": 4,
      "predecessors_crawled": 4,
      "academic_sources": 4,
      "crawl_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": true,
      "matching_idea_idx": 3,
      "input_tokens": 7645,
      "output_tokens": 903,
      "predecessor_details": [
        {
          "success": true,
          "title": "Systems of Spatial Reference in Human Memory - Semantic Scholar",
          "url": "https://www.semanticscholar.org/paper/Systems-of-Spatial-Reference-in-Human-Memory-Shelton-McNamara/ac9297ec4c6ba4e819c93f2d2681495506d87ae1",
          "content": "How Are the Locations of Objects in the Environment Represented in Memory? Psychology Spatial Cognition 2003 Evidence on the orientation dependence of spatial memories and recent results indicating that two representations may be formed when people learn a new environment; one preserves inter-object spatial relations and the other comprises visual memories of experienced views are reviewed. Intrinsic frames of reference in spatial memory. W. Mou T. McNamara Psychology Journal of experimental psychology. Learning, memory, and cognition 2002 The results indicate that spatial memories are defined with respect to intrinsic frames of reference, which are selected on the basis of egocentric experience and environmental cues. SHOWING 1-10 OF 45 REFERENCES Multiple views of spatial memory A. Shelton T. McNamara Psychology 1997 Recent evidence indicates that mental representations of large (i.e., navigable) spaces are viewpoint dependent when observers are restricted to a single view. The purpose of the present study was to\u2026 Object-array structure, frames of reference, and retrieval of spatial knowledge. R. D. Easton M. Sholl Psychology Journal of experimental psychology. Learning, memory, and cognition 1995 Assessment of the ability of people, without vision, to locate the positions of objects from imagined points of observation that are related to their actual position by rotational or translational components indicates that in the case of regularly structured object arrays, interobject relations are directly retrieved for the translation task, but for the rotation task, retrieval occurs by means of a body-centered coordinate system. ||||I|||| Skip to search formSkip to main contentSkip to account menu\n Semantic Scholar's Logo\n Search 210,748,839 papers from all fields of science\n Search\n Sign In Create Free Account\n * DOI: 10.1006/cogp.2001.0758\n * Corpus ID: 9510685\n\n Systems of Spatial Reference in Human Memory\n\n @article{Shelton2001SystemsOS,\n title={Systems of Spatial Reference in Human Memory},\n author={Amy Lynne Shelton and Timothy P. McNamara},\n journal={Cognitive Psychology},\n year={2001},\n volume={43},\n pages={274-310}\n }\n * A. Shelton , T. McNamara\n * Published 1 December 2001\n * Psychology\n * Cognitive Psychology\n Seven experiments examined the spatial reference systems used in memory to represent the locations of objects in the environment. Participants learned the locations of common objects in a room and then made judgments of relative direction using their memories of the layout (e.g., \"Imagine you are standing at the shoe, facing the lamp; point to the clock\"). The experiments manipulated the number of views that observers were allowed to experience, the presence or absence of local and global\u2026 Expand\n View on Elsevier\n psy.vanderbilt.edu\n Save to Library Save\n Create Alert Alert\n Cite\n Share This Paper\n 466 Citations\n Highly Influential Citations\n 55\n Background Citations\n 259\n Methods Citations\n 72\n Results Citations\n 53\n View All\n\n Figures from this paper\n\n * figure 1\n\n * figure 2\n\n * figure 3\n\n * figure 5\n\n * figure 6\n\n * figure 7\n\n * figure 8\n\n * figure 9\n\n * figure 10\n\n * figure 11\n\n View All 10 Figures & Tables\n\n 466 Citations\n\n Citation Type\n Has PDF\n Author\n More Filters\n More Filters\n Filters\n Sort by RelevanceSort by Most Influenced PapersSort by Citation CountSort by Recency\n\n How Are the Locations of Objects in the Environment Represented in Memory?\n\n * T. McNamara\n * Psychology\n Spatial Cognition\n * 2003\n TLDR\n Evidence on the orientation dependence of spatial memories and recent results indicating that two representations may be formed when people learn a new environment; one preserves inter-object spatial relations and the other comprises visual memories of experienced views are reviewed. Expand\n * 186\n * PDF\n * View 9 excerpts, cites methods, results and background\n Save\n Alert\n\n Object-centered reference systems and human spatial memory\n\n * Xiaoli Chen , T. McNamara\n * Psychology\n Psychonomic bulletin & review\n * 2011\n TLDR\n Results demonstrate that memories of the locations of objects are affected by object-centered reference systems and are consistent with conjectures that spatial memories are hierarchies of spatial reference systems, with higher levels corresponding to larger scales of space. Expand\n * 13\n * PDF\n * View 2 excerpts, cites background\n Save\n Alert\n\n Intrinsic frames of reference in spatial memory\n\n * W. Mou , T. McNamara\n * Psychology\n * 2002\n Three experiments investigated the frames of reference used in memory to represent the spatial structure of the environment. Participants learned the locations of objects in a room according to an \u2026 Expand\n * 123\n Save\n Alert\n\n Intrinsic frames of reference in spatial memory.\n\n * W. Mou , T. McNamara\n * Psychology\n Journal of experimental psychology. Learning, memory, and cognition\n * 2002\n TLDR\n The results indicate that spatial memories are defined with respect to intrinsic frames of reference, which are selected on the basis of egocentric experience and environmental cues. Expand\n * 335\n * PDF\n Save\n Alert\n\n Biased representations of the spatial structure of navigable environments\n\n * C. Valiquette , T. McNamara , J. Labrecque\n * Psychology\n Psychological research\n * 2007\n TLDR\n The findings indicate that the human spatial memory and navigation system is strongly biased to represent the spatial structure of navigable environments with reference directions or axes that are aligned with salient environmental frames of reference. Expand\n * 32\n * View 10 excerpts, cites background, methods and results\n Save\n Alert\n\n Reference frames in allocentric representations are invariant across static and active encoding\n\n * E. Chan , O. Baumann , M. Bellgrove , J. Mattingley\n * Psychology\n Front. Psychol.\n * 2013\n TLDR\n The findings suggest that the learning condition (static vs. active) does not affect the reference system employed to encode object-location information, and spatial reference systems appear to be a ubiquitous property of spatial representations, and might serve to reduce the cognitive demands of spatial processing. Expand\n * 9\n * PDF\n Save\n Alert\n\n Environmental reference systems for large-scale spaces\n\n * S. Werner , Kristine Schmidt\n * Psychology\n Spatial Cogn. Comput.\n * 1999\n TLDR\n To account for the simultaneous effect of an environmental and an egocentric reference system, this work presents a 2-level model of spatial memory access, which indicates that two independent reference systems underly the retrieval of spatial knowledge. Expand\n * 115\n * PDF\n Save\n Alert\n\n Egocentric and geocentric frames of reference in memory of large-scale space\n\n * T. McNamara , B. Rump , S. Werner\n * Psychology\n Psychonomic bulletin & review\n * 2003\n TLDR\n Results indicated that locations of objects were mentally represented in terms of frames of reference defined by the environment but selected on the basis of egocentric experience. Expand\n * 209\n * PDF\n * View 7 excerpts, cites methods, background and results\n Save\n Alert\n\n Allocentric and egocentric updating of spatial memories.\n\n * W. Mou , T. McNamara , C. Valiquette , B. Rump\n * Psychology\n Journal of experimental psychology. Learning, memory, and cognition\n * 2004\n TLDR\n These findings indicated that spatial reference directions in memory were not updated during locomotion, and pointing performance was best when the imagined heading was parallel to the learning view. Expand\n * 349\n * PDF\n * View 13 excerpts, cites background, results and methods\n Save\n Alert\n\n Representing Spatial Layout According to Intrinsic Frames of Reference\n\n * Chaoxiang Xie , Shiyi Li , Weidong Tao , Yiping Wei , Hong-jin Sun\n * Psychology\n Psychological reports\n * 2017\n TLDR\n The results of three experiments suggest that environmental reference frames and intrinsic axes can influence performance for novel headings, but their role in spatial memory depends on egocentric experience, layout regularity, and instructions. Expand\n * 2\n * View 1 ex",
          "original_query": "Systems of spatial reference in human memory",
          "cleaned_query": "Systems of spatial reference in human memory"
        },
        {
          "success": true,
          "title": "Endowing Vision-Language Models with Spatial Reasoning ... - arXiv",
          "url": "https://arxiv.org/abs/2401.12168",
          "content": "[2401.12168] SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2401.12168\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2401.12168**(cs)\n[Submitted on 22 Jan 2024]\n# Title:SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\nAuthors:[Boyuan Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+B),[Zhuo Xu](https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Z),[Sean Kirmani](https://arxiv.org/search/cs?searchtype=author&amp;query=Kirmani,+S),[Brian Ichter](https://arxiv.org/search/cs?searchtype=author&amp;query=Ichter,+B),[Danny Driess](https://arxiv.org/search/cs?searchtype=author&amp;query=Driess,+D),[Pete Florence](https://arxiv.org/search/cs?searchtype=author&amp;query=Florence,+P),[Dorsa Sadigh](https://arxiv.org/search/cs?searchtype=author&amp;query=Sadigh,+D),[Leonidas Guibas](https://arxiv.org/search/cs?searchtype=author&amp;query=Guibas,+L),[Fei Xia](https://arxiv.org/search/cs?searchtype=author&amp;query=Xia,+F)\nView a PDF of the paper titled SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities, by Boyuan Chen and 8 other authors\n[View PDF](https://arxiv.org/pdf/2401.12168)> > Abstract:\n> Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences. We hypothesize that VLMs&#39; limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture. Our work features the first internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability. Project website: [> this https URL\n](https://spatial-vlm.github.io/)> Subjects:|Computer Vision and Pattern Recognition (cs.CV); Computation and Language (cs.CL); Machine Learning (cs.LG); Robotics (cs.RO)|\nCite as:|[arXiv:2401.12168](https://arxiv.org/abs/2401.12168)[cs.CV]|\n|(or[arXiv:2401.12168v1](https://arxiv.org/abs/2401.12168v1)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2401.12168](https://doi.org/10.48550/arXiv.2401.12168)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Fei Xia [[view email](https://arxiv.org/show-email/28ea04d5/2401.12168)]\n**[v1]**Mon, 22 Jan 2024 18:01:01 UTC (4,607 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities, by Boyuan Chen and 8 other authors\n* [View PDF](https://arxiv.org/pdf/2401.12168)\n* [TeX Source](https://arxiv.org/src/2401.12168)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2401.12168&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2401.12168&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2024-01](https://arxiv.org/list/cs.CV/2024-01)\nChange to browse by:\n[cs](https://arxiv.org/abs/2401.12168?context=cs)\n[cs.CL](https://arxiv.org/abs/2401.12168?context=cs.CL)\n[cs.LG](https://arxiv.org/abs/2401.12168?context=cs.LG)\n[cs.RO](https://arxiv.org/abs/2401.12168?context=cs.RO)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2401.12168)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2401.12168)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2401.12168)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add",
          "original_query": "Spatialvlm: Endowing vision-language models with spatial reasoning capabilities",
          "cleaned_query": "Spatialvlm: Endowing vision-language models with spatial reasoning capabilities"
        },
        {
          "success": true,
          "title": "Learning to Answer Questions in Dynamic Audio-Visual Scenarios",
          "url": "https://ieeexplore.ieee.org/document/9879157/;jsessionid=EDAA458D8D144C65C901F3998AB7113F",
          "content": "Learning to Answer Questions in Dynamic Audio-Visual Scenarios \\| IEEE Conference Publication \\| IEEE Xplore\n\n### IEEE Account\n\n- [Change Username/Password](https://www.ieee.org/profile/changeusrpwd/showChangeUsrPwdPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Update Address](https://www.ieee.org/profile/address/getAddrInfoPage.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n\n### Purchase Details\n\n- [Payment Options](https://www.ieee.org/profile/payment/showPaymentHome.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Order History](https://www.ieee.org/profile/vieworder/showOrderHistory.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [View Purchased Documents](https://ieeexplore.ieee.org/articleSale/purchaseHistory.jsp)\n\n### Profile Information\n\n- [Communications Preferences](https://www.ieee.org/ieee-privacyportal/app/ibp?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Profession and Education](https://www.ieee.org/profile/profedu/getProfEduInformation.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n- [Technical Interests](https://www.ieee.org/profile/tips/getTipsInfo.html?refSite=https://ieeexplore.ieee.org&refSiteName=IEEE Xplore)\n\n### Need Help?\n\n- **US & Canada:** +1 800 678 4333\n- **Worldwide:** +1 732 981 0060\n\n- [Contact & Support](https://ieeexplore.ieee.org/xpl/contact)\n\n- [About IEEE _Xplore_](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/about-ieee-xplore)\n- [Contact Us](https://ieeexplore.ieee.org/xpl/contact)\n- [Help](https://ieeexplore.ieee.org/Xplorehelp)\n- [Accessibility](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/accessibility-statement)\n- [Terms of Use](https://ieeexplore.ieee.org/Xplorehelp/overview-of-ieee-xplore/terms-of-use)\n- [Nondiscrimination Policy](http://www.ieee.org/web/aboutus/whatis/policies/p9-26.html)\n- [Sitemap](https://ieeexplore.ieee.org/xpl/sitemap.jsp)\n- [Privacy & Opting Out of Cookies](http://www.ieee.org/about/help/security_privacy.html)\n\nA not-for-profit organization, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.\n\n\u00a9 Copyright 2025 IEEE - All rights reserved. Use of this web site signifies your agreement to the terms and conditions.",
          "original_query": "Learning to answer questions in dynamic audio-visual scenarios",
          "cleaned_query": "Learning to answer questions in dynamic audio-visual scenarios"
        },
        {
          "success": true,
          "title": "[2412.14171] Thinking in Space: How Multimodal Large ...",
          "url": "https://arxiv.org/abs/2412.14171",
          "content": "[2412.14171] Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2412.14171\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2412.14171**(cs)\n[Submitted on 18 Dec 2024 ([v1](https://arxiv.org/abs/2412.14171v1)), last revised 2 Jul 2025 (this version, v2)]\n# Title:Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces\nAuthors:[Jihan Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+J),[Shusheng Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+S),[Anjali W. Gupta](https://arxiv.org/search/cs?searchtype=author&amp;query=Gupta,+A+W),[Rilyn Han](https://arxiv.org/search/cs?searchtype=author&amp;query=Han,+R),[Li Fei-Fei](https://arxiv.org/search/cs?searchtype=author&amp;query=Fei-Fei,+L),[Saining Xie](https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+S)\nView a PDF of the paper titled Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces, by Jihan Yang and 5 other authors\n[View PDF](https://arxiv.org/pdf/2412.14171)[HTML (experimental)](https://arxiv.org/html/2412.14171v2)> > Abstract:\n> Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also ``think in space&#39;&#39; from videos? We present a novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitive - though subhuman - visual-spatial intelligence. We probe models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models. Notably, prevailing linguistic reasoning techniques (e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve performance, whereas explicitly generating cognitive maps during question-answering enhances MLLMs&#39; spatial distance ability. Comments:|Project page:[this https URL](https://vision-x-nyu.github.io/thinking-in-space.github.io/)|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:2412.14171](https://arxiv.org/abs/2412.14171)[cs.CV]|\n|(or[arXiv:2412.14171v2](https://arxiv.org/abs/2412.14171v2)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2412.14171](https://doi.org/10.48550/arXiv.2412.14171)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jihan Yang [[view email](https://arxiv.org/show-email/5bb2171e/2412.14171)]\n**[[v1]](https://arxiv.org/abs/2412.14171v1)**Wed, 18 Dec 2024 18:59:54 UTC (9,433 KB)\n**[v2]**Wed, 2 Jul 2025 21:00:36 UTC (9,554 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces, by Jihan Yang and 5 other authors\n* [View PDF](https://arxiv.org/pdf/2412.14171)\n* [HTML (experimental)](https://arxiv.org/html/2412.14171v2)\n* [TeX Source](https://arxiv.org/src/2412.14171)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2412.14171&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2412.14171&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2024-12](https://arxiv.org/list/cs.CV/2024-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/2412.14171?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2412.14171)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2412.14171)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2412.14171)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2412.14171)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces",
          "cleaned_query": "Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces"
        }
      ],
      "generated_ideas": [
        "Intrinsic Reference-Frame Induction for SpatialVLM\nTrain SpatialVLM-style models to explicitly predict the \u201cintrinsic\u201d reference direction of a scene (dominant axis/landmark-aligned frame) and then answer spatial VQA in that induced frame. Create supervision by procedurally rotating rooms/object layouts (or using estimated room axes) and scoring whether predicted frames align with stable environmental cues, mirroring human frame selection findings.",
        "Dual-Code Spatial Memory in MLLMs (View-Based + Relational Map)\nBuild an architecture with two coupled memories: (a) a view-embedding bank for experienced perspectives and (b) a graph/metric map preserving inter-object relations, reflecting the two-representation account in human spatial memory. Evaluate on VSI-Bench-like video QA with controlled single-view vs multi-view exposure to test whether each memory dominates under different encoding conditions.",
        "Hierarchical Reference Systems for Multi-Scale Spaces\nImplement a hierarchical spatial representation (object-level, room-level, building-level) where each level has its own learned reference frame and alignment constraints, inspired by \u201chierarchies of spatial reference systems.\u201d Train with mixed tasks: within-room metric VQA (SpatialVLM) and cross-room/topological queries (VSI-style videos), testing whether higher-level frames stabilize long-range reasoning.",
        "Audio-Visual Egocentric Cues as Spatial Frame Anchors\nAdd a module that uses dynamic audio cues (source localization, onset timing) to infer egocentric orientation and stabilize spatial recall during video QA in dynamic scenes. Construct datasets where sound sources provide consistent directional anchors even when visual landmarks are ambiguous, and measure gains on relative-direction and distance questions.",
        "Active Viewpoint Selection to Minimize Spatial Uncertainty\nTurn spatial QA into an embodied \u201cnext-best-view\u201d problem: given an incomplete cognitive map from partial video, the model chooses camera motions (or requests additional frames) to reduce uncertainty about queried relations. Optimize a policy using uncertainty estimates over the cognitive map and evaluate sample efficiency versus passive multi-view training.",
        "Relative-Direction \u201cJRD\u201d Tasks for MLLMs with Map-Consistent Decoding\nTranslate the classic Judgment of Relative Direction (JRD) paradigm into multimodal QA: \u201cImagine at object A facing object B; point to C,\u201d with answers as angles/bearings plus natural language rationales. Enforce consistency by decoding through a differentiable geometric layer operating on the model\u2019s predicted cognitive map, enabling direct penalties for impossible bearings.",
        "Learning Environmental Cue Weighting for Frame Selection\nCreate controlled environments/videos where global cues (room geometry, cardinal hints, repeated textures) and local cues (salient objects) conflict, and train a model to learn when to trust each cue for selecting a reference frame. The contribution is a measurable \u201ccue-weighting\u201d profile for MLLMs, enabling direct comparison to human strategies reported in spatial memory studies.",
        "Spatial Chain-of-Thought with Executable Map Programs\nReplace purely linguistic chain-of-thought (shown to be weak in VSI-Bench) with executable \u201cmap programs\u201d that operate on an explicit cognitive map (e.g., build nodes, set distances, compute shortest paths, rotate frames). Evaluate whether program execution improves quantitative distance/size reasoning (SpatialVLM) and sequential recall from videos (VSI-Bench), and analyze failure modes when the map is inconsistent or incomplete."
      ],
      "generated_ideas_raw": "1. **Intrinsic Reference-Frame Induction for SpatialVLM**\n   Train SpatialVLM-style models to explicitly predict the \u201cintrinsic\u201d reference direction of a scene (dominant axis/landmark-aligned frame) and then answer spatial VQA in that induced frame. Create supervision by procedurally rotating rooms/object layouts (or using estimated room axes) and scoring whether predicted frames align with stable environmental cues, mirroring human frame selection findings.\n\n2. **Dual-Code Spatial Memory in MLLMs (View-Based + Relational Map)**\n   Build an architecture with two coupled memories: (a) a view-embedding bank for experienced perspectives and (b) a graph/metric map preserving inter-object relations, reflecting the two-representation account in human spatial memory. Evaluate on VSI-Bench-like video QA with controlled single-view vs multi-view exposure to test whether each memory dominates under different encoding conditions.\n\n3. **Orientation-Dependence Benchmark for Video Spatial Recall**\n   Extend VSI-Bench with systematic \u201ctest heading\u201d manipulations (e.g., ask the same relative-direction question from imagined headings rotated by 0/90/180 degrees) to quantify orientation dependence in MLLMs. Use this benchmark to measure whether cognitive-map generation reduces orientation costs, paralleling human orientation-dependent retrieval effects.\n\n4. **Hierarchical Reference Systems for Multi-Scale Spaces**\n   Implement a hierarchical spatial representation (object-level, room-level, building-level) where each level has its own learned reference frame and alignment constraints, inspired by \u201chierarchies of spatial reference systems.\u201d Train with mixed tasks: within-room metric VQA (SpatialVLM) and cross-room/topological queries (VSI-style videos), testing whether higher-level frames stabilize long-range reasoning.\n\n5. **Audio-Visual Egocentric Cues as Spatial Frame Anchors**\n   Add a module that uses dynamic audio cues (source localization, onset timing) to infer egocentric orientation and stabilize spatial recall during video QA in dynamic scenes. Construct datasets where sound sources provide consistent directional anchors even when visual landmarks are ambiguous, and measure gains on relative-direction and distance questions.\n\n6. **Active Viewpoint Selection to Minimize Spatial Uncertainty**\n   Turn spatial QA into an embodied \u201cnext-best-view\u201d problem: given an incomplete cognitive map from partial video, the model chooses camera motions (or requests additional frames) to reduce uncertainty about queried relations. Optimize a policy using uncertainty estimates over the cognitive map and evaluate sample efficiency versus passive multi-view training.\n\n7. **Counterfactual Scene Rotations for Reference-Frame Robustness**\n   Introduce a counterfactual training objective: rotate the inferred 3D scene/map and require answers to transform equivariantly (or remain invariant when appropriate), explicitly targeting orientation dependence. This creates a controlled analogue of human rotation costs and tests whether models can learn stable allocentric reasoning while preserving view-based cues.\n\n8. **Relative-Direction \u201cJRD\u201d Tasks for MLLMs with Map-Consistent Decoding**\n   Translate the classic Judgment of Relative Direction (JRD) paradigm into multimodal QA: \u201cImagine at object A facing object B; point to C,\u201d with answers as angles/bearings plus natural language rationales. Enforce consistency by decoding through a differentiable geometric layer operating on the model\u2019s predicted cognitive map, enabling direct penalties for impossible bearings.\n\n9. **Learning Environmental Cue Weighting for Frame Selection**\n   Create controlled environments/videos where global cues (room geometry, cardinal hints, repeated textures) and local cues (salient objects) conflict, and train a model to learn when to trust each cue for selecting a reference frame. The contribution is a measurable \u201ccue-weighting\u201d profile for MLLMs, enabling direct comparison to human strategies reported in spatial memory studies.\n\n10. **Spatial Chain-of-Thought with Executable Map Programs**\n   Replace purely linguistic chain-of-thought (shown to be weak in VSI-Bench) with executable \u201cmap programs\u201d that operate on an explicit cognitive map (e.g., build nodes, set distances, compute shortest paths, rotate frames). Evaluate whether program execution improves quantitative distance/size reasoning (SpatialVLM) and sequential recall from videos (VSI-Bench), and analyze failure modes when the map is inconsistent or incomplete.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Intrinsic Reference-Frame Induction for SpatialVLM\nTrain SpatialVLM-style models to explicitly predict the \u201cintrinsic\u201d reference direction of a scene (dominant axis/landmark-aligned frame) and then an",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Dual-Code Spatial Memory in MLLMs (View-Based + Relational Map)\nBuild an architecture with two coupled memories: (a) a view-embedding bank for experienced perspectives and (b) a graph/metric map prese",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Hierarchical Reference Systems for Multi-Scale Spaces\nImplement a hierarchical spatial representation (object-level, room-level, building-level) where each level has its own learned reference frame an",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Audio-Visual Egocentric Cues as Spatial Frame Anchors\nAdd a module that uses dynamic audio cues (source localization, onset timing) to infer egocentric orientation and stabilize spatial recall during ",
          "is_match": true
        },
        {
          "idea_idx": 4,
          "idea_text": "Active Viewpoint Selection to Minimize Spatial Uncertainty\nTurn spatial QA into an embodied \u201cnext-best-view\u201d problem: given an incomplete cognitive map from partial video, the model chooses camera mot",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Relative-Direction \u201cJRD\u201d Tasks for MLLMs with Map-Consistent Decoding\nTranslate the classic Judgment of Relative Direction (JRD) paradigm into multimodal QA: \u201cImagine at object A facing object B; poin",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Learning Environmental Cue Weighting for Frame Selection\nCreate controlled environments/videos where global cues (room geometry, cardinal hints, repeated textures) and local cues (salient objects) con",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Spatial Chain-of-Thought with Executable Map Programs\nReplace purely linguistic chain-of-thought (shown to be weak in VSI-Bench) with executable \u201cmap programs\u201d that operate on an explicit cognitive ma",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 66,
      "paper_title": "A multiscale analysis of mean-field transformers in the moderate interaction regime",
      "contribution": "The paper provides a multiscale framework to analyze the dynamics of tokens in transformer models by treating them as mean-field interacting particles, especially in the moderate interaction regime.",
      "num_predecessors": 4,
      "predecessors_crawled": 4,
      "academic_sources": 4,
      "crawl_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 9114,
      "output_tokens": 940,
      "predecessor_details": [
        {
          "success": true,
          "title": "[1706.03762] Attention Is All You Need",
          "url": "https://arxiv.org/abs/1706.03762",
          "content": "[1706.03762] Attention Is All You Need[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1706.03762\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computation and Language\n**arXiv:1706.03762**(cs)\n[Submitted on 12 Jun 2017 ([v1](https://arxiv.org/abs/1706.03762v1)), last revised 2 Aug 2023 (this version, v7)]\n# Title:Attention Is All You Need\nAuthors:[Ashish Vaswani](https://arxiv.org/search/cs?searchtype=author&amp;query=Vaswani,+A),[Noam Shazeer](https://arxiv.org/search/cs?searchtype=author&amp;query=Shazeer,+N),[Niki Parmar](https://arxiv.org/search/cs?searchtype=author&amp;query=Parmar,+N),[Jakob Uszkoreit](https://arxiv.org/search/cs?searchtype=author&amp;query=Uszkoreit,+J),[Llion Jones](https://arxiv.org/search/cs?searchtype=author&amp;query=Jones,+L),[Aidan N. Gomez](https://arxiv.org/search/cs?searchtype=author&amp;query=Gomez,+A+N),[Lukasz Kaiser](https://arxiv.org/search/cs?searchtype=author&amp;query=Kaiser,+L),[Illia Polosukhin](https://arxiv.org/search/cs?searchtype=author&amp;query=Polosukhin,+I)\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n[View PDF](https://arxiv.org/pdf/1706.03762)[HTML (experimental)](https://arxiv.org/html/1706.03762v7)> > Abstract:\n> The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Comments:|15 pages, 5 figures|\nSubjects:|Computation and Language (cs.CL); Machine Learning (cs.LG)|\nCite as:|[arXiv:1706.03762](https://arxiv.org/abs/1706.03762)[cs.CL]|\n|(or[arXiv:1706.03762v7](https://arxiv.org/abs/1706.03762v7)[cs.CL]for this version)|\n|[https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Llion Jones [[view email](https://arxiv.org/show-email/f53b7360/1706.03762)]\n**[[v1]](https://arxiv.org/abs/1706.03762v1)**Mon, 12 Jun 2017 17:57:34 UTC (1,102 KB)\n**[[v2]](https://arxiv.org/abs/1706.03762v2)**Mon, 19 Jun 2017 16:49:45 UTC (1,125 KB)\n**[[v3]](https://arxiv.org/abs/1706.03762v3)**Tue, 20 Jun 2017 05:20:02 UTC (1,125 KB)\n**[[v4]](https://arxiv.org/abs/1706.03762v4)**Fri, 30 Jun 2017 17:29:30 UTC (1,124 KB)\n**[[v5]](https://arxiv.org/abs/1706.03762v5)**Wed, 6 Dec 2017 03:30:32 UTC (1,124 KB)\n**[[v6]](https://arxiv.org/abs/1706.03762v6)**Mon, 24 Jul 2023 00:48:54 UTC (1,124 KB)\n**[v7]**Wed, 2 Aug 2023 00:41:18 UTC (1,124 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Attention Is All You Need, by Ashish Vaswani and 7 other authors\n* [View PDF](https://arxiv.org/pdf/1706.03762)\n* [HTML (experimental)](https://arxiv.org/html/1706.03762v7)\n* [TeX Source](https://arxiv.org/src/1706.03762)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CL\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1706.03762&amp;function=prev&amp;context=cs.CL) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1706.03762&amp;function=next&amp;context=cs.CL)\n[new](https://arxiv.org/list/cs.CL/new)|[recent](https://arxiv.org/list/cs.CL/recent)|[2017-06](https://arxiv.org/list/cs.CL/2017-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/1706.03762?context=cs)\n[cs.LG](https://arxiv.org/abs/1706.03762?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1706.03762)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1706.03762)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1706.03762)\n### [123 blog links](https://arxiv.org/tb/1706.03762)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1706.html#VaswaniSPUJGKP17)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/VaswaniSPUJGKP17)\n[Ashish Vaswani]()\n[Noam Shazeer]()\n[Niki Parmar]()\n[Jakob Uszkoreit]()\n[Llion Jones]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces",
          "original_query": "Attention is all you need",
          "cleaned_query": "Attention is all you need"
        },
        {
          "success": true,
          "title": "Sinkformers: Transformers with Doubly Stochastic Attention - arXiv",
          "url": "https://arxiv.org/abs/2110.11773",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: Attention based models such as Transformers involve pairwise interactions\nbetween data points, modeled with a learnable attention matrix. Importantly,\nthis attention matrix is normalized with the SoftMax operator, which makes it\nrow-wise stochastic. In this paper, we propose instead to use Sinkhorn's\nalgorithm to make attention matrices doubly stochastic. We call the resulting\nmodel a Sinkformer. We show that the row-wise stochastic attention matrices in\nclassical Transformers get close to doubly stochastic matrices as the number of\nepochs increases, justifying the use of Sinkhorn normalization as an\ninformative prior. On the theoretical side, we show that, unlike the SoftMax\noperation, this normalization makes it possible to understand the iterations of\nself-attention modules as a discretized gradient-flow for the Wasserstein\nmetric. We also show in the infinite number of samples limit that, when\nrescaling both attention matrices and depth, Sinkformers operate a heat\ndiffusion. On the experimental side, we show that Sinkformers enhance model\naccuracy in vision and natural language processing tasks. In particular, on 3D\nshapes classification, Sinkformers lead to a significant improvement.\n \n \n \n \n Submission history From: Michael E. Sander [ view email]\n \n [v1] \n Fri, 22 Oct 2021 13:25:01 UTC (1,110 KB) [v2] \nMon, 24 Jan 2022 15:09:34 UTC (1,115 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > cs > arXiv:2110.11773\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Machine Learning\n\n arXiv:2110.11773 (cs)\n [Submitted on 22 Oct 2021 (v1), last revised 24 Jan 2022 (this version, v2)]\n\n Title: Sinkformers: Transformers with Doubly Stochastic Attention\n\n Authors: Michael E. Sander, Pierre Ablin, Mathieu Blondel, Gabriel Peyr\u00e9\n Download PDF\n Abstract: Attention based models such as Transformers involve pairwise interactions between data points, modeled with a learnable attention matrix. Importantly, this attention matrix is normalized with the SoftMax operator, which makes it row-wise stochastic. In this paper, we propose instead to use Sinkhorn's algorithm to make attention matrices doubly stochastic. We call the resulting model a Sinkformer. We show that the row-wise stochastic attention matrices in classical Transformers get close to doubly stochastic matrices as the number of epochs increases, justifying the use of Sinkhorn normalization as an informative prior. On the theoretical side, we show that, unlike the SoftMax operation, this normalization makes it possible to understand the iterations of self-attention modules as a discretized gradient-flow for the Wasserstein metric. We also show in the infinite number of samples limit that, when rescaling both attention matrices and depth, Sinkformers operate a heat diffusion. On the experimental side, we show that Sinkformers enhance model accuracy in vision and natural language processing tasks. In particular, on 3D shapes classification, Sinkformers lead to a significant improvement.\n Comments: Accepted at AISTATS \n Subjects: Machine Learning (cs.LG) ; Machine Learning (stat.ML)\n Cite as: arXiv:2110.11773 [cs.LG] \n (or arXiv:2110.11773v2 [cs.LG] for this version) \n https://doi.org/10.48550/arXiv.2110.11773 \n Focus to learn more \n arXiv-issued DOI via DataCite \n \n\n Submission history\n\n From: Michael E. Sander [view email]\n [v1] Fri, 22 Oct 2021 13:25:01 UTC (1,110 KB)\n [v2] Mon, 24 Jan 2022 15:09:34 UTC (1,115 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * Other formats\n Current browse context:\n cs.LG\n < prev | next >\n new | recent | 2110\n Change to browse by:\n cs\n stat\n stat.ML\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n\n DBLP - CS Bibliography\n\n listing | bibtex\n Pierre Ablin\n Mathieu Blondel\n Gabriel Peyr\u00e9\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n IArxiv recommender toggle\n IArxiv Recommender (What is IArxiv?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs and how to get involved.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Sinkformers: Transformers with doubly stochastic attention",
          "cleaned_query": "Sinkformers: Transformers with doubly stochastic attention"
        },
        {
          "success": true,
          "title": "[2305.05465] The emergence of clusters in self-attention dynamics",
          "url": "https://arxiv.org/abs/2305.05465",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2305.05465** (cs)\n\n\\[Submitted on 9 May 2023 ( [v1](https://arxiv.org/abs/2305.05465v1)), last revised 12 Feb 2024 (this version, v6)\\]\n\n# Title:The emergence of clusters in self-attention dynamics\n\nAuthors: [Borjan Geshkovski](https://arxiv.org/search/cs?searchtype=author&query=Geshkovski,+B), [Cyril Letrouit](https://arxiv.org/search/cs?searchtype=author&query=Letrouit,+C), [Yury Polyanskiy](https://arxiv.org/search/cs?searchtype=author&query=Polyanskiy,+Y), [Philippe Rigollet](https://arxiv.org/search/cs?searchtype=author&query=Rigollet,+P)\n\nView a PDF of the paper titled The emergence of clusters in self-attention dynamics, by Borjan Geshkovski and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2305.05465) [HTML (experimental)](https://arxiv.org/html/2305.05465v6)\n\n> Abstract:Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. Cluster locations are determined by the initial tokens, confirming context-awareness of representations learned by Transformers. Using techniques from dynamical systems and partial differential equations, we show that the type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. \\[VSP'17\\] that leaders appear in a sequence of tokens when processed by Transformers.\n\n| | |\n| --- | --- |\n| Subjects: | Machine Learning (cs.LG); Analysis of PDEs (math.AP); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2305.05465](https://arxiv.org/abs/2305.05465) \\[cs.LG\\] |\n| (or [arXiv:2305.05465v6](https://arxiv.org/abs/2305.05465v6) \\[cs.LG\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2305.05465](https://doi.org/10.48550/arXiv.2305.05465) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Borjan Geshkovski \\[ [view email](https://arxiv.org/show-email/080722b4/2305.05465)\\] **[\\[v1\\]](https://arxiv.org/abs/2305.05465v1)**\nTue, 9 May 2023 14:04:42 UTC (70,588 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2305.05465v2)**\nWed, 17 May 2023 15:16:55 UTC (50,995 KB)\n**[\\[v3\\]](https://arxiv.org/abs/2305.05465v3)**\nTue, 17 Oct 2023 11:20:44 UTC (28,354 KB)\n**[\\[v4\\]](https://arxiv.org/abs/2305.05465v4)**\nMon, 8 Jan 2024 07:48:21 UTC (28,355 KB)\n**[\\[v5\\]](https://arxiv.org/abs/2305.05465v5)**\nTue, 6 Feb 2024 07:30:11 UTC (28,355 KB)\n**\\[v6\\]**\nMon, 12 Feb 2024 10:21:08 UTC (28,355 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled The emergence of clusters in self-attention dynamics, by Borjan Geshkovski and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2305.05465)\n- [HTML (experimental)](https://arxiv.org/html/2305.05465v6)\n- [TeX Source](https://arxiv.org/src/2305.05465)\n- [Other Formats](https://arxiv.org/format/2305.05465)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2305.05465&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2305.05465&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2023-05](https://arxiv.org/list/cs.LG/2023-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2305.05465?context=cs) [math](https://arxiv.org/abs/2305.05465?context=math) [math.AP](https://arxiv.org/abs/2305.05465?context=math.AP) [stat](https://arxiv.org/abs/2305.05465?context=stat) [stat.ML](https://arxiv.org/abs/2305.05465?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2305.05465)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2305.05465)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2305.05465)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2305.05465) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "The emergence of clusters in self-attention dynamics",
          "cleaned_query": "The emergence of clusters in self-attention dynamics"
        },
        {
          "success": true,
          "title": "Quantitative Clustering in Mean-Field Transformer Models - arXiv",
          "url": "https://arxiv.org/abs/2504.14697",
          "content": "[2504.14697] Quantitative Clustering in Mean-Field Transformer Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2504.14697\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2504.14697**(cs)\n[Submitted on 20 Apr 2025 ([v1](https://arxiv.org/abs/2504.14697v1)), last revised 30 Apr 2025 (this version, v2)]\n# Title:Quantitative Clustering in Mean-Field Transformer Models\nAuthors:[Shi Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+S),[Zhengjiang Lin](https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+Z),[Yury Polyanskiy](https://arxiv.org/search/cs?searchtype=author&amp;query=Polyanskiy,+Y),[Philippe Rigollet](https://arxiv.org/search/cs?searchtype=author&amp;query=Rigollet,+P)\nView a PDF of the paper titled Quantitative Clustering in Mean-Field Transformer Models, by Shi Chen and 3 other authors\n[View PDF](https://arxiv.org/pdf/2504.14697)> > Abstract:\n> The evolution of tokens through a deep transformer models can be modeled as an interacting particle system that has been shown to exhibit an asymptotic clustering behavior akin to the synchronization phenomenon in Kuramoto models. In this work, we investigate the long-time clustering of mean-field transformer models. More precisely, we establish exponential rates of contraction to a Dirac point mass for any suitably regular initialization under some assumptions on the parameters of transformer models, any suitably regular mean-field initialization synchronizes exponentially fast with some quantitative rates. Comments:|48 pages, 4 figures|\nSubjects:|Machine Learning (cs.LG); Analysis of PDEs (math.AP); Dynamical Systems (math.DS); Machine Learning (stat.ML)|\nCite as:|[arXiv:2504.14697](https://arxiv.org/abs/2504.14697)[cs.LG]|\n|(or[arXiv:2504.14697v2](https://arxiv.org/abs/2504.14697v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2504.14697](https://doi.org/10.48550/arXiv.2504.14697)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Zhengjiang Lin [[view email](https://arxiv.org/show-email/c0d5feb0/2504.14697)]\n**[[v1]](https://arxiv.org/abs/2504.14697v1)**Sun, 20 Apr 2025 18:21:34 UTC (506 KB)\n**[v2]**Wed, 30 Apr 2025 13:35:39 UTC (507 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Quantitative Clustering in Mean-Field Transformer Models, by Shi Chen and 3 other authors\n* [View PDF](https://arxiv.org/pdf/2504.14697)\n* [TeX Source](https://arxiv.org/src/2504.14697)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2504.14697&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2504.14697&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2025-04](https://arxiv.org/list/cs.LG/2025-04)\nChange to browse by:\n[cs](https://arxiv.org/abs/2504.14697?context=cs)\n[math](https://arxiv.org/abs/2504.14697?context=math)\n[math.AP](https://arxiv.org/abs/2504.14697?context=math.AP)\n[math.DS](https://arxiv.org/abs/2504.14697?context=math.DS)\n[stat](https://arxiv.org/abs/2504.14697?context=stat)\n[stat.ML](https://arxiv.org/abs/2504.14697?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2504.14697)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2504.14697)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2504.14697)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2504.14697)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Quantitative clustering in mean-field transformer models",
          "cleaned_query": "Quantitative clustering in mean-field transformer models"
        }
      ],
      "generated_ideas": [
        "Entropy-Controlled Sinkhorn Attention to Prevent Representation Collapse\nIntroduce an explicit entropy (or KL-to-uniform) regularizer on doubly stochastic Sinkhorn attention to tune how quickly tokens cluster, leveraging the mean-field clustering results as a stability target. Evaluate whether controlling attention entropy delays or shapes leader/cluster formation and improves long-context accuracy (e.g., retrieval-heavy QA) without degrading optimization.",
        "Multi-Head \u201cCluster Diversity\u201d Constraints for Sinkformers\nDesign a training objective that penalizes different heads collapsing to the same low-rank/Boolean-like attention patterns, inspired by the proven low-rank limit of self-attention dynamics. Implement a head-wise repulsion term (e.g., orthogonality between attention matrices or between induced cluster assignments) and test whether it increases expressive power and robustness on tasks where multiple relations must be tracked simultaneously.",
        "Learning Value-Matrix Spectra to Control Cluster Types\nBuild an architecture that parameterizes (or regularizes) the value projection so its spectrum matches desired dynamical regimes identified in the clustering theory (e.g., enforcing eigenvalue gaps or sign patterns). Concretely, add spectral penalties or low-rank-plus-diagonal decompositions for the value matrix and measure whether predicted limiting objects/cluster behaviors correlate with improved performance in parsing or structured prediction.",
        "Depth-as-Time Calibration: Predicting Optimal Layer Counts from Contraction Rates\nUse the exponential contraction rates from mean-field transformer models to derive a principled \u201csufficient depth\u201d criterion for a given data distribution and initialization smoothness. Implement a layer-adaptive stopping rule (or dynamic depth during training) that halts stacking when contraction-to-cluster saturates, then validate compute\u2013accuracy tradeoffs on translation and vision benchmarks.",
        "Wasserstein-Flow Regularized Transformers for Stable Long-Horizon Dynamics\nExploit the Sinkformer interpretation as discretized Wasserstein gradient flow by adding a loss that encourages successive layers to follow a smooth transport path (e.g., penalize large Wasserstein distances between token distributions across layers). Test whether this reduces training instabilities, improves calibration, and yields more predictable clustering behavior across depths and seeds.",
        "Mean-Field Theory for Cross-Attention: When Do Encoder\u2013Decoder Tokens Synchronize?\nExtend the interacting-particle/mean-field analysis from self-attention to cross-attention in encoder\u2013decoder Transformers, characterizing conditions under which decoder tokens synchronize around specific encoder \u201cleaders.\u201d Empirically test derived predictions on translation (alignment quality, fertility) and on multimodal tasks (text attending to image regions) using Sinkhorn vs SoftMax normalization.",
        "Token Clustering as a Compression Primitive: Cluster-and-Refine Transformers\nCreate an inference-time (or training-time) module that explicitly clusters tokens according to attention-induced dynamics (using Sinkhorn attention as a principled soft assignment), replaces each cluster by a prototype token, and then refines with a smaller Transformer. Evaluate whether this controlled clustering yields predictable compression rates and preserves performance on long-context classification and retrieval tasks.",
        "Spectral-Transport Co-Design: Jointly Learning Sinkhorn Temperature and Value Scaling\nPropose a coupled parameterization where Sinkhorn normalization strength (number of Sinkhorn iterations / entropic regularization) and value-matrix scaling are jointly scheduled to target specific contraction/cluster rates predicted by theory. Implement a scheduler learned by gradient-based meta-optimization to balance early mixing (diffusion-like) with later specialization (cluster formation), and benchmark on tasks sensitive to both global and local dependencies."
      ],
      "generated_ideas_raw": "1. **Entropy-Controlled Sinkhorn Attention to Prevent Representation Collapse**  \nIntroduce an explicit entropy (or KL-to-uniform) regularizer on doubly stochastic Sinkhorn attention to tune how quickly tokens cluster, leveraging the mean-field clustering results as a stability target. Evaluate whether controlling attention entropy delays or shapes leader/cluster formation and improves long-context accuracy (e.g., retrieval-heavy QA) without degrading optimization.\n\n2. **Multi-Head \u201cCluster Diversity\u201d Constraints for Sinkformers**  \nDesign a training objective that penalizes different heads collapsing to the same low-rank/Boolean-like attention patterns, inspired by the proven low-rank limit of self-attention dynamics. Implement a head-wise repulsion term (e.g., orthogonality between attention matrices or between induced cluster assignments) and test whether it increases expressive power and robustness on tasks where multiple relations must be tracked simultaneously.\n\n3. **Learning Value-Matrix Spectra to Control Cluster Types**  \nBuild an architecture that parameterizes (or regularizes) the value projection so its spectrum matches desired dynamical regimes identified in the clustering theory (e.g., enforcing eigenvalue gaps or sign patterns). Concretely, add spectral penalties or low-rank-plus-diagonal decompositions for the value matrix and measure whether predicted limiting objects/cluster behaviors correlate with improved performance in parsing or structured prediction.\n\n4. **Depth-as-Time Calibration: Predicting Optimal Layer Counts from Contraction Rates**  \nUse the exponential contraction rates from mean-field transformer models to derive a principled \u201csufficient depth\u201d criterion for a given data distribution and initialization smoothness. Implement a layer-adaptive stopping rule (or dynamic depth during training) that halts stacking when contraction-to-cluster saturates, then validate compute\u2013accuracy tradeoffs on translation and vision benchmarks.\n\n5. **Leader Token Identification via Boolean-Attention Convergence Diagnostics**  \nDevelop an online diagnostic that monitors attention matrices for convergence toward low-rank Boolean structure (as predicted in 1D) and uses it to identify emergent \u201cleader\u201d tokens early in inference. Turn this into an actionable method: prune non-leader tokens or compress attention to leader-centric edges, and quantify speedups and accuracy impacts on long sequences.\n\n6. **Wasserstein-Flow Regularized Transformers for Stable Long-Horizon Dynamics**  \nExploit the Sinkformer interpretation as discretized Wasserstein gradient flow by adding a loss that encourages successive layers to follow a smooth transport path (e.g., penalize large Wasserstein distances between token distributions across layers). Test whether this reduces training instabilities, improves calibration, and yields more predictable clustering behavior across depths and seeds.\n\n7. **Diffusion-to-Attention Hybrid: Heat-Kernel Initialization for Sinkhorn Attention**  \nInitialize (or partially tie) attention logits to approximate a heat kernel on token similarity graphs, directly aligning with the \u201cSinkformers operate a heat diffusion\u201d limit. Then learn residual attention on top, comparing convergence speed, sample efficiency, and the geometry of learned clusters on 3D shape classification and language modeling.\n\n8. **Mean-Field Theory for Cross-Attention: When Do Encoder\u2013Decoder Tokens Synchronize?**  \nExtend the interacting-particle/mean-field analysis from self-attention to cross-attention in encoder\u2013decoder Transformers, characterizing conditions under which decoder tokens synchronize around specific encoder \u201cleaders.\u201d Empirically test derived predictions on translation (alignment quality, fertility) and on multimodal tasks (text attending to image regions) using Sinkhorn vs SoftMax normalization.\n\n9. **Token Clustering as a Compression Primitive: Cluster-and-Refine Transformers**  \nCreate an inference-time (or training-time) module that explicitly clusters tokens according to attention-induced dynamics (using Sinkhorn attention as a principled soft assignment), replaces each cluster by a prototype token, and then refines with a smaller Transformer. Evaluate whether this controlled clustering yields predictable compression rates and preserves performance on long-context classification and retrieval tasks.\n\n10. **Spectral-Transport Co-Design: Jointly Learning Sinkhorn Temperature and Value Scaling**  \nPropose a coupled parameterization where Sinkhorn normalization strength (number of Sinkhorn iterations / entropic regularization) and value-matrix scaling are jointly scheduled to target specific contraction/cluster rates predicted by theory. Implement a scheduler learned by gradient-based meta-optimization to balance early mixing (diffusion-like) with later specialization (cluster formation), and benchmark on tasks sensitive to both global and local dependencies.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Entropy-Controlled Sinkhorn Attention to Prevent Representation Collapse\nIntroduce an explicit entropy (or KL-to-uniform) regularizer on doubly stochastic Sinkhorn attention to tune how quickly tokens",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Multi-Head \u201cCluster Diversity\u201d Constraints for Sinkformers\nDesign a training objective that penalizes different heads collapsing to the same low-rank/Boolean-like attention patterns, inspired by the p",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Learning Value-Matrix Spectra to Control Cluster Types\nBuild an architecture that parameterizes (or regularizes) the value projection so its spectrum matches desired dynamical regimes identified in th",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Depth-as-Time Calibration: Predicting Optimal Layer Counts from Contraction Rates\nUse the exponential contraction rates from mean-field transformer models to derive a principled \u201csufficient depth\u201d cri",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Wasserstein-Flow Regularized Transformers for Stable Long-Horizon Dynamics\nExploit the Sinkformer interpretation as discretized Wasserstein gradient flow by adding a loss that encourages successive la",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Mean-Field Theory for Cross-Attention: When Do Encoder\u2013Decoder Tokens Synchronize?\nExtend the interacting-particle/mean-field analysis from self-attention to cross-attention in encoder\u2013decoder Transfo",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Token Clustering as a Compression Primitive: Cluster-and-Refine Transformers\nCreate an inference-time (or training-time) module that explicitly clusters tokens according to attention-induced dynamics ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Spectral-Transport Co-Design: Jointly Learning Sinkhorn Temperature and Value Scaling\nPropose a coupled parameterization where Sinkhorn normalization strength (number of Sinkhorn iterations / entropic",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 67,
      "paper_title": "Exploring Diffusion Transformer Designs via Grafting",
      "contribution": "The paper introduces grafting as a method for efficiently modifying pretrained diffusion transformer architectures to explore new designs without extensive retraining.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 7,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10396,
      "output_tokens": 1028,
      "predecessor_details": [
        {
          "success": true,
          "title": "Denoising Diffusion Probabilistic Models in Six Simple Steps",
          "url": "https://arxiv.org/abs/2402.04384",
          "content": "[2402.04384] Denoising Diffusion Probabilistic Models in Six Simple Steps\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2402.04384\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2402.04384**(cs)\n[Submitted on 6 Feb 2024 ([v1](https://arxiv.org/abs/2402.04384v1)), last revised 10 Feb 2024 (this version, v2)]\n# Title:Denoising Diffusion Probabilistic Models in Six Simple Steps\nAuthors:[Richard E. Turner](https://arxiv.org/search/cs?searchtype=author&amp;query=Turner,+R+E),[Cristiana-Diana Diaconu](https://arxiv.org/search/cs?searchtype=author&amp;query=Diaconu,+C),[Stratis Markou](https://arxiv.org/search/cs?searchtype=author&amp;query=Markou,+S),[Aliaksandra Shysheya](https://arxiv.org/search/cs?searchtype=author&amp;query=Shysheya,+A),[Andrew Y. K. Foong](https://arxiv.org/search/cs?searchtype=author&amp;query=Foong,+A+Y+K),[Bruno Mlodozeniec](https://arxiv.org/search/cs?searchtype=author&amp;query=Mlodozeniec,+B)\nView a PDF of the paper titled Denoising Diffusion Probabilistic Models in Six Simple Steps, by Richard E. Turner and 4 other authors\n[View PDF](https://arxiv.org/pdf/2402.04384)> > Abstract:\n> Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. In this note, we distill down the formulation of the DDPM into six simple steps each of which comes with a clear rationale. We assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, Gaussian distributions, maximum likelihood estimation, and deep learning. Subjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2402.04384](https://arxiv.org/abs/2402.04384)[cs.LG]|\n|(or[arXiv:2402.04384v2](https://arxiv.org/abs/2402.04384v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2402.04384](https://doi.org/10.48550/arXiv.2402.04384)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Richard Turner [[view email](https://arxiv.org/show-email/66f8e9a9/2402.04384)]\n**[[v1]](https://arxiv.org/abs/2402.04384v1)**Tue, 6 Feb 2024 20:43:04 UTC (911 KB)\n**[v2]**Sat, 10 Feb 2024 19:19:34 UTC (911 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Denoising Diffusion Probabilistic Models in Six Simple Steps, by Richard E. Turner and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2402.04384)\n* [TeX Source](https://arxiv.org/src/2402.04384)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2402.04384&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2402.04384&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2024-02](https://arxiv.org/list/cs.LG/2024-02)\nChange to browse by:\n[cs](https://arxiv.org/abs/2402.04384?context=cs)\n[stat](https://arxiv.org/abs/2402.04384?context=stat)\n[stat.ML](https://arxiv.org/abs/2402.04384?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2402.04384)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2402.04384)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2402.04384)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's com",
          "original_query": "Denoising diffusion probabilistic models",
          "cleaned_query": "Denoising diffusion probabilistic models"
        },
        {
          "success": true,
          "title": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture",
          "url": "https://arxiv.org/abs/2310.12109",
          "content": "[2310.12109] Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2310.12109\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2310.12109**(cs)\n[Submitted on 18 Oct 2023]\n# Title:Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture\nAuthors:[Daniel Y. Fu](https://arxiv.org/search/cs?searchtype=author&amp;query=Fu,+D+Y),[Simran Arora](https://arxiv.org/search/cs?searchtype=author&amp;query=Arora,+S),[Jessica Grogan](https://arxiv.org/search/cs?searchtype=author&amp;query=Grogan,+J),[Isys Johnson](https://arxiv.org/search/cs?searchtype=author&amp;query=Johnson,+I),[Sabri Eyuboglu](https://arxiv.org/search/cs?searchtype=author&amp;query=Eyuboglu,+S),[Armin W. Thomas](https://arxiv.org/search/cs?searchtype=author&amp;query=Thomas,+A+W),[Benjamin Spector](https://arxiv.org/search/cs?searchtype=author&amp;query=Spector,+B),[Michael Poli](https://arxiv.org/search/cs?searchtype=author&amp;query=Poli,+M),[Atri Rudra](https://arxiv.org/search/cs?searchtype=author&amp;query=Rudra,+A),[Christopher R\u00e9](https://arxiv.org/search/cs?searchtype=author&amp;query=R\u00e9,+C)\nView a PDF of the paper titled Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture, by Daniel Y. Fu and 9 other authors\n[View PDF](https://arxiv.org/pdf/2310.12109)> > Abstract:\n> Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs. Comments:|NeurIPS 2023 (Oral)|\nSubjects:|Machine Learning (cs.LG)|\nCite as:|[arXiv:2310.12109](https://arxiv.org/abs/2310.12109)[cs.LG]|\n|(or[arXiv:2310.12109v1](https://arxiv.org/abs/2310.12109v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2310.12109](https://doi.org/10.48550/arXiv.2310.12109)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Daniel Y. Fu [[view email](https://arxiv.org/show-email/da3a8ddb/2310.12109)]\n**[v1]**Wed, 18 Oct 2023 17:06:22 UTC (1,346 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture, by Daniel Y. Fu and 9 other authors\n* [View PDF](https://arxiv.org/pdf/2310.12109)\n* [TeX Source](https://arxiv.org/src/2310.12109)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2310.12109&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2310.12109&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2023-10](https://arxiv.org/list/cs.LG/2023-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/2310.12109?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2310.12109)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2310.12109)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2310.12109)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and",
          "original_query": "Monarch mixer: A simple sub-quadratic gemm-based architecture",
          "cleaned_query": "Monarch mixer: A simple sub-quadratic gemm-based architecture"
        },
        {
          "success": true,
          "title": "[1503.02531] Distilling the Knowledge in a Neural Network - arXiv",
          "url": "https://arxiv.org/abs/1503.02531",
          "content": "[1503.02531] Distilling the Knowledge in a Neural Network\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[stat](https://arxiv.org/list/stat/recent)&gt;arXiv:1503.02531\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Statistics \\> Machine Learning\n**arXiv:1503.02531**(stat)\n[Submitted on 9 Mar 2015]\n# Title:Distilling the Knowledge in a Neural Network\nAuthors:[Geoffrey Hinton](https://arxiv.org/search/stat?searchtype=author&amp;query=Hinton,+G),[Oriol Vinyals](https://arxiv.org/search/stat?searchtype=author&amp;query=Vinyals,+O),[Jeff Dean](https://arxiv.org/search/stat?searchtype=author&amp;query=Dean,+J)\nView a PDF of the paper titled Distilling the Knowledge in a Neural Network, by Geoffrey Hinton and 2 other authors\n[View PDF](https://arxiv.org/pdf/1503.02531)> > Abstract:\n> A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel. Comments:|NIPS 2014 Deep Learning Workshop|\nSubjects:|Machine Learning (stat.ML); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)|\nCite as:|[arXiv:1503.02531](https://arxiv.org/abs/1503.02531)[stat.ML]|\n|(or[arXiv:1503.02531v1](https://arxiv.org/abs/1503.02531v1)[stat.ML]for this version)|\n|[https://doi.org/10.48550/arXiv.1503.02531](https://doi.org/10.48550/arXiv.1503.02531)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Oriol Vinyals [[view email](https://arxiv.org/show-email/c8a7998e/1503.02531)]\n**[v1]**Mon, 9 Mar 2015 15:44:49 UTC (18 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Distilling the Knowledge in a Neural Network, by Geoffrey Hinton and 2 other authors\n* [View PDF](https://arxiv.org/pdf/1503.02531)\n* [TeX Source](https://arxiv.org/src/1503.02531)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\nstat.ML\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1503.02531&amp;function=prev&amp;context=stat.ML) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1503.02531&amp;function=next&amp;context=stat.ML)\n[new](https://arxiv.org/list/stat.ML/new)|[recent](https://arxiv.org/list/stat.ML/recent)|[2015-03](https://arxiv.org/list/stat.ML/2015-03)\nChange to browse by:\n[cs](https://arxiv.org/abs/1503.02531?context=cs)\n[cs.LG](https://arxiv.org/abs/1503.02531?context=cs.LG)\n[cs.NE](https://arxiv.org/abs/1503.02531?context=cs.NE)\n[stat](https://arxiv.org/abs/1503.02531?context=stat)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1503.02531)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1503.02531)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1503.02531)\n### [14 blog links](https://arxiv.org/tb/1503.02531)\n([what is this?](https://info.arxiv.org/help/trackback.html))\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1503.02531)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Distilling the knowledge in a neural network",
          "cleaned_query": "Distilling the knowledge in a neural network"
        },
        {
          "success": true,
          "title": "[PDF] Revisiting Scaling Laws for Language Models: The Role of Data ...",
          "url": "https://aclanthology.org/2025.acl-long.1163.pdf",
          "content": "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 23881\u201323899\nJuly 27 - August 1, 2025 \u00a92025 Association for Computational Linguistics\nRevisiting Scaling Laws for Language Models: The Role of Data Quality\nand Training Strategies\nZhengyu Chen1*, Siqi Wang 2*, Teng Xiao3, Yudong Wang 4,\nShiqi Chen 5, Xunliang Cai1, Junxian He 5, Jingang Wang1\n1 Meituan Inc. 2 The University of Hong Kong 3 Pennsylvania State University\n4 Peking University 5 The Hong Kong University of Science and Technology\n{chenzhengyu04,wangjingang02}@meituan.com\nAbstract\nTraditional scaling laws in natural language\nprocessing suggest that increasing model size\nand training data enhances performance. How\u0002ever, recent studies reveal deviations, partic\u0002ularly in large language models, where per\u0002formance improvements decelerate\u2014a phe\u0002nomenon known as sub-scaling. This paper\nrevisits these scaling laws by examining the\nimpact of data quality and training strategies\non model performance. Through extensive em\u0002pirical analysis of over 400 models, we iden\u0002tify high data density and non-optimal resource\nallocation as key factors contributing to sub\u0002scaling. High data density leads to diminishing\nreturns due to redundant information, while\noptimal resource allocation is crucial for sus\u0002tained performance improvements. We propose\na sub-optimal scaling law that better predicts\nperformance in sub-scaling regimes, highlight\u0002ing the importance of data quality and diversity.\n1 Introduction\nThe rapid advancement in natural language pro\u0002cessing (NLP) has been significantly driven by the\ndevelopment of increasingly large language mod\u0002els. These models, such as LLaMA (Touvron et al.,\n2023), Chinchilla (70B) (Hoffmann et al., 2022),\nGopher (280B) (Rae et al., 2021), and Megatron\u0002Turing NLG (530B) (Smith et al., 2022), have\nset new benchmarks across a variety of linguistic\ntasks. There is also a growing body of research on\nscaling strategies (McCandlish et al., 2018; Yang\net al., 2022, 2023; Wang et al., 2024), which could\nbe beneficial for large language models (LLMs).\nThe conventional wisdom suggests that augmenting\nmodel size and corresponding training data gener\u0002ally results in enhanced performance. This trend\nhas led to the popularization of a \u2019bigger is better\u2019\nparadigm within the field. This scaling up has been\n*Equal Contribution.\ndriven by the empirical observation that larger mod\u0002els trained on vast amounts of data tend to perform\nbetter on various natural language processing tasks\n(Komatsuzaki, 2019; Hernandez et al., 2022a).\nHowever, recent empirical studies (Hernandez\net al., 2022a; Hu et al., 2023; Porian et al., 2024;\nMuennighoff et al., 2024) have observed devia\u0002tions from this expected trend, particularly in the\ncontext of exceptionally large language models.\nThese deviations manifest as sub-scaling growth,\nwhere the rate of performance improvement de\u0002celerates as the model or dataset size continues to\nincrease. Specifically, (Hernandez et al., 2022a;\nMuennighoff et al., 2024) observe that sub-scaling\noccurs in scenarios involving repeated training data,\nleading to diminishing returns in performance. (Hu\net al., 2023) highlight that sub-scaling is particu\u0002larly pronounced in tasks requiring complex reason\u0002ing or multi-step processes. Furthermore, (Porian\net al., 2024) find that sub-scaling exists under non\u0002optimal training strategies with sub-optimal hyper\u0002parameters. Figure 1 provides a visualization of the\ndiminishing returns, clearly showing that as train\u0002ing progresses, the actual training loss values tend\nto be higher than those extrapolated from earlier\nstages, indicating how traditional scaling laws fall\nshort when dealing with extensive datasets and sug\u0002gests the need for a modified approach. Moreover,\n(Hernandez et al., 2022a; Muennighoff et al., 2024)\nhave similar sub-scaling observations in repeated\ndata and non-optimal training strategy. However,\nthere is a lack of systematic research on the sub\u0002scaling behavior of large language models (LLMs).\nFurther extending this observation to model per\u0002formance, Figure 2 displays the results of our tests\non the performance scaling law (Yang et al., 2024;\nIsik et al., 2024; Wu and Tang, 2024) with LLaMA\n2 and LLaMA 3 models. Despite LLaMA 3 incor\u0002porating advanced training strategies and improved\ndata quality, the performance improvements from\nLLaMA 2 to LLaMA 3 decelerate as the training\n23881\nFigure 1: Sub-scaling phenomenon in loss. Scaling law fits well with 5B training tokens, but as tokens increase,\nloss curve shows greater curvature, and fitting accuracy decreases, especially for larger models.\nFigure 2: (left) LLaMA 2\u2019s scaling curve outperforms LLaMA 3\u2019s, despite LLaMA 3\u2019s advanced strategies. (right)\nHigher-density datasets lead to sub-scaling. We propose metric density to measure redundancy and diversity: higher\ndensity indicates more redundancy and less diversity, leading to sub-scaling (see Section 2.1).\n10\n7 10\n8 109 1010\nModel Size\n2.5\n3.0\n3.5\n4.0\nLoss\n1e+18\n3e+18\n6e+18\n1e+19\n3e+19\n6e+19\n1e+20\n3e+20\nOptimal Model/Data Allocation\nScaling Law\n10\n6 10\n7 10\n8 109 1010 1011 1012\nModel Size\n2.0\n2.5\n3.0\n3.5\n4.0\nLoss\nLlama3 80B-1.5T\nLlama3 8B-15T\n1e+18\n3e+18\n6e+18\n1e+19\n3e+19\n6e+19\n1e+20\n3e+20\nScaling Law\nFigure 3: (left) With a fixed total compute budget, we adjust the model-to-data allocation ratio and plot the training\nloss against model size. A black curve connects the minimum points of each curve, illustrating the optimal\nChinchilla law. (right) However, current large language models, such as Llama3 8B, are trained on 15T tokens, with\na model-to-data allocation strategy that significantly deviates from the optimal Chinchilla law.\nflops increase, LLaMA 2 with 70B parameters out\u0002performs LLaMA 3 with 8B parameters.This dis\u0002crepancy, depicted in Figure 2, underscores the\ninadequacies of traditional scaling laws. Addition\u0002ally, when the scale of training data surpasses an\noptimal threshold relative to the available computa\u0002tional resources, sub-scaling law happens with such\nover-training (Gadre et al., 2024), potentially lead\u0002ing to diminishing returns in model performance.\nMoreover, there is a lack of understanding of the\ntraining dynamics of large language models and\nthe sub-scaling laws governing the training strate\u0002gies of language models. This motivates the ques\u0002tion: Under what conditions do sub-scaling laws\ninfluence the performance and efficiency of large\nlanguage models?\nThis study aims to systematically investigate the\nsub-scaling law phenomenon through an extensive\nempirical analysis involving over 400 models, rang\u0002ing from 20 million to 7 billion parameters, with\nvarying datasets and training strategies. Our find\u0002ings indicate that sub-scaling laws arise primarily\n23882\nfrom high data density and non-optimal training\nresource allocations. Specifically, we observed that\nboth factors contribute more significantly to per\u0002formance deceleration than previously anticipated.\nWe examine the sub-scaling phenomenon from two\nperspectives: data density and training strategy.\nHigh data density leads to diminishing marginal\ngains in performance as shown in Figure 2, while\noptimal resource allocation is crucial for sustain\u0002ing performance improvements as shown in Figure\n3. Further, we propose a sub-optimal scaling law\nthat generalizes the Chinchilla scaling law (Hoff\u0002mann et al., 2022) to better predict performance\nand loss in sub-scaling regimes. Our analysis re\u0002veals that the quality and diversity of training data\nare paramount, often outweighing the benefits of\nmere scale in model size. Key findings from our\nstudy include:\n1. Sub-Scaling Law Phenomenon: Traditional\nscaling laws fail to predict performance improve\u0002ments for large models and datasets. The per\u0002formance gains decelerate, leading to sub-scaling\ngrowth, especially in high data density scenarios\nand with non-optimal resource allocation.\n2. Training Strategies under Over-Training:\nCompared to Gadre et a",
          "original_query": "Scaling laws for neural language models",
          "cleaned_query": "Scaling laws for neural language models"
        },
        {
          "success": true,
          "title": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models",
          "url": "https://arxiv.org/abs/2408.15237",
          "content": "[2408.15237] The Mamba in the Llama: Distilling and Accelerating Hybrid Models[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2408.15237\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2408.15237**(cs)\n[Submitted on 27 Aug 2024 ([v1](https://arxiv.org/abs/2408.15237v1)), last revised 27 Jun 2025 (this version, v4)]\n# Title:The Mamba in the Llama: Distilling and Accelerating Hybrid Models\nAuthors:[Junxiong Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+J),[Daniele Paliotta](https://arxiv.org/search/cs?searchtype=author&amp;query=Paliotta,+D),[Avner May](https://arxiv.org/search/cs?searchtype=author&amp;query=May,+A),[Alexander M. Rush](https://arxiv.org/search/cs?searchtype=author&amp;query=Rush,+A+M),[Tri Dao](https://arxiv.org/search/cs?searchtype=author&amp;query=Dao,+T)\nView a PDF of the paper titled The Mamba in the Llama: Distilling and Accelerating Hybrid Models, by Junxiong Wang and 4 other authors\n[View PDF](https://arxiv.org/pdf/2408.15237)[HTML (experimental)](https://arxiv.org/html/2408.15237v4)> > Abstract:\n> Linear RNN architectures, like Mamba, can be competitive with Transformer models in language modeling while having advantageous deployment characteristics. Given the focus on training large-scale Transformer models, we consider the challenge of converting these pretrained models for deployment. We demonstrate that it is feasible to distill large Transformers into linear RNNs by reusing the linear projection weights from attention layers with academic GPU resources. The resulting hybrid model, which incorporates a quarter of the attention layers, achieves performance comparable to the original Transformer in chat benchmarks and outperforms open-source hybrid Mamba models trained from scratch with trillions of tokens in both chat benchmarks and general benchmarks. Moreover, we introduce a hardware-aware speculative decoding algorithm that accelerates the inference speed of Mamba and hybrid models. Overall we show how, with limited computation resources, we can remove many of the original attention layers and generate from the resulting model more efficiently. Our top-performing model, distilled from Llama3-8B-Instruct, achieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and 7.35 on MT-Bench, surpassing the best 8B scale instruction-tuned linear RNN model. We also find that the distilled model has natural length extrapolation, showing almost perfect accuracy in the needle-in-a-haystack test at 20x the distillation length. Code and pre-trained checkpoints are open-sourced at [> this https URL\n](https://github.com/jxiw/MambaInLlama)> and [> this https URL\n](https://github.com/itsdaniele/speculative_mamba)> . Comments:|NeurIPS 2024. v4 updates: mention concurrent work of speculative decoding for SSM|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2408.15237](https://arxiv.org/abs/2408.15237)[cs.LG]|\n|(or[arXiv:2408.15237v4](https://arxiv.org/abs/2408.15237v4)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2408.15237](https://doi.org/10.48550/arXiv.2408.15237)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Junxiong Wang [[view email](https://arxiv.org/show-email/7b91cd33/2408.15237)]\n**[[v1]](https://arxiv.org/abs/2408.15237v1)**Tue, 27 Aug 2024 17:56:11 UTC (3,726 KB)\n**[[v2]](https://arxiv.org/abs/2408.15237v2)**Thu, 26 Dec 2024 05:27:51 UTC (4,508 KB)\n**[[v3]](https://arxiv.org/abs/2408.15237v3)**Wed, 8 Jan 2025 20:34:02 UTC (4,500 KB)\n**[v4]**Fri, 27 Jun 2025 07:54:57 UTC (935 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled The Mamba in the Llama: Distilling and Accelerating Hybrid Models, by Junxiong Wang and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2408.15237)\n* [HTML (experimental)](https://arxiv.org/html/2408.15237v4)\n* [TeX Source](https://arxiv.org/src/2408.15237)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2408.15237&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2408.15237&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2024-08](https://arxiv.org/list/cs.LG/2024-08)\nChange to browse by:\n[cs](https://arxiv.org/abs/2408.15237?context=cs)\n[cs.AI](https://arxiv.org/abs/2408.15237?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2408.15237)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2408.15237)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2408.15237)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to In",
          "original_query": "The mamba in the llama: Distilling and accelerating hybrid models",
          "cleaned_query": "The mamba in the llama: Distilling and accelerating hybrid models"
        }
      ],
      "generated_ideas": [
        "Hybrid Attention\u2013SSM LLMs with Diffusion-Based Layer Replacement Search\nBuild on \u201cMamba in the Llama\u201d by using a diffusion-style discrete-to-continuous relaxation to search which attention layers to keep vs. replace with linear RNN (SSM) layers under a compute/latency constraint. The key deliverable is an automated procedure that outputs a Pareto frontier of hybrid architectures (attention count, placement, width) that preserve instruction-following while improving throughput.",
        "Data-Density-Aware Knowledge Distillation for Sub-Scaling Regimes\nUse the \u201cdata density\u201d redundancy metric from the scaling-law paper to *weight* distillation examples: assign higher weight to rare/low-density regions and lower weight to redundant high-density text. Evaluate whether a student can match a teacher with fewer tokens and less sub-scaling, yielding a practical training strategy that couples dataset curation and distillation loss weighting.",
        "Speculative Decoding for Diffusion Models Using a Linear-RNN Proposal Network\nAdapt the hardware-aware speculative decoding idea to diffusion sampling: a fast Mamba/SSM proposal predicts multiple denoising updates (or latent residuals) that are verified/corrected by a larger diffusion model. This would create a deployment-friendly diffusion sampler with measurable wall-clock gains on GPUs while maintaining exactness (or bounded error) via accept/reject or correction steps.",
        "Layer-Transfer Distillation: Reusing Attention Projections to Initialize Monarch Mixers\nExtend \u201creuse attention projection weights\u201d by mapping pretrained QKV/O projections into equivalent (or approximating) Monarch structured matrices, then fine-tuning under distillation. This yields an actionable conversion pipeline from Transformer checkpoints to Monarch-based mixers, testing whether structured GEMM-friendly mixing can inherit capabilities with minimal retraining compute.",
        "Sub-Optimal Scaling Laws for Architecture Conversion (Transformer \u2192 Hybrid/Monarch/SSM)\nFit a scaling model that predicts performance loss from *conversion knobs* (fraction of attention removed, Monarch rank/structure size, SSM state size) under fixed compute and data density. The outcome is a practical \u201cconversion scaling law\u201d that tells practitioners how much data/fine-tuning compute is needed to regain baseline quality after accelerating architectural changes.",
        "Diffusion-Style Curriculum for Training Efficient Sequence Models Under Data Redundancy\nUse the DDPM \u201cnoise schedule\u201d idea as a curriculum on text: progressively corrupt inputs (token dropout/shuffle/semantic noise) and train Monarch/Mamba hybrids to denoise/reconstruct, emphasizing diverse low-density samples late in training. This provides a concrete training strategy to combat sub-scaling by forcing models to learn non-redundant structure rather than memorizing repeated high-density patterns.",
        "Specialist Distillation for Hybrid LLMs Targeting Failure Modes (Reasoning, Long Context, Tool Use)\nRevive Hinton\u2019s \u201cspecialist models\u201d by training lightweight specialists (e.g., small attention modules or small diffusion-based refiners) for specific confusion sets\u2014multi-step reasoning, long-context retrieval, or tool-call formatting\u2014then distill their behavior into a main hybrid Mamba/Transformer model. The contribution is a modular, parallelizable distillation framework that improves targeted capabilities without scaling the full model or adding quadratic attention everywhere."
      ],
      "generated_ideas_raw": "1. **Monarch-Parameterized Diffusion Backbones for Long-Sequence Generative Modeling**  \n   Replace standard dense linear layers inside DDPM U-Nets (or 1D diffusion transformers) with Monarch Mixer blocks to reduce compute from quadratic to sub-quadratic in both sequence length and channel dimension. Benchmark on long-context audio and token/latent sequence diffusion (e.g., waveform diffusion, video latents) to quantify speed/quality tradeoffs and identify where structured mixing harms denoising fidelity.\n\n2. **Distilling Multi-Step DDPM Samplers into Few-Step Students via Soft-Target Denoising**  \n   Train a compact student diffusion model to match the *teacher\u2019s* conditional Gaussian reverse transitions (means/variances) across timesteps, using Hinton-style temperature-scaled soft targets over predicted noise/velocity and step-wise KL objectives. The actionable contribution is a distillation recipe that yields high-quality 4\u20138 step samplers without adversarial losses, directly aligned with the \u201csix steps\u201d DDPM construction.\n\n3. **Hybrid Attention\u2013SSM LLMs with Diffusion-Based Layer Replacement Search**  \n   Build on \u201cMamba in the Llama\u201d by using a diffusion-style discrete-to-continuous relaxation to search which attention layers to keep vs. replace with linear RNN (SSM) layers under a compute/latency constraint. The key deliverable is an automated procedure that outputs a Pareto frontier of hybrid architectures (attention count, placement, width) that preserve instruction-following while improving throughput.\n\n4. **Data-Density-Aware Knowledge Distillation for Sub-Scaling Regimes**  \n   Use the \u201cdata density\u201d redundancy metric from the scaling-law paper to *weight* distillation examples: assign higher weight to rare/low-density regions and lower weight to redundant high-density text. Evaluate whether a student can match a teacher with fewer tokens and less sub-scaling, yielding a practical training strategy that couples dataset curation and distillation loss weighting.\n\n5. **Speculative Decoding for Diffusion Models Using a Linear-RNN Proposal Network**  \n   Adapt the hardware-aware speculative decoding idea to diffusion sampling: a fast Mamba/SSM proposal predicts multiple denoising updates (or latent residuals) that are verified/corrected by a larger diffusion model. This would create a deployment-friendly diffusion sampler with measurable wall-clock gains on GPUs while maintaining exactness (or bounded error) via accept/reject or correction steps.\n\n6. **Causal Monarch Diffusion for Autoregressive-Like Text or Code Generation**  \n   Combine Monarch Mixer\u2019s causal parameterization (polynomial interpolation view) with a diffusion-on-tokens (or diffusion-in-embedding) generator that enforces causality without quadratic masking. The contribution is a new text/code diffusion architecture that scales to 32k\u2013128k contexts using sub-quadratic causal mixing, plus an empirical comparison against causal Transformers and SSMs.\n\n7. **Layer-Transfer Distillation: Reusing Attention Projections to Initialize Monarch Mixers**  \n   Extend \u201creuse attention projection weights\u201d by mapping pretrained QKV/O projections into equivalent (or approximating) Monarch structured matrices, then fine-tuning under distillation. This yields an actionable conversion pipeline from Transformer checkpoints to Monarch-based mixers, testing whether structured GEMM-friendly mixing can inherit capabilities with minimal retraining compute.\n\n8. **Sub-Optimal Scaling Laws for Architecture Conversion (Transformer \u2192 Hybrid/Monarch/SSM)**  \n   Fit a scaling model that predicts performance loss from *conversion knobs* (fraction of attention removed, Monarch rank/structure size, SSM state size) under fixed compute and data density. The outcome is a practical \u201cconversion scaling law\u201d that tells practitioners how much data/fine-tuning compute is needed to regain baseline quality after accelerating architectural changes.\n\n9. **Diffusion-Style Curriculum for Training Efficient Sequence Models Under Data Redundancy**  \n   Use the DDPM \u201cnoise schedule\u201d idea as a curriculum on text: progressively corrupt inputs (token dropout/shuffle/semantic noise) and train Monarch/Mamba hybrids to denoise/reconstruct, emphasizing diverse low-density samples late in training. This provides a concrete training strategy to combat sub-scaling by forcing models to learn non-redundant structure rather than memorizing repeated high-density patterns.\n\n10. **Specialist Distillation for Hybrid LLMs Targeting Failure Modes (Reasoning, Long Context, Tool Use)**  \n   Revive Hinton\u2019s \u201cspecialist models\u201d by training lightweight specialists (e.g., small attention modules or small diffusion-based refiners) for specific confusion sets\u2014multi-step reasoning, long-context retrieval, or tool-call formatting\u2014then distill their behavior into a main hybrid Mamba/Transformer model. The contribution is a modular, parallelizable distillation framework that improves targeted capabilities without scaling the full model or adding quadratic attention everywhere.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Hybrid Attention\u2013SSM LLMs with Diffusion-Based Layer Replacement Search\nBuild on \u201cMamba in the Llama\u201d by using a diffusion-style discrete-to-continuous relaxation to search which attention layers to k",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Data-Density-Aware Knowledge Distillation for Sub-Scaling Regimes\nUse the \u201cdata density\u201d redundancy metric from the scaling-law paper to *weight* distillation examples: assign higher weight to rare/lo",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Speculative Decoding for Diffusion Models Using a Linear-RNN Proposal Network\nAdapt the hardware-aware speculative decoding idea to diffusion sampling: a fast Mamba/SSM proposal predicts multiple deno",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Layer-Transfer Distillation: Reusing Attention Projections to Initialize Monarch Mixers\nExtend \u201creuse attention projection weights\u201d by mapping pretrained QKV/O projections into equivalent (or approxim",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Sub-Optimal Scaling Laws for Architecture Conversion (Transformer \u2192 Hybrid/Monarch/SSM)\nFit a scaling model that predicts performance loss from *conversion knobs* (fraction of attention removed, Monar",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Diffusion-Style Curriculum for Training Efficient Sequence Models Under Data Redundancy\nUse the DDPM \u201cnoise schedule\u201d idea as a curriculum on text: progressively corrupt inputs (token dropout/shuffle/",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Specialist Distillation for Hybrid LLMs Targeting Failure Modes (Reasoning, Long Context, Tool Use)\nRevive Hinton\u2019s \u201cspecialist models\u201d by training lightweight specialists (e.g., small attention modul",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 68,
      "paper_title": "Generalized Gradient Norm Clipping & Non-Euclidean $(L_0,L_1)$-Smoothness",
      "contribution": "The paper introduces a novel hybrid optimization method that combines steepest descent and conditional gradient approaches under a generalized notion of (L0,L1)-smoothness, facilitating efficient training in non-Euclidean spaces.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 4,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": true,
      "matching_idea_idx": 8,
      "input_tokens": 12468,
      "output_tokens": 1089,
      "predecessor_details": [
        {
          "success": true,
          "title": "The Frank-Wolfe algorithm: a short introduction",
          "url": "https://arxiv.org/abs/2311.05313",
          "content": "# Mathematics > Optimization and Control\n\n**arXiv:2311.05313** (math)\n\n\\[Submitted on 9 Nov 2023 ( [v1](https://arxiv.org/abs/2311.05313v1)), last revised 28 Nov 2023 (this version, v3)\\]\n\n# Title:The Frank-Wolfe algorithm: a short introduction\n\nAuthors: [Sebastian Pokutta](https://arxiv.org/search/math?searchtype=author&query=Pokutta,+S)\n\nView a PDF of the paper titled The Frank-Wolfe algorithm: a short introduction, by Sebastian Pokutta\n\n[View PDF](https://arxiv.org/pdf/2311.05313)\n\n> Abstract:In this paper we provide an introduction to the Frank-Wolfe algorithm, a method for smooth convex optimization in the presence of (relatively) complicated constraints. We will present the algorithm, introduce key concepts, and establish important baseline results, such as e.g., primal and dual convergence. We will also discuss some of its properties, present a new adaptive step-size strategy as well as applications.\n\n| | |\n| --- | --- |\n| Comments: | Introductory article for the Jahresbericht der Deutschen Mathematiker Vereinigung |\n| Subjects: | Optimization and Control (math.OC) |\n| Cite as: | [arXiv:2311.05313](https://arxiv.org/abs/2311.05313) \\[math.OC\\] |\n| | (or [arXiv:2311.05313v3](https://arxiv.org/abs/2311.05313v3) \\[math.OC\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2311.05313](https://doi.org/10.48550/arXiv.2311.05313) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Sebastian Pokutta \\[ [view email](https://arxiv.org/show-email/50a0520b/2311.05313)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2311.05313v1)**\nThu, 9 Nov 2023 12:18:50 UTC (1,702 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2311.05313v2)**\nSun, 19 Nov 2023 21:02:24 UTC (1,701 KB)\n\n**\\[v3\\]**\nTue, 28 Nov 2023 20:13:51 UTC (1,933 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled The Frank-Wolfe algorithm: a short introduction, by Sebastian Pokutta\n\n- [View PDF](https://arxiv.org/pdf/2311.05313)\n- [TeX Source](https://arxiv.org/src/2311.05313)\n- [Other Formats](https://arxiv.org/format/2311.05313)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nmath.OC\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2311.05313&function=prev&context=math.OC)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2311.05313&function=next&context=math.OC)\n\n[new](https://arxiv.org/list/math.OC/new) \\| [recent](https://arxiv.org/list/math.OC/recent) \\| [2023-11](https://arxiv.org/list/math.OC/2023-11)\n\nChange to browse by:\n\n[math](https://arxiv.org/abs/2311.05313?context=math)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2311.05313)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2311.05313)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2311.05313)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2311.05313&description=The Frank-Wolfe algorithm: a short introduction) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2311.05313&title=The Frank-Wolfe algorithm: a short introduction)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2311.05313) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Frank-Wolfe Algorithm: A Survey",
          "cleaned_query": "Frank-Wolfe Algorithm: A Survey"
        },
        {
          "success": true,
          "title": "[PDF] Trade-offs of Local SGD at Scale: An Empirical Study",
          "url": "https://josejg.com/papers/psgd-postlocal_neurips2020.pdf",
          "content": "Trade-offs of Local SGD at Scale: An Empirical Study\nTrade-offs of Local SGD at Scale: An Empirical Study\nJose Javier Gonzalez Ortiz josejg@mit.edu\nMIT CSAIL\nJonathan Frankle jfrankle@mit.edu\nMIT CSAIL\nMike Rabbat mikerabbat@fb.com\nFacebook AI Research\nAri Morcos arimorcos@fb.com\nFacebook AI Research\nNicolas Ballas ballasn@fb.com\nFacebook AI Research\nAbstract\nAs datasets and models become increasingly large, distributed training has become a\nnecessary component to allow deep neural networks to train in reasonable amounts of time.\nHowever, distributed training can have substantial communication overhead that hinders its\nscalability. One strategy for reducing this overhead is to perform multiple unsynchronized\nSGD steps independently on each worker between synchronization steps, a technique known\nas local SGD. We conduct a comprehensive empirical study of local SGD and related methods\non a large scale image classification task. We find that performing local SGD comes at a price:\nlower communication costs (and thereby faster training) are accompanied by lower accuracy.\nThis finding is in contrast from the smaller-scale experiments in prior work, suggesting that\nlocal SGD encounters challenges at scale. We further show that incorporating the slow\nmomentum framework of Wang et al. (2020) consistently improves accuracy without requiring\nadditional communication, hinting at future directions for potentially escaping this trade-off.\nKeywords: Deep Learning, Distributed Optimization, Local SGD, Convolutional Neural\nNetworks\n1. Introduction\nAs datasets and models continue to grow in size, it has become a common practice to train\ndeep neural networks in a distributed manner across multiple hardware workers Goyal et al.\n(2017); Shallue et al. (2018). Most deep learning models are currently optimized using some\nvariant of stochastic gradient descent Robbins and Monro (1951), often using a mini-batch\napproach Bottou (2010); Dekel et al. (2012). In distributed scenarios, the communication\noverhead necessary to synchronize gradients between workers can quickly dominate the time\nnecessary to compute the model updates, hindering the scalability of this approach. Moreover,\nbecause of the serial nature of neural network training, all the nodes must wait until the\nsynchronization completes, and performance is therefore dependent on the slowest node Dutta\net al. (2018); Ferdinand et al. (2020).\nThese issues have motivated the development of optimization algorithms that reduce the\namount of communication between workers. A simple yet practical example islocal SGD (closely\nrelated to federated averaging (McMahan et al., 2017)), where, instead of synchronizing the\n1\n0\n200\n400\n600\nTime [ms]\nK = 8 K = 16 K = 32 K = 64 K = 128\nResNet-50\nK = 256\n1 2 4 8\nH\n0\n200\n400\n600\nTime [ms]\n1 2 4 8\nH\n1 2 4 8\nH\n1 2 4 8\nH\n1 2 4 8\nH\n1 2 4 8\nH\nResNet-101\nCommunication\nOptimizer\nBackward Pass\nForward Pass\nData Loading\nFigure 1: Breakdown of the average wall-clock time per iteration during the training process.\nResults are reported for various numbers of workers (K) and numbers of local steps (H). Every\nnode has 8 workers. The main difference is the communication time, which decreases as we\nreduce the frequency of model averaging. For minibatch SGD (which we label as H = 1) with\nmultiple nodes (K >8), the communication time dominates other parts of training.\ngradients at every iteration, each worker performs multiple SGD steps locally and then averages\nthe model weights across all workers Zhang et al. (2016). Local SGD has been shown to have\ngood optimization properties from a theoretical standpoint Stich (2018); Zhang et al. (2016);\nWoodworth et al. (2020). However, while local SGD does speed up training, the resulting models\nare often less accurate compared to a synchronous minibatch SGD baseline (Lin et al., 2018).\nPost-local SGD is a variant of local SGD introduced by Lin et al. (2018) with the goal\nof remedying these problems. Post-local SGD divides training into two phases. In the first\nphase, workers perform synchronous minibatch SGD; in the second, they switch to local SGD.\nLin et al. claim that this approach generalizes better on large batch training than both local\nSGD and minibatch SGD while reducing communication for the second phase of training. The\nmajority of the analysis of local SGD and post-local SGD reported in Lin et al. (2018) is on\nCIFAR-10 Krizhevsky et al. (2009).\nMotivated by these results, we perform a thorough analysis of local SGD and post-local\nSGD on the ImageNet-1k Russakovsky et al. (2015) classification task, a de facto benchmark\nfor large-scale vision classification problems. As Figure 1 shows, inter-node communication\ncan dominate training time, becoming a bottleneck in the training process. We complement\nthe analysis of Lin et al. (2018), studying how the choice of learning rate schedule and the point\nat which to switch phases affect the generalization accuracy of models trained with post-local\nSGD. We find that post-local SGD at ImageNet-scale is a double-edged sword: decreases in\ncommunication costs (by increasing the number of local steps) are accompanied by decreases\nin accuracy. As a result, practitioners interested in post-local SGD must weigh the trade-offs\nbetween training speedup and reduction in the accuracy of the final model. Looking ahead,\nour analysis of the interaction between post-local SGD, learning rates, and momentum points\ntoward potential opportunities to escape these trade-offs.\nContributions. Our main contributions are as follows:\n1. We perform a comprehensive empirical study on ImageNet that identifies previously\nunreported scalability limitations of local and post-local SGD. Our analysis highlights\nhow, when compared to the fully synchronous baseline, local and post-local SGD suffer\nfrom non-trivial accuracy drops as workers or local steps increase.\n2\nTrade-offs of Local SGD at Scale: An Empirical Study\n2. Our analysis is the first to identify that post-local SGD performance heavily relies on\nthe choice of hyperparameters, including learning rate schedule and switching point.\n3. We show that using slow momentum Wang et al. (2020) together with post-local SGD\nachieves a better quality-performance trade-off.\n4. We show that switching to local SGD has a regularization effect on optimization that\nis only beneficial in the short term, suggesting it is always better to make the switch\nlater in training.\n2. Background and Related Work\nMinibatch SGD. Neural networks are typically trained with minibatch SGD. In minibatch\nSGD, the dataset D={(xi,yi)}i\u2208[N]is divided into non-overlapping subsets of size B known as\nminibatches. Gradient descent is performed sequentially on these minibatches, passing through\nthe entire dataset over the course of an epoch. The dataset is typically randomly shuffled\nbefore each epoch, meaning the minibatch composition and order are different on each pass\nthrough the dataset. See Algorithm 1 below for full details. In practice, networks are often\ntrained in a distributed data-parallel fashion across K workers Li et al. (2020). Each worker\nhas a separate copy of the weights and computes gradients using a disjoint subset of the data.\nThe entire dataset is reshuffled and split among workers at the beginning of every epoch. After\nthe backward pass, gradients are averaged across workers before updating the model weights.\nLocal SGD. In local SGD, described in Algorithm 2, the workers update the local copies\nof their weights, and every H >1 iterations they synchronize the weights across workers by\naveraging the weights stored on each worker. Papers on local SGD typically credit McDonald\net al. (2009), Zinkevich et al. (2010) and McDonald et al. (2010) with pioneering local SGD\nin pre-deep learning settings; these works train models to completion and average the final pa\u0002rameters. Zhang et al. (2016) explore local SGD with periodic averaging of models throughout\ntraining; they prove that it converges in convex settings (in fa",
          "original_query": "Stochastic Gradient Descent and the Trade-offs between Local and Global Performance",
          "cleaned_query": "Stochastic Gradient Descent and the Trade-offs between Local and Global Performance",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "Gradient Clipping: Preventing Exploding Gradients in Deep Learning",
          "url": "https://mbrenndoerfer.com/writing/gradient-clipping-deep-learning",
          "content": "Gradient Clipping: Preventing Exploding Gradients in Deep Learning - Interactive | Michael Brenndoerfer | Michael Brenndoerfer\n[Writing](https://mbrenndoerfer.com/writing)[Books](https://mbrenndoerfer.com/books)[About](https://mbrenndoerfer.com/about)[Community](https://mbrenndoerfer.com/community)[Contact](https://mbrenndoerfer.com/contact)[Community](https://mbrenndoerfer.com/community)\nSign-in\n![](https://mbrenndoerfer.com/_next/image?url=https%3A%2F%2Fcnassets.uk%2Fheaders%2Fdefault_dark_fx.png&amp;w=1920&amp;q=75)![](https://mbrenndoerfer.com/_next/image?url=https%3A%2F%2Fcnassets.uk%2Fheaders%2Fdefault_light_fx.png&amp;w=1920&amp;q=75)\nBack# Gradient Clipping: Preventing Exploding Gradients in Deep Learning\nMichael BrenndoerferPublished:April 22, 2025Updated:April 24, 2025\u2022UpdatedApril 24, 2025\u202231min read\n[Data, Analytics &amp; AI](https://mbrenndoerfer.com/writing/categories/data-analytics-ai)[Machine Learning](https://mbrenndoerfer.com/writing/categories/machine-learning)[Language AI Handbook](https://mbrenndoerfer.com/writing/categories/language-ai-handbook)\nLearn how gradient clipping prevents training instability by capping gradient magnitudes. Master clip by value vs clip by norm strategies with PyTorch implementation.\n[\n![Language AI Handbook Cover](https://mbrenndoerfer.com/_next/image?url=%2Flanguage-ai-handbook%2Flanguage-ai-handbook-cover.jpg&amp;w=1920&amp;q=75)\nPart ofLanguage AI Handbook\nThis article is part of the free-to-readLanguage AI Handbook\n](https://mbrenndoerfer.com/books/language-ai-handbook)\nReading Level\nChoose your expertise level to adjust how many terms are explained. Beginners see more tooltips, experts see fewer to maintain reading flow. Hover overunderlined termsfor instant definitions.\nBeginnerMaximum helpIntermediateMedium helpExpertMinimal helpHide AllNo tooltips\n## [](#gradient-clipping)Gradient ClippingLink Copied\nDeep neural networks are powerful function approximators, but training them can be surprisingly fragile. One moment your loss is decreasing steadily, the next it explodes to infinity. The culprit?Exploding gradients. When gradients grow too large during[backpropagation](https://mbrenndoerfer.com/writing/backpropagation-algorithm-deep-learning-neural-networks),[weight updates](https://mbrenndoerfer.com/writing/mathematics-llm-fine-tuning-how-and-why-it-works-explained)become catastrophically destabilizing, and training collapses.\nGradient clippingprovides a simple but effective solution. By capping gradients at a maximumthresholdbefore applying updates, we prevent the runaway feedback loops that cause explosions. This technique is essential for training recurrent neural networks, transformers, and many other deep architectures. Without it, models with long computational paths would be nearly impossible to train.\nThis chapter covers how to detectexploding gradients, the two main clipping strategies (by value and by globalnorm), and when to apply each approach. We&#x27;ll implementgradient clippingfrom scratch and explore how to choose appropriate thresholds throughgradientnormmonitoring.\n## [](#the-exploding-gradient-problem)The Exploding Gradient ProblemLink Copied\nDuringbackpropagation, gradients flow backward through the network, accumulating contributions from each layer. In deep networks or recurrent architectures, this can create a[feedback loop](https://mbrenndoerfer.com/writing/continuous-feedback-and-improvement-ai-agents)where gradients compound multiplicatively.\nExploding Gradients\nExploding gradientsoccur whengradientmagnitudes grow exponentially duringbackpropagation. This happens when thechain ruleproduces products of values greater than 1 that compound across many layers, resulting in weight updates so large they destabilize training.\nConsider a simple[recurrent network](https://mbrenndoerfer.com/writing/history-rnn-recurrent-neural-networks)processing a sequence ofTTTtimesteps. At each step, thegradientis multiplied by the weight matrixW\\\\mathbf{W}W. If the largest[eigenvalue](https://mbrenndoerfer.com/writing/principal-component-analysis-complete-guide)of that matrix exceeds 1, gradients grow exponentially. AfterTTTmultiplications, thegradientmagnitude scales as:\n\u2225gT\u2225\u2248\u2225g0\u2225\u22c5\u03bbmax\u2061T\\\\|\\\\mathbf{g}\\_T\\\\| \\\\approx \\\\|\\\\mathbf{g}\\_0\\\\| \\\\cdot \\\\lambda\\_{\\\\max}^T\u2225gT\u200b\u2225\u2248\u2225g0\u200b\u2225\u22c5\u03bbmaxT\u200b\nwhere:\n* \u2225g0\u2225\\\\|\\\\mathbf{g}\\_0\\\\|\u2225g0\u200b\u2225: the initialgradientmagnitude at the output layer\n* \u2225gT\u2225\\\\|\\\\mathbf{g}\\_T\\\\|\u2225gT\u200b\u2225: thegradientmagnitude after backpropagating throughTTTtimesteps\n* \u03bbmax\u2061\\\\lambda\\_{\\\\max}\u03bbmax\u200b: the largesteigenvalueof the weight matrix (or itsspectral norm)\n* TTT: the number of timesteps (or layers) thegradientpasses through\nWithT=100T = 100T=100timesteps and\u03bbmax\u2061=1.1\\\\lambda\\_{\\\\max} = 1.1\u03bbmax\u200b=1.1, thegradientgrows by a factor of1.1100\u224813,7811.1^{100} \\\\approx 13,7811.1100\u224813,781. Even this modest 10% amplification per step compounds into a catastrophic explosion.\nIn[2]:\nCode\n```\n`# Simulating gradient growth in a recurrent networktimesteps=100growth\\_factor=1.1# Gradient multiplier per step# Track gradient magnitude over timegradient\\_magnitudes=[1.0]# Start with unit gradientfortinrange(timesteps):gradient\\_magnitudes.append(gradient\\_magnitudes[-1]\\*growth\\_factor)`\n```\n```\n`# Simulating gradient growth in a recurrent networktimesteps=100growth\\_factor=1.1# Gradient multiplier per step# Track gradient magnitude over timegradient\\_magnitudes=[1.0]# Start with unit gradientfortinrange(timesteps):gradient\\_magnitudes.append(gradient\\_magnitudes[-1]\\*growth\\_factor)`\n```\nOut[3]:\nConsole\n```\nGradient Growth Over Timesteps\n--------------------------------------------------\nInitial gradient magnitude: 1.00\nAfter 10 steps: 2.59\nAfter 50 steps: 117.39\nAfter 100 steps: 13780.61\nGrowth factor: 13781x\n```\nOut[4]:\nVisualization\n![Line plot showing exponential growth curve of gradient magnitude from 1 to nearly 14000 over 100 timesteps on linear scale.](https://mbrenndoerfer.com/_next/image?url=https%3A%2F%2Fcnassets.uk%2Fnotebooks%2F13_gradient_clipping_files%2Fexponential-gradient-growth-linear.png&amp;w=1920&amp;q=75)\nGradient magnitude on linear scale showing the characteristic hockey-stick curve of exponential growth.\n![Line plot showing exponential growth on log scale as a straight line with annotations at key points.](https://mbrenndoerfer.com/_next/image?url=https%3A%2F%2Fcnassets.uk%2Fnotebooks%2F13_gradient_clipping_files%2Fexponential-gradient-growth-log.png&amp;w=1920&amp;q=75)\nGradient magnitude on logarithmic scale confirms exponential growth with annotations showing 117\u00d7 amplification at step 50 and 13,781\u00d7 at step 100.\nThe exponential growth is striking. With just a 10% amplification per step, thegradientgrows nearly 14,000 times larger by the end of the sequence. This illustrates why even seemingly small spectral norms above 1.0 become catastrophic in deep or recurrent networks. In practice, these explosive gradients lead toNaNlosses, parameter values shooting to infinity, and complete training failure.\n### [](#detecting-gradient-explosions)Detecting Gradient ExplosionsLink Copied\nBefore clipping, you need to know when gradients are problematic. The clearest signal is thegradientnorm, which measures the overall magnitude of gradients across all parameters. For a model with parameters\u03b81,\u03b82,\u2026,\u03b8n\\\\theta\\_1, \\\\theta\\_2, \\\\ldots, \\\\theta\\_n\u03b81\u200b,\u03b82\u200b,\u2026,\u03b8n\u200b, each withgradient\u2207\u03b8iL\\\\nabla\\_{\\\\theta\\_i} L\u2207\u03b8i\u200b\u200bL, the globalL2 normis:\n\u2225\u2207L\u22252=\u2211i=1n\u2225\u2207\u03b8iL\u222522\\\\|\\\\nabla L\\\\|\\_2 = \\\\sqrt{\\\\sum\\_{i=1}^{n} \\\\|\\\\nabla\\_{\\\\theta\\_i} L\\\\|\\_2^2}\u2225\u2207L\u22252\u200b=i=1\u2211n\u200b\u2225\u2207\u03b8i\u200b\u200bL\u222522\u200b\u200b\nwhere:\n* \u2207\u03b8iL\\\\nabla\\_{\\\\theta\\_i} L\u2207\u03b8i\u200b\u200bL: thegradientof the lossLLLwith respect to parameter\u03b8i\\\\theta\\_i\u03b8i\u200b(a tensor)\n* \u2225\u2207\u03b8iL\u22252\\\\|\\\\nabla\\_{\\\\theta\\_i} L\\\\|\\_2\u2225\u2207\u03b8i\u200b\u200bL\u22252\u200b: theL2 normof that parameter&#x27;sgradient\n* \u2225\u2207L\u22252\\\\|\\\\nabla L\\\\|\\_2\u2225\u2207L\u22252\u200b: theglobal gradient normacross all parameters\nThis single scalar summarizes the entiregradient&#x27;s magnitude, making it easy to monitor and compare across training steps.\nIn[5]:\nCode\n```\n`importtor",
          "original_query": "Gradient Clipping: A Technique for Reducing Exploding Gradients",
          "cleaned_query": "Gradient Clipping: A Technique for Reducing Exploding Gradients",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "[2511.11466] Non-Euclidean SGD for Structured Optimization - arXiv",
          "url": "https://arxiv.org/abs/2511.11466",
          "content": "# Mathematics > Optimization and Control\n\n**arXiv:2511.11466** (math)\n\n\\[Submitted on 14 Nov 2025\\]\n\n# Title:Non-Euclidean SGD for Structured Optimization: Unified Analysis and Improved Rates\n\nAuthors: [Dmitry Kovalev](https://arxiv.org/search/math?searchtype=author&query=Kovalev,+D), [Ekaterina Borodich](https://arxiv.org/search/math?searchtype=author&query=Borodich,+E)\n\nView a PDF of the paper titled Non-Euclidean SGD for Structured Optimization: Unified Analysis and Improved Rates, by Dmitry Kovalev and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2511.11466)\n\n> Abstract:Recently, several instances of non-Euclidean SGD, including SignSGD, Lion, and Muon, have attracted significant interest from the optimization community due to their practical success in training deep neural networks. Consequently, a number of works have attempted to explain this success by developing theoretical convergence analyses. Unfortunately, these results cannot properly justify the superior performance of these methods, as they could not beat the convergence rate of vanilla Euclidean SGD. We resolve this important open problem by developing a new unified convergence analysis under the structured smoothness and gradient noise assumption. In particular, our results indicate that non-Euclidean SGD (i) can exploit the sparsity or low-rank structure of the upper bounds on the Hessian and gradient noise, (ii) can provably benefit from popular algorithmic tools such as extrapolation or momentum variance reduction, and (iii) can match the state-of-the-art convergence rates of adaptive and more complex optimization algorithms such as AdaGrad and Shampoo.\n\n| | |\n| --- | --- |\n| Subjects: | Optimization and Control (math.OC); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2511.11466](https://arxiv.org/abs/2511.11466) \\[math.OC\\] |\n| (or [arXiv:2511.11466v1](https://arxiv.org/abs/2511.11466v1) \\[math.OC\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2511.11466](https://doi.org/10.48550/arXiv.2511.11466) Focus to learn more arXiv-issued DOI via DataCite (pending registration) |\n\n## Submission history\n\nFrom: Dmitry Kovalev \\[ [view email](https://arxiv.org/show-email/ce16cf5b/2511.11466)\\] **\\[v1\\]**\nFri, 14 Nov 2025 16:38:15 UTC (26 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Non-Euclidean SGD for Structured Optimization: Unified Analysis and Improved Rates, by Dmitry Kovalev and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2511.11466)\n- [TeX Source](https://arxiv.org/src/2511.11466)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nmath.OC\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2511.11466&function=prev&context=math.OC)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2511.11466&function=next&context=math.OC)\n\n[new](https://arxiv.org/list/math.OC/new) \\| [recent](https://arxiv.org/list/math.OC/recent) \\| [2025-11](https://arxiv.org/list/math.OC/2025-11)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2511.11466?context=cs) [cs.LG](https://arxiv.org/abs/2511.11466?context=cs.LG) [math](https://arxiv.org/abs/2511.11466?context=math)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2511.11466)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2511.11466)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2511.11466)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2511.11466) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "On the Optimization of Non-Euclidean Structures",
          "cleaned_query": "On the Optimization of Non-Euclidean Structures"
        },
        {
          "success": true,
          "title": "Adaptive Gradient Methods with Dynamic Bound of ...",
          "url": "https://arxiv.org/abs/1902.09843",
          "content": "[1902.09843] Adaptive Gradient Methods with Dynamic Bound of Learning Rate\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1902.09843\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1902.09843**(cs)\n[Submitted on 26 Feb 2019]\n# Title:Adaptive Gradient Methods with Dynamic Bound of Learning Rate\nAuthors:[Liangchen Luo](https://arxiv.org/search/cs?searchtype=author&amp;query=Luo,+L),[Yuanhao Xiong](https://arxiv.org/search/cs?searchtype=author&amp;query=Xiong,+Y),[Yan Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Y),[Xu Sun](https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+X)\nView a PDF of the paper titled Adaptive Gradient Methods with Dynamic Bound of Learning Rate, by Liangchen Luo and 3 other authors\n[View PDF](https://arxiv.org/pdf/1902.09843)> > Abstract:\n> Adaptive optimization methods such as AdaGrad, RMSprop and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with SGD or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as AMSGrad to tackle this issue but they failed to achieve considerable improvement over existing methods. In our paper, we demonstrate that extreme learning rates can lead to poor performance. We provide new variants of Adam and AMSGrad, called AdaBound and AMSBound respectively, which employ dynamic bounds on learning rates to achieve a gradual and smooth transition from adaptive methods to SGD and give a theoretical proof of convergence. We further conduct experiments on various popular tasks and models, which is often insufficient in previous work. Experimental results show that new variants can eliminate the generalization gap between adaptive methods and SGD and maintain higher learning speed early in training at the same time. Moreover, they can bring significant improvement over their prototypes, especially on complex deep networks. The implementation of the algorithm can be found at [> this https URL\n](https://github.com/Luolc/AdaBound)> . Comments:|Accepted to ICLR 2019. arXiv admin note: text overlap with[arXiv:1904.09237](https://arxiv.org/abs/1904.09237)by other authors|\nSubjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:1902.09843](https://arxiv.org/abs/1902.09843)[cs.LG]|\n|(or[arXiv:1902.09843v1](https://arxiv.org/abs/1902.09843v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1902.09843](https://doi.org/10.48550/arXiv.1902.09843)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Liangchen Luo [[view email](https://arxiv.org/show-email/268d8cc7/1902.09843)]\n**[v1]**Tue, 26 Feb 2019 10:22:48 UTC (218 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Adaptive Gradient Methods with Dynamic Bound of Learning Rate, by Liangchen Luo and 3 other authors\n* [View PDF](https://arxiv.org/pdf/1902.09843)\n* [TeX Source](https://arxiv.org/src/1902.09843)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1902.09843&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1902.09843&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2019-02](https://arxiv.org/list/cs.LG/2019-02)\nChange to browse by:\n[cs](https://arxiv.org/abs/1902.09843?context=cs)\n[stat](https://arxiv.org/abs/1902.09843?context=stat)\n[stat.ML](https://arxiv.org/abs/1902.09843?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1902.09843)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1902.09843)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1902.09843)\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1902.html#abs-1902-09843)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1902-09843)\n[Liangchen Luo]()\n[Yuanhao Xiong]()\n[Yan Liu]()\n[Xu Sun]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1902.098",
          "original_query": "Adaptive Gradient Methods with Dynamic Bound for Regularization",
          "cleaned_query": "Adaptive Gradient Methods with Dynamic Bound for Regularization"
        },
        {
          "success": true,
          "title": "Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance",
          "url": "https://arxiv.org/abs/2406.04142",
          "content": "# Mathematics > Optimization and Control\n\n**arXiv:2406.04142** (math)\n\n\\[Submitted on 6 Jun 2024 ( [v1](https://arxiv.org/abs/2406.04142v1)), last revised 4 Mar 2025 (this version, v2)\\]\n\n# Title:Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance\n\nAuthors: [Dimitris Oikonomou](https://arxiv.org/search/math?searchtype=author&query=Oikonomou,+D), [Nicolas Loizou](https://arxiv.org/search/math?searchtype=author&query=Loizou,+N)\n\nView a PDF of the paper titled Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance, by Dimitris Oikonomou and 1 other authors\n\n[View PDF](https://arxiv.org/pdf/2406.04142) [HTML (experimental)](https://arxiv.org/html/2406.04142v2)\n\n> Abstract:Stochastic gradient descent with momentum, also known as Stochastic Heavy Ball method (SHB), is one of the most popular algorithms for solving large-scale stochastic optimization problems in various machine learning tasks. In practical scenarios, tuning the step-size and momentum parameters of the method is a prohibitively expensive and time-consuming process. In this work, inspired by the recent advantages of stochastic Polyak step-size in the performance of stochastic gradient descent (SGD), we propose and explore new Polyak-type variants suitable for the update rule of the SHB method. In particular, using the Iterate Moving Average (IMA) viewpoint of SHB, we propose and analyze three novel step-size selections: MomSPS$\\_{\\\\max}$, MomDecSPS, and MomAdaSPS. For MomSPS$\\_{\\\\max}$, we provide convergence guarantees for SHB to a neighborhood of the solution for convex and smooth problems (without assuming interpolation). If interpolation is also satisfied, then using MomSPS$\\_{\\\\max}$, SHB converges to the true solution at a fast rate matching the deterministic HB. The other two variants, MomDecSPS and MomAdaSPS, are the first adaptive step-size for SHB that guarantee convergence to the exact minimizer - without a priori knowledge of the problem parameters and without assuming interpolation. Our convergence analysis of SHB is tight and obtains the convergence guarantees of stochastic Polyak step-size for SGD as a special case. We supplement our analysis with experiments validating our theory and demonstrating the effectiveness and robustness of our algorithms.\n\n| | |\n| --- | --- |\n| Comments: | 13th International Conference on Learning Representations (ICLR 2025) |\n| Subjects: | Optimization and Control (math.OC); Machine Learning (cs.LG); Machine Learning (stat.ML) |\n| Cite as: | [arXiv:2406.04142](https://arxiv.org/abs/2406.04142) \\[math.OC\\] |\n| | (or [arXiv:2406.04142v2](https://arxiv.org/abs/2406.04142v2) \\[math.OC\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2406.04142](https://doi.org/10.48550/arXiv.2406.04142) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Dimitris Oikonomou \\[ [view email](https://arxiv.org/show-email/211cded8/2406.04142)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2406.04142v1)**\nThu, 6 Jun 2024 15:08:06 UTC (1,342 KB)\n\n**\\[v2\\]**\nTue, 4 Mar 2025 03:36:14 UTC (1,002 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance, by Dimitris Oikonomou and 1 other authors\n\n- [View PDF](https://arxiv.org/pdf/2406.04142)\n- [HTML (experimental)](https://arxiv.org/html/2406.04142v2)\n- [TeX Source](https://arxiv.org/src/2406.04142)\n- [Other Formats](https://arxiv.org/format/2406.04142)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\nmath.OC\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2406.04142&function=prev&context=math.OC)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2406.04142&function=next&context=math.OC)\n\n[new](https://arxiv.org/list/math.OC/new) \\| [recent](https://arxiv.org/list/math.OC/recent) \\| [2024-06](https://arxiv.org/list/math.OC/2024-06)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2406.04142?context=cs)\n\n[cs.LG](https://arxiv.org/abs/2406.04142?context=cs.LG)\n\n[math](https://arxiv.org/abs/2406.04142?context=math)\n\n[stat](https://arxiv.org/abs/2406.04142?context=stat)\n\n[stat.ML](https://arxiv.org/abs/2406.04142?context=stat.ML)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2406.04142)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2406.04142)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2406.04142)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2406.04142&description=Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2406.04142&title=Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2406.04142) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Momentum Methods for Stochastic Optimization",
          "cleaned_query": "Momentum Methods for Stochastic Optimization"
        }
      ],
      "generated_ideas": [
        "Communication-Aware Polyak Local SGD (CAP-LSGD)\nDevelop a local/post-local SGD variant where the number of local steps \\(H_t\\) and the learning rate are jointly adapted using stochastic Polyak step-sizes (SPS) computed on each worker (and optionally a periodic global loss estimate). The key contribution is a practical rule that increases \\(H_t\\) when progress per communication is high and decreases it when worker drift harms loss reduction, targeting the accuracy/communication trade-off observed at ImageNet scale.",
        "Non-Euclidean Local SGD with Structured Noise-Aware Geometry\nCombine local SGD with non-Euclidean SGD updates (e.g., SignSGD/Lion/Muon-style mirror maps) guided by the \u201cstructured smoothness + structured noise\u201d assumptions to reduce sensitivity to worker drift. The contribution is a theory + implementation showing when non-Euclidean geometry can offset the accuracy drop from infrequent averaging by exploiting sparsity/low-rank structure in gradient noise/Hessian surrogates.",
        "Adaptive Gradient Clipping Driven by Dual-Gap/Polyak Signals\nDesign an adaptive clipping threshold schedule where the clip norm is adjusted online using either (i) a Polyak-style progress estimate (loss decrease per step) or (ii) a Frank\u2013Wolfe-inspired stationarity/duality measure (when constraints are present). The contribution is a clipping controller that is less heuristic than fixed thresholds, aiming to stabilize large-scale/local training without the accuracy degradation caused by overly aggressive clipping.",
        "Dynamic-Bounded Non-Euclidean Optimizers (AdaBound-Mirror)\nExtend AdaBound\u2019s dynamic learning-rate bounds to non-Euclidean SGD updates by bounding mirror-step magnitudes (or dual-variable step sizes) instead of elementwise Euclidean learning rates. The contribution is a concrete algorithm that transitions from aggressive non-Euclidean adaptation early to a more \u201cSGD-like\u201d regime later, with convergence guarantees under the structured assumptions used to improve non-Euclidean SGD rates.",
        "Slow-Momentum Meets Polyak: Drift-Corrected Local Momentum (SM-Polyak)\nCreate a unified local SGD optimizer that couples slow momentum (shown empirically to help at scale) with momentum Polyak step-size rules (MomSPS variants) to automatically tune both effective step size and momentum under infrequent synchronization. The contribution is a principled drift-correction mechanism: workers adjust their local momentum/step using SPS-style signals to reduce divergence between local models without extra communication.",
        "Extrapolated Non-Euclidean Local SGD for Scale (ENL-SGD)\nLeverage the non-Euclidean paper\u2019s claim that extrapolation can provably help, and implement an extrapolated two-step scheme (predict\u2013correct) inside each worker\u2019s local loop before averaging. The contribution is an actionable algorithmic template plus an empirical study on ImageNet-scale settings testing whether extrapolation reduces the \u201caccuracy tax\u201d as \\(H\\) and worker count grow.",
        "Frank\u2013Wolfe as a Communication-Efficient Averaging Operator\nReplace naive parameter averaging in local SGD with a Frank\u2013Wolfe step over the convex hull of worker models (or their low-rank/sparse decompositions), using the global gradient estimate when synchronizing. The contribution is a new synchronization primitive that can bias aggregation toward workers that provide better descent directions (measured via linear minimization), potentially reducing the degradation seen with many local steps.",
        "Clip-and-Bound Schedules for Large-Batch/Local Regimes\nJointly schedule gradient clipping and dynamic learning-rate bounds (AdaBound/AMSBound style) specifically for regimes where communication is reduced (large \\(H\\), many workers) and gradients are bursty. The contribution is a set of experimentally validated schedules (and ablations) demonstrating when clipping should tighten while bounds widen (or vice versa) to maintain stability without sacrificing final accuracy.",
        "Structured Constraint Learning via FW + Non-Euclidean SGD Hybrid\nBuild a hybrid optimizer that uses non-Euclidean SGD updates for unconstrained layers/parameters and Frank\u2013Wolfe steps for constrained blocks (e.g., enforcing sparsity on attention projections or low-rank adapters), all under a shared step-size policy (adaptive or Polyak-type). The contribution is a modular \u201cblock geometry\u201d training framework with clear implementation hooks and measurable benefits: fewer projections, better exploitation of structure, and improved robustness under noisy/heterogeneous gradients."
      ],
      "generated_ideas_raw": "1. **Communication-Aware Polyak Local SGD (CAP-LSGD)**\n   Develop a local/post-local SGD variant where the number of local steps \\(H_t\\) and the learning rate are jointly adapted using stochastic Polyak step-sizes (SPS) computed on each worker (and optionally a periodic global loss estimate). The key contribution is a practical rule that increases \\(H_t\\) when progress per communication is high and decreases it when worker drift harms loss reduction, targeting the accuracy/communication trade-off observed at ImageNet scale.\n\n2. **Non-Euclidean Local SGD with Structured Noise-Aware Geometry**\n   Combine local SGD with non-Euclidean SGD updates (e.g., SignSGD/Lion/Muon-style mirror maps) guided by the \u201cstructured smoothness + structured noise\u201d assumptions to reduce sensitivity to worker drift. The contribution is a theory + implementation showing when non-Euclidean geometry can offset the accuracy drop from infrequent averaging by exploiting sparsity/low-rank structure in gradient noise/Hessian surrogates.\n\n3. **Frank\u2013Wolfe Fine-Tuning Phase for Post-Local SGD (FW-Post)**\n   Add a late-stage \u201cprojection-free constraint enforcement\u201d phase using Frank\u2013Wolfe to fine-tune models under explicit constraints (e.g., \\(\\ell_1\\) sparsity budgets, nuclear-norm compression, or quantization-feasible polytopes) after the post-local SGD phase. The contribution is an end-to-end recipe (and convergence diagnostics via FW dual gaps) to obtain constrained, deployable models without costly projections, while retaining most of the speedups from local training.\n\n4. **Adaptive Gradient Clipping Driven by Dual-Gap/Polyak Signals**\n   Design an adaptive clipping threshold schedule where the clip norm is adjusted online using either (i) a Polyak-style progress estimate (loss decrease per step) or (ii) a Frank\u2013Wolfe-inspired stationarity/duality measure (when constraints are present). The contribution is a clipping controller that is less heuristic than fixed thresholds, aiming to stabilize large-scale/local training without the accuracy degradation caused by overly aggressive clipping.\n\n5. **Dynamic-Bounded Non-Euclidean Optimizers (AdaBound-Mirror)**\n   Extend AdaBound\u2019s dynamic learning-rate bounds to non-Euclidean SGD updates by bounding mirror-step magnitudes (or dual-variable step sizes) instead of elementwise Euclidean learning rates. The contribution is a concrete algorithm that transitions from aggressive non-Euclidean adaptation early to a more \u201cSGD-like\u201d regime later, with convergence guarantees under the structured assumptions used to improve non-Euclidean SGD rates.\n\n6. **Slow-Momentum Meets Polyak: Drift-Corrected Local Momentum (SM-Polyak)**\n   Create a unified local SGD optimizer that couples slow momentum (shown empirically to help at scale) with momentum Polyak step-size rules (MomSPS variants) to automatically tune both effective step size and momentum under infrequent synchronization. The contribution is a principled drift-correction mechanism: workers adjust their local momentum/step using SPS-style signals to reduce divergence between local models without extra communication.\n\n7. **Extrapolated Non-Euclidean Local SGD for Scale (ENL-SGD)**\n   Leverage the non-Euclidean paper\u2019s claim that extrapolation can provably help, and implement an extrapolated two-step scheme (predict\u2013correct) inside each worker\u2019s local loop before averaging. The contribution is an actionable algorithmic template plus an empirical study on ImageNet-scale settings testing whether extrapolation reduces the \u201caccuracy tax\u201d as \\(H\\) and worker count grow.\n\n8. **Frank\u2013Wolfe as a Communication-Efficient Averaging Operator**\n   Replace naive parameter averaging in local SGD with a Frank\u2013Wolfe step over the convex hull of worker models (or their low-rank/sparse decompositions), using the global gradient estimate when synchronizing. The contribution is a new synchronization primitive that can bias aggregation toward workers that provide better descent directions (measured via linear minimization), potentially reducing the degradation seen with many local steps.\n\n9. **Clip-and-Bound Schedules for Large-Batch/Local Regimes**\n   Jointly schedule gradient clipping and dynamic learning-rate bounds (AdaBound/AMSBound style) specifically for regimes where communication is reduced (large \\(H\\), many workers) and gradients are bursty. The contribution is a set of experimentally validated schedules (and ablations) demonstrating when clipping should tighten while bounds widen (or vice versa) to maintain stability without sacrificing final accuracy.\n\n10. **Structured Constraint Learning via FW + Non-Euclidean SGD Hybrid**\n   Build a hybrid optimizer that uses non-Euclidean SGD updates for unconstrained layers/parameters and Frank\u2013Wolfe steps for constrained blocks (e.g., enforcing sparsity on attention projections or low-rank adapters), all under a shared step-size policy (adaptive or Polyak-type). The contribution is a modular \u201cblock geometry\u201d training framework with clear implementation hooks and measurable benefits: fewer projections, better exploitation of structure, and improved robustness under noisy/heterogeneous gradients.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Communication-Aware Polyak Local SGD (CAP-LSGD)\nDevelop a local/post-local SGD variant where the number of local steps \\(H_t\\) and the learning rate are jointly adapted using stochastic Polyak step-si",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Non-Euclidean Local SGD with Structured Noise-Aware Geometry\nCombine local SGD with non-Euclidean SGD updates (e.g., SignSGD/Lion/Muon-style mirror maps) guided by the \u201cstructured smoothness + structu",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Adaptive Gradient Clipping Driven by Dual-Gap/Polyak Signals\nDesign an adaptive clipping threshold schedule where the clip norm is adjusted online using either (i) a Polyak-style progress estimate (lo",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Dynamic-Bounded Non-Euclidean Optimizers (AdaBound-Mirror)\nExtend AdaBound\u2019s dynamic learning-rate bounds to non-Euclidean SGD updates by bounding mirror-step magnitudes (or dual-variable step sizes) ",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Slow-Momentum Meets Polyak: Drift-Corrected Local Momentum (SM-Polyak)\nCreate a unified local SGD optimizer that couples slow momentum (shown empirically to help at scale) with momentum Polyak step-si",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Extrapolated Non-Euclidean Local SGD for Scale (ENL-SGD)\nLeverage the non-Euclidean paper\u2019s claim that extrapolation can provably help, and implement an extrapolated two-step scheme (predict\u2013correct) ",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Frank\u2013Wolfe as a Communication-Efficient Averaging Operator\nReplace naive parameter averaging in local SGD with a Frank\u2013Wolfe step over the convex hull of worker models (or their low-rank/sparse decom",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Clip-and-Bound Schedules for Large-Batch/Local Regimes\nJointly schedule gradient clipping and dynamic learning-rate bounds (AdaBound/AMSBound style) specifically for regimes where communication is red",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Structured Constraint Learning via FW + Non-Euclidean SGD Hybrid\nBuild a hybrid optimizer that uses non-Euclidean SGD updates for unconstrained layers/parameters and Frank\u2013Wolfe steps for constrained ",
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 69,
      "paper_title": "Rethinking Multimodal Learning from the Perspective of Mitigating Classification Ability Disproportion",
      "contribution": "The paper introduces a novel multimodal learning approach that dynamically balances the classification abilities of strong and weak modalities using a sustained boosting algorithm and adaptive classifier assignment.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 10,
      "hit_at_k": true,
      "matching_idea_idx": 1,
      "input_tokens": 8744,
      "output_tokens": 1035,
      "predecessor_details": [
        {
          "success": true,
          "title": "A Decision-Theoretic Generalization of On-Line Learning and an ...",
          "url": "https://www.sciencedirect.com/science/article/pii/S002200009791504X",
          "content": "References REFERENCES 1 What size net gives valid generalization? Adv. Neural Inform. Process. Systems I (1989), pp. 81-90 2 D. Blackwell An analog of the minimax theorem for vector payoffs Pacific J. Math., 6 (Spring 1956), pp. 1-8 3 L. Breiman, 1996, Bias, variance, and arcing classifiers, Statistics Dept. University of California 4 N. Cesa-Bianchi, Y. Freund, D. P. Helmhold, D. Haussler, R. E. Schapire, M. K. Warmuth, How to use expert advice, Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, 1993, 382, 391 5 T. H. Chung, Approximate methods for sequential decision making using expert advice, Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, 1994, 183, 189 6 T.M. Cover Universal portfolios Math. Finance, 1 (Jan. 1991), pp. 1-29 7 T.G. Dietterich, G. Bakiri Solving multiclass learning problems via error-correcting output codes J. Artif. Intell. Res., 2 (January 1995), pp. 263-286 8 H. Drucker, C. Cortes Boosting decision trees Adv. Neural Inform. Process. Systems, 8 (1996) 9 H. Drucker, R. Schapire, P. Simard Boosting performance in neural networks Int. J. Pattern Recognition Artif. Intell., 7 (1993), pp. 705-719 10 Y. Freund, 1993, Data Filtering and Distribution Modeling Algorithms for Machine Learning, University of California at Santa Cruz 11 Y. Freund Boosting a weak learning algorithm by majority Inform. and Comput., 121 (September 1995), pp. 256-285 12 Y. Freund, R. E. Schapire, Experiments with a new boosting algorithm, Machine Learning: Proceedings of the Thirteenth International Conference, 1996, 148, 156 13 Y. Freund, R. E. Schapire, Game theory, on-line prediction and boosting, Proceedings of the Ninth Annual Conference on Computational Learning Theory, 1996, 325, 332 14 J. Hannan Approximation to Bayes risk in repeated play Contributions to the Theory of Games, Princeton Univ. Press, Princeton (1957) 15 D. Haussler, J. Kivinen, M.K. Warmuth Tight worst-case loss bounds for predicting with expert advice Computational Learning Theory: Second European Conference, EuroCOLT '95, Springer-Verlag, New York/Berlin (1995) 16 J.C. Jackson, M.W. Craven Learning sparse perceptrons Adv. Neural Inform. Process. Systems, 8 (1996) 17 M. Kearns, Y. Mansour, A. Y. Ng, D. Ron, An experimental and theoretical comparison of model selection methods, Proceedings of the Eighth Annual Conference on Computational Learning Theory, 1995 18 M.J. Kearns, U.V. Vazirani An Introduction to Computational Learning Theory, MIT Press, Cambridge (1994) 19 J. Kivinen, M.K. Warmuth Using experts for predicting continuous outcomes Computational Learning Theory: EuroCOLT '93, Springer-Verlag, New York/Berlin (1994) 20 N. Littlestone, M.K. Warmuth The weighted majority algorithm Inform. and Comput., 108 (1994), pp. 212-261 21 J. R. Quinlan, Bagging, boosting, and C4.5, Proceedings, Fourteenth National Conference on Artificial Intelligence, 1996 22 R.E. Schapire The strength of weak learnability Machine Learning, 5 (1990), pp. 197-227 23 Estimation of Dependences Based on Empirical Data, Springer-Verlag, New York/Berlin (1982) 24 V. G. Vovk, A game of prediction with expert advice, Proceedings of the Eighth Annual Conference on Computational Learning Theory, 1995 25 V. G. Vovk, Aggregating strategies, Proceedings of the Third Annual Workshop on Computational Learning Theory, 1990, 321, 383 26 R.S. Wenocur, R.M. Dudley Some special Vapnik\u2013Chervonenkis classes Discrete Mathematics, 33 (1981), pp. 313-318 Copyright \u00a9 1997 Academic Press. All rights reserved.",
          "original_query": "A decision-theoretic generalization of on-line learning and an application to boosting",
          "cleaned_query": "A decision-theoretic generalization of on-line learning and an application to boosting"
        },
        {
          "success": true,
          "title": "(PDF) Greedy Function Approximation: A Gradient Boosting Machine",
          "url": "https://www.researchgate.net/publication/2424824_Greedy_Function_Approximation_A_Gradient_Boosting_Machine",
          "content": "- [Home](https://www.researchgate.net/directory/publications)\n- [Classification Algorithms](https://www.researchgate.net/topic/Classification-Algorithms/publications)\n- [Computer Algorithm](https://www.researchgate.net/topic/Computer-Algorithm/publications)\n- [Computer Science](https://www.researchgate.net/topic/Computer-Science/publications)\n- [Boosting](https://www.researchgate.net/topic/Boosting/publications)\n\nArticlePDF Available\n\n# Greedy Function Approximation: A Gradient Boosting Machine\n\n- November 2000\n- [The Annals of Statistics](https://www.researchgate.net/journal/The-Annals-of-Statistics-0090-5364) 29(5)\n\nDOI: [10.1214/aos/1013203451](https://doi.org/10.1214/aos/1013203451)\n\nAuthors:\n\n[Jerome H. Friedman](https://www.researchgate.net/profile/Jerome-Friedman)\n\n- [Stanford University](https://www.researchgate.net/institution/Stanford-University)\n\n[Download full-text PDF](https://www.researchgate.net/profile/Jerome-Friedman/publication/2424824_Greedy_Function_Approximation_A_Gradient_Boosting_Machine/links/6589e1ae0bb2c7472b0fc00e/Greedy-Function-Approximation-A-Gradient-Boosting-Machine.pdf)\n\n[Read full-text](https://www.researchgate.net/publication/2424824_Greedy_Function_Approximation_A_Gradient_Boosting_Machine#read)\n\n[Download citation](https://www.researchgate.net/publication/2424824_Greedy_Function_Approximation_A_Gradient_Boosting_Machine/citation/download)\n\nCopy link Link copied\n\n[Read full-text](https://www.researchgate.net/publication/2424824_Greedy_Function_Approximation_A_Gradient_Boosting_Machine#read) [Download citation](https://www.researchgate.net/publication/2424824_Greedy_Function_Approximation_A_Gradient_Boosting_Machine/citation/download)\nCopy link Link copied\n\n## Abstract\n\nFunction approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest--descent minimization. A general gradient--descent \"boosting\" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least--squares, least--absolute--deviation, and Huber--M loss functions for regression, and multi--class logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are decision trees, and tools for interpreting such \"TreeBoost\" models are presented. Gradient boosting of decision trees produces competitive, highly robust, interpretable procedures for regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire 1996, and Fr...\n\n**Discover the world's research**\n\n- 25+ million members\n- 160+ million publication pages\n- 2.3+ billion citations\n\n[Join for free](https://www.researchgate.net/signup.SignUp.html)\n\nContent uploaded by [Jerome H. Friedman](https://www.researchgate.net/profile/Jerome-Friedman)\n\nAuthor content\n\nAll content in this area was uploaded by Jerome H. Friedman on Dec 25, 2023\n\nContent may be subject to copyright.\n\nhttp://www.jstor.org\n\nGreedy Function Approximation: A Gradient Boosting Machine\n\nAuthor(s): Jerome H. Friedman\n\nSource:\n\nThe Annals of Statistics,\n\nVol. 29, No. 5 (Oct., 2001), pp. 1189-1232\n\nPublished by: Institute of Mathematical Statistics\n\nStable URL: http://www.jstor.org/stable/2699986\n\nAccessed: 05/09/2008 10:04\n\nYour use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at\n\nhttp://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless\n\nyou have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you\n\nmay use content in the JSTOR archive only for your personal, non-commercial use.\n\nPlease contact the publisher regarding any further use of this work. Publisher contact information may be obtained at\n\nhttp://www.jstor.org/action/showPublisher?publisherCode=ims.\n\nEach copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed\n\npage of such transmission.\n\nJSTOR is a not-for-profit organization founded in 1995 to build trusted digital archives for scholarship. We work with the\n\nscholarly community to preserve their work and the materials they rely upon, and to build a common research platform that\n\npromotes the discovery and use of these resources. For more information about JSTOR, please contact support@jstor.org.\n\n... Support Vector Regression \\[43\\] with radial basis function (RBF) kernels was deployed for meteorological feature sets, leveraging its effectiveness in high-dimensional spaces with complex non-linear relationships. The optimization problem was formulated as: ...\n\n[A Novel Application of Choquet Integral for Multi-Model Fusion in Urban PM 10 Forecasting](https://www.researchgate.net/publication/397453601_A_Novel_Application_of_Choquet_Integral_for_Multi-Model_Fusion_in_Urban_PM_10_Forecasting)\n\nArticle\n\nFull-text available\n\n- Nov 2025\n\n- [Houria Bouzghiba](https://www.researchgate.net/profile/Houria-Bouzghiba-2)\n- [Amine Ajdour](https://www.researchgate.net/profile/Amine-Ajdour)\n\n[View](https://www.researchgate.net/publication/397453601_A_Novel_Application_of_Choquet_Integral_for_Multi-Model_Fusion_in_Urban_PM_10_Forecasting)\n\n[Assessment of Tropical Cyclone Disaster Damage Based on Learnable Inter-City Interaction GNN\u57fa\u4e8e\u53ef\u5b66\u4e60\u57ce\u9645\u4ea4\u4e92\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u70ed\u5e26\u6c14\u65cb\u707e\u5bb3\u635f\u5931\u8bc4\u4f30](https://www.researchgate.net/publication/397452794_Assessment_of_Tropical_Cyclone_Disaster_Damage_Based_on_Learnable_Inter-City_Interaction_GNNjiyukexuexichengjijiaohutushenjingwangluoderedaiqixuanzaihaisunshipinggu)\n\nArticle\n\n- Nov 2025\n\nTropical cyclones (TCs) are one of the most frequent disastrous weather events in China, causing widespread damage. Traditional approaches for assessing TC disaster damage treat the TC affected regions as isolated units and ignore inter-regional interactions, resulting in underestimation of complex dynamics in disaster damage assessment. In this paper, we developed an original TC disaster damage dataset, with each sample representing a unique disaster event, incorporating city-specific multi-dimensional features and damage indicators. Then, using provincial administrative divisions in China as examples, we innovatively assigned cities as nodes and constructed inter-city interaction graphs. To align with the physical interactions, a deep learning model named TC-Damage is specifically established. It includes an edge building module and a backbone. The edge building module aims to construct inter-city interaction features from multiple perspectives. The backbone employs a multi-layer Graph Neural Network (GNN) based on Graph Sample and Aggregate (GraphSAGE) and Jumping Knowledge Network (JKNet) to learn comprehensive and hierarchical features of inter-city interactions. A loss function combined with focal loss and node-level loss is proposed to address data imbalance and to enforce representation node distribution. Multiple experiments demonstrate that TC-Damage outperforms other assessment methods and effectively identifies high-contribution factors. Explainability analysis of Super Typhoon Lekima reveals that key edges are adjacent to cities with high disaster factors and social development levels and significantly overlap with edges exhibiting strong inter-city interactions.\n\n[View](https://www.researchgate.net/publication/397452794_Assessment_of_Tropical_Cyclone_Disaster_Damage_Based_on_Learnable_Inter-City_Interaction_GNNjiyukexuexichengjijiaohutushenjingwangluoderedaiqixuanzaihaisunshipinggu)\n\nShow abstract\n\n[Identifying Predictors of Utilization of Skilled Birth Attendance in Uganda Through Interpretable Machine Learning](https://www.researchgate.net/publication/397451251_Identifying_Predictors_of_Utilization_of_Skilled_Birth_Attendance_in_Uganda_Throug",
          "original_query": "Greedy function approximation: a gradient boosting machine",
          "cleaned_query": "Greedy function approximation: a gradient boosting machine"
        },
        {
          "success": true,
          "title": "ReconBoost: Boosting Can Achieve Modality Reconcilement - arXiv",
          "url": "https://arxiv.org/abs/2405.09321",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "Reconboost: Boosting can achieve modality reconcilement",
          "cleaned_query": "Reconboost: Boosting can achieve modality reconcilement"
        },
        {
          "success": true,
          "title": "What Makes Training Multi-Modal Classification Networks Hard?",
          "url": "https://arxiv.org/abs/1905.12681",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:1905.12681** (cs)\n\n\\[Submitted on 29 May 2019 ( [v1](https://arxiv.org/abs/1905.12681v1)), last revised 3 Apr 2020 (this version, v5)\\]\n\n# Title:What Makes Training Multi-Modal Classification Networks Hard?\n\nAuthors: [Weiyao Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+W), [Du Tran](https://arxiv.org/search/cs?searchtype=author&query=Tran,+D), [Matt Feiszli](https://arxiv.org/search/cs?searchtype=author&query=Feiszli,+M)\n\nView a PDF of the paper titled What Makes Training Multi-Modal Classification Networks Hard?, by Weiyao Wang and Du Tran and Matt Feiszli\n\n[View PDF](https://arxiv.org/pdf/1905.12681)\n\n> Abstract:Consider end-to-end training of a multi-modal vs. a single-modal network on a task with multiple input modalities: the multi-modal network receives more information, so it should match or outperform its single-modal counterpart. In our experiments, however, we observe the opposite: the best single-modal network always outperforms the multi-modal network. This observation is consistent across different combinations of modalities and on different tasks and benchmarks.\n>\n> This paper identifies two main causes for this performance drop: first, multi-modal networks are often prone to overfitting due to increased capacity. Second, different modalities overfit and generalize at different rates, so training them jointly with a single optimization strategy is sub-optimal. We address these two problems with a technique we call Gradient Blending, which computes an optimal blend of modalities based on their overfitting behavior. We demonstrate that Gradient Blending outperforms widely-used baselines for avoiding overfitting and achieves state-of-the-art accuracy on various tasks including human action recognition, ego-centric action recognition, and acoustic event detection.\n\n| | |\n| --- | --- |\n| Comments: | CVPR 2020 |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG) |\n| Cite as: | [arXiv:1905.12681](https://arxiv.org/abs/1905.12681) \\[cs.CV\\] |\n| | (or [arXiv:1905.12681v5](https://arxiv.org/abs/1905.12681v5) \\[cs.CV\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.1905.12681](https://doi.org/10.48550/arXiv.1905.12681) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Weiyao Wang \\[ [view email](https://arxiv.org/show-email/e79a8e9f/1905.12681)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/1905.12681v1)**\nWed, 29 May 2019 19:10:06 UTC (5,232 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/1905.12681v2)**\nThu, 20 Jun 2019 18:24:31 UTC (5,232 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/1905.12681v3)**\nSat, 29 Jun 2019 07:04:17 UTC (5,062 KB)\n\n**[\\[v4\\]](https://arxiv.org/abs/1905.12681v4)**\nMon, 9 Dec 2019 22:49:19 UTC (5,583 KB)\n\n**\\[v5\\]**\nFri, 3 Apr 2020 00:36:42 UTC (6,872 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled What Makes Training Multi-Modal Classification Networks Hard?, by Weiyao Wang and Du Tran and Matt Feiszli\n\n- [View PDF](https://arxiv.org/pdf/1905.12681)\n- [TeX Source](https://arxiv.org/src/1905.12681)\n- [Other Formats](https://arxiv.org/format/1905.12681)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=1905.12681&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=1905.12681&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2019-05](https://arxiv.org/list/cs.CV/2019-05)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/1905.12681?context=cs)\n\n[cs.LG](https://arxiv.org/abs/1905.12681?context=cs.LG)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1905.12681)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1905.12681)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:1905.12681)\n\n### [DBLP](https://dblp.uni-trier.de) \\- CS Bibliography\n\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1905.html#abs-1905-12681) \\| [bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1905-12681)\n\n[Weiyao Wang](https://dblp.uni-trier.de/search/author?author=Weiyao%20Wang)\n\n[Du Tran](https://dblp.uni-trier.de/search/author?author=Du%20Tran)\n\n[Matt Feiszli](https://dblp.uni-trier.de/search/author?author=Matt%20Feiszli)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/1905.12681&description=What Makes Training Multi-Modal Classification Networks Hard?) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/1905.12681&title=What Makes Training Multi-Modal Classification Networks Hard?)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1905.12681) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "What makes training multi-modal classification networks hard?",
          "cleaned_query": "What makes training multi-modal classification networks hard?"
        },
        {
          "success": true,
          "title": "Balanced Multimodal Learning via On-the-fly Gradient Modulation",
          "url": "https://arxiv.org/abs/2203.15332",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2203.15332** (cs)\n\n\\[Submitted on 29 Mar 2022\\]\n\n# Title:Balanced Multimodal Learning via On-the-fly Gradient Modulation\n\nAuthors: [Xiaokang Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng,+X), [Yake Wei](https://arxiv.org/search/cs?searchtype=author&query=Wei,+Y), [Andong Deng](https://arxiv.org/search/cs?searchtype=author&query=Deng,+A), [Dong Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+D), [Di Hu](https://arxiv.org/search/cs?searchtype=author&query=Hu,+D)\n\nView a PDF of the paper titled Balanced Multimodal Learning via On-the-fly Gradient Modulation, by Xiaokang Peng and 3 other authors\n\n[View PDF](https://arxiv.org/pdf/2203.15332)\n\n> Abstract:Multimodal learning helps to comprehensively understand the world, by integrating different senses. Accordingly, multiple input modalities are expected to boost model performance, but we actually find that they are not fully exploited even when the multimodal model outperforms its uni-modal counterpart. Specifically, in this paper we point out that existing multimodal discriminative models, in which uniform objective is designed for all modalities, could remain under-optimized uni-modal representations, caused by another dominated modality in some scenarios, e.g., sound in blowing wind event, vision in drawing picture event, etc. To alleviate this optimization imbalance, we propose on-the-fly gradient modulation to adaptively control the optimization of each modality, via monitoring the discrepancy of their contribution towards the learning objective. Further, an extra Gaussian noise that changes dynamically is introduced to avoid possible generalization drop caused by gradient modulation. As a result, we achieve considerable improvement over common fusion methods on different multimodal tasks, and this simple strategy can also boost existing multimodal methods, which illustrates its efficacy and versatility. The source code is available at \\\\url{ [this https URL](https://github.com/GeWu-Lab/OGM-GE_CVPR2022)}.\n\n| | |\n| --- | --- |\n| Comments: | Accepted by CVPR 2022 (ORAL) |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI) |\n| Cite as: | [arXiv:2203.15332](https://arxiv.org/abs/2203.15332) \\[cs.CV\\] |\n| (or [arXiv:2203.15332v1](https://arxiv.org/abs/2203.15332v1) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2203.15332](https://doi.org/10.48550/arXiv.2203.15332) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Yake Wei \\[ [view email](https://arxiv.org/show-email/c269f548/2203.15332)\\] **\\[v1\\]**\nTue, 29 Mar 2022 08:26:38 UTC (505 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Balanced Multimodal Learning via On-the-fly Gradient Modulation, by Xiaokang Peng and 3 other authors\n\n- [View PDF](https://arxiv.org/pdf/2203.15332)\n- [TeX Source](https://arxiv.org/src/2203.15332)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2203.15332&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2203.15332&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2022-03](https://arxiv.org/list/cs.CV/2022-03)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2203.15332?context=cs) [cs.AI](https://arxiv.org/abs/2203.15332?context=cs.AI)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2203.15332)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2203.15332)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2203.15332)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2203.15332) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Balanced multimodal learning via on-the-fly gradient modulation",
          "cleaned_query": "Balanced multimodal learning via on-the-fly gradient modulation"
        }
      ],
      "generated_ideas": [
        "1. **Online No-Regret Gradient Blending for Multimodal Deep Nets**  \nDevelop an online learning variant of Gradient Blending (Paper 4) where modality weights are updated with a weighted-majority / expert-advice rule (Paper 1) using per-modality validation regret as losses. Provide theoretical no-regret guarantees on the weighting process and evaluate on audio-visual action recognition under non-stationary modality quality (e.g., changing noise levels).",
        "**Boosted Modality Specialists: Stagewise Additive Multimodal Fusion**  \nCast each modality pathway (and simple cross-modal interaction blocks) as \u201cweak learners\u201d and build a stagewise additive multimodal predictor via gradient boosting in function space (Paper 2). Each boosting iteration selects the modality (or interaction) that best fits the current residual/negative gradient, creating an interpretable sequence of \u201cwhich modality helped when,\u201d and reducing overfitting from jointly training a large fused model (Paper 4).",
        "**Generalization-Aware On-the-Fly Gradient Modulation via PAC-Bayes Bounds**  \nExtend OGM (Paper 5) by replacing heuristic discrepancy measures with a bound-driven control signal: estimate per-modality generalization gaps online using PAC-Bayes-style proxies or held-out sharpness/curvature indicators, then modulate gradients to minimize an upper bound on test risk. The actionable output is a concrete algorithm that dynamically down-weights modalities whose complexity-adjusted generalization bound worsens during training.",
        "**ReconBoost for Missing/Corrupted Modalities with Boosted Reconstruction Residuals**  \nBuild on ReconBoost (Paper 3) by explicitly boosting reconstruction modules: at each stage, add a small reconstructor that predicts one modality\u2019s features from others to reduce reconciliation error, then feed reconciled features into the classifier. Test robustness when modalities are missing at inference, and quantify tradeoffs between reconstruction boosting depth and classification accuracy.",
        "**Adversarial Modality-Quality Games: Robust Fusion via Decision-Theoretic Updates**  \nModel modality noise/corruption as an adversary and the fusion rule as the learner in a repeated game (Paper 1), where each modality is an \u201cexpert\u201d whose loss depends on current corruption. Derive a robust training procedure that updates modality weights to guarantee performance close to the best modality in hindsight, then integrate it with Gradient Blending/OGM as the inner optimizer for end-to-end networks.",
        "**Multi-Objective Boosting for Accuracy\u2013Calibration\u2013Fairness in Multimodal Systems**  \nUse vector-payoff/approachability ideas (Blackwell-style, Paper 1 references) to treat accuracy, calibration error, and group fairness metrics as simultaneous objectives. Implement a boosting-style procedure (Paper 2) where each stage chooses updates (per modality or shared) to steer the metric vector toward a target set, producing a practical algorithm that balances modalities without sacrificing calibration or fairness.",
        "**TreeBoost-Inspired Interpretable Gating Policies for Multimodal Fusion**  \nTrain a lightweight decision-tree ensemble (Paper 2) to predict per-sample modality weights using features like modality-specific confidence, gradient norms, and overfitting indicators (Papers 4\u20135). This yields an interpretable gating policy (\u201cwhen audio is noisy, trust vision\u201d) that can be plugged into deep multimodal classifiers and audited post hoc.",
        "**Curriculum Boosting Across Modalities Based on Differential Overfitting Rates**  \nOperationalize Paper 4\u2019s finding that modalities overfit at different rates by designing a boosting/curriculum schedule: start training with the modality that generalizes fastest, then gradually add others as new boosting stages that fit residual errors. Provide an ablation-driven recipe (stage timing, shrinkage, early stopping per stage) and test whether it consistently beats simultaneous fusion on small-data regimes.",
        "**Shrinkage and Early-Stopping Rules for OGM/Blending Derived from Functional Gradient Descent**  \nReinterpret OGM and Gradient Blending as variants of functional gradient descent (Paper 2) with modality-dependent step sizes, then derive principled shrinkage/early-stopping criteria per modality using validation line-search or stability-based rules. The contribution is a concrete, plug-and-play training protocol that reduces the generalization drop sometimes induced by aggressive gradient modulation (Paper 5).",
        "**Cross-Modal Ensemble Distillation via Expert Advice and Boosting**  \nTrain strong uni-modal experts and a multimodal student; use an expert-advice scheme (Paper 1) to produce a time-varying mixture teacher signal, then distill into the student with a gradient-boosting-inspired residual objective (Paper 2). This directly targets the empirical phenomenon that uni-modal models can outperform naive multimodal models (Paper 4) by ensuring the student never underperforms the best expert during training."
      ],
      "generated_ideas_raw": "1. **Online No-Regret Gradient Blending for Multimodal Deep Nets**  \nDevelop an online learning variant of Gradient Blending (Paper 4) where modality weights are updated with a weighted-majority / expert-advice rule (Paper 1) using per-modality validation regret as losses. Provide theoretical no-regret guarantees on the weighting process and evaluate on audio-visual action recognition under non-stationary modality quality (e.g., changing noise levels).\n\n2. **Boosted Modality Specialists: Stagewise Additive Multimodal Fusion**  \nCast each modality pathway (and simple cross-modal interaction blocks) as \u201cweak learners\u201d and build a stagewise additive multimodal predictor via gradient boosting in function space (Paper 2). Each boosting iteration selects the modality (or interaction) that best fits the current residual/negative gradient, creating an interpretable sequence of \u201cwhich modality helped when,\u201d and reducing overfitting from jointly training a large fused model (Paper 4).\n\n3. **Generalization-Aware On-the-Fly Gradient Modulation via PAC-Bayes Bounds**  \nExtend OGM (Paper 5) by replacing heuristic discrepancy measures with a bound-driven control signal: estimate per-modality generalization gaps online using PAC-Bayes-style proxies or held-out sharpness/curvature indicators, then modulate gradients to minimize an upper bound on test risk. The actionable output is a concrete algorithm that dynamically down-weights modalities whose complexity-adjusted generalization bound worsens during training.\n\n4. **ReconBoost for Missing/Corrupted Modalities with Boosted Reconstruction Residuals**  \nBuild on ReconBoost (Paper 3) by explicitly boosting reconstruction modules: at each stage, add a small reconstructor that predicts one modality\u2019s features from others to reduce reconciliation error, then feed reconciled features into the classifier. Test robustness when modalities are missing at inference, and quantify tradeoffs between reconstruction boosting depth and classification accuracy.\n\n5. **Adversarial Modality-Quality Games: Robust Fusion via Decision-Theoretic Updates**  \nModel modality noise/corruption as an adversary and the fusion rule as the learner in a repeated game (Paper 1), where each modality is an \u201cexpert\u201d whose loss depends on current corruption. Derive a robust training procedure that updates modality weights to guarantee performance close to the best modality in hindsight, then integrate it with Gradient Blending/OGM as the inner optimizer for end-to-end networks.\n\n6. **Multi-Objective Boosting for Accuracy\u2013Calibration\u2013Fairness in Multimodal Systems**  \nUse vector-payoff/approachability ideas (Blackwell-style, Paper 1 references) to treat accuracy, calibration error, and group fairness metrics as simultaneous objectives. Implement a boosting-style procedure (Paper 2) where each stage chooses updates (per modality or shared) to steer the metric vector toward a target set, producing a practical algorithm that balances modalities without sacrificing calibration or fairness.\n\n7. **TreeBoost-Inspired Interpretable Gating Policies for Multimodal Fusion**  \nTrain a lightweight decision-tree ensemble (Paper 2) to predict per-sample modality weights using features like modality-specific confidence, gradient norms, and overfitting indicators (Papers 4\u20135). This yields an interpretable gating policy (\u201cwhen audio is noisy, trust vision\u201d) that can be plugged into deep multimodal classifiers and audited post hoc.\n\n8. **Curriculum Boosting Across Modalities Based on Differential Overfitting Rates**  \nOperationalize Paper 4\u2019s finding that modalities overfit at different rates by designing a boosting/curriculum schedule: start training with the modality that generalizes fastest, then gradually add others as new boosting stages that fit residual errors. Provide an ablation-driven recipe (stage timing, shrinkage, early stopping per stage) and test whether it consistently beats simultaneous fusion on small-data regimes.\n\n9. **Shrinkage and Early-Stopping Rules for OGM/Blending Derived from Functional Gradient Descent**  \nReinterpret OGM and Gradient Blending as variants of functional gradient descent (Paper 2) with modality-dependent step sizes, then derive principled shrinkage/early-stopping criteria per modality using validation line-search or stability-based rules. The contribution is a concrete, plug-and-play training protocol that reduces the generalization drop sometimes induced by aggressive gradient modulation (Paper 5).\n\n10. **Cross-Modal Ensemble Distillation via Expert Advice and Boosting**  \nTrain strong uni-modal experts and a multimodal student; use an expert-advice scheme (Paper 1) to produce a time-varying mixture teacher signal, then distill into the student with a gradient-boosting-inspired residual objective (Paper 2). This directly targets the empirical phenomenon that uni-modal models can outperform naive multimodal models (Paper 4) by ensuring the student never underperforms the best expert during training.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "1. **Online No-Regret Gradient Blending for Multimodal Deep Nets**  \nDevelop an online learning variant of Gradient Blending (Paper 4) where modality weights are updated with a weighted-majority / exp",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "**Boosted Modality Specialists: Stagewise Additive Multimodal Fusion**  \nCast each modality pathway (and simple cross-modal interaction blocks) as \u201cweak learners\u201d and build a stagewise additive multim",
          "is_match": true
        },
        {
          "idea_idx": 2,
          "idea_text": "**Generalization-Aware On-the-Fly Gradient Modulation via PAC-Bayes Bounds**  \nExtend OGM (Paper 5) by replacing heuristic discrepancy measures with a bound-driven control signal: estimate per-modalit",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "**ReconBoost for Missing/Corrupted Modalities with Boosted Reconstruction Residuals**  \nBuild on ReconBoost (Paper 3) by explicitly boosting reconstruction modules: at each stage, add a small reconstr",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "**Adversarial Modality-Quality Games: Robust Fusion via Decision-Theoretic Updates**  \nModel modality noise/corruption as an adversary and the fusion rule as the learner in a repeated game (Paper 1), ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "**Multi-Objective Boosting for Accuracy\u2013Calibration\u2013Fairness in Multimodal Systems**  \nUse vector-payoff/approachability ideas (Blackwell-style, Paper 1 references) to treat accuracy, calibration erro",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "**TreeBoost-Inspired Interpretable Gating Policies for Multimodal Fusion**  \nTrain a lightweight decision-tree ensemble (Paper 2) to predict per-sample modality weights using features like modality-sp",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "**Curriculum Boosting Across Modalities Based on Differential Overfitting Rates**  \nOperationalize Paper 4\u2019s finding that modalities overfit at different rates by designing a boosting/curriculum sched",
          "is_match": true
        },
        {
          "idea_idx": 8,
          "idea_text": "**Shrinkage and Early-Stopping Rules for OGM/Blending Derived from Functional Gradient Descent**  \nReinterpret OGM and Gradient Blending as variants of functional gradient descent (Paper 2) with modal",
          "is_match": false
        },
        {
          "idea_idx": 9,
          "idea_text": "**Cross-Modal Ensemble Distillation via Expert Advice and Boosting**  \nTrain strong uni-modal experts and a multimodal student; use an expert-advice scheme (Paper 1) to produce a time-varying mixture ",
          "is_match": true
        }
      ]
    },
    {
      "paper_idx": 70,
      "paper_title": "TransferTraj: A Vehicle Trajectory Learning Model for Region and Task Transferability",
      "contribution": "TransferTraj introduces a unified vehicle trajectory learning model that effectively achieves both region and task transferability without retraining.",
      "num_predecessors": 4,
      "predecessors_crawled": 4,
      "academic_sources": 4,
      "crawl_rate": 1.0,
      "ideas_generated": 6,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 6980,
      "output_tokens": 968,
      "predecessor_details": [
        {
          "success": true,
          "title": "Modeling trajectories with recurrent neural networks",
          "url": "https://dl.acm.org/doi/10.5555/3172077.3172319",
          "content": "Modeling trajectories with recurrent neural networks | Proceedings of the 26th International Joint Conference on Artificial Intelligence[skip to main content](#skip-to-main-content)\n[](#global-menu)\nSearch ACM Digital Library\nSearchSearch\n[Advanced Search](https://dl.acm.org/search/advanced)\n10.5555/3172077.3172319guideproceedingsArticle/Chapter ViewAbstractPublication PagesConference Proceedingsacm-pubtype\n[Browse](#)\n**## Export Citations\nSelect Citation formatBibTeXEndNoteACM Ref**\n* Please download or close your previous search result export first before starting a new bulk export.\nPreview is not available.\nBy clicking download,**a status dialog**will open to start the export process. The process may take**a few minutes**but once it finishes a file will be downloadable from your browser. You may continue to browse the DL while the export process is in progress.\n* ```\n```\n* [Download citation**](javascript:void(0))\n* [Copy citation**](javascript:void(0))\nArticle\nShare on\n* **\n* **\n* **\n* **\n* **\n# Modeling trajectories with recurrent neural networks\nAuthors:[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)HaoWu](#artseq-00001),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)ZiyangChen](#artseq-00002),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)WeiweiSun](#artseq-00003),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)BaihuaZheng](#artseq-00004),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)WeiWang](#artseq-00005)[Authors Info &amp; Claims](#tab-contributors)\n[IJCAI'17: Proceedings of the 26th International Joint Conference on Artificial Intelligence](https://dl.acm.org/doi/proceedings/10.5555/3172077)\nPages3083-3090\nPublished:19 August 2017[Publication History](#core-history)\n**12citation**0Downloads\nMetrics\n[\nTotal Citations12\n](#tab-citations)[\nTotal Downloads0\n](#tab-metrics-inner)\nLast 12 Months0\nLast 6 weeks0\n**Get Citation Alerts\n**## New Citation Alert added!\nThis alert has been successfully added and will be sent to:\nYou will be notified whenever a record that you have chosen has been cited.\nTo manage your alert preferences, click on the button below.\n[Manage my Alerts](https://dl.acm.org/action/showPreferences?menuTab=Alerts)\n**## New Citation Alert!\nPlease[log in to your account](https://dl.acm.org/action/showLogin?redirectUri=/doi/10.5555/3172077.3172319)\n**\n**\n[**Publisher Site](https://www.ijcai.org/proceedings/2017/0430.pdf)\n**Contents\n## Abstract\nModeling trajectory data is a building block for many smart-mobility initiatives. Existing approaches apply shallow models such as Markov chain and inverse reinforcement learning to model trajectories, which cannot capture the long-term dependencies. On the other hand, deep models such as Recurrent Neural Network (RNN) have demonstrated their strength of modeling variable length sequences. However, directly adopting RNN to model trajectories is not appropriate because of the unique topological constraints faced by trajectories. Motivated by these findings, we design two RNN-based models which can make full advantage of the strength of RNN to capture variable length sequence and meanwhile to address the constraints of topological structure on trajectory modeling. Our experimental study based on real taxi trajectory datasets shows that both of our approaches largely outperform the existing approaches.\n## References\n[1]\nKyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation.*arXiv preprint arXiv:1406.1078*, 2014.\n[Google Scholar](https://scholar.google.com/scholar?q=Kyunghyun+Cho,+Bart+Van+Merri\u00ebnboer,+Caglar+Gulcehre,+Dzmitry+Bahdanau,+Fethi+Bougares,+Holger+Schwenk,+and+Yoshua+Bengio.+Learning+phrase+representations+using+rnn+encoder-decoder+for+statistical+machine+translation.+arXiv+preprint+arXiv:1406.1078+,+2014.)\n[2]\nJeffrey L. Elman. Finding structure in time.*Cognitive science*, 14(2):179-211, 1990.\n[Google Scholar](https://scholar.google.com/scholar?q=Jeffrey+L.+Elman.+Finding+structure+in+time.+Cognitive+science+,+14(2):179-211,+1990.)\n[3]\nAlex Graves. Generating sequences with recurrent neural networks.*arXiv preprint arXiv:1308.0850*, 2013.\n[Google Scholar](https://scholar.google.com/scholar?q=Alex+Graves.+Generating+sequences+with+recurrent+neural+networks.+arXiv+preprint+arXiv:1308.0850+,+2013.)\n[4]\nAnders Gustavsson, Anders Magnuson, Bj\u00f6rn Blomberg, Magnus Andersson, Jonas Halfvarson, and Curt Tysk. On the difficulty of training recurrent neural networks.*Computer Science*, 52(3):337-345, 2012.\n[Google Scholar](https://scholar.google.com/scholar?q=Anders+Gustavsson,+Anders+Magnuson,+Bj\u00f6rn+Blomberg,+Magnus+Andersson,+Jonas+Halfvarson,+and+Curt+Tysk.+On+the+difficulty+of+training+recurrent+neural+networks.+Computer+Science+,+52(3):337-345,+2012.)\n[5]\nMichael Gutmann and Aapo Hyv\u00e4rinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In*AISTATS'10*, pages 297-304, 2010.\n[Google Scholar](https://scholar.google.com/scholar?q=Michael+Gutmann+and+Aapo+Hyv\u00e4rinen.+Noise-contrastive+estimation:+A+new+estimation+principle+for+unnormalized+statistical+models.+In+AISTATS'10+,+pages+297-304,+2010.)\n[6]\nGeoffrey Hinton. Neural networks for machine learning. Coursera video lectures, 2012.\n[Google Scholar](https://scholar.google.com/scholar?q=Geoffrey+Hinton.+Neural+networks+for+machine+learning.+Coursera+video+lectures,+2012.)\n[7]\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory.*Neural computation*, 9(8):1735-1780, 1997.\n[Digital Library](https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1162/neco.1997.9.8.1735)\n[8]\nSepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\n[Google Scholar](https://scholar.google.com/scholar?q=Sepp+Hochreiter,+Yoshua+Bengio,+Paolo+Frasconi,+and+J\u00fcrgen+Schmidhuber.+Gradient+flow+in+recurrent+nets:+the+difficulty+of+learning+long-term+dependencies,+2001.)\n[9]\nS\u00e9bastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target vocabulary for neural machine translation. In*ACL'15*, 2015.\n[Crossref](https://doi.org/10.3115/v1/P15-1001)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.3115/v1/P15-1001)\n[10]\nChen Liang, Jonathan Berant, Quoc Le, Kenneth D Forbus, and Ni Lao. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision.*arXiv preprint arXiv:1611.00020*, 2016.\n[Google Scholar](https://scholar.google.com/scholar?q=Chen+Liang,+Jonathan+Berant,+Quoc+Le,+Kenneth+D+Forbus,+and+Ni+Lao.+Neural+symbolic+machines:+Learning+semantic+parsers+on+freebase+with+weak+supervision.+arXiv+preprint+arXiv:1611.00020+,+2016.)\n[11]\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE.*Journal of Machine Learning Research*, 9(Nov):2579-2605, 2008.\n[Google Scholar](https://scholar.google.com/scholar?q=Laurens+van+der+Maaten+and+Geoffrey+Hinton.+Visualizing+data+using+t-SNE.+Journal+of+Machine+Learning+Research+,+9(Nov):2579-2605,+2008.)\n[12]\nChristopher D Manning, Prabhakar Raghavan, Hinrich Sch\u00fctze, et al.*Introduction to Information Retrieval*, volume 1. Cambridge University Press, 2008.\n[Digital Library](https://dl.acm.org/doi/10.5555/1394399)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.5555/1394399)\n[13]\nTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In*NIPS'13*, pages 3111-3119, 2013.\n[Digital Library](https://dl.acm.org/doi/10.5555/2999792.2999959)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.555",
          "original_query": "Modeling trajectories with recurrent neural networks",
          "cleaned_query": "Modeling trajectories with recurrent neural networks"
        },
        {
          "success": true,
          "title": "MTrajRec: Map-Constrained Trajectory Recovery via ...",
          "url": "https://dl.acm.org/doi/10.1145/3447548.3467238",
          "content": "MTrajRec | Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining[skip to main content](#skip-to-main-content)\n[](#global-menu)\nSearch ACM Digital Library\nSearchSearch\n[Advanced Search](https://dl.acm.org/search/advanced)\n10.1145/3447548.3467238acmconferencesArticle/Chapter ViewAbstractPublication PageskddConference Proceedingsconference-collections\n[kdd](#)\n**## Export Citations\nSelect Citation formatBibTeXEndNoteACM Ref**\n* Please download or close your previous search result export first before starting a new bulk export.\nPreview is not available.\nBy clicking download,**a status dialog**will open to start the export process. The process may take**a few minutes**but once it finishes a file will be downloadable from your browser. You may continue to browse the DL while the export process is in progress.\n* ```\n```\n* [Download citation**](javascript:void(0))\n* [Copy citation**](javascript:void(0))\nresearch-article\n**Public Access\nShare on\n* **\n* **\n* **\n* **\n* **\n# MTrajRec:Map-Constrained Trajectory Recovery via Seq2Seq Multi-task Learning\nAuthors:[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)HuiminRen](#artseq-00001),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)SijieRuan](#artseq-00002),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)YanhuaLi](#artseq-00003),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)JieBao](#artseq-00004),+ 3,[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)ChuishiMeng](#artseq-00005),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)RuiyuanLi](#artseq-00006),[![](https://dl.acm.org/action/showDoPubAsset?doi=10.1145/contrib-81350600388&amp;format=rel-imgonly&amp;assetId=tiny_head_yuzheng.jpg)YuZheng](#artseq-00007)(Less)[Authors Info &amp; Claims](#tab-contributors)\n[KDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining](https://dl.acm.org/doi/proceedings/10.1145/3447548)\nPages1410-1419\n[https://doi.org/10.1145/3447548.3467238](https://doi.org/10.1145/3447548.3467238)\nPublished:14 August 2021[Publication History](#core-history)[](#)\n**70citation**2,398Downloads\nMetrics\n[\nTotal Citations70\n](#tab-citations)[\nTotal Downloads2,398\n](#tab-metrics-inner)\nLast 12 Months548\nLast 6 weeks53\n**Get Citation Alerts\n**## New Citation Alert added!\nThis alert has been successfully added and will be sent to:\nYou will be notified whenever a record that you have chosen has been cited.\nTo manage your alert preferences, click on the button below.\n[Manage my Alerts](https://dl.acm.org/action/showPreferences?menuTab=Alerts)\n**## New Citation Alert!\nPlease[log in to your account](https://dl.acm.org/action/showLogin?redirectUri=/doi/10.1145/3447548.3467238)\n**\n**\n[**PDF](https://dl.acm.org/doi/pdf/10.1145/3447548.3467238)[**eReader](https://dl.acm.org/doi/epdf/10.1145/3447548.3467238)\n**Contents\n## Abstract\nWith the increasing adoption of GPS modules, there are a wide range of urban applications based on trajectory data analysis, such as vehicle navigation, travel time estimation, and driver behavior analysis. The effectiveness of urban applications relies greatly on the high sampling rates of trajectories precisely matched to the map. However, a large number of trajectories are collected under a low sampling rate in real-world practice, due to certain communication loss and energy constraints. To enhance the trajectory data and support the urban applications more effectively, many trajectory recovery methods are proposed to infer the trajectories in free space. In addition, the recovered trajectory still needs to be mapped to the road network, before it can be used in the applications. However, the two-stage pipeline, which first infers high-sampling-rate trajectories and then performs the map matching, is inaccurate and inefficient. In this paper, we propose a Map-constrained Trajectory Recovery framework, MTrajRec, to recover the fine-grained points in trajectories and map match them on the road network in an end-to-end manner. MTrajRec implements a multi-task sequence-to-sequence learning architecture to predict road segment and moving ratio simultaneously. Constraint mask, attention mechanism, and attribute module are proposed to overcome the limits of coarse grid representation and improve the performance. Extensive experiments based on large-scale real-world trajectory data confirm the effectiveness and efficiency of our approach.\n## Formats available\nYou can view the full content in the following formats:\n[**PDF](https://dl.acm.org/doi/pdf/10.1145/3447548.3467238)\n## Supplementary Material\nMP4 File(mtrajrec\\_mapconstrained\\_trajectory\\_recovery\\_via-huimin\\_ren.mp4)\n20-minute talk that goes with our paper \"MTrajRec: Map-Constrained Trajectory Recovery via Seq2Seq Multi-task Learning\", presented at ACM SIGKDD in August 2021.\n* [Download](https://dl.acm.org/doi/suppl/10.1145/3447548.3467238/suppl_file/mtrajrec_mapconstrained_trajectory_recovery_via-huimin_ren.mp4)\n* 40.80 MB\n## References\n[1]\nD. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv:1409.0473, 2014.\n[Google Scholar](https://scholar.google.com/scholar?q=D.+Bahdanau,+K.+Cho,+and+Y.+Bengio.+Neural+machine+translation+by+jointly+learning+to+align+and+translate.+arXiv:1409.0473,+2014.)\n[2]\nR. Caruana. Multitask learning. Machine learning, 28(1):41--75, 1997.\n[Digital Library](https://dl.acm.org/doi/10.1023/A:1007379606734)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1023/A:1007379606734)\n[3]\nK. Cho, B. Van Merri\u00ebnboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv:1406.1078, 2014.\n[Google Scholar](https://scholar.google.com/scholar?q=K.+Cho,+B.+Van+Merri\u00ebnboer,+C.+Gulcehre,+D.+Bahdanau,+F.+Bougares,+H.+Schwenk,+and+Y.+Bengio.+Learning+phrase+representations+using+rnn+encoder-decoder+for+statistical+machine+translation.+arXiv:1406.1078,+2014.)\n[4]\nG. Cui, J. Luo, and X. Wang. Personalized travel route recommendation using collaborative filtering based on gps trajectories. International journal of digital earth, 11(3):284--307, 2018.\n[Crossref](https://doi.org/10.1080/17538947.2017.1326535)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1080/17538947.2017.1326535)\n[5]\nW. Dong, J. Li, R. Yao, C. Li, T. Yuan, and L. Wang. Characterizing driving styles with deep learning. arXiv:1607.03611, 2016.\n[Google Scholar](https://scholar.google.com/scholar?q=W.+Dong,+J.+Li,+R.+Yao,+C.+Li,+T.+Yuan,+and+L.+Wang.+Characterizing+driving+styles+with+deep+learning.+arXiv:1607.03611,+2016.)\n[6]\nX. Fang, J. Huang, F. Wang, L. Zeng, H. Liang, and H. Wang. Constgat: Contextual spatial-temporal graph attention network for travel time estimation at baidu maps. In Proc. of the 26th ACM SIGKDD, pages 2697--2705, 2020.\n[Digital Library](https://dl.acm.org/doi/10.1145/3394486.3403320)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/3394486.3403320)\n[7]\nJ. Feng, Y. Li, C. Zhang, F. Sun, F. Meng, A. Guo, and D. Jin. Deepmove: Predicting human mobility with attentional recurrent networks. In Proc. of the 2018 WWW conference, pages 1459--1468, 2018.\n[Digital Library](https://dl.acm.org/doi/10.1145/3178876.3186058)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/3178876.3186058)\n[8]\nY. Gal and Z. Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. arXiv:1512.05287, 2015.\n[Google Scholar](https://scholar.google.com/scholar?q=Y.+Gal+and+Z.+Ghahramani.+A+theoretically+grounded+application+of+dropout+in+recurrent+neural+networks.+arXiv:1512.05287,+2015.)\n[9]\nH. Hong, Y. Lin, X. Yang, Z. Li, K. Fu, Z. Wang, X. Qie, and J. Ye. Heteta: Heterogeneous information network embeddi",
          "original_query": "Mtrajrec: Map-constrained trajectory recovery via seq2seq multi-task learning",
          "cleaned_query": "Mtrajrec: Map-constrained trajectory recovery via seq2seq multi-task learning"
        },
        {
          "success": true,
          "title": "Trembr: Road Networks for Trajectory Representation Learning",
          "url": "https://dl.acm.org/doi/10.1145/3361741",
          "content": "In this article, we propose a novel representation learning framework, namely TRajectory EMBedding via Road networks (Trembr) , to learn trajectory embeddings (low-dimensional feature vectors) for use in a variety of trajectory applications. The novelty of Trembr lies in (1) the design of a recurrent neural network\u2013(RNN) based encoder\u2013decoder model, namely Traj2Vec , that encodes spatial and temporal properties inherent in trajectories into trajectory embeddings by exploiting the underlying road networks to constrain the learning process in accordance with the matched road segments obtained using road network matching techniques (e.g., Barefoot [24, 27]), and (2) the design of a neural network\u2013based model, namely Road2Vec , to learn road segment embeddings in road networks that captures various relationships amongst road segments in preparation for trajectory representation learning. In addition to model design, several unique technical issues raising in Trembr, including data preparation in Road2Vec, the road segment relevance-aware loss, and the network topology constraint in Traj2Vec, are examined. To validate our ideas, we learn trajectory embeddings using multiple large-scale real-world trajectory datasets and use them in three tasks, including trajectory similarity measure, travel time prediction, and destination prediction. Empirical results show that Trembr soundly outperforms the state-of-the-art trajectory representation learning models, trajectory2vec and t2vec , by at least one order of magnitude in terms of mean rank in trajectory similaritymeasure, 23.3% to 41.7% in terms of mean absolute error (MAE) in travel time prediction, and 39.6% to 52.4% in terms of MAE in destination prediction.",
          "original_query": "TremBR: Exploring road networks for trajectory representation learning",
          "cleaned_query": "TremBR: Exploring road networks for trajectory representation learning"
        },
        {
          "success": true,
          "title": "Self-supervised Trajectory Representation Learning with ...",
          "url": "https://arxiv.org/abs/2211.09510",
          "content": "# Computer Science > Machine Learning\n\n**arXiv:2211.09510** (cs)\n\n\\[Submitted on 17 Nov 2022 ( [v1](https://arxiv.org/abs/2211.09510v1)), last revised 7 Mar 2024 (this version, v4)\\]\n\n# Title:Self-supervised Trajectory Representation Learning with Temporal Regularities and Travel Semantics\n\nAuthors: [Jiawei Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang,+J), [Dayan Pan](https://arxiv.org/search/cs?searchtype=author&query=Pan,+D), [Houxing Ren](https://arxiv.org/search/cs?searchtype=author&query=Ren,+H), [Xiaohan Jiang](https://arxiv.org/search/cs?searchtype=author&query=Jiang,+X), [Chao Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+C), [Jingyuan Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+J)\n\nView a PDF of the paper titled Self-supervised Trajectory Representation Learning with Temporal Regularities and Travel Semantics, by Jiawei Jiang and 5 other authors\n\n[View PDF](https://arxiv.org/pdf/2211.09510) [HTML (experimental)](https://arxiv.org/html/2211.09510v4)\n\n> Abstract:Trajectory Representation Learning (TRL) is a powerful tool for spatial-temporal data analysis and management. TRL aims to convert complicated raw trajectories into low-dimensional representation vectors, which can be applied to various downstream tasks, such as trajectory classification, clustering, and similarity computation. Existing TRL works usually treat trajectories as ordinary sequence data, while some important spatial-temporal characteristics, such as temporal regularities and travel semantics, are not fully exploited. To fill this gap, we propose a novel Self-supervised trajectory representation learning framework with TemporAl Regularities and Travel semantics, namely START. The proposed method consists of two stages. The first stage is a Trajectory Pattern-Enhanced Graph Attention Network (TPE-GAT), which converts the road network features and travel semantics into representation vectors of road segments. The second stage is a Time-Aware Trajectory Encoder (TAT-Enc), which encodes representation vectors of road segments in the same trajectory as a trajectory representation vector, meanwhile incorporating temporal regularities with the trajectory representation. Moreover, we also design two self-supervised tasks, i.e., span-masked trajectory recovery and trajectory contrastive learning, to introduce spatial-temporal characteristics of trajectories into the training process of our START framework. The effectiveness of the proposed method is verified by extensive experiments on two large-scale real-world datasets for three downstream tasks. The experiments also demonstrate that our method can be transferred across different cities to adapt heterogeneous trajectory datasets.\n\n| | |\n| --- | --- |\n| Comments: | 13 pages, 10 figures, Accepted by ICDE 2023 |\n| Subjects: | Machine Learning (cs.LG) |\n| Cite as: | [arXiv:2211.09510](https://arxiv.org/abs/2211.09510) \\[cs.LG\\] |\n| | (or [arXiv:2211.09510v4](https://arxiv.org/abs/2211.09510v4) \\[cs.LG\\] for this version) |\n| | [https://doi.org/10.48550/arXiv.2211.09510](https://doi.org/10.48550/arXiv.2211.09510) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Jiawei Jiang \\[ [view email](https://arxiv.org/show-email/53dd1e74/2211.09510)\\]\n\n**[\\[v1\\]](https://arxiv.org/abs/2211.09510v1)**\nThu, 17 Nov 2022 13:14:47 UTC (3,029 KB)\n\n**[\\[v2\\]](https://arxiv.org/abs/2211.09510v2)**\nMon, 21 Nov 2022 09:53:16 UTC (3,021 KB)\n\n**[\\[v3\\]](https://arxiv.org/abs/2211.09510v3)**\nThu, 19 Jan 2023 07:56:28 UTC (3,022 KB)\n\n**\\[v4\\]**\nThu, 7 Mar 2024 16:15:54 UTC (3,022 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Self-supervised Trajectory Representation Learning with Temporal Regularities and Travel Semantics, by Jiawei Jiang and 5 other authors\n\n- [View PDF](https://arxiv.org/pdf/2211.09510)\n- [HTML (experimental)](https://arxiv.org/html/2211.09510v4)\n- [TeX Source](https://arxiv.org/src/2211.09510)\n- [Other Formats](https://arxiv.org/format/2211.09510)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.LG\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2211.09510&function=prev&context=cs.LG)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2211.09510&function=next&context=cs.LG)\n\n[new](https://arxiv.org/list/cs.LG/new) \\| [recent](https://arxiv.org/list/cs.LG/recent) \\| [2022-11](https://arxiv.org/list/cs.LG/2022-11)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2211.09510?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2211.09510)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2211.09510)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2211.09510)\n\n[a](https://arxiv.org/static/browse/0.3.4/css/cite.css) export BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2211.09510&description=Self-supervised Trajectory Representation Learning with Temporal Regularities and Travel Semantics) [![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2211.09510&title=Self-supervised Trajectory Representation Learning with Temporal Regularities and Travel Semantics)\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\nIArxiv recommender toggle\n\nIArxiv Recommender _( [What is IArxiv?](https://iarxiv.org/about))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/",
          "original_query": "Self-supervised trajectory representation learning with temporal regularities and travel semantics",
          "cleaned_query": "Self-supervised trajectory representation learning with temporal regularities and travel semantics"
        }
      ],
      "generated_ideas": [
        "Probabilistic Map-Constrained Recovery with Uncertainty-Calibrated Outputs\nAdd a probabilistic decoding layer to MTrajRec that outputs distributions over candidate road segments and continuous moving ratios, not just point estimates. Use constrained beam search on the road network plus calibration losses (e.g., reliability diagrams for segment probabilities) so downstream systems can trade off accuracy vs. risk (e.g., discard high-uncertainty recovered portions).",
        "Topology-Aware Contrastive Learning with Hard Negatives from Network Proximity\nDesign a contrastive objective where negatives are selected by road-network-aware rules: (a) same origin-destination but different route, (b) nearby subpaths with different turn sequences, and (c) paths with similar length but different functional road classes. Integrate this into START\u2019s trajectory contrastive learning so representations distinguish subtle route-choice behaviors that naive random negatives fail to separate.",
        "Multi-Resolution Trajectory Modeling from Grids to Road Segments to Lanes\nBuild a hierarchical encoder-decoder that first predicts coarse grid transitions (for robustness under noise), then refines to road segments (as in MTrajRec/Trembr), and finally to lane-level sequences where available. Use consistency constraints between levels (e.g., segment must lie within predicted grid cell; lane must lie on predicted segment) to improve recovery and representation for high-precision applications.",
        "Causally Regularized Time-Aware Encoders for Irregular Sampling and Stops\nExtend START\u2019s Time-Aware Trajectory Encoder by explicitly modeling dwell times and stop events as first-class tokens with causal/monotonic constraints (time must increase; speed/stop-state transitions must be plausible). Train with masked \u201ctime-gap reconstruction\u201d and \u201cstop-span recovery\u201d tasks to better handle low-rate sampling and urban congestion patterns.",
        "Joint Trajectory Embedding and Destination/ETA Heads via Multi-Task Self-Training\nCombine Trembr\u2019s strong downstream performance (destination prediction, travel time prediction) with START\u2019s self-supervised pretraining by adding lightweight prediction heads during pretraining. Use pseudo-labeling: generate noisy ETA/destination labels from historical averages or routing engines, then refine jointly with representation learning to reduce domain shift between pretraining and deployment tasks.",
        "Trajectory Representation Learning under Map Errors and Network Drift\nCreate a robustness benchmark and model extension where the road network is partially wrong (missing roads, outdated turn restrictions) and trajectories are matched imperfectly. Modify Trembr/START encoders to incorporate \u201cnetwork uncertainty\u201d (multiple plausible maps/edits) and train with consistency losses across perturbed maps, yielding embeddings and recovery that degrade gracefully under real-world map drift."
      ],
      "generated_ideas_raw": "1. **Unified Self-Supervised Map-Constrained Trajectory Recovery (SS-MTrajRec)**\n   Extend MTrajRec by replacing purely supervised seq2seq training with START-style pretext tasks (span-masked recovery + contrastive learning) over low-sampling GPS. Train the model to jointly impute points, predict road segments/moving ratios, and align recovered trajectories with their own augmented views, improving robustness when labeled map-matched data is scarce.\n\n2. **Probabilistic Map-Constrained Recovery with Uncertainty-Calibrated Outputs**\n   Add a probabilistic decoding layer to MTrajRec that outputs distributions over candidate road segments and continuous moving ratios, not just point estimates. Use constrained beam search on the road network plus calibration losses (e.g., reliability diagrams for segment probabilities) so downstream systems can trade off accuracy vs. risk (e.g., discard high-uncertainty recovered portions).\n\n3. **Road2Vec + TPE-GAT Hybrid Road Embeddings for Cold-Start Segments**\n   Combine Trembr\u2019s Road2Vec (topology/relationship modeling) with START\u2019s TPE-GAT (pattern/semantic features) into a single road-segment encoder trained with multi-view self-supervision. Evaluate on \u201cnew road\u201d or \u201csparsely traveled\u201d segments by masking traversal data and forcing the encoder to infer embeddings from network structure + attributes, improving transfer to rapidly changing cities.\n\n4. **Topology-Aware Contrastive Learning with Hard Negatives from Network Proximity**\n   Design a contrastive objective where negatives are selected by road-network-aware rules: (a) same origin-destination but different route, (b) nearby subpaths with different turn sequences, and (c) paths with similar length but different functional road classes. Integrate this into START\u2019s trajectory contrastive learning so representations distinguish subtle route-choice behaviors that naive random negatives fail to separate.\n\n5. **Multi-Resolution Trajectory Modeling from Grids to Road Segments to Lanes**\n   Build a hierarchical encoder-decoder that first predicts coarse grid transitions (for robustness under noise), then refines to road segments (as in MTrajRec/Trembr), and finally to lane-level sequences where available. Use consistency constraints between levels (e.g., segment must lie within predicted grid cell; lane must lie on predicted segment) to improve recovery and representation for high-precision applications.\n\n6. **Causally Regularized Time-Aware Encoders for Irregular Sampling and Stops**\n   Extend START\u2019s Time-Aware Trajectory Encoder by explicitly modeling dwell times and stop events as first-class tokens with causal/monotonic constraints (time must increase; speed/stop-state transitions must be plausible). Train with masked \u201ctime-gap reconstruction\u201d and \u201cstop-span recovery\u201d tasks to better handle low-rate sampling and urban congestion patterns.\n\n7. **Joint Trajectory Embedding and Destination/ETA Heads via Multi-Task Self-Training**\n   Combine Trembr\u2019s strong downstream performance (destination prediction, travel time prediction) with START\u2019s self-supervised pretraining by adding lightweight prediction heads during pretraining. Use pseudo-labeling: generate noisy ETA/destination labels from historical averages or routing engines, then refine jointly with representation learning to reduce domain shift between pretraining and deployment tasks.\n\n8. **Constrained Generative Trajectory Simulation for Data Augmentation**\n   Develop an RNN/seq2seq generator (building on Paper 1 and MTrajRec) that samples realistic trajectories under road-network constraints, conditioned on time-of-day and coarse OD cells. Use the generator to augment rare patterns (e.g., nighttime routes, event traffic) and quantify gains in recovery/embedding tasks, while enforcing validity through constraint masks and topology-aware decoding.\n\n9. **City-to-City Transfer with Network Alignment and Semantic Anchors**\n   Improve START\u2019s cross-city transfer by explicitly aligning road embeddings across cities using shared semantic anchors (road class, speed limit bins, intersection type) plus graph matching of local motifs (e.g., 4-way intersections, highway ramps). Train with a bilingual-style alignment loss so a trajectory encoder trained in City A can embed City B trajectories into a comparable space without extensive fine-tuning.\n\n10. **Trajectory Representation Learning under Map Errors and Network Drift**\n   Create a robustness benchmark and model extension where the road network is partially wrong (missing roads, outdated turn restrictions) and trajectories are matched imperfectly. Modify Trembr/START encoders to incorporate \u201cnetwork uncertainty\u201d (multiple plausible maps/edits) and train with consistency losses across perturbed maps, yielding embeddings and recovery that degrade gracefully under real-world map drift.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Probabilistic Map-Constrained Recovery with Uncertainty-Calibrated Outputs\nAdd a probabilistic decoding layer to MTrajRec that outputs distributions over candidate road segments and continuous moving ",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Topology-Aware Contrastive Learning with Hard Negatives from Network Proximity\nDesign a contrastive objective where negatives are selected by road-network-aware rules: (a) same origin-destination but ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Multi-Resolution Trajectory Modeling from Grids to Road Segments to Lanes\nBuild a hierarchical encoder-decoder that first predicts coarse grid transitions (for robustness under noise), then refines to",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Causally Regularized Time-Aware Encoders for Irregular Sampling and Stops\nExtend START\u2019s Time-Aware Trajectory Encoder by explicitly modeling dwell times and stop events as first-class tokens with cau",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Joint Trajectory Embedding and Destination/ETA Heads via Multi-Task Self-Training\nCombine Trembr\u2019s strong downstream performance (destination prediction, travel time prediction) with START\u2019s self-supe",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Trajectory Representation Learning under Map Errors and Network Drift\nCreate a robustness benchmark and model extension where the road network is partially wrong (missing roads, outdated turn restrict",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 71,
      "paper_title": "PhySense: Sensor Placement Optimization for Accurate Physics Sensing",
      "contribution": "PhySense introduces a synergistic two-stage framework that jointly optimizes sensor placements and reconstructs physical fields for improved accuracy in physics sensing.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 4,
      "crawl_rate": 1.0,
      "ideas_generated": 9,
      "hit_at_k": true,
      "matching_idea_idx": 3,
      "input_tokens": 9269,
      "output_tokens": 994,
      "predecessor_details": [
        {
          "success": true,
          "title": "Global field reconstruction from sparse sensors with ...",
          "url": "https://arxiv.org/abs/2101.00554",
          "content": "\n \n \n \n \n \n \n Download PDF \n Abstract: Achieving accurate and robust global situational awareness of a complex\ntime-evolving field from a limited number of sensors has been a longstanding\nchallenge. This reconstruction problem is especially difficult when sensors are\nsparsely positioned in a seemingly random or unorganized manner, which is often\nencountered in a range of scientific and engineering problems. Moreover, these\nsensors can be in motion and can become online or offline over time. The key\nleverage in addressing this scientific issue is the wealth of data accumulated\nfrom the sensors. As a solution to this problem, we propose a data-driven\nspatial field recovery technique founded on a structured grid-based\ndeep-learning approach for arbitrary positioned sensors of any numbers. It\nshould be noted that the na\u00efve use of machine learning becomes prohibitively\nexpensive for global field reconstruction and is furthermore not adaptable to\nan arbitrary number of sensors. In the present work, we consider the use of\nVoronoi tessellation to obtain a structured-grid representation from sensor\nlocations enabling the computationally tractable use of convolutional neural\nnetworks. One of the central features of the present method is its\ncompatibility with deep-learning based super-resolution reconstruction\ntechniques for structured sensor data that are established for image\nprocessing. The proposed reconstruction technique is demonstrated for unsteady\nwake flow, geophysical data, and three-dimensional turbulence. The current\nframework is able to handle an arbitrary number of moving sensors, and thereby\novercomes a major limitation with existing reconstruction methods. The\npresented technique opens a new pathway towards the practical use of neural\nnetworks for real-time global field estimation.\n \n \n \n \n Submission history From: Kai Fukami [ view email]\n \n [v1] \n Sun, 3 Jan 2021 03:43:53 UTC (5,624 KB) [v2] \nThu, 22 Jul 2021 09:34:11 UTC (6,026 KB) ||||I|||| Skip to main content\n We gratefully acknowledge support from\n the Simons Foundation and member institutions.\n > physics > arXiv:2101.00554\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Physics > Fluid Dynamics\n\n arXiv:2101.00554 (physics)\n [Submitted on 3 Jan 2021 (v1), last revised 22 Jul 2021 (this version, v2)]\n\n Title: Global field reconstruction from sparse sensors with Voronoi tessellation-assisted deep learning\n\n Authors: Kai Fukami, Romit Maulik, Nesar Ramachandra, Koji Fukagata, Kunihiko Taira\n Download PDF\n Abstract: Achieving accurate and robust global situational awareness of a complex time-evolving field from a limited number of sensors has been a longstanding challenge. This reconstruction problem is especially difficult when sensors are sparsely positioned in a seemingly random or unorganized manner, which is often encountered in a range of scientific and engineering problems. Moreover, these sensors can be in motion and can become online or offline over time. The key leverage in addressing this scientific issue is the wealth of data accumulated from the sensors. As a solution to this problem, we propose a data-driven spatial field recovery technique founded on a structured grid-based deep-learning approach for arbitrary positioned sensors of any numbers. It should be noted that the na\u00efve use of machine learning becomes prohibitively expensive for global field reconstruction and is furthermore not adaptable to an arbitrary number of sensors. In the present work, we consider the use of Voronoi tessellation to obtain a structured-grid representation from sensor locations enabling the computationally tractable use of convolutional neural networks. One of the central features of the present method is its compatibility with deep-learning based super-resolution reconstruction techniques for structured sensor data that are established for image processing. The proposed reconstruction technique is demonstrated for unsteady wake flow, geophysical data, and three-dimensional turbulence. The current framework is able to handle an arbitrary number of moving sensors, and thereby overcomes a major limitation with existing reconstruction methods. The presented technique opens a new pathway towards the practical use of neural networks for real-time global field estimation.\n Subjects: Fluid Dynamics (physics.flu-dyn) ; Machine Learning (cs.LG); Computational Physics (physics.comp-ph)\n Cite as: arXiv:2101.00554 [physics.flu-dyn] \n (or arXiv:2101.00554v2 [physics.flu-dyn] for this version) \n https://doi.org/10.48550/arXiv.2101.00554 \n Focus to learn more \n arXiv-issued DOI via DataCite \n https://doi.org/10.1038/s42256-021-00402-2 \n Related DOI: Focus to learn more \n DOI(s) linking to related resources \n \n\n Submission history\n\n From: Kai Fukami [view email]\n [v1] Sun, 3 Jan 2021 03:43:53 UTC (5,624 KB)\n [v2] Thu, 22 Jul 2021 09:34:11 UTC (6,026 KB)\n Full-text links:\n\n Download:\n\n * PDF\n * Other formats\n Current browse context:\n physics.flu-dyn\n < prev | next >\n new | recent | 2101\n Change to browse by:\n cs\n cs.LG\n physics\n physics.comp-ph\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export bibtex citation Loading...\n\n Bibtex formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Related Papers\n\n Recommenders and Search Tools\n\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs and how to get involved.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Global field reconstruction from sparse sensors with Voronoi tessellation-assisted deep learning",
          "cleaned_query": "Global field reconstruction from sparse sensors with Voronoi tessellation-assisted deep learning"
        },
        {
          "success": true,
          "title": "Denoising Diffusion Probabilistic Models in Six Simple Steps",
          "url": "https://arxiv.org/abs/2402.04384",
          "content": "[2402.04384] Denoising Diffusion Probabilistic Models in Six Simple Steps\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2402.04384\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2402.04384**(cs)\n[Submitted on 6 Feb 2024 ([v1](https://arxiv.org/abs/2402.04384v1)), last revised 10 Feb 2024 (this version, v2)]\n# Title:Denoising Diffusion Probabilistic Models in Six Simple Steps\nAuthors:[Richard E. Turner](https://arxiv.org/search/cs?searchtype=author&amp;query=Turner,+R+E),[Cristiana-Diana Diaconu](https://arxiv.org/search/cs?searchtype=author&amp;query=Diaconu,+C),[Stratis Markou](https://arxiv.org/search/cs?searchtype=author&amp;query=Markou,+S),[Aliaksandra Shysheya](https://arxiv.org/search/cs?searchtype=author&amp;query=Shysheya,+A),[Andrew Y. K. Foong](https://arxiv.org/search/cs?searchtype=author&amp;query=Foong,+A+Y+K),[Bruno Mlodozeniec](https://arxiv.org/search/cs?searchtype=author&amp;query=Mlodozeniec,+B)\nView a PDF of the paper titled Denoising Diffusion Probabilistic Models in Six Simple Steps, by Richard E. Turner and 4 other authors\n[View PDF](https://arxiv.org/pdf/2402.04384)> > Abstract:\n> Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. In this note, we distill down the formulation of the DDPM into six simple steps each of which comes with a clear rationale. We assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, Gaussian distributions, maximum likelihood estimation, and deep learning. Subjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2402.04384](https://arxiv.org/abs/2402.04384)[cs.LG]|\n|(or[arXiv:2402.04384v2](https://arxiv.org/abs/2402.04384v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2402.04384](https://doi.org/10.48550/arXiv.2402.04384)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Richard Turner [[view email](https://arxiv.org/show-email/66f8e9a9/2402.04384)]\n**[[v1]](https://arxiv.org/abs/2402.04384v1)**Tue, 6 Feb 2024 20:43:04 UTC (911 KB)\n**[v2]**Sat, 10 Feb 2024 19:19:34 UTC (911 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Denoising Diffusion Probabilistic Models in Six Simple Steps, by Richard E. Turner and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2402.04384)\n* [TeX Source](https://arxiv.org/src/2402.04384)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2402.04384&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2402.04384&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2024-02](https://arxiv.org/list/cs.LG/2024-02)\nChange to browse by:\n[cs](https://arxiv.org/abs/2402.04384?context=cs)\n[stat](https://arxiv.org/abs/2402.04384?context=stat)\n[stat.ML](https://arxiv.org/abs/2402.04384?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2402.04384)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2402.04384)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2402.04384)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's com",
          "original_query": "Denoising diffusion probabilistic models",
          "cleaned_query": "Denoising diffusion probabilistic models"
        },
        {
          "success": true,
          "title": "Near-Optimal Sensor Placements in Gaussian Processes",
          "url": "https://dl.acm.org/doi/10.5555/1390681.1390689",
          "content": "Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies | The Journal of Machine Learning Research[skip to main content](#skip-to-main-content)\n[](#global-menu)\nSearch ACM Digital Library\nSearchSearch\n[Advanced Search](https://dl.acm.org/search/advanced)\n[The Journal of Machine Learning Research](#)\n**## Export Citations\nSelect Citation formatBibTeXEndNoteACM Ref**\n* Please download or close your previous search result export first before starting a new bulk export.\nPreview is not available.\nBy clicking download,**a status dialog**will open to start the export process. The process may take**a few minutes**but once it finishes a file will be downloadable from your browser. You may continue to browse the DL while the export process is in progress.\n* ```\n```\n* [Download citation**](javascript:void(0))\n* [Copy citation**](javascript:void(0))\narticle\n**Free access\nShare on\n* **\n* **\n* **\n* **\n* **\n# Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies\nAuthors:[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)AndreasKrause](#artseq-00001),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)AjitSingh](#artseq-00002),[![](https://dl.acm.org/pb-assets/icons/DOs/default-profile-1543932446943.svg)CarlosGuestrin](#artseq-00003)[Authors Info &amp; Claims](#tab-contributors)\n[The Journal of Machine Learning Research,Volume9](https://dl.acm.org/toc/10.5555/1390681)\nPages235-284\nPublished:01 June 2008[Publication History](#core-history)\n**373citation**1,916Downloads\nMetrics\n[\nTotal Citations373\n](#tab-citations)[\nTotal Downloads1,916\n](#tab-metrics-inner)\nLast 12 Months186\nLast 6 weeks18\n**Get Citation Alerts\n**## New Citation Alert added!\nThis alert has been successfully added and will be sent to:\nYou will be notified whenever a record that you have chosen has been cited.\nTo manage your alert preferences, click on the button below.\n[Manage my Alerts](https://dl.acm.org/action/showPreferences?menuTab=Alerts)\n**## New Citation Alert!\nPlease[log in to your account](https://dl.acm.org/action/showLogin?redirectUri=/doi/10.5555/1390681.1390689)\n**\n**\n[**PDF](https://dl.acm.org/doi/pdf/10.5555/1390681.1390689)[**eReader](https://dl.acm.org/doi/epdf/10.5555/1390681.1390689)\n**Contents\n## Abstract\nWhen monitoring spatial phenomena, which can often be modeled as Gaussian processes (GPs), choosing sensor locations is a fundamental task. There are several common strategies to address this task, for example, geometry or disk models, placing sensors at the points of highest entropy (variance) in the GP model, and A-, D-, or E-optimal design. In this paper, we tackle the combinatorial optimization problem of maximizing the*mutual information*between the chosen locations and the locations which are not selected. We prove that the problem of finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1-1/*e*) of the optimum by exploiting the*submodularity*of mutual information. We also show how submodularity can be used to obtain online bounds, and design branch and bound search procedures. We then extend our algorithm to exploit lazy evaluations and local structure in the GP, yielding significant speedups. We also extend our approach to find placements which are robust against node failures and uncertainties in the model. These extensions are again associated with rigorous theoretical approximation guarantees, exploiting the submodularity of the objective function. We demonstrate the advantages of our approach towards optimizing mutual information in a very extensive empirical study on two real-world data sets.\n## Formats available\nYou can view the full content in the following formats:\n[**PDF](https://dl.acm.org/doi/pdf/10.5555/1390681.1390689)\n## References\n[1]\nA. C. Atkinson. Recent developments in the methods of optimum and related experimental designs.*International Statistical Review / Revue Internationale de Statistique*, 56(2):99-115, Aug. 1988.\n[Google Scholar](https://scholar.google.com/scholar?q=A.+C.+Atkinson.+Recent+developments+in+the+methods+of+optimum+and+related+experimental+designs.+International+Statistical+Review+/+Revue+Internationale+de+Statistique,+56(2):99-115,+Aug.+1988.)\n[2]\nA. C. Atkinson. The usefulness of optimum experimental designs.*Journal of the Royal Statistical Society. Series B (Methodological)*, 58(1):59-76, 1996.\n[Crossref](https://doi.org/10.1111/j.2517-6161.1996.tb02067.x)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1111/j.2517-6161.1996.tb02067.x)\n[3]\nS. Axelrod, S. Fine, R. Gilad-Bachrach, R. Mendelson, and N. Tishby. The information of observations and application for active learning with uncertainty. Technical report, Jerusalem: Leibniz Center, Hebrew University, 2001.\n[Google Scholar](https://scholar.google.com/scholar?q=S.+Axelrod,+S.+Fine,+R.+Gilad-Bachrach,+R.+Mendelson,+and+N.+Tishby.+The+information+of+observations+and+application+for+active+learning+with+uncertainty.+Technical+report,+Jerusalem:+Leibniz+Center,+Hebrew+University,+2001.)\n[4]\nX. Bai, S. Kumar, Z. Yun, D. Xuan, and T. H. Lai. Deploying wireless sensors to achieve both coverage and connectivity. In*ACM International Symposium on Mobile Ad Hoc Networking and Computing*, Florence, Italy, 2006.\n[Digital Library](https://dl.acm.org/doi/10.1145/1132905.1132921)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1145/1132905.1132921)\n[5]\nJ. M. Bernardo. Expected information as expected utility.*Annals of Statistics*, 7(3):686-690, May 1979.\n[Crossref](https://doi.org/10.1214/aos/1176344689)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1214/aos/1176344689)\n[6]\nS. Boyd and L. Vandenberghe.*Convex Optimization*. Cambridge UP, March 2004.\n[Digital Library](https://dl.acm.org/doi/10.5555/993483)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.5555/993483)\n[7]\nW. F. Caselton and T. Hussain. Hydrologic networks: Information transmission.*Journal of Water Resources Planning and Management*, WR2:503-520, 1980.\n[Google Scholar](https://scholar.google.com/scholar?q=W.+F.+Caselton+and+T.+Hussain.+Hydrologic+networks:+Information+transmission.+Journal+of+Water+Resources+Planning+and+Management,+WR2:503-520,+1980.)\n[8]\nW. F. Caselton and J. V. Zidek. Optimal monitoring network designs.*Statistics and Probability Letters*, 2(4):223-227, 1984.\n[Crossref](https://doi.org/10.1016/0167-7152(84)90020-8)\n[Google Scholar](https://scholar.google.com/scholar_lookup?doi=10.1016/0167-7152(84)90020-8)\n[9]\nW. F. Caselton, L. Kan, and J. V. Zidek.*Statistics in the Environmental and Earth Sciences*, chapter Quality data networks that minimize entropy, pages 10-38. Halsted Press, 1992.\n[Google Scholar](https://scholar.google.com/scholar?q=W.+F.+Caselton,+L.+Kan,+and+J.+V.+Zidek.+Statistics+in+the+Environmental+and+Earth+Sciences,+chapter+Quality+data+networks+that+minimize+entropy,+pages+10-38.+Halsted+Press,+1992.)\n[10]\nK. Chaloner and I. Verdinelli. Bayesian experimental design: A review.*Statistical Science*, 10(3): 273-304, Aug. 1995. ISSN 08834237.\n[Google Scholar](https://scholar.google.com/scholar?q=K.+Chaloner+and+I.+Verdinelli.+Bayesian+experimental+design:+A+review.+Statistical+Science,+10(3):+273-304,+Aug.+1995.+ISSN+08834237.)\n[11]\nD. A. Cohn. Neural network exploration using optimal experiment design. In Jack D. Cowan, Gerald Tesauro, and Joshua Alspector, editors,*Advances in Neural Information Processing Systems*, volume 6, pages 679-686. Morgan Kaufmann Publishers, Inc., 1994.\n[Google Scholar](https://scholar.google.com/scholar?q=D.+A.+Cohn.+Neural+network+exploration+using+optimal+experiment+design.+In+Jack+D.+Cowan,+Gerald+Tesauro,+and+Joshua+Alspector,+editors,+Advances+in+Neural+Information+Processing+Systems,+volume+6,+pages+679-686.+Morgan+Kaufmann+Publish",
          "original_query": "Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies",
          "cleaned_query": "Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies"
        },
        {
          "success": true,
          "title": "Principles of Optimal Design 3rd edition | 9781107132672, 9781316868171 | VitalSource",
          "url": "https://www.vitalsource.com/products/principles-of-optimal-design-panos-y-papalambros-douglass-v9781316868171",
          "content": "[Skip to main content](https://www.vitalsource.com/www.vitalsource.com#content)\n\n# Principles of Optimal Design\n\n## Modeling and Computation\n\n3rd\u00a0Edition\n\n- Author(s)\nPanos Y. Papalambros; Douglass J. Wilde\n\n- Publisher Published by Cambridge University Press\n- ## Print ISBN: 9781107132672\n\n\"ISBN-13\n9781107132672\"\n- Format\nFixed Layout\n- Available from\n$\n68.80\nUSD\nSKU:\n9781316868171R180\n\n## eBook\n\n## eText ISBN:9781316868171\n\n180 Days\n$68.80\n\nLifetime\n$85.99\n\neTextbook LicenseOpen digital license dialog\n\n$68.80 USD\n\n\u200c\u200c\u200c\u200c\u200c\n\n- ### Study Tools\n\n\n\nBuilt-in study tools like highlights and more\n\n- ### Read Aloud\n\n\n\nListen and follow along as Bookshelf reads to you\n\n- ### Offline Access\n\n\n\nAccess your eTextbook anytime and anywhere\n\n- ### Global Search\n\n\n\nSearch across book content, figures, and your workbook\n\n\n[Learn More](https://www.vitalsource.com/bookshelf-features)\n\n## 25+ Years of Digital Transformation\n\n- 4500\nInstitutions\n\n- 230+\nCountries & Territories\n\n- 10K+\nPublishers\n\n- 18M+\nActive Users\n\n\n### Principles of Optimal Design: Modeling and Computation 3rd Edition is written by Panos Y. Papalambros; Douglass J. Wilde and published by Cambridge University Press. The Digital and eTextbook ISBNs for Principles of Optimal Design are 9781316868171, 1316868176 and the print ISBNs are 9781107132672, 1107132673. Save up to 80% versus print by going digital with VitalSource.\n\nPrinciples of Optimal Design: Modeling and Computation 3rd Edition is written by Panos Y. Papalambros; Douglass J. Wilde and published by Cambridge University Press. The Digital and eTextbook ISBNs for Principles of Optimal Design are 9781316868171, 1316868176 and the print ISBNs are 9781107132672, 1107132673. Save up to 80% versus print by going digital with VitalSource.\n\n[Back to Top](https://www.vitalsource.com/www.vitalsource.com#scroll-to-top)",
          "original_query": "Principles of optimal design: modeling and computation",
          "cleaned_query": "Principles of optimal design: modeling and computation",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "Inverse and Ill-Posed Problems $$\\star $$",
          "url": "https://link.springer.com/content/pdf/10.1007/978-3-031-68566-8_13.pdf?pdf=inline+link",
          "content": "References J.B. Keller, Inverse problems. Am. Math. Monthly 83, 107 (1976) Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n A.N. Tikhonov, V. Arsenin, Solutions of ill-posed problems (John Wiley &amp; Sons, New York, 1977) MATH \u00a0\n \n Google Scholar \u00a0\n A.\u00a0G.\u00a0Ramm, Inverse problems. Mathematical and analytical techniques with applications to engineering (Springer Science+Business Media, New York, 2005) \n Google Scholar \u00a0\n J.L. Mueller, S. Siltanen, Linear and nonlinear inverse problems with practical applications (SIAM, Philadelphia, 2012) Book \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n S.I. Kabanikhin, Definitions and examples of inverse and ill-posed problems. J. Inv. Ill-Posed Problems 16, 317 (2008) Article \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n M.\u00a0Richter, Inverse problems. Basics, theory and applications in geophysics (Springer International Publishing AG, Cham, 2016) \n Google Scholar \u00a0\n A.B. Weglein et al., Inverse scattering series and inverse seismic exploration. Inverse Problems 19, R27 (2003) Article \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n F.\u00a0D.\u00a0M.\u00a0Neto, A.\u00a0J.\u00a0da S.\u00a0Neto, An introduction to inverse problems with applications (Springer-Verlag, Berlin, 2013) \n Google Scholar \u00a0\n M.\u00a0Benning, M.\u00a0J.\u00a0Ehrhardt, Inverse problems in imaging, Lecture Notes (Cambridge University, 2016) \n Google Scholar \u00a0\n A.\u00a0Ben-Israel, T.\u00a0N.\u00a0E.\u00a0Greville, Generalized inverses. Theory and applications \\(2^{\\rm {nd}}\\) edition (Springer-Verlag, New York, 2003) \n Google Scholar \u00a0\n H.W. Engl, M. Hanke, A. Neubauer, Regularization of inverse problems (Kluwer Academic Publishers, Dordrecht, 1996) Book \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n A.H. Hasano\u011flu, V.G. Romanov, Introduction to inverse problem for differential equations (Springer International Publishing AG, Cham, 2017) Book \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n A. Kirsch, An introduction to the mathematical theory of inverse problems, \\(2^{\\rm {nd}}\\) edition (Springer Science+Business Media, New York, 2011) \n Google Scholar \u00a0\n I. Stein, Conjugate gradient methods in Banach spaces. Nonlin. Anal. 63, e2621 (2005) Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n F. Heber, F. Sch\u00f6pfer, T. Schuster, Acceleration of sequential subspace optimization in Banach spaces by orthogonal search directions. J. Comput. Appl. Math. 345, 1 (2019) Article \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n F. Lenti, F. Nunziata, C. Estatico, M. Migliaccio, Conjugate gradient method in Hilbert and Banach spaces to enhance the spatial resolution of radiometer data. IEEE Trans. Geosci. Rem. Sens. 54, 397 (2016) Article \u00a0\n ADS \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n C. Estatico, S. Gratton, F. Lenti, D. Titley-Peloquin, A conjugate gradient like method for \\(p\\) -norm minimization in functional spaces. Numer. Math. 137, 895 (2017) Article \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n C.R. Vogel, Computational methods for inverse problems (SIAM, Philadelphia, 2002) Book \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n A. Tarantola, Inverse problem theory and methods for model parameter estimation (SIAM, Philadelphia, 2005) Book \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n L. Tenorio, Statistical regularization of inverse problems. SIAM Review 43, 347 (2001) Article \u00a0\n ADS \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n A.\u00a0N.\u00a0Tikhonov, A.\u00a0S.\u00a0Leonov, A.\u00a0G.\u00a0Yagola, Nonlinear ill-posed problems, Vols. I and II (Chapman and Hall, London, 1998) \n Google Scholar \u00a0\n H.W. Engl, K. Kunisch, A. Neubauer, Convergence rates for Tikhonov regularization of non-linear ill-posed problems. Inverse Problems 5, 523 (1989) Article \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n A. Neubauer, Tikhonov regularization for non-linear ill-posed problems: optimal convergence rates and finite-dimensional approximation. Inverse Problems 5, 541 (1989) Article \u00a0\n ADS \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n S.S. Pereverzyev, R. Pinnau, N. Siedow, Regularized fixed-point iterations for nonlinear inverse problems. Inverse Problems 22, 1 (2006) Article \u00a0\n ADS \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n M. Bachmayr, M. Burger, Iterative total variation schemes for nonlinear inverse problems. Inverse Problems 25, 105004 (2009) Article \u00a0\n ADS \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n A.-M. Wazwaz, A first course in integral equations, \\(2^{\\rm {nd}}\\) edition (World Scientific, Singapore, 2015) \n Google Scholar \u00a0\n C.\u00a0W.\u00a0Groetsch, Integral equations of the first kind, inverse problems and regularization: a crash course, J. Phys.: Conf. Ser. 73, 012001 (2007) \n Google Scholar \u00a0\n P. Linz, Analytical and numerical methods for Volterra equations (SIAM, Philadelphia, 1985) Book \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n G. Izzo, E. Russo, C. Chiapparelli, Highly stable Runga-Kutta methods for Volterra integral equations. Appl. Num. Math. 62, 1002 (2012) Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n P.C. Hansen, Numerical tools for analysis and solution of Fredholm equations of the first kind. Inverse Problems 8, 849 (1992) Article \u00a0\n ADS \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n S.I. Kabanikhin, Inverse and ill-posed problems (Theory and applications (De Gruyter, Berlin, 2012) MATH \u00a0\n \n Google Scholar \u00a0\n C.W. Groetsch, The theory of Tikhonov regularization for Fredholm equations of the first kind (Pitman Publishing, London, 1984) MATH \u00a0\n \n Google Scholar \u00a0\n E. de Micheli, N. Magnoli, G.A. Viano, On the regularization of Fredholm integral equations of the first kind. SIAM J. Math. Anal. 29, 855 (1998) Article \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n P.P.B. Eggermont, Maximum entropy regularization for Fredholm integral equations of the first kind. SIAM J. Math. Anal. 24, 1557 (1993) Article \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n U. Amato, W. Hughes, Maximum entropy regularization of Fredholm integral equations of the first kind. Inverse Problems 7, 793 (1991) Article \u00a0\n ADS \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n A.-M. Wazwaz, The regularization method for Fredholm integral equations of the first kind. Comp. Math. Appl. 61, 2981 (2011) Article \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n V.\u00a0M.\u00a0Fridman, Method of successive approximations for Fredholm integral equations of the first kind, Russian version in Usp. Mat. Nauk, Tom XI, 1(67), 233 (1956) \n Google Scholar \u00a0\n H.J.J. Riele, A program for solving first kind Fredholm equations by means of regularization. Comp. Phys. Comm. 36, 423 (1985) Article \u00a0\n ADS \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n P.\u00a0K.\u00a0Lamm, A survey of regularization methods for first kind Volterra equations, in: D.\u00a0Colton, H.\u00a0W.\u00a0Engl, A.\u00a0K.\u00a0Louis, J.\u00a0R.\u00a0McLaughlin, W.\u00a0Rundell (eds.), Surveys on solution methods for inverse problems, p.\u00a053 (Springer-Verlag, Wien, 2000) \n Google Scholar \u00a0\n J.\u00a0Radon, \u00dcber die Bestimmung von Funktionen durch ihre Integralwerte l\u00e4ngs gewisser Mannigfaltigkeiten, Berichte \u00fcber die Verhandlungen der K\u00f6niglich-S\u00e4chsischen Gesellschaft der Wissenschaften zu Leipzig, Math.-Phys. Klasse, 69, 262 (1917) \n Google Scholar \u00a0\n F. Natterer, The mathematics of computerized tomography (SIAM, Philadelphia, 2001) Book \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n S. Helgason, Integral geometry and Radon transforms (Springer Science+Business Media, New York, 2010) Book \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n R.M. Mersereau, Recovering multidimensional signals from their projections. Computer Graphics and Image Processing 1, 179 (1973) Article \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n P.\u00a0A.\u00a0Toft, J.\u00a0A.\u00a0S\u00f8rensen, The Radon transform \u2014 theory and implementation, PhD Thesis (Technical University of Denmark, Lyngby, 1996) \n Google Scholar \u00a0\n W.H. Press, Discrete Radon transform has an exact, fast inverse and generalizes to operations other than sums along lines. Proc. Nat. Acad. Sci. 103, 19249 (2006) Article \u00a0\n ADS \u00a0\n MathSciNet \u00a0\n MATH \u00a0\n \n Google Scholar \u00a0\n M.\u00a0Kac, Can one hear the shape of a drum?, Am. Math. Monthly 73, No.\u00a04, p.\u00a01 (1966) \n Google Scholar \u00a0\n C.\u00a0Gordon, D.\u00a0Webb, S.\u00a0Wolpert, One cannot hear the shape of the drum, Bulletin Am. Math. Soc. 27, No.\u00a01, p.\u00a0134 (1992) \n Google Scholar \u00a0\n S.\u00a0J.\u00a0Chapman, Drums that sound the same, Am. Math. Monthly 102, No.\u00a02, p",
          "original_query": "Inverse and ill-posed problems: theory and applications",
          "cleaned_query": "Inverse and ill-posed problems: theory and applications"
        }
      ],
      "generated_ideas": [
        "Diffusion-Based Uncertainty-Aware Field Reconstruction from Sparse Voronoi Grids\nTrain a conditional DDPM that generates full-field reconstructions conditioned on Voronoi-tessellated sensor inputs, producing a *distribution* over plausible fields rather than a single estimate. Evaluate calibration (e.g., CRPS, coverage) under sensor dropout/motion and compare against deterministic CNN super-resolution baselines to quantify uncertainty benefits for situational awareness.",
        "Sequential Active Sensing with Diffusion Posterior Sampling\nDevelop an online policy that alternates between (i) sampling multiple candidate full fields from a conditional diffusion model and (ii) selecting the next sensor move/activation that maximizes expected information gain over the sampled posterior (approximating mutual information). Demonstrate improved reconstruction per measurement in scenarios with moving sensors and intermittent availability.",
        "Physics-Regularized Diffusion Reconstruction via Tikhonov/PDE Residual Penalties\nIncorporate inverse-problem regularization into diffusion training/inference by adding Tikhonov-style penalties and/or PDE residual constraints (e.g., incompressibility, Navier\u2013Stokes residual) on generated samples. Compare stability and generalization under extreme sparsity/noise to purely data-driven Voronoi-CNN methods, framing the task explicitly as an ill-posed inverse problem with learned priors.",
        "Failure-Robust Sensor Network Design Coupled to Learned Reconstructors\nExtend robust placement ideas (node failures/uncertain models) by optimizing sensor layouts against *worst-case* or *distributional* sensor dropout while using the Voronoi-based reconstructor as the estimator. Produce a tractable algorithm (e.g., submodular robust objective or scenario-based optimization) and quantify resilience: reconstruction degradation vs. fraction of failed sensors.",
        "Adaptive Voronoi Tessellation for Anisotropic and Multi-Scale Fields\nReplace standard Voronoi cells with anisotropic or weighted (power diagram) tessellations whose metrics adapt to flow features (e.g., shear layers, fronts) learned from historical data. Implement a pipeline that learns spatially varying distance metrics and shows improved recovery of sharp gradients compared to uniform Voronoi gridding.",
        "Spatiotemporal Reconstruction with Moving Sensors Using Latent Trajectory Conditioning\nAugment the Voronoi-assisted framework to explicitly condition on sensor trajectories and timestamps using a temporal backbone (e.g., ConvLSTM/transformer on structured Voronoi grids). Target time-evolving fields where sensor motion creates nonstationary sampling, and measure gains in short-term forecasting and gap-filling when sensors go offline.",
        "Hybrid GP\u2013Deep Reconstruction: GP for Acquisition, Voronoi-Deep for High-Res Recovery\nUse a GP model primarily for fast uncertainty estimates and mutual-information acquisition, while the high-capacity Voronoi-CNN/diffusion model performs final super-resolution reconstruction. Study how GP kernel choices and GP posterior uncertainty correlate with deep-model error, and design a principled two-stage system that is both sample-efficient (GP-guided) and accurate (deep reconstruction).",
        "Multi-Modal Sensor Fusion on Voronoi Grids (Scalar + Vector + Indirect Measurements)\nGeneralize the structured-grid input to handle heterogeneous sensors (e.g., pressure, velocity components, temperature, line-integral/radiometer-like measurements) by encoding each modality into aligned Voronoi channels with modality-specific noise models. Evaluate on geophysical/turbulence cases to show improved reconstruction when direct field observations are supplemented by indirect or cheaper measurements.",
        "Certified Reconstruction Guarantees via Learned Priors and Inverse-Problem Stability Analysis\nDevelop a framework that combines classical inverse-problem tools (regularization, stability bounds, discrepancy principles) with learned Voronoi-based priors to provide *a posteriori* error indicators or bounds. Concretely, derive computable certificates based on measurement residuals, Lipschitz/contractivity estimates of the reconstructor, and sensor geometry metrics, then validate that certificates correlate with true reconstruction error across sparsity regimes."
      ],
      "generated_ideas_raw": "1. **Diffusion-Based Uncertainty-Aware Field Reconstruction from Sparse Voronoi Grids**  \nTrain a conditional DDPM that generates full-field reconstructions conditioned on Voronoi-tessellated sensor inputs, producing a *distribution* over plausible fields rather than a single estimate. Evaluate calibration (e.g., CRPS, coverage) under sensor dropout/motion and compare against deterministic CNN super-resolution baselines to quantify uncertainty benefits for situational awareness.\n\n2. **Mutual-Information Sensor Placement for Voronoi-CNN/Diffusion Reconstructors**  \nUse Krause et al.\u2019s submodular mutual-information objective to select sensor locations that *maximize downstream reconstruction quality* for the Voronoi-assisted deep model (CNN or diffusion) rather than for a GP alone. Implement a greedy (1\u22121/e)-approximate placement loop where the reconstruction model provides a differentiable or surrogate score (e.g., expected error/entropy reduction) and test on wake flows and geophysical fields.\n\n3. **Sequential Active Sensing with Diffusion Posterior Sampling**  \nDevelop an online policy that alternates between (i) sampling multiple candidate full fields from a conditional diffusion model and (ii) selecting the next sensor move/activation that maximizes expected information gain over the sampled posterior (approximating mutual information). Demonstrate improved reconstruction per measurement in scenarios with moving sensors and intermittent availability.\n\n4. **Physics-Regularized Diffusion Reconstruction via Tikhonov/PDE Residual Penalties**  \nIncorporate inverse-problem regularization into diffusion training/inference by adding Tikhonov-style penalties and/or PDE residual constraints (e.g., incompressibility, Navier\u2013Stokes residual) on generated samples. Compare stability and generalization under extreme sparsity/noise to purely data-driven Voronoi-CNN methods, framing the task explicitly as an ill-posed inverse problem with learned priors.\n\n5. **Failure-Robust Sensor Network Design Coupled to Learned Reconstructors**  \nExtend robust placement ideas (node failures/uncertain models) by optimizing sensor layouts against *worst-case* or *distributional* sensor dropout while using the Voronoi-based reconstructor as the estimator. Produce a tractable algorithm (e.g., submodular robust objective or scenario-based optimization) and quantify resilience: reconstruction degradation vs. fraction of failed sensors.\n\n6. **Adaptive Voronoi Tessellation for Anisotropic and Multi-Scale Fields**  \nReplace standard Voronoi cells with anisotropic or weighted (power diagram) tessellations whose metrics adapt to flow features (e.g., shear layers, fronts) learned from historical data. Implement a pipeline that learns spatially varying distance metrics and shows improved recovery of sharp gradients compared to uniform Voronoi gridding.\n\n7. **Spatiotemporal Reconstruction with Moving Sensors Using Latent Trajectory Conditioning**  \nAugment the Voronoi-assisted framework to explicitly condition on sensor trajectories and timestamps using a temporal backbone (e.g., ConvLSTM/transformer on structured Voronoi grids). Target time-evolving fields where sensor motion creates nonstationary sampling, and measure gains in short-term forecasting and gap-filling when sensors go offline.\n\n8. **Hybrid GP\u2013Deep Reconstruction: GP for Acquisition, Voronoi-Deep for High-Res Recovery**  \nUse a GP model primarily for fast uncertainty estimates and mutual-information acquisition, while the high-capacity Voronoi-CNN/diffusion model performs final super-resolution reconstruction. Study how GP kernel choices and GP posterior uncertainty correlate with deep-model error, and design a principled two-stage system that is both sample-efficient (GP-guided) and accurate (deep reconstruction).\n\n9. **Multi-Modal Sensor Fusion on Voronoi Grids (Scalar + Vector + Indirect Measurements)**  \nGeneralize the structured-grid input to handle heterogeneous sensors (e.g., pressure, velocity components, temperature, line-integral/radiometer-like measurements) by encoding each modality into aligned Voronoi channels with modality-specific noise models. Evaluate on geophysical/turbulence cases to show improved reconstruction when direct field observations are supplemented by indirect or cheaper measurements.\n\n10. **Certified Reconstruction Guarantees via Learned Priors and Inverse-Problem Stability Analysis**  \nDevelop a framework that combines classical inverse-problem tools (regularization, stability bounds, discrepancy principles) with learned Voronoi-based priors to provide *a posteriori* error indicators or bounds. Concretely, derive computable certificates based on measurement residuals, Lipschitz/contractivity estimates of the reconstructor, and sensor geometry metrics, then validate that certificates correlate with true reconstruction error across sparsity regimes.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Diffusion-Based Uncertainty-Aware Field Reconstruction from Sparse Voronoi Grids\nTrain a conditional DDPM that generates full-field reconstructions conditioned on Voronoi-tessellated sensor inputs, pr",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Sequential Active Sensing with Diffusion Posterior Sampling\nDevelop an online policy that alternates between (i) sampling multiple candidate full fields from a conditional diffusion model and (ii) sel",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Physics-Regularized Diffusion Reconstruction via Tikhonov/PDE Residual Penalties\nIncorporate inverse-problem regularization into diffusion training/inference by adding Tikhonov-style penalties and/or ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Failure-Robust Sensor Network Design Coupled to Learned Reconstructors\nExtend robust placement ideas (node failures/uncertain models) by optimizing sensor layouts against *worst-case* or *distribution",
          "is_match": true
        },
        {
          "idea_idx": 4,
          "idea_text": "Adaptive Voronoi Tessellation for Anisotropic and Multi-Scale Fields\nReplace standard Voronoi cells with anisotropic or weighted (power diagram) tessellations whose metrics adapt to flow features (e.g",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Spatiotemporal Reconstruction with Moving Sensors Using Latent Trajectory Conditioning\nAugment the Voronoi-assisted framework to explicitly condition on sensor trajectories and timestamps using a temp",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Hybrid GP\u2013Deep Reconstruction: GP for Acquisition, Voronoi-Deep for High-Res Recovery\nUse a GP model primarily for fast uncertainty estimates and mutual-information acquisition, while the high-capacit",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Multi-Modal Sensor Fusion on Voronoi Grids (Scalar + Vector + Indirect Measurements)\nGeneralize the structured-grid input to handle heterogeneous sensors (e.g., pressure, velocity components, temperat",
          "is_match": false
        },
        {
          "idea_idx": 8,
          "idea_text": "Certified Reconstruction Guarantees via Learned Priors and Inverse-Problem Stability Analysis\nDevelop a framework that combines classical inverse-problem tools (regularization, stability bounds, discr",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 72,
      "paper_title": "InfinityStar: Uni\ufb01ed Spacetime AutoRegressive Modeling for Visual Generation",
      "contribution": "InfinityStar introduces a unified autoregressive framework that effectively generates high-quality images and videos by integrating spatial and temporal modeling within a discrete approach.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": true,
      "matching_idea_idx": 0,
      "input_tokens": 9615,
      "output_tokens": 1007,
      "predecessor_details": [
        {
          "success": true,
          "title": "Scalable Image Generation via Next-Scale Prediction - arXiv",
          "url": "https://arxiv.org/abs/2404.02905",
          "content": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
          "original_query": "Visual autoregressive modeling: Scalable image generation via next-scale prediction",
          "cleaned_query": "Visual autoregressive modeling: Scalable image generation via next-scale prediction"
        },
        {
          "success": true,
          "title": "Infinity: Scaling Bitwise AutoRegressive Modeling for High ... - arXiv",
          "url": "https://arxiv.org/abs/2412.04431",
          "content": "[2412.04431] Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2412.04431\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2412.04431**(cs)\n[Submitted on 5 Dec 2024 ([v1](https://arxiv.org/abs/2412.04431v1)), last revised 17 Jun 2025 (this version, v2)]\n# Title:Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis\nAuthors:[Jian Han](https://arxiv.org/search/cs?searchtype=author&amp;query=Han,+J),[Jinlai Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J),[Yi Jiang](https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+Y),[Bin Yan](https://arxiv.org/search/cs?searchtype=author&amp;query=Yan,+B),[Yuqi Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Y),[Zehuan Yuan](https://arxiv.org/search/cs?searchtype=author&amp;query=Yuan,+Z),[Bingyue Peng](https://arxiv.org/search/cs?searchtype=author&amp;query=Peng,+B),[Xiaobing Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+X)\nView a PDF of the paper titled Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis, by Jian Han and 7 other authors\n[View PDF](https://arxiv.org/pdf/2412.04431)[HTML (experimental)](https://arxiv.org/html/2412.04431v2)> > Abstract:\n> We present Infinity, a Bitwise Visual AutoRegressive Modeling capable of generating high-resolution, photorealistic images following language instruction. Infinity redefines visual autoregressive model under a bitwise token prediction framework with an infinite-vocabulary tokenizer &amp; classifier and bitwise self-correction mechanism, remarkably improving the generation capacity and details. By theoretically scaling the tokenizer vocabulary size to infinity and concurrently scaling the transformer size, our method significantly unleashes powerful scaling capabilities compared to vanilla VAR. Infinity sets a new record for autoregressive text-to-image models, outperforming top-tier diffusion models like SD3-Medium and SDXL. Notably, Infinity surpasses SD3-Medium by improving the GenEval benchmark score from 0.62 to 0.73 and the ImageReward benchmark score from 0.87 to 0.96, achieving a win rate of 66%. Without extra optimization, Infinity generates a high-quality 1024x1024 image in 0.8 seconds, making it 2.6x faster than SD3-Medium and establishing it as the fastest text-to-image model. Models and codes will be released to promote further exploration of Infinity for visual generation and unified tokenizer modeling. Comments:|17 pages, 14 figures|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:2412.04431](https://arxiv.org/abs/2412.04431)[cs.CV]|\n|(or[arXiv:2412.04431v2](https://arxiv.org/abs/2412.04431v2)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2412.04431](https://doi.org/10.48550/arXiv.2412.04431)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jian Han [[view email](https://arxiv.org/show-email/70d7e286/2412.04431)]\n**[[v1]](https://arxiv.org/abs/2412.04431v1)**Thu, 5 Dec 2024 18:53:02 UTC (14,791 KB)\n**[v2]**Tue, 17 Jun 2025 15:32:20 UTC (14,792 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis, by Jian Han and 7 other authors\n* [View PDF](https://arxiv.org/pdf/2412.04431)\n* [HTML (experimental)](https://arxiv.org/html/2412.04431v2)\n* [TeX Source](https://arxiv.org/src/2412.04431)\n[![license icon](https://arxiv.org/icons/licenses/zero-1.0.png)view license](http://creativecommons.org/publicdomain/zero/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2412.04431&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2412.04431&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2024-12](https://arxiv.org/list/cs.CV/2024-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/2412.04431?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2412.04431)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2412.04431)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2412.04431)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Wh",
          "original_query": "Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis",
          "cleaned_query": "Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis"
        },
        {
          "success": true,
          "title": "VideoPoet: A Large Language Model for Zero-Shot Video Generation",
          "url": "https://arxiv.org/abs/2312.14125",
          "content": "[2312.14125] VideoPoet: A Large Language Model for Zero-Shot Video Generation\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2312.14125\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2312.14125**(cs)\n[Submitted on 21 Dec 2023 ([v1](https://arxiv.org/abs/2312.14125v1)), last revised 4 Jun 2024 (this version, v4)]\n# Title:VideoPoet: A Large Language Model for Zero-Shot Video Generation\nAuthors:[Dan Kondratyuk](https://arxiv.org/search/cs?searchtype=author&amp;query=Kondratyuk,+D),[Lijun Yu](https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+L),[Xiuye Gu](https://arxiv.org/search/cs?searchtype=author&amp;query=Gu,+X),[Jos\u00e9 Lezama](https://arxiv.org/search/cs?searchtype=author&amp;query=Lezama,+J),[Jonathan Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+J),[Grant Schindler](https://arxiv.org/search/cs?searchtype=author&amp;query=Schindler,+G),[Rachel Hornung](https://arxiv.org/search/cs?searchtype=author&amp;query=Hornung,+R),[Vighnesh Birodkar](https://arxiv.org/search/cs?searchtype=author&amp;query=Birodkar,+V),[Jimmy Yan](https://arxiv.org/search/cs?searchtype=author&amp;query=Yan,+J),[Ming-Chang Chiu](https://arxiv.org/search/cs?searchtype=author&amp;query=Chiu,+M),[Krishna Somandepalli](https://arxiv.org/search/cs?searchtype=author&amp;query=Somandepalli,+K),[Hassan Akbari](https://arxiv.org/search/cs?searchtype=author&amp;query=Akbari,+H),[Yair Alon](https://arxiv.org/search/cs?searchtype=author&amp;query=Alon,+Y),[Yong Cheng](https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng,+Y),[Josh Dillon](https://arxiv.org/search/cs?searchtype=author&amp;query=Dillon,+J),[Agrim Gupta](https://arxiv.org/search/cs?searchtype=author&amp;query=Gupta,+A),[Meera Hahn](https://arxiv.org/search/cs?searchtype=author&amp;query=Hahn,+M),[Anja Hauth](https://arxiv.org/search/cs?searchtype=author&amp;query=Hauth,+A),[David Hendon](https://arxiv.org/search/cs?searchtype=author&amp;query=Hendon,+D),[Alonso Martinez](https://arxiv.org/search/cs?searchtype=author&amp;query=Martinez,+A),[David Minnen](https://arxiv.org/search/cs?searchtype=author&amp;query=Minnen,+D),[Mikhail Sirotenko](https://arxiv.org/search/cs?searchtype=author&amp;query=Sirotenko,+M),[Kihyuk Sohn](https://arxiv.org/search/cs?searchtype=author&amp;query=Sohn,+K),[Xuan Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+X),[Hartwig Adam](https://arxiv.org/search/cs?searchtype=author&amp;query=Adam,+H),[Ming-Hsuan Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+M),[Irfan Essa](https://arxiv.org/search/cs?searchtype=author&amp;query=Essa,+I),[Huisheng Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+H),[David A. Ross](https://arxiv.org/search/cs?searchtype=author&amp;query=Ross,+D+A),[Bryan Seybold](https://arxiv.org/search/cs?searchtype=author&amp;query=Seybold,+B),[Lu Jiang](https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+L)\nView a PDF of the paper titled VideoPoet: A Large Language Model for Zero-Shot Video Generation, by Dan Kondratyuk and Lijun Yu and Xiuye Gu and Jos\\\\&#39;&#39;e Lezama and Jonathan Huang and Grant Schindler and Rachel Hornung and Vighnesh Birodkar and Jimmy Yan and Ming-Chang Chiu and Krishna Somandepalli and Hassan Akbari and Yair Alon and Yong Cheng and Josh Dillon and Agrim Gupta and Meera Hahn and Anja Hauth and David Hendon and Alonso Martinez and David Minnen and Mikhail Sirotenko and Kihyuk Sohn and Xuan Yang and Hartwig Adam and Ming-Hsuan Yang and Irfan Essa and Huisheng Wang and David A. Ross and Bryan Seybold and Lu Jiang\n[View PDF](https://arxiv.org/pdf/2312.14125)[HTML (experimental)](https://arxiv.org/html/2312.14125v4)> > Abstract:\n> We present VideoPoet, a language model capable of synthesizing high-quality video, with matching audio, from a large variety of conditioning signals. VideoPoet employs a decoder-only transformer architecture that processes multimodal inputs -- including images, videos, text, and audio. The training protocol follows that of Large Language Models (LLMs), consisting of two stages: pretraining and task-specific adaptation. During pretraining, VideoPoet incorporates a mixture of multimodal generative objectives within an autoregressive Transformer framework. The pretrained LLM serves as a foundation that can be adapted for a range of video generation tasks. We present empirical results demonstrating the model&#39;s state-of-the-art capabilities in zero-shot video generation, specifically highlighting VideoPoet&#39;s ability to generate high-fidelity motions. Project page: [> this http URL\n](http://sites.research.google/videopoet/)> Comments:|To appear at ICML 2024; Project page:[this http URL](http://sites.research.google/videopoet/)|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2312.14125](https://arxiv.org/abs/2312.14125)[cs.CV]|\n|(or[arXiv:2312.14125v4](https://arxiv.org/abs/2312.14125v4)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2312.14125](https://doi.org/10.48550/arXiv.2312.14125)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jonathan Huang [[view email](https://arxiv.org/show-email/9ee285a3/2312.14125)]\n**[[v1]](https://arxiv.org/abs/2312.14125v1)**Thu, 21 Dec 2023 18:46:41 UTC (39,735 KB)\n**[[v2]](https://arxiv.org/abs/2312.14125v2)**Thu, 14 Mar 2024 18:08:11 UTC (41,255 KB)\n**[[v3]](https://arxiv.org/abs/2312.14125v3)**Fri, 22 Mar 2024 17:06:53 UTC (41,255 KB)\n**[v4]**Tue, 4 Jun 2024 17:25:20 UTC (41,035 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled VideoPoet: A Large Language Model for Zero-Shot Video Generation, by Dan Kondratyuk and Lijun Yu and Xiuye Gu and Jos\\\\&#39;&#39;e Lezama and Jonathan Huang and Grant Schindler and Rachel Hornung and Vighnesh Birodkar and Jimmy Yan and Ming-Chang Chiu and Krishna Somandepalli and Hassan Akbari and Yair Alon and Yong Cheng and Josh Dillon and Agrim Gupta and Meera Hahn and Anja Hauth and David Hendon and Alonso Martinez and David Minnen and Mikhail Sirotenko and Kihyuk Sohn and Xuan Yang and Hartwig Adam and Ming-Hsuan Yang and Irfan Essa and Huisheng Wang and David A. Ross and Bryan Seybold and Lu Jiang\n* [View PDF](https://arxiv.org/pdf/2312.14125)\n* [HTML (experimental)](https://arxiv.org/html/2312.14125v4)\n* [TeX Source](https://arxiv.org/src/2312.14125)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2312.14125&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2312.14125&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2023-12](https://arxiv.org/list/cs.CV/2023-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/2312.1412",
          "original_query": "Videopoet: A vector quantized approach to video generation",
          "cleaned_query": "Videopoet: A vector quantized approach to video generation"
        },
        {
          "success": true,
          "title": "Autoregressive Video Generation without Vector Quantization - arXiv",
          "url": "https://arxiv.org/abs/2412.14169",
          "content": "[2412.14169] Autoregressive Video Generation without Vector Quantization\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2412.14169\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2412.14169**(cs)\n[Submitted on 18 Dec 2024 ([v1](https://arxiv.org/abs/2412.14169v1)), last revised 2 Mar 2025 (this version, v2)]\n# Title:Autoregressive Video Generation without Vector Quantization\nAuthors:[Haoge Deng](https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+H),[Ting Pan](https://arxiv.org/search/cs?searchtype=author&amp;query=Pan,+T),[Haiwen Diao](https://arxiv.org/search/cs?searchtype=author&amp;query=Diao,+H),[Zhengxiong Luo](https://arxiv.org/search/cs?searchtype=author&amp;query=Luo,+Z),[Yufeng Cui](https://arxiv.org/search/cs?searchtype=author&amp;query=Cui,+Y),[Huchuan Lu](https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+H),[Shiguang Shan](https://arxiv.org/search/cs?searchtype=author&amp;query=Shan,+S),[Yonggang Qi](https://arxiv.org/search/cs?searchtype=author&amp;query=Qi,+Y),[Xinlong Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+X)\nView a PDF of the paper titled Autoregressive Video Generation without Vector Quantization, by Haoge Deng and 8 other authors\n[View PDF](https://arxiv.org/pdf/2412.14169)[HTML (experimental)](https://arxiv.org/html/2412.14169v2)> > Abstract:\n> This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at [> this https URL\n](https://github.com/baaivision/NOVA)> . Comments:|Accepted to ICLR 2025. Project page at[this https URL](https://github.com/baaivision/NOVA)|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:2412.14169](https://arxiv.org/abs/2412.14169)[cs.CV]|\n|(or[arXiv:2412.14169v2](https://arxiv.org/abs/2412.14169v2)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2412.14169](https://doi.org/10.48550/arXiv.2412.14169)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Haoge Deng [[view email](https://arxiv.org/show-email/3c2ae3c7/2412.14169)]\n**[[v1]](https://arxiv.org/abs/2412.14169v1)**Wed, 18 Dec 2024 18:59:53 UTC (11,196 KB)\n**[v2]**Sun, 2 Mar 2025 08:09:39 UTC (11,130 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Autoregressive Video Generation without Vector Quantization, by Haoge Deng and 8 other authors\n* [View PDF](https://arxiv.org/pdf/2412.14169)\n* [HTML (experimental)](https://arxiv.org/html/2412.14169v2)\n* [TeX Source](https://arxiv.org/src/2412.14169)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2412.14169&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2412.14169&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2024-12](https://arxiv.org/list/cs.CV/2024-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/2412.14169?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2412.14169)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2412.14169)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2412.14169)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about ar",
          "original_query": "Autoregressive video generation without vector quantization",
          "cleaned_query": "Autoregressive video generation without vector quantization"
        },
        {
          "success": true,
          "title": "A Systematic Framework For Large Video Generative Models - arXiv",
          "url": "https://arxiv.org/abs/2412.03603",
          "content": "[2412.03603] HunyuanVideo: A Systematic Framework For Large Video Generative Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2412.03603\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2412.03603**(cs)\n[Submitted on 3 Dec 2024 ([v1](https://arxiv.org/abs/2412.03603v1)), last revised 11 Mar 2025 (this version, v6)]\n# Title:HunyuanVideo: A Systematic Framework For Large Video Generative Models\nAuthors:[Weijie Kong](https://arxiv.org/search/cs?searchtype=author&amp;query=Kong,+W),[Qi Tian](https://arxiv.org/search/cs?searchtype=author&amp;query=Tian,+Q),[Zijian Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Z),[Rox Min](https://arxiv.org/search/cs?searchtype=author&amp;query=Min,+R),[Zuozhuo Dai](https://arxiv.org/search/cs?searchtype=author&amp;query=Dai,+Z),[Jin Zhou](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+J),[Jiangfeng Xiong](https://arxiv.org/search/cs?searchtype=author&amp;query=Xiong,+J),[Xin Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+X),[Bo Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+B),[Jianwei Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+J),[Kathrina Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+K),[Qin Lin](https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+Q),[Junkun Yuan](https://arxiv.org/search/cs?searchtype=author&amp;query=Yuan,+J),[Yanxin Long](https://arxiv.org/search/cs?searchtype=author&amp;query=Long,+Y),[Aladdin Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+A),[Andong Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+A),[Changlin Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+C),[Duojun Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+D),[Fang Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+F),[Hao Tan](https://arxiv.org/search/cs?searchtype=author&amp;query=Tan,+H),[Hongmei Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+H),[Jacob Song](https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+J),[Jiawang Bai](https://arxiv.org/search/cs?searchtype=author&amp;query=Bai,+J),[Jianbing Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+J),[Jinbao Xue](https://arxiv.org/search/cs?searchtype=author&amp;query=Xue,+J),[Joey Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+J),[Kai Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+K),[Mengyang Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+M),[Pengyu Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+P),[Shuai Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+S),[Weiyan Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+W),[Wenqing Yu](https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+W),[Xinchi Deng](https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+X),[Yang Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y),[Yi Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Y),[Yutao Cui](https://arxiv.org/search/cs?searchtype=author&amp;query=Cui,+Y),[Yuanbo Peng](https://arxiv.org/search/cs?searchtype=author&amp;query=Peng,+Y),[Zhentao Yu](https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+Z),[Zhiyu He](https://arxiv.org/search/cs?searchtype=author&amp;query=He,+Z),[Zhiyong Xu](https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Z),[Zixiang Zhou](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+Z),[Zunnan Xu](https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Z),[Yangyu Tao](https://arxiv.org/search/cs?searchtype=author&amp;query=Tao,+Y),[Qinglin Lu](https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+Q),[Songtao Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+S),[Dax Zhou](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+D),[Hongfa Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+H),[Yong Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+Y),[Di Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+D),[Yuhong Liu](https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Y),[Jie Jiang](https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+J),[Caesar Zhong](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhong,+C)(refer to the report for detailed contributions)\nView a PDF of the paper titled HunyuanVideo: A Systematic Framework For Large Video Generative Models, by Weijie Kong and 51 other authors\n[View PDF](https://arxiv.org/pdf/2412.03603)[HTML (experimental)](https://arxiv.org/html/2412.03603v6)> > Abstract:\n> Recent advancements in video generation have significantly impacted daily life for both individuals and industries. However, the leading video generation models remain closed-source, resulting in a notable performance gap between industry capabilities and those available to the public. In this report, we introduce HunyuanVideo, an innovative open-source video foundation model that demonstrates performance in video generation comparable to, or even surpassing, that of leading closed-source models. HunyuanVideo encompasses a comprehensive framework that integrates several key elements, including data curation, advanced architectural design, progressive model scaling and training, and an efficient infrastructure tailored for large-scale model training and inference. As a result, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to evaluations by professionals, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and three top-performing Chinese video generative models. By releasing the code for the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower individuals within the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at [> this https URL\n](https://github.com/Tencent/HunyuanVideo)> . Subjects:|Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:2412.03603](https://arxiv.org/abs/2412.03603)[cs.CV]|\n|(or[arXiv:2412.03603v6](https://arxiv.org/abs/2412.03603v6)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2412.03603](https://doi.org/10.48550/arXiv.2412.03603)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Zijian Zhang [[view email](https://arxiv.org/show-email/8f10f11e/2412.03603)]\n**[[v1]](https://arxiv.",
          "original_query": "Hunyuanvideo: A systematic framework for large video generative models",
          "cleaned_query": "Hunyuanvideo: A systematic framework for large video generative models"
        },
        {
          "success": true,
          "title": "[2409.18869] Emu3: Next-Token Prediction is All You Need",
          "url": "https://arxiv.org/abs/2409.18869",
          "content": "arXiv reCAPTCHA\n[![Cornell University](https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n[We gratefully acknowledge support from\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)\n# [![arxiv logo](https://static.arxiv.org/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)",
          "original_query": "Emu3: Next-token prediction is all you need",
          "cleaned_query": "Emu3: Next-token prediction is all you need"
        }
      ],
      "generated_ideas": [
        "Bitwise Next-Scale Video Generator (Infinity \u00d7 Next-Scale Prediction)\nCombine Infinity\u2019s bitwise infinite-vocabulary prediction with next-scale prediction to generate videos by first sampling a low-res/low-fps \u201csketch\u201d and then autoregressively refining both spatial resolution and frame rate in stages. Implement a scale schedule that interleaves spatial upscaling and temporal densification, and measure gains in speed/quality versus single-pass NOVA-style generation.",
        "Self-Correcting Autoregressive Decoding for Long-Horizon Video Consistency\nExtend Infinity\u2019s bitwise self-correction to video by adding a \u201ctemporal correction pass\u201d that revisits earlier frames\u2019 latent bits/tokens when downstream frames reveal inconsistencies (identity drift, object count errors). Make the correction causal by limiting edits to a small window and constraining changes via learned \u201cedit budgets,\u201d then evaluate improvements on long-duration prompts and multi-entity tracking metrics.",
        "Non-Quantized Bit-Plane Autoregression for Continuous-Pixel Video (NOVA \u00d7 Infinity)\nReplace vector quantization entirely by modeling RGB (or latent) values through bit-plane (MSB\u2192LSB) autoregression within NOVA\u2019s frame-by-frame, set-by-set factorization. This yields a unified discrete autoregressive objective without a learned codebook while retaining fine detail control; benchmark training stability, bitrate-efficiency, and fidelity against VQ-based and diffusion baselines.",
        "Unified Multimodal Tokenizer for Text\u2013Image\u2013Audio\u2013Video with \u201cInfinite Vocabulary\u201d\nBuild a single tokenizer that encodes image patches, video blocks, and audio segments into a shared bitwise representation, inspired by Infinity\u2019s infinite-vocabulary tokenizer and VideoPoet\u2019s multimodal AR training. Pretrain with mixed objectives (text\u2192video, video\u2192audio, audio\u2192video) to enable cross-modal in-context learning (e.g., continue a video given a few frames + a hummed melody).",
        "Causal-within-Time, Bidirectional-within-Frame Attention via Learned Set Partitions\nGeneralize NOVA\u2019s bidirectional modeling inside frames by learning adaptive spatial \u201csets\u201d (superpixels/regions) that are predicted in parallel, while keeping strict causality across time. Train a partitioner jointly with the AR model so static regions (background) become large sets and dynamic regions become small sets, improving efficiency and motion fidelity.",
        "In-Context Video Editing via Autoregressive \u201cPatch Continuations\u201d\nLeverage GPT-style causal flexibility (NOVA/VideoPoet) to perform localized edits by conditioning on unedited frames/regions and autoregressively regenerating only masked spatiotemporal blocks. Implement a blockwise attention mask plus a likelihood-based acceptance criterion (self-correction) to keep identity and background fixed while changing objects, actions, or style.",
        "Data-Efficient Autoregressive Video Pretraining with Bitwise Distillation from Diffusion\nUse a strong diffusion video teacher to generate soft targets (denoised predictions or guidance gradients) and distill them into an AR student that predicts bits/tokens (Infinity/NOVA style). Study whether distillation reduces the amount of real video needed for high fidelity, and quantify trade-offs in diversity vs. alignment on benchmarks like GenEval-style compositional tests for video.",
        "Benchmarking Autoregressive Factorizations: Raster vs Set-by-Set vs Next-Scale on the Same Backbone\nCreate a controlled benchmark suite where the same transformer backbone is trained under multiple factorizations: raster-scan AR, NOVA\u2019s frame/set-by-set AR, next-scale prediction, and bitwise AR. Provide standardized evaluations (speed\u2013quality curves, long-horizon drift, prompt adherence, temporal metrics) to isolate which factorization choices actually drive gains and where hybrids are optimal."
      ],
      "generated_ideas_raw": "1. **Bitwise Next-Scale Video Generator (Infinity \u00d7 Next-Scale Prediction)**\n   Combine Infinity\u2019s bitwise infinite-vocabulary prediction with next-scale prediction to generate videos by first sampling a low-res/low-fps \u201csketch\u201d and then autoregressively refining both spatial resolution and frame rate in stages. Implement a scale schedule that interleaves spatial upscaling and temporal densification, and measure gains in speed/quality versus single-pass NOVA-style generation.\n\n2. **Self-Correcting Autoregressive Decoding for Long-Horizon Video Consistency**\n   Extend Infinity\u2019s bitwise self-correction to video by adding a \u201ctemporal correction pass\u201d that revisits earlier frames\u2019 latent bits/tokens when downstream frames reveal inconsistencies (identity drift, object count errors). Make the correction causal by limiting edits to a small window and constraining changes via learned \u201cedit budgets,\u201d then evaluate improvements on long-duration prompts and multi-entity tracking metrics.\n\n3. **Non-Quantized Bit-Plane Autoregression for Continuous-Pixel Video (NOVA \u00d7 Infinity)**\n   Replace vector quantization entirely by modeling RGB (or latent) values through bit-plane (MSB\u2192LSB) autoregression within NOVA\u2019s frame-by-frame, set-by-set factorization. This yields a unified discrete autoregressive objective without a learned codebook while retaining fine detail control; benchmark training stability, bitrate-efficiency, and fidelity against VQ-based and diffusion baselines.\n\n4. **Unified Multimodal Tokenizer for Text\u2013Image\u2013Audio\u2013Video with \u201cInfinite Vocabulary\u201d**\n   Build a single tokenizer that encodes image patches, video blocks, and audio segments into a shared bitwise representation, inspired by Infinity\u2019s infinite-vocabulary tokenizer and VideoPoet\u2019s multimodal AR training. Pretrain with mixed objectives (text\u2192video, video\u2192audio, audio\u2192video) to enable cross-modal in-context learning (e.g., continue a video given a few frames + a hummed melody).\n\n5. **Causal-within-Time, Bidirectional-within-Frame Attention via Learned Set Partitions**\n   Generalize NOVA\u2019s bidirectional modeling inside frames by learning adaptive spatial \u201csets\u201d (superpixels/regions) that are predicted in parallel, while keeping strict causality across time. Train a partitioner jointly with the AR model so static regions (background) become large sets and dynamic regions become small sets, improving efficiency and motion fidelity.\n\n6. **Next-Token \u201cChain-of-Thought\u201d Planning for Video: Script Tokens \u2192 Motion Tokens**\n   Inspired by LLM-style pretraining (VideoPoet/Emu3), introduce intermediate discrete \u201cplan tokens\u201d (shot list, actor actions, camera moves) that are autoregressively generated before pixels/latents. Supervise plan tokens with weak labels from video captioners or action/camera classifiers, and test whether explicit plans improve compositional prompts and reduce temporal incoherence.\n\n7. **Streaming, Real-Time Text-to-Video with Progressive Guarantees**\n   Design a streaming decoder that outputs a playable low-res video immediately and refines it continuously, using next-scale prediction for spatial detail and bitwise refinement for texture. Add a compute-aware scheduler that allocates refinement steps based on scene complexity (detected online), targeting stable 24\u201330 FPS interactive generation.\n\n8. **In-Context Video Editing via Autoregressive \u201cPatch Continuations\u201d**\n   Leverage GPT-style causal flexibility (NOVA/VideoPoet) to perform localized edits by conditioning on unedited frames/regions and autoregressively regenerating only masked spatiotemporal blocks. Implement a blockwise attention mask plus a likelihood-based acceptance criterion (self-correction) to keep identity and background fixed while changing objects, actions, or style.\n\n9. **Data-Efficient Autoregressive Video Pretraining with Bitwise Distillation from Diffusion**\n   Use a strong diffusion video teacher to generate soft targets (denoised predictions or guidance gradients) and distill them into an AR student that predicts bits/tokens (Infinity/NOVA style). Study whether distillation reduces the amount of real video needed for high fidelity, and quantify trade-offs in diversity vs. alignment on benchmarks like GenEval-style compositional tests for video.\n\n10. **Benchmarking Autoregressive Factorizations: Raster vs Set-by-Set vs Next-Scale on the Same Backbone**\n   Create a controlled benchmark suite where the same transformer backbone is trained under multiple factorizations: raster-scan AR, NOVA\u2019s frame/set-by-set AR, next-scale prediction, and bitwise AR. Provide standardized evaluations (speed\u2013quality curves, long-horizon drift, prompt adherence, temporal metrics) to isolate which factorization choices actually drive gains and where hybrids are optimal.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Bitwise Next-Scale Video Generator (Infinity \u00d7 Next-Scale Prediction)\nCombine Infinity\u2019s bitwise infinite-vocabulary prediction with next-scale prediction to generate videos by first sampling a low-re",
          "is_match": true
        },
        {
          "idea_idx": 1,
          "idea_text": "Self-Correcting Autoregressive Decoding for Long-Horizon Video Consistency\nExtend Infinity\u2019s bitwise self-correction to video by adding a \u201ctemporal correction pass\u201d that revisits earlier frames\u2019 laten",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Non-Quantized Bit-Plane Autoregression for Continuous-Pixel Video (NOVA \u00d7 Infinity)\nReplace vector quantization entirely by modeling RGB (or latent) values through bit-plane (MSB\u2192LSB) autoregression w",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Unified Multimodal Tokenizer for Text\u2013Image\u2013Audio\u2013Video with \u201cInfinite Vocabulary\u201d\nBuild a single tokenizer that encodes image patches, video blocks, and audio segments into a shared bitwise represent",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Causal-within-Time, Bidirectional-within-Frame Attention via Learned Set Partitions\nGeneralize NOVA\u2019s bidirectional modeling inside frames by learning adaptive spatial \u201csets\u201d (superpixels/regions) tha",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "In-Context Video Editing via Autoregressive \u201cPatch Continuations\u201d\nLeverage GPT-style causal flexibility (NOVA/VideoPoet) to perform localized edits by conditioning on unedited frames/regions and autor",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Data-Efficient Autoregressive Video Pretraining with Bitwise Distillation from Diffusion\nUse a strong diffusion video teacher to generate soft targets (denoised predictions or guidance gradients) and ",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Benchmarking Autoregressive Factorizations: Raster vs Set-by-Set vs Next-Scale on the Same Backbone\nCreate a controlled benchmark suite where the same transformer backbone is trained under multiple fa",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 73,
      "paper_title": "Does Stochastic Gradient really succeed for bandits?",
      "contribution": "The paper characterizes regret regimes for Stochastic Gradient Bandit (SGB) as a function of learning rates, identifying critical thresholds that delineate logarithmic from polynomial regret.",
      "num_predecessors": 3,
      "predecessors_crawled": 3,
      "academic_sources": 2,
      "crawl_rate": 1.0,
      "ideas_generated": 3,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 5484,
      "output_tokens": 925,
      "predecessor_details": [
        {
          "success": true,
          "title": "A Stochastic Approximation Method - Project Euclid",
          "url": "https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full",
          "content": "Email\n\nPassword [Forgot your password?](https://projecteuclid.org/)\n\nShow\n\nRemember Email on this computer\n\nRemember Password\n\n![](https://projecteuclid.org/Content/themes/SPIEImages/Loading.gif)Please wait...\n\nNo Project Euclid account? [Create an account](https://projecteuclid.org/)\n\nor [Sign in with your institutional credentials](https://projecteuclid.org/Account/institutionalsignin?redirect=https%3a%2f%2fprojecteuclid.org%2fjournals%2fannals-of-mathematical-statistics%2fvolume-22%2fissue-3%2fA-Stochastic-Approximation-Method%2f10.1214%2faoms%2f1177729586.full)\n\nWe can help you reset your password using the email address linked to your Project Euclid account.\n\nEmail\n\nRegistered users receive a variety of benefits including the ability to customize email alerts, create favorite journals list, and save searches.\nPlease note that a Project Euclid web account does not automatically grant access to full-text content. An institutional or society member subscription is required to view non-Open Access content.\nContact [customer\\_support@projecteuclid.org](mailto:customer_support@projecteuclid.org) with any questions.\n\n[View Project Euclid Privacy Policy](https://projecteuclid.org/policies)\n\nAll Fields are Required\n\n\\*First Name\n\n\\*Last/Family Name\n\n\\*Email\n\n\\*Password\n\nPassword Requirements: Minimum 8 characters, must include as least one uppercase, one lowercase letter, and one number or permitted symbol\nValid Symbols for password:\n\n~ Tilde\n\n! Exclamation Mark\n\n@ At sign\n\n$ Dollar sign\n\n^ Caret\n\n( Opening Parenthesis\n\n) Closing Parenthesis\n\n\\_ Underscore\n\n. Period\n![](https://projecteuclid.org/Content/themes/SPIEImages/InformationQuestionMark.png)\n\n\\*Confirm Password\n\n![](https://projecteuclid.org/Content/themes/SPIEImages/Loading.gif)Please wait...\n\nWeb Account created successfully\n\n![Open Access](https://projecteuclid.org/Content/themes/SPIEImages/OpenAccessIcon.svg)\n\nSeptember, 1951A Stochastic Approximation Method\n\nHerbert Robbins,\nSutton Monro\n\nAnn. Math. Statist. 22(3): 400-407 (September, 1951). DOI: 10.1214/aoms/1177729586\n\n- ABOUT\n- FIRST PAGE\n- CITED BY\n- [DOWNLOAD PAPER](https://projecteuclid.org/journalArticle/Download?urlid=10.1214%2Faoms%2F1177729586) [SAVE TO MY LIBRARY](https://projecteuclid.org/)\n\n\nPERSONAL SIGN IN\n\nFull access may be available with your subscription\n\nEmail\n\nPasswordForgot your password?\n\nShow\n\nRemember Email on this computer\n\nRemember Password\n\nNo Project Euclid account? [Create an account](https://projecteuclid.org/)\n\nor [Sign in with your institutional credentials](https://projecteuclid.org/Account/institutionalsignin?redirect=https%3a%2f%2fprojecteuclid.org%2fjournals%2fannals-of-mathematical-statistics%2fvolume-22%2fissue-3%2fA-Stochastic-Approximation-Method%2f10.1214%2faoms%2f1177729586.full)\n\nPURCHASE SINGLE ARTICLE\n\nThis article is only available to [subscribers](https://projecteuclid.org/subscriptions-and-access). It is not available for individual sale.\n\nThis will count as one of your downloads.\n\nYou will have access to both the presentation and article (if available).\n\n[DOWNLOAD NOW](https://projecteuclid.org/)\n\nThis content is available for download via your institution's subscription. To access this item, please sign in to your personal account.\n\nEmail\n\nPasswordForgot your password?\n\nShow\n\nRemember Email on this computer\n\nRemember Password\n\nNo Project Euclid account? [Create an account](https://projecteuclid.org/)\n\nMy Library\n\nYou currently do not have any folders to save your paper to! Create a new folder below.\n\nCreate New Folder\n\nSAVE >\n\n## Abstract\n\nLet $M(x)$ denote the expected value at level $x$ of the response to a certain experiment. $M(x)$ is assumed to be a monotone function of $x$ but is unknown to the experimenter, and it is desired to find the solution $x = \\\\theta$ of the equation $M(x) = \\\\alpha$, where $\\\\alpha$ is a given constant. We give a method for making successive experiments at levels $x\\_1,x\\_2,\\\\cdots$ in such a way that $x\\_n$ will tend to $\\\\theta$ in probability.\n\n## Citation\n\n[Download Citation](https://projecteuclid.org/)\n\nHerbert Robbins.Sutton Monro.\"A Stochastic Approximation Method.\"Ann. Math. Statist.22(3)400 - 407,September, 1951.https://doi.org/10.1214/aoms/1177729586\n\n## Information\n\nPublished: September, 1951\n\nFirst available in Project Euclid: 28 April 2007\n\nzbMATH: [0054.05901](https://zbmath.org/?q=an:0054.05901)\n\nMathSciNet: [MR42668](https://mathscinet.ams.org/mathscinet-getitem?mr=MR42668)\n\nDigital Object Identifier: 10.1214/aoms/1177729586\n\nRights: Copyright \u00a9 1951 Institute of Mathematical Statistics\n\nJOURNAL ARTICLE\n\n8 PAGES\n\n* * *\n\n[DOWNLOAD PDF](https://projecteuclid.org/journalArticle/Download?urlId=10.1214%2Faoms%2F1177729586)+SAVE TO MY LIBRARY\n\n* * *\n\n![](https://projecteuclid.org/content/themes/spieimages/GetCitation.png)\nGET CITATION\n\nMy Library\n\nYou currently do not have any folders to save your paper to! Create a new folder below.\n\nCreate New Folder\n\nSAVE >\n\nFolder Name\n\nFolder Description\n\nSAVE\n\n< [Previous Article](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/On-the-Translation-Parameter-Problem-for-Discrete-Variables/10.1214/aoms/1177729585.full)\n\n\\|\n\n[Next Article](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/Some-Bounded-Significance-Level-Properties-of-the-Equal-Tail-Sign/10.1214/aoms/1177729587.full) >\n\n[**Ann. Math. Statist.**](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3)\n\nVol.22 \u2022 No. 3 \u2022 September, 1951\n\n![](https://projecteuclid.org/images/journals/cover_aoms.jpg)\n\n[Institute of Mathematical Statistics](https://projecteuclid.org/publishers/Institute-of-Mathematical-Statistics)\n\n[![](https://projecteuclid.org/images/publishers/thumbnail/publisher_ims.png)](https://projecteuclid.org/publishers/Institute-of-Mathematical-Statistics)\n\n[![](https://projecteuclid.org/Content/themes/SPIEImages/Subscribe_Check.png)Subscribe to Project Euclid](https://projecteuclid.org/subscribe)\n\n[![](https://projecteuclid.org/Content/themes/SPIEImages/Alerts_Mark.png)Receive erratum alerts for this article](https://projecteuclid.org/)\n\nHerbert Robbins, Sutton Monro \"A Stochastic Approximation Method,\" The Annals of Mathematical Statistics, Ann. Math. Statist. 22(3), 400-407, (September, 1951)\n\nInclude:\n\nCitation Only\n\nCitation & Abstract\n\nFormat:\n\nRIS\n\nEndNote\n\nBibTex\n\nPrint Friendly Version (PDF)\n\nDOWNLOAD CITATION\n\n[![Back to Top](https://projecteuclid.org/Images/Project%20Euclid/Back-Top_Icon.png)](https://projecteuclid.org/#top)\n\n## KEYWORDS/PHRASES\n\nKeywords\n\nin\n\nAll FieldsAbstractAuthor NameAffiliationDOI/ISSN/ISBNFigure & Table CaptionsKeywordsTitle\n\nRemove\n\nANDORNOT\n\nin\n\nAll FieldsAbstractAuthor NameAffiliationDOI/ISSN/ISBNFigure & Table CaptionsKeywordsTitle\n\nRemove\n\nANDORNOT\n\nin\n\nAll FieldsAbstractAuthor NameAffiliationDOI/ISSN/ISBNFigure & Table CaptionsKeywordsTitle\n\nRemove\n\n\\+ Add another field\n\n* * *\n\n## PUBLICATION TITLE:\n\nAll Titles\n\nChoose Title(s)\n\nSelect a title\n\n* * *\n\n## PUBLICATION YEARS\n\nRange\n\nSingle Year\n\n* * *\n\nClear Form",
          "original_query": "A stochastic approximation method",
          "cleaned_query": "A stochastic approximation method",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "[2402.17235] Stochastic Gradient Succeeds for Bandits - arXiv",
          "url": "https://arxiv.org/abs/2402.17235",
          "content": "\n Download PDF \n HTML (experimental) \nWe show that the \\emph{stochastic gradient} bandit algorithm converges to a \\emph{globally optimal} policy at an $O(1/t)$ rate, even with a \\emph{constant} step size. Remarkably, global convergence of the stochastic gradient bandit algorithm has not been previously established, even though it is an old algorithm known to be applicable to bandits. The new result is achieved by establishing two novel technical findings: first, the noise of the stochastic updates in the gradient bandit algorithm satisfies a strong ``growth condition'' property, where the variance diminishes whenever progress becomes small, implying that additional noise control via diminishing step sizes is unnecessary; second, a form of ``weak exploration'' is automatically achieved through the stochastic gradient updates, since they prevent the action probabilities from decaying faster than $O(1/t)$, thus ensuring that every action is sampled infinitely often with probability $1$. These two findings can be used to show that the stochastic gradient update is already ``sufficient'' for bandits in the sense that exploration versus exploitation is automatically balanced in a manner that ensures almost sure convergence to a global optimum. These novel theoretical findings are further verified by experimental results.\n \n \n Submission history From: Jincheng Mei [ view email] [v1] \n Tue, 27 Feb 2024 06:05:01 UTC (2,734 KB) \n ||||I|||| Skip to main content\n We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate\n > cs > arXiv:2402.17235\n\n Help | Advanced Search\n\n All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text\n Search\n GO\n\n quick links\n\n * Login\n * Help Pages\n * About\n\n Computer Science > Machine Learning\n\n arXiv:2402.17235 (cs)\n [Submitted on 27 Feb 2024]\n\n Title: Stochastic Gradient Succeeds for Bandits\n\n Authors: Jincheng Mei, Zixin Zhong, Bo Dai, Alekh Agarwal, Csaba Szepesvari, Dale Schuurmans\n Download a PDF of the paper titled Stochastic Gradient Succeeds for Bandits, by Jincheng Mei and Zixin Zhong and Bo Dai and Alekh Agarwal and Csaba Szepesvari and Dale Schuurmans\n Download PDF HTML (experimental)\n Abstract: We show that the \\emph{stochastic gradient} bandit algorithm converges to a \\emph{globally optimal} policy at an $O(1/t)$ rate, even with a \\emph{constant} step size. Remarkably, global convergence of the stochastic gradient bandit algorithm has not been previously established, even though it is an old algorithm known to be applicable to bandits. The new result is achieved by establishing two novel technical findings: first, the noise of the stochastic updates in the gradient bandit algorithm satisfies a strong ``growth condition'' property, where the variance diminishes whenever progress becomes small, implying that additional noise control via diminishing step sizes is unnecessary; second, a form of ``weak exploration'' is automatically achieved through the stochastic gradient updates, since they prevent the action probabilities from decaying faster than $O(1/t)$, thus ensuring that every action is sampled infinitely often with probability $1$. These two findings can be used to show that the stochastic gradient update is already ``sufficient'' for bandits in the sense that exploration versus exploitation is automatically balanced in a manner that ensures almost sure convergence to a global optimum. These novel theoretical findings are further verified by experimental results.\n Comments: 39 pages; Correction for a previous version published at ICML 2023 conference\n Subjects: Machine Learning (cs.LG) \n Cite as: arXiv:2402.17235 [cs.LG] \n (or arXiv:2402.17235v1 [cs.LG] for this version) \n https://doi.org/10.48550/arXiv.2402.17235 \n Focus to learn more \n arXiv-issued DOI via DataCite \n \n\n Submission history\n\n From: Jincheng Mei [view email]\n [v1] Tue, 27 Feb 2024 06:05:01 UTC (2,734 KB)\n Full-text links:\n\n Access Paper:\n\n Download a PDF of the paper titled Stochastic Gradient Succeeds for Bandits, by Jincheng Mei and Zixin Zhong and Bo Dai and Alekh Agarwal and Csaba Szepesvari and Dale Schuurmans\n * Download PDF\n * HTML (experimental)\n * TeX Source\n * Other Formats\n view license\n Current browse context:\n cs.LG\n < prev | next >\n new | recent | 2402\n Change to browse by:\n cs\n\n References & Citations\n\n * NASA ADS\n * Google Scholar\n * Semantic Scholar\n a export BibTeX citation Loading...\n\n BibTeX formatted citation\n\n \u00d7\n loading...\n Data provided by:\n\n Bookmark\n\n Bibliographic Tools\n\n Bibliographic and Citation Tools\n\n Bibliographic Explorer Toggle\n Bibliographic Explorer (What is the Explorer?)\n Litmaps Toggle\n Litmaps (What is Litmaps?)\n scite.ai Toggle\n scite Smart Citations (What are Smart Citations?)\n Code, Data, Media\n\n Code, Data and Media Associated with this Article\n\n Links to Code Toggle\n CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n DagsHub Toggle\n DagsHub (What is DagsHub?)\n GotitPub Toggle\n Gotit.pub (What is GotitPub?)\n Links to Code Toggle\n Papers with Code (What is Papers with Code?)\n ScienceCast Toggle\n ScienceCast (What is ScienceCast?)\n Demos\n\n Demos\n\n Replicate Toggle\n Replicate (What is Replicate?)\n Spaces Toggle\n Hugging Face Spaces (What is Spaces?)\n Spaces Toggle\n TXYZ.AI (What is TXYZ.AI?)\n Related Papers\n\n Recommenders and Search Tools\n\n Link to Influence Flower\n Influence Flower (What are Influence Flowers?)\n Connected Papers Toggle\n Connected Papers (What is Connected Papers?)\n Core recommender toggle\n CORE Recommender (What is CORE?)\n IArxiv recommender toggle\n IArxiv Recommender (What is IArxiv?)\n About arXivLabs\n\n arXivLabs: experimental projects with community collaborators\n\n arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\n Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\n Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n\n Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n * About\n * Help\n * Click here to contact arXiv Contact\n * Click here to subscribe Subscribe\n * Copyright\n * Privacy Policy\n * Web Accessibility Assistance\n\n * arXiv Operational Status\n Get status notifications via email or slack",
          "original_query": "Stochastic gradient succeeds for bandits",
          "cleaned_query": "Stochastic gradient succeeds for bandits"
        },
        {
          "success": true,
          "title": "Finite-time Analysis of the Multiarmed Bandit Problem",
          "url": "https://link.springer.com/article/10.1023/A:1013689704352",
          "content": "Finite-time Analysis of the Multiarmed Bandit Problem | Machine Learning\n[Skip to main content](#main)\nAdvertisement\n[![Springer Nature Link](https://link.springer.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1023/A:1013689704352?)\n# Finite-time Analysis of the Multiarmed Bandit Problem\n* Published:May 2002\n* Volume\u00a047,\u00a0pages 235\u2013256, (2002)\n* [Cite this article](#citeas)\n[Download PDF](https://link.springer.com/content/pdf/10.1023/A:1013689704352.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/10994?as=webp)Machine Learning](https://link.springer.com/journal/10994)[Aims and scope](https://link.springer.com/journal/10994/aims-and-scope)[Submit manuscript](https://submission.springernature.com/new-submission/10994/3)\nFinite-time Analysis of the Multiarmed Bandit Problem\n[Download PDF](https://link.springer.com/content/pdf/10.1023/A:1013689704352.pdf)\n* [Peter Auer](#auth-Peter-Auer-Aff1)[1](#Aff1),\n* [Nicol\u00f2 Cesa-Bianchi](#auth-Nicol_-Cesa_Bianchi-Aff2)[2](#Aff2)&amp;\n* [Paul Fischer](#auth-Paul-Fischer-Aff3)[3](#Aff3)\n* 51kAccesses\n* 4179Citations\n* 49Altmetric\n* 3Mentions\n* [Explore all metrics](https://link.springer.com/article/10.1023/A:1013689704352/metrics)\n## Abstract\nReinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.\n## Article PDF\n[Download](https://link.springer.com/content/pdf/10.1023/A:1013689704352.pdf)to read the full article text\n### Similar content being viewed by others\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10994-022-06271-z/MediaObjects/10994_2022_6271_Fig1_HTML.png)\n### [Multi-armed bandits with censored consumption of resources](https://link.springer.com/10.1007/s10994-022-06271-z?fromPaywallRec=false)\nArticleOpen access16 November 2022\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10994-022-06291-9/MediaObjects/10994_2022_6291_Fig1_HTML.png)\n### [Constrained regret minimization for multi-criterion multi-armed bandits](https://link.springer.com/10.1007/s10994-022-06291-9?fromPaywallRec=false)\nArticle06 January 2023\n![](https://media.springernature.com/w215h120/springer-static/image/art%3A10.1007%2Fs10994-021-05956-1/MediaObjects/10994_2021_5956_Figa_HTML.png)\n### [Multi-objective multi-armed bandit with lexicographically ordered and satisficing objectives](https://link.springer.com/10.1007/s10994-021-05956-1?fromPaywallRec=false)\nArticle05 May 2021\n### Explore related subjects\nDiscover the latest articles, books and news in related subjects, suggested using machine learning.\n* [Continuous Optimization](https://link.springer.com/subjects/continuous-optimization)\n* [Discrete Optimization](https://link.springer.com/subjects/discrete-optimization)\n* [Learning algorithms](https://link.springer.com/subjects/learning-algorithms)\n* [Operations Research and Decision Theory](https://link.springer.com/subjects/operations-research-and-decision-theory)\n* [Stochastic Learning and Adaptive Control](https://link.springer.com/subjects/stochastic-learning-and-adaptive-control)\n* [Calculus of Variations and Optimization](https://link.springer.com/subjects/calculus-of-variations-and-optimization)\n[Use our pre-submission checklist](https://beta.springernature.com/pre-submission?journalId=10994)\nAvoid common mistakes on your manuscript.\n## References\n* Agrawal, R. (1995). Sample mean based index policies with*O(*log*n)*regret for the multi-armed bandit problem.*Advances in Applied Probability*,*27*, 1054\u20131078.\n[Google Scholar]()\n* Berry, D., &amp; Fristedt, B. (1985).*Bandit problems*. London: Chapman and Hall.\n[Google Scholar]()\n* Burnetas, A., &amp; Katehakis, M. (1996). Optimal adaptive policies for sequential allocation problems.*Advances in Applied Mathematics*,*17:2*, 122\u2013142.\n[Google Scholar]()\n* Duff, M. (1995). Q-learning for bandit problems. In*Proceedings of the 12th International Conference on Machine Learning*(pp. 209-217).\n* Gittins, J. (1989).*Multi-armed bandit allocation indices*, Wiley-Interscience series in Systems and Optimization. New York: John Wiley and Sons.\n[Google Scholar]()\n* Holland, J. (1992).*Adaptation in natural and artificial systems*. Cambridge: MIT Press/Bradford Books.\n[Google Scholar]()\n* Ishikida, T., &amp; Varaiya, P. (1994). Multi-armed bandit problem revisited.*Journal of Optimization Theory and Applications*,*83:1*, 113\u2013154.\n[Google Scholar]()\n* Lai, T., &amp; Robbins, H. (1985). Asymptotically efficient adaptive allocation rules.*Advances in Applied Mathematics*,*6*, 4\u201322.\n[Google Scholar]()\n* Pollard, D. (1984).*Convergence of stochastic processes*. Berlin: Springer.\n[Google Scholar]()\n* Sutton, R., &amp; Barto, A. (1998).*Reinforcement learning, an introduction*. Cambridge: MIT Press/Bradford Books.\n[Google Scholar]()\n* Wilks, S. (1962).*Matematical statistics*. New York: John Wiley and Sons.\n[Google Scholar]()\n* Yakowitz, S., &amp; Lowe, W. (1991). Nonparametric bandit methods.*Annals of Operations Research*,*28*, 297\u2013312.\n[Google Scholar]()\n[Download references](https://citation-needed.springer.com/v2/references/10.1023/A:1013689704352?format=refman&amp;flavour=references)\n## Author information\n### Authors and Affiliations\n1. University of Technology Graz, A-8010, Graz, Austria\nPeter Auer\n2. DTI, University of Milan, via Bramante 65, I-26013, Crema, Italy\nNicol\u00f2 Cesa-Bianchi\n3. Lehrstuhl Informatik II, Universit\u00e4t Dortmund, D-44221, Dortmund, Germany\nPaul Fischer\nAuthors\n1. Peter Auer\n[View author publications](https://link.springer.com/search?sortBy=newestFirst&amp;dc.creator=Peter%20Auer)\nSearch author on:[PubMed]()[Google Scholar]()\n2. Nicol\u00f2 Cesa-Bianchi\n[View author publications](https://link.springer.com/search?sortBy=newestFirst&amp;dc.creator=Nicol%C3%B2%20Cesa-Bianchi)\nSearch author on:[PubMed]()[Google Scholar]()\n3. Paul Fischer\n[View author publications](https://link.springer.com/search?sortBy=newestFirst&amp;dc.creator=Paul%20Fischer)\nSearch author on:[PubMed]()[Google Scholar]()\n## Rights and permissions\n[Reprints and permissions]()\n## About this article\n### Cite this article\nAuer, P., Cesa-Bianchi, N. &amp; Fischer, P. Finite-time Analysis of the Multiarmed Bandit Problem.*Machine Learning***47**, 235\u2013256 (2002). https://doi.org/10.1023/A:1013689704352\n[Download citation](https://citation-needed.springer.com/v2/references/10.1023/A:1013689704352?format=refman&amp;flavour=citation)\n* Issue date:May 2002\n* DOI:https://doi.org/10.1023/A:1013689704352\n### Share this article\nAnyone you share the following link with will be able to read this content:\nGet shareable link\nSorry, a shareable link is not currently available for this article.\nCopy shareable link to clipboard\nProvided by the Springer Nature SharedIt content-sharing initiative\n* [bandit problems](https://link.springer.com/search?query=bandit%20problems&amp;facet-discipline=&#34;Computer%20Science&#34;)\n* [adaptive allocation rules](https://link.spring",
          "original_query": "Finite-time analysis of the multiarmed bandit problem",
          "cleaned_query": "Finite-time analysis of the multiarmed bandit problem"
        }
      ],
      "generated_ideas": [
        "Robbins\u2013Monro Root-Finding for Best-Arm Identification (BAI) with Continuous Arms\nUse Robbins\u2013Monro stochastic approximation to solve for the threshold where the gap between the best arm and a contender equals a target confidence margin, enabling adaptive sampling on a continuous action space. Provide finite-time stopping rules with explicit sample complexity, and compare against discretization-based BAI baselines.",
        "Non-Stationary Bandits: Constant-Step Gradient Bandits with Drift Tracking Guarantees\nAnalyze gradient bandits with constant step size as a stochastic approximation tracker of a moving optimum when reward means drift over time. Derive dynamic regret bounds in terms of a variation budget, and characterize when the growth condition still yields stable tracking without diminishing step sizes.",
        "Unifying Robbins\u2013Monro and Gradient Bandits via ODE/Stochastic Differential Equation Limits\nDevelop a unified limiting ODE/SDE framework where both Robbins\u2013Monro root-finding and policy-gradient bandit updates appear as instances of a common stochastic approximation dynamic. Use this to derive new sufficient conditions for global convergence (e.g., Lyapunov functions) and to predict when constant step sizes succeed or fail across bandit parameterizations."
      ],
      "generated_ideas_raw": "1. **Robbins\u2013Monro Root-Finding for Best-Arm Identification (BAI) with Continuous Arms**  \n   Use Robbins\u2013Monro stochastic approximation to solve for the threshold where the gap between the best arm and a contender equals a target confidence margin, enabling adaptive sampling on a continuous action space. Provide finite-time stopping rules with explicit sample complexity, and compare against discretization-based BAI baselines.\n\n2. **A Growth-Condition Lens for UCB-Style Regret Proofs**  \n   Formalize an analogue of the \u201cstrong growth condition\u201d (variance shrinks when progress is small) for index policies like UCB1 by expressing index updates as stochastic approximation steps. Derive new finite-time regret bounds that replace worst-case variance terms with data-dependent variance that tightens automatically as arms become well-separated.\n\n3. **Constant-Step-Size Gradient Bandits under Heavy-Tailed Rewards via Robust Gradient Estimators**  \n   Extend the constant-step-size global convergence result to reward distributions beyond bounded support (Paper 3) by integrating robust mean/gradient estimators (e.g., truncation or median-of-means) into the stochastic gradient bandit update. Prove an \\(O(1/t)\\)-type convergence (or best-possible alternative rate) under finite-variance or bounded-moment assumptions, with explicit robustness\u2013bias tradeoffs.\n\n4. **Automatically-Calibrated Exploration Floors from Stochastic Approximation Theory**  \n   Turn the \u201cweak exploration\u201d phenomenon (action probabilities not decaying faster than \\(O(1/t)\\)) into an explicit design principle: add a Robbins\u2013Monro-style correction term that enforces a target minimum sampling rate per arm without manual \\(\\epsilon\\)-greedy schedules. Provide guarantees that the floor is just enough to ensure infinite exploration while minimizing regret impact, with a finite-time analysis.\n\n5. **Non-Stationary Bandits: Constant-Step Gradient Bandits with Drift Tracking Guarantees**  \n   Analyze gradient bandits with constant step size as a stochastic approximation tracker of a moving optimum when reward means drift over time. Derive dynamic regret bounds in terms of a variation budget, and characterize when the growth condition still yields stable tracking without diminishing step sizes.\n\n6. **Unifying Robbins\u2013Monro and Gradient Bandits via ODE/Stochastic Differential Equation Limits**  \n   Develop a unified limiting ODE/SDE framework where both Robbins\u2013Monro root-finding and policy-gradient bandit updates appear as instances of a common stochastic approximation dynamic. Use this to derive new sufficient conditions for global convergence (e.g., Lyapunov functions) and to predict when constant step sizes succeed or fail across bandit parameterizations.\n\n7. **Finite-Time Last-Iterate Guarantees for Softmax Gradient Bandits**  \n   Move beyond asymptotic/global convergence statements by proving explicit finite-time bounds on last-iterate suboptimality for softmax policies updated by stochastic gradients. Leverage ideas from Paper 3\u2019s uniform-in-time analysis to give high-probability bounds that hold at every time \\(t\\), not only on averages or in expectation.\n\n8. **Best-of-Both-Worlds Algorithm: UCB Initialization + Constant-Step Gradient Refinement**  \n   Design a two-phase policy that starts with UCB1 (for rapid identification and bounded-support finite-time guarantees) and then switches to constant-step gradient updates once gaps are estimated reliably. Prove improved constants or instance-dependent regret compared to pure UCB or pure gradient bandits, with a principled switching criterion based on confidence intervals.\n\n9. **Bandits with Monotone Structure: Robbins\u2013Monro-Guided Action Selection**  \n   Consider structured bandits where arm means are monotone in an ordered action parameter (mirroring Paper 1\u2019s monotone \\(M(x)\\)). Use Robbins\u2013Monro to home in on the optimal arm by querying actions that shrink uncertainty around the maximizer, and provide regret bounds that improve over unstructured \\(K\\)-arm rates by exploiting monotonicity.\n\n10. **Variance-Adaptive Step Sizes that Preserve Constant-Step Benefits**  \n   Create a step-size rule that is \u201ceffectively constant\u201d but shrinks only when an empirical growth-condition diagnostic fails (e.g., when gradient noise does not diminish with progress). Prove that the algorithm retains \\(O(1/t)\\) convergence on problems satisfying the growth condition (Paper 2) while reverting to safe diminishing-step behavior otherwise, with verifiable finite-time conditions.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Robbins\u2013Monro Root-Finding for Best-Arm Identification (BAI) with Continuous Arms\nUse Robbins\u2013Monro stochastic approximation to solve for the threshold where the gap between the best arm and a contend",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Non-Stationary Bandits: Constant-Step Gradient Bandits with Drift Tracking Guarantees\nAnalyze gradient bandits with constant step size as a stochastic approximation tracker of a moving optimum when re",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Unifying Robbins\u2013Monro and Gradient Bandits via ODE/Stochastic Differential Equation Limits\nDevelop a unified limiting ODE/SDE framework where both Robbins\u2013Monro root-finding and policy-gradient bandi",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 74,
      "paper_title": "Perception Encoder: The best visual embeddings are not at the output of the network",
      "contribution": "The paper introduces the Perception Encoder, a family of vision encoding models that leverage intermediate layer embeddings for superior performance across various computer vision tasks.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 5,
      "crawl_rate": 1.0,
      "ideas_generated": 4,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10343,
      "output_tokens": 1166,
      "predecessor_details": [
        {
          "success": true,
          "title": "Multimodal Autoregressive Pre-training of Large Vision Encoders",
          "url": "https://arxiv.org/abs/2411.14402",
          "content": "[2411.14402] Multimodal Autoregressive Pre-training of Large Vision Encoders\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nIn just 5 minutes help us improve arXiv:\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2411.14402\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2411.14402**(cs)\n[Submitted on 21 Nov 2024]\n# Title:Multimodal Autoregressive Pre-training of Large Vision Encoders\nAuthors:[Enrico Fini](https://arxiv.org/search/cs?searchtype=author&amp;query=Fini,+E),[Mustafa Shukor](https://arxiv.org/search/cs?searchtype=author&amp;query=Shukor,+M),[Xiujun Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+X),[Philipp Dufter](https://arxiv.org/search/cs?searchtype=author&amp;query=Dufter,+P),[Michal Klein](https://arxiv.org/search/cs?searchtype=author&amp;query=Klein,+M),[David Haldimann](https://arxiv.org/search/cs?searchtype=author&amp;query=Haldimann,+D),[Sai Aitharaju](https://arxiv.org/search/cs?searchtype=author&amp;query=Aitharaju,+S),[Victor Guilherme Turrisi da Costa](https://arxiv.org/search/cs?searchtype=author&amp;query=da+Costa,+V+G+T),[Louis B\u00e9thune](https://arxiv.org/search/cs?searchtype=author&amp;query=B\u00e9thune,+L),[Zhe Gan](https://arxiv.org/search/cs?searchtype=author&amp;query=Gan,+Z),[Alexander T Toshev](https://arxiv.org/search/cs?searchtype=author&amp;query=Toshev,+A+T),[Marcin Eichner](https://arxiv.org/search/cs?searchtype=author&amp;query=Eichner,+M),[Moin Nabi](https://arxiv.org/search/cs?searchtype=author&amp;query=Nabi,+M),[Yinfei Yang](https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+Y),[Joshua M. Susskind](https://arxiv.org/search/cs?searchtype=author&amp;query=Susskind,+J+M),[Alaaeldin El-Nouby](https://arxiv.org/search/cs?searchtype=author&amp;query=El-Nouby,+A)\nView a PDF of the paper titled Multimodal Autoregressive Pre-training of Large Vision Encoders, by Enrico Fini and 15 other authors\n[View PDF](https://arxiv.org/pdf/2411.14402)> > Abstract:\n> We introduce a novel method for pre-training of large-scale vision encoders. Building on recent advancements in autoregressive pre-training of vision models, we extend this framework to a multimodal setting, i.e., images and text. In this paper, we present AIMV2, a family of generalist vision encoders characterized by a straightforward pre-training process, scalability, and remarkable performance across a range of downstream tasks. This is achieved by pairing the vision encoder with a multimodal decoder that autoregressively generates raw image patches and text tokens. Our encoders excel not only in multimodal evaluations but also in vision benchmarks such as localization, grounding, and classification. Notably, our AIMV2-3B encoder achieves 89.5% accuracy on ImageNet-1k with a frozen trunk. Furthermore, AIMV2 consistently outperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) in multimodal image understanding across diverse settings. Comments:|[this https URL](https://github.com/apple/ml-aim)|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)|\nCite as:|[arXiv:2411.14402](https://arxiv.org/abs/2411.14402)[cs.CV]|\n|(or[arXiv:2411.14402v1](https://arxiv.org/abs/2411.14402v1)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2411.14402](https://doi.org/10.48550/arXiv.2411.14402)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Alaaeldin El-Nouby [[view email](https://arxiv.org/show-email/164eafba/2411.14402)]\n**[v1]**Thu, 21 Nov 2024 18:31:25 UTC (4,240 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Multimodal Autoregressive Pre-training of Large Vision Encoders, by Enrico Fini and 15 other authors\n* [View PDF](https://arxiv.org/pdf/2411.14402)\n* [TeX Source](https://arxiv.org/src/2411.14402)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2411.14402&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2411.14402&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2024-11](https://arxiv.org/list/cs.CV/2024-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/2411.14402?context=cs)\n[cs.LG](https://arxiv.org/abs/2411.14402?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2411.14402)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2411.14402)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2411.14402)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLab",
          "original_query": "Multimodal autoregressive pre-training of large vision encoders",
          "cleaned_query": "Multimodal autoregressive pre-training of large vision encoders"
        },
        {
          "success": true,
          "title": "[2312.14238] InternVL: Scaling up Vision Foundation Models and ...",
          "url": "https://arxiv.org/abs/2312.14238",
          "content": "# Computer Science > Computer Vision and Pattern Recognition\n\n**arXiv:2312.14238** (cs)\n\n\\[Submitted on 21 Dec 2023 ( [v1](https://arxiv.org/abs/2312.14238v1)), last revised 15 Jan 2024 (this version, v3)\\]\n\n# Title:InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks\n\nAuthors: [Zhe Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+Z), [Jiannan Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu,+J), [Wenhai Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+W), [Weijie Su](https://arxiv.org/search/cs?searchtype=author&query=Su,+W), [Guo Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen,+G), [Sen Xing](https://arxiv.org/search/cs?searchtype=author&query=Xing,+S), [Muyan Zhong](https://arxiv.org/search/cs?searchtype=author&query=Zhong,+M), [Qinglong Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+Q), [Xizhou Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu,+X), [Lewei Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu,+L), [Bin Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+B), [Ping Luo](https://arxiv.org/search/cs?searchtype=author&query=Luo,+P), [Tong Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu,+T), [Yu Qiao](https://arxiv.org/search/cs?searchtype=author&query=Qiao,+Y), [Jifeng Dai](https://arxiv.org/search/cs?searchtype=author&query=Dai,+J)\n\nView a PDF of the paper titled InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks, by Zhe Chen and 14 other authors\n\n[View PDF](https://arxiv.org/pdf/2312.14238) [HTML (experimental)](https://arxiv.org/html/2312.14238v3)\n\n> Abstract:The exponential growth of large language models (LLMs) has opened up numerous possibilities for multimodal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foundation model (InternVL), which scales up the vision foundation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision-language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models. Code and models are available at [this https URL](https://github.com/OpenGVLab/InternVL).\n\n| | |\n| --- | --- |\n| Comments: | 25 pages, 5 figures, 28 tables |\n| Subjects: | Computer Vision and Pattern Recognition (cs.CV) |\n| Cite as: | [arXiv:2312.14238](https://arxiv.org/abs/2312.14238) \\[cs.CV\\] |\n| (or [arXiv:2312.14238v3](https://arxiv.org/abs/2312.14238v3) \\[cs.CV\\] for this version) |\n| [https://doi.org/10.48550/arXiv.2312.14238](https://doi.org/10.48550/arXiv.2312.14238) Focus to learn more arXiv-issued DOI via DataCite |\n\n## Submission history\n\nFrom: Zhe Chen \\[ [view email](https://arxiv.org/show-email/23f1d40e/2312.14238)\\] **[\\[v1\\]](https://arxiv.org/abs/2312.14238v1)**\nThu, 21 Dec 2023 18:59:31 UTC (1,993 KB)\n**[\\[v2\\]](https://arxiv.org/abs/2312.14238v2)**\nTue, 26 Dec 2023 18:03:16 UTC (1,824 KB)\n**\\[v3\\]**\nMon, 15 Jan 2024 15:23:55 UTC (1,824 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks, by Zhe Chen and 14 other authors\n\n- [View PDF](https://arxiv.org/pdf/2312.14238)\n- [HTML (experimental)](https://arxiv.org/html/2312.14238v3)\n- [TeX Source](https://arxiv.org/src/2312.14238)\n\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\n\nCurrent browse context:\n\ncs.CV\n\n[<\u00a0prev](https://arxiv.org/prevnext?id=2312.14238&function=prev&context=cs.CV)\u00a0 \\| \u00a0[next\u00a0>](https://arxiv.org/prevnext?id=2312.14238&function=next&context=cs.CV)\n\n[new](https://arxiv.org/list/cs.CV/new) \\| [recent](https://arxiv.org/list/cs.CV/recent) \\| [2023-12](https://arxiv.org/list/cs.CV/2023-12)\n\nChange to browse by:\n\n[cs](https://arxiv.org/abs/2312.14238?context=cs)\n\n### References & Citations\n\n- [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2312.14238)\n- [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2312.14238)\n- [Semantic Scholar](https://api.semanticscholar.org/arXiv:2312.14238)\n\nexport BibTeX citationLoading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\nBibliographic Tools\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer Toggle\n\nBibliographic Explorer _( [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_\n\nConnected Papers Toggle\n\nConnected Papers _( [What is Connected Papers?](https://www.connectedpapers.com/about))_\n\nLitmaps Toggle\n\nLitmaps _( [What is Litmaps?](https://www.litmaps.co/))_\n\nscite.ai Toggle\n\nscite Smart Citations _( [What are Smart Citations?](https://www.scite.ai/))_\n\nCode, Data, Media\n\n# Code, Data and Media Associated with this Article\n\nalphaXiv Toggle\n\nalphaXiv _( [What is alphaXiv?](https://alphaxiv.org/))_\n\nLinks to Code Toggle\n\nCatalyzeX Code Finder for Papers _( [What is CatalyzeX?](https://www.catalyzex.com))_\n\nDagsHub Toggle\n\nDagsHub _( [What is DagsHub?](https://dagshub.com/))_\n\nGotitPub Toggle\n\nGotit.pub _( [What is GotitPub?](http://gotit.pub/faq))_\n\nHuggingface Toggle\n\nHugging Face _( [What is Huggingface?](https://huggingface.co/huggingface))_\n\nLinks to Code Toggle\n\nPapers with Code _( [What is Papers with Code?](https://paperswithcode.com/))_\n\nScienceCast Toggle\n\nScienceCast _( [What is ScienceCast?](https://sciencecast.org/welcome))_\n\nDemos\n\n# Demos\n\nReplicate Toggle\n\nReplicate _( [What is Replicate?](https://replicate.com/docs/arxiv/about))_\n\nSpaces Toggle\n\nHugging Face Spaces _( [What is Spaces?](https://huggingface.co/docs/hub/spaces))_\n\nSpaces Toggle\n\nTXYZ.AI _( [What is TXYZ.AI?](https://txyz.ai))_\n\nRelated Papers\n\n# Recommenders and Search Tools\n\nLink to Influence Flower\n\nInfluence Flower _( [What are Influence Flowers?](https://influencemap.cmlab.dev/))_\n\nCore recommender toggle\n\nCORE Recommender _( [What is CORE?](https://core.ac.uk/services/recommender))_\n\n- Author\n- Venue\n- Institution\n- Topic\n\nAbout arXivLabs\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n\nHave an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2312.14238) \\|\n[Disable MathJax](javascript:setMathjaxCookie()) ( [What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks",
          "cleaned_query": "InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks"
        },
        {
          "success": true,
          "title": "[2010.11929] An Image is Worth 16x16 Words: Transformers ... - arXiv",
          "url": "https://arxiv.org/abs/2010.11929",
          "content": "[2010.11929] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2010.11929\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2010.11929**(cs)\n[Submitted on 22 Oct 2020 ([v1](https://arxiv.org/abs/2010.11929v1)), last revised 3 Jun 2021 (this version, v2)]\n# Title:An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\nAuthors:[Alexey Dosovitskiy](https://arxiv.org/search/cs?searchtype=author&amp;query=Dosovitskiy,+A),[Lucas Beyer](https://arxiv.org/search/cs?searchtype=author&amp;query=Beyer,+L),[Alexander Kolesnikov](https://arxiv.org/search/cs?searchtype=author&amp;query=Kolesnikov,+A),[Dirk Weissenborn](https://arxiv.org/search/cs?searchtype=author&amp;query=Weissenborn,+D),[Xiaohua Zhai](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhai,+X),[Thomas Unterthiner](https://arxiv.org/search/cs?searchtype=author&amp;query=Unterthiner,+T),[Mostafa Dehghani](https://arxiv.org/search/cs?searchtype=author&amp;query=Dehghani,+M),[Matthias Minderer](https://arxiv.org/search/cs?searchtype=author&amp;query=Minderer,+M),[Georg Heigold](https://arxiv.org/search/cs?searchtype=author&amp;query=Heigold,+G),[Sylvain Gelly](https://arxiv.org/search/cs?searchtype=author&amp;query=Gelly,+S),[Jakob Uszkoreit](https://arxiv.org/search/cs?searchtype=author&amp;query=Uszkoreit,+J),[Neil Houlsby](https://arxiv.org/search/cs?searchtype=author&amp;query=Houlsby,+N)\nView a PDF of the paper titled An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, by Alexey Dosovitskiy and 10 other authors\n[View PDF](https://arxiv.org/pdf/2010.11929)> > Abstract:\n> While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. Comments:|Fine-tuning code and pre-trained models are available at[this https URL](https://github.com/google-research/vision_transformer). ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)|\nCite as:|[arXiv:2010.11929](https://arxiv.org/abs/2010.11929)[cs.CV]|\n|(or[arXiv:2010.11929v2](https://arxiv.org/abs/2010.11929v2)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2010.11929](https://doi.org/10.48550/arXiv.2010.11929)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Alexey Dosovitskiy [[view email](https://arxiv.org/show-email/375200f6/2010.11929)]\n**[[v1]](https://arxiv.org/abs/2010.11929v1)**Thu, 22 Oct 2020 17:55:59 UTC (3,194 KB)\n**[v2]**Thu, 3 Jun 2021 13:08:56 UTC (3,033 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, by Alexey Dosovitskiy and 10 other authors\n* [View PDF](https://arxiv.org/pdf/2010.11929)\n* [TeX Source](https://arxiv.org/src/2010.11929)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2010.11929&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2010.11929&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2020-10](https://arxiv.org/list/cs.CV/2020-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/2010.11929?context=cs)\n[cs.AI](https://arxiv.org/abs/2010.11929?context=cs.AI)\n[cs.LG](https://arxiv.org/abs/2010.11929?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2010.11929)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2010.11929)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2010.11929)\n### [31 blog links](https://arxiv.org/tb/2010.11929)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2010.html#abs-2010-11929)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2010-11929)\n[Alexey Dosovitskiy]()\n[Lucas Beyer]()\n[Alexander Kolesnikov]()\n[Dirk Weissenborn]()\n[Xiaohua Zhai]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence F",
          "original_query": "An image is worth 16x16 words: Transformers for image recognition at scale",
          "cleaned_query": "An image is worth 16x16 words: Transformers for image recognition at scale"
        },
        {
          "success": true,
          "title": "Masked Autoencoders Are Scalable Vision Learners - arXiv",
          "url": "https://arxiv.org/abs/2111.06377",
          "content": "[2111.06377] Masked Autoencoders Are Scalable Vision Learners[![close this message](https://arxiv.org/static/browse/0.3.4/images/icons/close-slider.png)](#)\n![arXiv smileybones](https://arxiv.org/static/browse/0.3.4/images/icons/smileybones-pixel.png)\n## Happy Open Access Week from arXiv!\nYOU make open access possible! Tell us why you support #openaccess and give to arXiv this week to help keep science open for all.\n[**Donate!**](https://arxiv.salsalabs.org/arXivOAWeek2025)\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2111.06377\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2111.06377**(cs)\n[Submitted on 11 Nov 2021 ([v1](https://arxiv.org/abs/2111.06377v1)), last revised 19 Dec 2021 (this version, v3)]\n# Title:Masked Autoencoders Are Scalable Vision Learners\nAuthors:[Kaiming He](https://arxiv.org/search/cs?searchtype=author&amp;query=He,+K),[Xinlei Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+X),[Saining Xie](https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+S),[Yanghao Li](https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y),[Piotr Doll\u00e1r](https://arxiv.org/search/cs?searchtype=author&amp;query=Doll\u00e1r,+P),[Ross Girshick](https://arxiv.org/search/cs?searchtype=author&amp;query=Girshick,+R)\nView a PDF of the paper titled Masked Autoencoders Are Scalable Vision Learners, by Kaiming He and 5 other authors\n[View PDF](https://arxiv.org/pdf/2111.06377)> > Abstract:\n> This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior. Comments:|Tech report. arXiv v2: add more transfer learning results; v3: add robustness evaluation|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:2111.06377](https://arxiv.org/abs/2111.06377)[cs.CV]|\n|(or[arXiv:2111.06377v3](https://arxiv.org/abs/2111.06377v3)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2111.06377](https://doi.org/10.48550/arXiv.2111.06377)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Kaiming He [[view email](https://arxiv.org/show-email/612bcab6/2111.06377)]\n**[[v1]](https://arxiv.org/abs/2111.06377v1)**Thu, 11 Nov 2021 18:46:40 UTC (6,839 KB)\n**[[v2]](https://arxiv.org/abs/2111.06377v2)**Thu, 2 Dec 2021 18:30:33 UTC (6,840 KB)\n**[v3]**Sun, 19 Dec 2021 19:23:25 UTC (6,841 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Masked Autoencoders Are Scalable Vision Learners, by Kaiming He and 5 other authors\n* [View PDF](https://arxiv.org/pdf/2111.06377)\n* [TeX Source](https://arxiv.org/src/2111.06377)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2111.06377&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2111.06377&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2021-11](https://arxiv.org/list/cs.CV/2021-11)\nChange to browse by:\n[cs](https://arxiv.org/abs/2111.06377?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2111.06377)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2111.06377)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2111.06377)\n### [5 blog links](https://arxiv.org/tb/2111.06377)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2111.html#abs-2111-06377)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2111-06377)\n[Kaiming He]()\n[Xinlei Chen]()\n[Saining Xie]()\n[Yanghao Li]()\n[Piotr Doll\u00e1r]()\n&hellip;\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators ",
          "original_query": "Masked autoencoders are scalable vision learners",
          "cleaned_query": "Masked autoencoders are scalable vision learners"
        },
        {
          "success": true,
          "title": "[1512.03385] Deep Residual Learning for Image Recognition",
          "url": "https://arxiv.org/abs/1512.03385",
          "content": "[1512.03385] Deep Residual Learning for Image Recognition\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1512.03385\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:1512.03385**(cs)\n[Submitted on 10 Dec 2015]\n# Title:Deep Residual Learning for Image Recognition\nAuthors:[Kaiming He](https://arxiv.org/search/cs?searchtype=author&amp;query=He,+K),[Xiangyu Zhang](https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X),[Shaoqing Ren](https://arxiv.org/search/cs?searchtype=author&amp;query=Ren,+S),[Jian Sun](https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+J)\nView a PDF of the paper titled Deep Residual Learning for Image Recognition, by Kaiming He and 3 other authors\n[View PDF](https://arxiv.org/pdf/1512.03385)> > Abstract:\n> Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\n> The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. Comments:|Tech report|\nSubjects:|Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:1512.03385](https://arxiv.org/abs/1512.03385)[cs.CV]|\n|(or[arXiv:1512.03385v1](https://arxiv.org/abs/1512.03385v1)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.1512.03385](https://doi.org/10.48550/arXiv.1512.03385)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Kaiming He [[view email](https://arxiv.org/show-email/aa803c8a/1512.03385)]\n**[v1]**Thu, 10 Dec 2015 19:51:55 UTC (494 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Deep Residual Learning for Image Recognition, by Kaiming He and 3 other authors\n* [View PDF](https://arxiv.org/pdf/1512.03385)\n* [TeX Source](https://arxiv.org/src/1512.03385)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1512.03385&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1512.03385&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2015-12](https://arxiv.org/list/cs.CV/2015-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/1512.03385?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1512.03385)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1512.03385)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1512.03385)\n### [107 blog links](https://arxiv.org/tb/1512.03385)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1512.html#HeZRS15)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/HeZRS15)\n[Kaiming He]()\n[Xiangyu Zhang]()\n[Shaoqing Ren]()\n[Jian Sun]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1512.03385)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Deep residual learning for image recognition",
          "cleaned_query": "Deep residual learning for image recognition"
        }
      ],
      "generated_ideas": [
        "Token-Budget Adaptive Vision Encoders for LLM Alignment\nInspired by InternVL\u2019s progressive alignment, train a ViT encoder that can dynamically output variable numbers of visual tokens based on an instruction/query (e.g., \u201ccount cars\u201d vs \u201cread sign\u201d). Use a lightweight controller that predicts a token budget and performs patch merging/pruning before cross-attention into the LLM. Benchmark accuracy\u2013latency trade-offs on multimodal chat, VQA, OCR-centric tasks, and dense grounding where token sparsity is critical.",
        "Patch-Level Contrastive\u2013Autoregressive Hybrid Alignment (CLIP/SigLIP + AIMV2)\nBuild a single model that learns global image\u2013text alignment via contrastive loss while simultaneously training an autoregressive multimodal decoder to generate patches/tokens. Constrain the autoregressive decoder\u2019s hidden states to be predictive of the contrastive embedding (e.g., via mutual information or distillation losses), aiming to unify retrieval strength (contrastive) with fine-grained understanding (autoregressive). Evaluate on zero-shot retrieval/classification (contrastive strength) and phrase grounding/localization (autoregressive strength).",
        "Multimodal Autoregressive Pre-training with OCR-Aware Text Tokenization\nExtend multimodal decoding to include explicit \u201cscene text\u201d token streams predicted from image patches (separate from caption tokens), using OCR-like pseudo-labels from a teacher OCR system on web data. Train the vision encoder to support reading-centric tasks without task-specific fine-tuning by forcing patch representations to support character/word generation. Benchmark on TextVQA, DocVQA, ICDAR, and multimodal chat requiring faithful reading.",
        "Uncertainty-Calibrated Patch Generation to Improve Robustness and OOD Detection\nUse autoregressive patch likelihoods from the multimodal decoder as a calibrated signal of visual uncertainty, aggregating token-level negative log-likelihood into spatial uncertainty maps. Train with auxiliary calibration losses (temperature scaling per layer, or distributionally robust objectives) so likelihood correlates with errors under corruptions and domain shifts. Validate on ImageNet-C, OOD detection benchmarks, and safety-critical settings (e.g., \u201crefuse to answer\u201d in multimodal chat when patch likelihood is low)."
      ],
      "generated_ideas_raw": "1. **Joint Autoregressive + Masked Reconstruction Pre-training for Vision Encoders**\n   Combine AIMV2-style multimodal autoregressive decoding (image patches + text tokens) with an MAE-style high-mask-rate reconstruction head on the same ViT encoder. Train with a schedule that alternates (or mixes) objectives to test whether autoregressive token prediction improves MAE transfer on localization/grounding while MAE stabilizes and accelerates autoregressive convergence. Evaluate on frozen-trunk ImageNet, RefCOCO grounding, and dense tasks (ADE20K/COCO panoptic) to quantify complementary gains.\n\n2. **Curriculum Masking for Multimodal Autoregressive Patch Generation**\n   Extend AIMV2 by replacing fixed patch ordering with a learned curriculum: start with easy-to-predict low-frequency patches (or large patch sizes) and gradually move to finer patches/high-frequency regions, while text tokens are generated conditionally throughout. Implement a masking-and-ordering policy network that selects the next patch region to decode to minimize uncertainty. Measure improvements in data efficiency and robustness under distribution shift compared with static raster-scan decoding.\n\n3. **Token-Budget Adaptive Vision Encoders for LLM Alignment**\n   Inspired by InternVL\u2019s progressive alignment, train a ViT encoder that can dynamically output variable numbers of visual tokens based on an instruction/query (e.g., \u201ccount cars\u201d vs \u201cread sign\u201d). Use a lightweight controller that predicts a token budget and performs patch merging/pruning before cross-attention into the LLM. Benchmark accuracy\u2013latency trade-offs on multimodal chat, VQA, OCR-centric tasks, and dense grounding where token sparsity is critical.\n\n4. **Patch-Level Contrastive\u2013Autoregressive Hybrid Alignment (CLIP/SigLIP + AIMV2)**\n   Build a single model that learns global image\u2013text alignment via contrastive loss while simultaneously training an autoregressive multimodal decoder to generate patches/tokens. Constrain the autoregressive decoder\u2019s hidden states to be predictive of the contrastive embedding (e.g., via mutual information or distillation losses), aiming to unify retrieval strength (contrastive) with fine-grained understanding (autoregressive). Evaluate on zero-shot retrieval/classification (contrastive strength) and phrase grounding/localization (autoregressive strength).\n\n5. **Residualized ViT Blocks for Extremely Deep Autoregressive Vision Pre-training**\n   Bring explicit ResNet-style residual design analysis into very deep ViT encoders trained with autoregressive multimodal objectives (AIMV2) to test stability at depth (e.g., 100+ transformer blocks). Experiment with residual scaling, gated residual pathways, and pre-norm variants to reduce training instabilities observed in long-horizon autoregressive decoding. Report scaling laws (depth vs compute vs downstream transfer) and identify configurations that enable deeper encoders without degradation.\n\n6. **Region-Aware Autoregressive Decoding for Grounding and Localization**\n   Modify AIMV2\u2019s patch generation to decode object-centric regions first by integrating a coarse region proposal module (or unsupervised slot attention) that defines patch groups. Autoregressively generate grouped patches and text tokens conditioned on region identities, encouraging representations aligned with object boundaries. Evaluate improvements on RefCOCO/Visual Genome grounding, DETR-style detection transfer, and segmentation, especially under frozen-trunk settings.\n\n7. **Multimodal Autoregressive Pre-training with OCR-Aware Text Tokenization**\n   Extend multimodal decoding to include explicit \u201cscene text\u201d token streams predicted from image patches (separate from caption tokens), using OCR-like pseudo-labels from a teacher OCR system on web data. Train the vision encoder to support reading-centric tasks without task-specific fine-tuning by forcing patch representations to support character/word generation. Benchmark on TextVQA, DocVQA, ICDAR, and multimodal chat requiring faithful reading.\n\n8. **Progressive Alignment: Encoder-First, Cross-Attention Later, Full Multimodal Decoder Last**\n   Systematically ablate alignment stages (as in InternVL\u2019s progressive strategy) for AIMV2-like autoregressive training: (i) unimodal MAE on images, (ii) add text-conditioned cross-attention with frozen LLM, (iii) finally train full multimodal autoregressive decoder. The contribution is a reproducible recipe showing which stages are necessary for strong frozen-trunk vision performance versus strong multimodal dialogue. Provide guidance on when to invest compute in alignment vs pure vision pre-training depending on target tasks.\n\n9. **Uncertainty-Calibrated Patch Generation to Improve Robustness and OOD Detection**\n   Use autoregressive patch likelihoods from the multimodal decoder as a calibrated signal of visual uncertainty, aggregating token-level negative log-likelihood into spatial uncertainty maps. Train with auxiliary calibration losses (temperature scaling per layer, or distributionally robust objectives) so likelihood correlates with errors under corruptions and domain shifts. Validate on ImageNet-C, OOD detection benchmarks, and safety-critical settings (e.g., \u201crefuse to answer\u201d in multimodal chat when patch likelihood is low).\n\n10. **Unified Image\u2013Video Autoregressive Pre-training via 3D Patch Tokens**\n   Extend AIMV2\u2019s patch-token generation from 2D images to short video clips using spatiotemporal patch tokens (tubelets) while keeping text token generation in the same decoder. Compare designs: factorized temporal decoding vs full 3D autoregression, and integrate MAE-style high masking in time to control compute. Evaluate on zero-shot video classification/retrieval and video grounding, testing whether the same encoder can serve both image and video tasks without separate backbones.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Token-Budget Adaptive Vision Encoders for LLM Alignment\nInspired by InternVL\u2019s progressive alignment, train a ViT encoder that can dynamically output variable numbers of visual tokens based on an inst",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Patch-Level Contrastive\u2013Autoregressive Hybrid Alignment (CLIP/SigLIP + AIMV2)\nBuild a single model that learns global image\u2013text alignment via contrastive loss while simultaneously training an autoreg",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Multimodal Autoregressive Pre-training with OCR-Aware Text Tokenization\nExtend multimodal decoding to include explicit \u201cscene text\u201d token streams predicted from image patches (separate from caption to",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Uncertainty-Calibrated Patch Generation to Improve Robustness and OOD Detection\nUse autoregressive patch likelihoods from the multimodal decoder as a calibrated signal of visual uncertainty, aggregati",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 75,
      "paper_title": "PlayerOne: Egocentric World Simulator",
      "contribution": "PlayerOne is the first egocentric realistic world simulator that allows for immersive exploration with real-time human motion captured.",
      "num_predecessors": 5,
      "predecessors_crawled": 5,
      "academic_sources": 4,
      "crawl_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 11062,
      "output_tokens": 985,
      "predecessor_details": [
        {
          "success": true,
          "title": "NVIDIA Cosmos - Physical AI with World Foundation Models",
          "url": "https://www.nvidia.com/en-us/ai/cosmos/",
          "content": "Physical AI with World Foundation Models | NVIDIA Cosmos\nNVIDIA HomeNVIDIA HomeMenuMenu iconMenuMenu iconCloseClose iconCloseClose iconCloseClose iconCaret down iconAccordion is closed, click to open.Caret down iconAccordion is closed, click to open.Caret up iconAccordion is open, click to close.Caret right iconClick to expandCaret right iconClick to expandCaret right iconClick to expand menu.Caret left iconClick to collapse menu.Caret left iconClick to collapse menu.Caret left iconClick to collapse menu.Shopping CartClick to see cart itemsSearch iconClick to search\n**Physical AI**\n# NVIDIA Cosmos\nDevelop world foundation models to advance physical AI.\n[Download from GitHub](https://github.com/nvidia-cosmos)\n[Cookbook](https://nvidia-cosmos.github.io/cosmos-cookbook/) | [Documentation](https://docs.nvidia.com/cosmos/) | [Forum](https://docs.nvidia.com/cosmos/latest/community.html)\n* [Overview](#overview)\n* [Models](#models)\n* [Use Cases](#use-cases)\n* [Ecosystem](#ecosystem)\n* [Next Steps](#next-steps)\n* [Resources](#resources)\n* [FAQs](#faqs)\n* [Overview](#overview)\n* [Models](#models)\n* [Use Cases](#use-cases)\n* [Ecosystem](#ecosystem)\n* [Next Steps](#next-steps)\n* [Resources](#resources)\n* [FAQs](#faqs)\n* [Overview](#overview)\n* [Models](#models)\n* [Use Cases](#use-cases)\n* [Ecosystem](#ecosystem)\n* [Next Steps](#next-steps)\n* [Resources](#resources)\n* [FAQs](#faqs)\n[Download Now](https://github.com/nvidia-cosmos)\nOverview\n## What Is NVIDIA Cosmos?\nNVIDIA Cosmos\u2122 is a platform purpose-built for physical AI, featuring state-of-the-art generative[world foundation models](https://www.nvidia.com/en-us/glossary/world-models/)(WFMs), guardrails, and an accelerated data processing and curation pipeline. Developers use Cosmos to accelerate the development of[physical AI](https://www.nvidia.com/en-us/glossary/generative-physical-ai/)for[autonomous vehicles](https://www.nvidia.com/en-us/use-cases/autonomous-vehicle-simulation/)(AVs),[robots,](https://www.nvidia.com/en-us/use-cases/robotics-simulation/)and[video analytics AI agents.](https://www.nvidia.com/en-us/use-cases/video-analytics-ai-agents/)\n### Customizing NVIDIA Cosmos for Any Physical AI Use Case\nThe Cosmos Cookbook offers a comprehensive guide for physical AI developers to learn, use, and post-train NVIDIA's Cosmos-based models, including new applications like LidarGen, Cosmos Policy, and more.\n[Read Blog](https://blogs.nvidia.com/blog/neurips-open-source-digital-physical-ai/)\n### How to Scale Data Generation for Physical AI with the NVIDIA Cosmos Cookbook\nIn this blog, we\u2019ll sample Cosmos Transfer recipes to change video backgrounds, add new environmental conditions to driving data, generate data for robotics navigation, and generate synthetic data for urban traffic scenarios.\n[Read Blog](https://developer.nvidia.com/blog/how-to-scale-data-generation-for-physical-ai-with-the-nvidia-cosmos-cookbook/)\nOpen Models\n## Cosmos Models for Physical AI\nPretrained multimodal generative models that developers can use out-of-the-box for world generation or reasoning, or post-train to develop physical AI models.\n### Cosmos Predict\nA state-of-the-art world state prediction model that can generate up to 30 seconds of continuous video from multimodal inputs with superior speed, fidelity, and prompt adherence. Unlock advanced forecasting and scenario planning for robotics and AI agents by predicting future states of dynamic environments.\n[Get Started on GitHub](https://github.com/nvidia-cosmos/cosmos-predict2.5)\n### Cosmos Transfer\nMulticontrol model scales a single simulation or spatial video quickly across various environments and lighting conditions. Accelerate 3D inputs from physical AI simulation frameworks, like CARLA or[NVIDIA Isaac Sim\u2122,](https://developer.nvidia.com/isaac/sim)to enable fully controllable data augmentation and[synthetic data generation](https://www.nvidia.com/en-us/glossary/synthetic-data-generation/)pipelines.\n[Get Started on GitHub](https://github.com/nvidia-cosmos/cosmos-transfer2.5)\n### Cosmos Reason\nOpen, customizable, reasoning vision language model (VLM) for physical AI lets robots and vision AI agents reason like humans. It can utilize prior knowledge, physics understanding, and common sense to comprehend the real world and how to interact with it.\n[Get Started on GitHub](https://github.com/nvidia-cosmos/cosmos-reason1)\n## Data Processing\nNVIDIA Cosmos Curator is a framework enabling developers to quickly filter, annotate, and deduplicate large amounts of sensor data necessary for physical AI development, creating tailored datasets to meet model needs. With NVIDIA Cosmos Dataset Search (CDS), developers can instantly query these datasets and retrieve scenarios for targeted post-training.\nSpeed up efficient dataset processing and generation.\n[Download Cosmos Curator](https://github.com/nvidia-cosmos/cosmos-curate)\n[Try Cosmos Dataset Search](https://build.nvidia.com/nvidia/cosmos-dataset-search)\nUse Cases\n## How Cosmos Accelerates AI Across Industries\nUse Cosmos WFMs to simulate, reason, and generate data for downstream pipelines in robotics, autonomous vehicles, and industrial vision systems.\n1. Robot Learning\n2. Autonomous Vehicle Training\n3. Video Analytics AI Agents\n### Robot Learning\nRobots need vast, diverse training data to effectively perceive and interact with their environments. With Cosmos WFMs, developers can generate controllable, high-fidelity synthetic data to train robot perception and policy models.\n[Learn More](https://www.nvidia.com/en-us/use-cases/robot-learning/)\n### Autonomous Vehicle Training\nDiverse, high-fidelity sensor data is critical for safely training, testing, and validating autonomous vehicles. With Cosmos WFMs post-trained on vehicle data, developers can amplify existing data diversity with new weather, lighting, and geolocations, or expand into multi-sensor views\u2014saving significant time and cost.\n[Learn More](https://www.nvidia.com/en-us/solutions/autonomous-vehicles/)\n### Video Analytics AI Agents\nThese AI agents can analyze, summarize, and interact with real-time or recorded video streams to enhance automation, safety, and operational efficiency across industrial and urban environments. Cosmos Reason powers video analytics AI agents to deliver real-time question-answering, rapid alerts, and rich contextual insights\u2014powering smarter, more responsive systems across edge and cloud deployments.\n[Learn More](https://www.nvidia.com/en-us/use-cases/video-analytics-ai-agents/)\nTrustworthy AI\n## Supporting the Physical AI Community\nCosmos models, guardrails, and tokenizers are available on Hugging Face and GitHub, with resources to tackle data scarcity in training physical AI models.\n[](https://github.com/nvidia-cosmos)\n[](https://huggingface.co/nvidia)\nAI Infrastructure\n## Get the Best Performance With NVIDIA Blackwell\nNVIDIA RTX PRO 6000 Blackwell Series Servers accelerate physical AI development for robots, autonomous vehicles, and AI agents across training, synthetic data generation, simulation, and inference.\nUnlock peak performance for Cosmos world foundation models on NVIDIA Blackwell GB200 for industrial post-training and inference workloads.\n[Learn More](https://www.nvidia.com/en-us/data-center/rtx-pro-6000-blackwell-server-edition/)\nEcosystem\n## Adopted by Leading Physical AI Innovators\nModel developers from the robotics, autonomous vehicles, and vision AI industries are using Cosmos to accelerate physical AI development.\n## Next Steps\n### Ready to Get Started?\nTest-drive a world foundation model in the NVIDIA API catalog or start building your own world models using Cosmos.\n[Try Now](https://build.nvidia.com/search?q=\"cosmos\"+-nemotron)[Start Developing](https://developer.nvidia.com/cosmos)\n#### Cosmos Cookbook\nA comprehensive guide for working with the NVIDIA Cosmos ecosystem for real-world, domain-specific applications across robotics, simulation, autonomous systems, and physical scene understanding.\n[Learn More](https://nvidia-cosmos.gith",
          "original_query": "Cosmos world foundation model platform for physical ai",
          "cleaned_query": "Cosmos world foundation model platform for physical ai",
          "note": "Non-academic source"
        },
        {
          "success": true,
          "title": "DayDreamer: World Models for Physical Robot Learning - arXiv",
          "url": "https://arxiv.org/abs/2206.14176",
          "content": "[2206.14176] DayDreamer: World Models for Physical Robot Learning\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2206.14176\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Robotics\n**arXiv:2206.14176**(cs)\n[Submitted on 28 Jun 2022]\n# Title:DayDreamer: World Models for Physical Robot Learning\nAuthors:[Philipp Wu](https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+P),[Alejandro Escontrela](https://arxiv.org/search/cs?searchtype=author&amp;query=Escontrela,+A),[Danijar Hafner](https://arxiv.org/search/cs?searchtype=author&amp;query=Hafner,+D),[Ken Goldberg](https://arxiv.org/search/cs?searchtype=author&amp;query=Goldberg,+K),[Pieter Abbeel](https://arxiv.org/search/cs?searchtype=author&amp;query=Abbeel,+P)\nView a PDF of the paper titled DayDreamer: World Models for Physical Robot Learning, by Philipp Wu and 4 other authors\n[View PDF](https://arxiv.org/pdf/2206.14176)> > Abstract:\n> To solve tasks in complex environments, robots need to learn from experience. Deep reinforcement learning is a common approach to robot learning but requires a large amount of trial and error to learn, limiting its deployment in the physical world. As a consequence, many advances in robot learning rely on simulators. On the other hand, learning inside of simulators fails to capture the complexity of the real world, is prone to simulator inaccuracies, and the resulting behaviors do not adapt to changes in the world. The Dreamer algorithm has recently shown great promise for learning from small amounts of interaction by planning within a learned world model, outperforming pure reinforcement learning in video games. Learning a world model to predict the outcomes of potential actions enables planning in imagination, reducing the amount of trial and error needed in the real environment. However, it is unknown whether Dreamer can facilitate faster learning on physical robots. In this paper, we apply Dreamer to 4 robots to learn online and directly in the real world, without simulators. Dreamer trains a quadruped robot to roll off its back, stand up, and walk from scratch and without resets in only 1 hour. We then push the robot and find that Dreamer adapts within 10 minutes to withstand perturbations or quickly roll over and stand back up. On two different robotic arms, Dreamer learns to pick and place multiple objects directly from camera images and sparse rewards, approaching human performance. On a wheeled robot, Dreamer learns to navigate to a goal position purely from camera images, automatically resolving ambiguity about the robot orientation. Using the same hyperparameters across all experiments, we find that Dreamer is capable of online learning in the real world, establishing a strong baseline. We release our infrastructure for future applications of world models to robot learning. Comments:|Website:[this https URL](https://danijar.com/daydreamer)|\nSubjects:|Robotics (cs.RO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)|\nCite as:|[arXiv:2206.14176](https://arxiv.org/abs/2206.14176)[cs.RO]|\n|(or[arXiv:2206.14176v1](https://arxiv.org/abs/2206.14176v1)[cs.RO]for this version)|\n|[https://doi.org/10.48550/arXiv.2206.14176](https://doi.org/10.48550/arXiv.2206.14176)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Danijar Hafner [[view email](https://arxiv.org/show-email/42b2c958/2206.14176)]\n**[v1]**Tue, 28 Jun 2022 17:44:48 UTC (2,791 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled DayDreamer: World Models for Physical Robot Learning, by Philipp Wu and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2206.14176)\n* [TeX Source](https://arxiv.org/src/2206.14176)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.RO\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2206.14176&amp;function=prev&amp;context=cs.RO) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2206.14176&amp;function=next&amp;context=cs.RO)\n[new](https://arxiv.org/list/cs.RO/new)|[recent](https://arxiv.org/list/cs.RO/recent)|[2022-06](https://arxiv.org/list/cs.RO/2022-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/2206.14176?context=cs)\n[cs.AI](https://arxiv.org/abs/2206.14176?context=cs.AI)\n[cs.LG](https://arxiv.org/abs/2206.14176?context=cs.LG)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2206.14176)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2206.14176)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2206.14176)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these",
          "original_query": "Daydreamer: World models for physical robot learning",
          "cleaned_query": "Daydreamer: World models for physical robot learning"
        },
        {
          "success": true,
          "title": "Ego-Exo4D: Understanding Skilled Human Activity from First",
          "url": "https://link.springer.com/article/10.1007/s11263-025-02557-6",
          "content": "Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives | International Journal of Computer Vision\n[Skip to main content](#main)\n[![Springer Nature Link](https://link.springer.com/oscar-static/images/darwin/header/img/logo-springer-nature-link-3149409f62.svg)](https://link.springer.com)\n[Log in](https://idp.springer.com/auth/personal/springernature?redirect_uri=https://link.springer.com/article/10.1007/s11263-025-02557-6?)\n# Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives\n* [Open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)\n* Published:24 November 2025\n* Volume\u00a0133,\u00a0pages 8356\u20138435, (2025)\n* [Cite this article](#citeas)\nYou have full access to this[open access](https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research)article\n[Download PDF](https://link.springer.com/content/pdf/10.1007/s11263-025-02557-6.pdf)\n[![](https://media.springernature.com/w72/springer-static/cover-hires/journal/11263?as=webp)International Journal of Computer Vision](https://link.springer.com/journal/11263)[Aims and scope](https://link.springer.com/journal/11263/aims-and-scope)[Submit manuscript](https://www.editorialmanager.com/visi)\nEgo-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives\n[Download PDF](https://link.springer.com/content/pdf/10.1007/s11263-025-02557-6.pdf)\n* [Kristen Grauman](#auth-Kristen-Grauman-Aff1)[ORCID:orcid.org/0000-0002-9591-5873](https://orcid.org/0000-0002-9591-5873)[1](#Aff1),\n* [Andrew Westbury](#auth-Andrew-Westbury-Aff2)[2](#Aff2),\n* [Lorenzo Torresani](#auth-Lorenzo-Torresani-Aff3)[3](#Aff3),\n* [Kris Kitani](#auth-Kris-Kitani-Aff2-Aff4)[2](#Aff2),[4](#Aff4),\n* [Jitendra Malik](#auth-Jitendra-Malik-Aff2-Aff5)[2](#Aff2),[5](#Aff5),\n* [Triantafyllos Afouras](#auth-Triantafyllos-Afouras-Aff2)[2](#Aff2),\n* [Kumar Ashutosh](#auth-Kumar-Ashutosh-Aff1-Aff2)[1](#Aff1),[2](#Aff2),\n* [Vijay Baiyya](#auth-Vijay-Baiyya-Aff6)[6](#Aff6),\n* [Siddhant Bansal](#auth-Siddhant-Bansal-Aff7-Aff8)[7](#Aff7),[8](#Aff8),\n* [Bikram Boote](#auth-Bikram-Boote-Aff9)[9](#Aff9),\n* [Eugene Byrne](#auth-Eugene-Byrne-Aff2)[2](#Aff2),\n* [Zach Chavis](#auth-Zach-Chavis-Aff11)[11](#Aff11),\n* [Joya Chen](#auth-Joya-Chen-Aff12)[12](#Aff12),\n* [Feng Cheng](#auth-Feng-Cheng-Aff2)[2](#Aff2),\n* [Fu-Jen Chu](#auth-Fu_Jen-Chu-Aff2)[2](#Aff2),\n* [Sean Crane](#auth-Sean-Crane-Aff10)[10](#Aff10),\n* [Avijit Dasgupta](#auth-Avijit-Dasgupta-Aff8)[8](#Aff8),\n* [Jing Dong](#auth-Jing-Dong-Aff6)[6](#Aff6),\n* [Maria Escobar](#auth-Maria-Escobar-Aff13)[13](#Aff13),\n* [Cristhian Forigua](#auth-Cristhian-Forigua-Aff13)[13](#Aff13),\n* [Abrham Gebreselasie](#auth-Abrham-Gebreselasie-Aff10)[10](#Aff10),\n* [Sanjay Haresh](#auth-Sanjay-Haresh-Aff14)[14](#Aff14),\n* [Jing Huang](#auth-Jing-Huang-Aff2)[2](#Aff2),\n* [Md Mohaiminul Islam](#auth-Md_Mohaiminul-Islam-Aff15)[15](#Aff15),\n* [Suyog Jain](#auth-Suyog-Jain-Aff2)[2](#Aff2),\n* [Rawal Khirodkar](#auth-Rawal-Khirodkar-Aff10)[10](#Aff10),\n* [Devansh Kukreja](#auth-Devansh-Kukreja-Aff2)[2](#Aff2),\n* [Kevin J. Liang](#auth-Kevin_J_-Liang-Aff2)[2](#Aff2),\n* [Jia-Wei Liu](#auth-Jia_Wei-Liu-Aff12)[12](#Aff12),\n* [Sagnik Majumder](#auth-Sagnik-Majumder-Aff1-Aff2)[1](#Aff1),[2](#Aff2),\n* [Yongsen Mao](#auth-Yongsen-Mao-Aff14)[14](#Aff14),\n* [Miguel Martin](#auth-Miguel-Martin-Aff2)[2](#Aff2),\n* [Effrosyni Mavroudi](#auth-Effrosyni-Mavroudi-Aff2)[2](#Aff2),\n* [Tushar Nagarajan](#auth-Tushar-Nagarajan-Aff2)[2](#Aff2),\n* [Francesco Ragusa](#auth-Francesco-Ragusa-Aff16)[16](#Aff16),\n* [Santhosh Kumar Ramakrishnan](#auth-Santhosh_Kumar-Ramakrishnan-Aff1)[1](#Aff1),\n* [Luigi Seminara](#auth-Luigi-Seminara-Aff16)[16](#Aff16),\n* [Arjun Somayazulu](#auth-Arjun-Somayazulu-Aff1)[1](#Aff1),\n* [Yale Song](#auth-Yale-Song-Aff2)[2](#Aff2),\n* [Shan Su](#auth-Shan-Su-Aff17)[17](#Aff17),\n* [Zihui Xue](#auth-Zihui-Xue-Aff1-Aff2)[1](#Aff1),[2](#Aff2),\n* [Edward Zhang](#auth-Edward-Zhang-Aff17)[17](#Aff17),\n* [Jinxu Zhang](#auth-Jinxu-Zhang-Aff17)[17](#Aff17),\n* [Angela Castillo](#auth-Angela-Castillo-Aff13)[13](#Aff13),\n* [Changan Chen](#auth-Changan-Chen-Aff1)[1](#Aff1),\n* [Xinzhu Fu](#auth-Xinzhu-Fu-Aff12)[12](#Aff12),\n* [Ryosuke Furuta](#auth-Ryosuke-Furuta-Aff18)[18](#Aff18),\n* [Cristina Gonz\u00e1lez](#auth-Cristina-Gonz_lez-Aff13)[13](#Aff13),\n* [Prince Gupta](#auth-Prince-Gupta-Aff6)[6](#Aff6),\n* [Jiabo Hu](#auth-Jiabo-Hu-Aff19)[19](#Aff19),\n* [Yifei Huang](#auth-Yifei-Huang-Aff18)[18](#Aff18),\n* [Yiming Huang](#auth-Yiming-Huang-Aff17)[17](#Aff17),\n* [Weslie Khoo](#auth-Weslie-Khoo-Aff20)[20](#Aff20),\n* [Anush Kumar](#auth-Anush-Kumar-Aff11)[11](#Aff11),\n* [Robert Kuo](#auth-Robert-Kuo-Aff19)[19](#Aff19),\n* [Sach Lakhavani](#auth-Sach-Lakhavani-Aff6)[6](#Aff6),\n* [Miao Liu](#auth-Miao-Liu-Aff19)[19](#Aff19),\n* [Mi Luo](#auth-Mi-Luo-Aff1)[1](#Aff1),\n* [Zhengyi Luo](#auth-Zhengyi-Luo-Aff4)[4](#Aff4),\n* [Brighid Meredith](#auth-Brighid-Meredith-Aff19)[19](#Aff19),\n* [Austin Miller](#auth-Austin-Miller-Aff19)[19](#Aff19),\n* [Oluwatumininu Oguntola](#auth-Oluwatumininu-Oguntola-Aff15)[15](#Aff15),\n* [Xiaqing Pan](#auth-Xiaqing-Pan-Aff6)[6](#Aff6),\n* [Penny Peng](#auth-Penny-Peng-Aff19)[19](#Aff19),\n* [Shraman Pramanick](#auth-Shraman-Pramanick-Aff21)[21](#Aff21),\n* [Merey Ramazanova](#auth-Merey-Ramazanova-Aff22)[22](#Aff22),\n* [Fiona Ryan](#auth-Fiona-Ryan-Aff23)[23](#Aff23),\n* [Wei Shan](#auth-Wei-Shan-Aff15)[15](#Aff15),\n* [Kiran Somasundaram](#auth-Kiran-Somasundaram-Aff6)[6](#Aff6),\n* [Chenan Song](#auth-Chenan-Song-Aff12)[12](#Aff12),\n* [Audrey Southerland](#auth-Audrey-Southerland-Aff23)[23](#Aff23),\n* [Masatoshi Tateno](#auth-Masatoshi-Tateno-Aff18)[18](#Aff18),\n* [Huiyu Wang](#auth-Huiyu-Wang-Aff2)[2](#Aff2),\n* [Yuchen Wang](#auth-Yuchen-Wang-Aff20)[20](#Aff20),\n* [Takuma Yagi](#auth-Takuma-Yagi-Aff18)[18](#Aff18),\n* [Mingfei Yan](#auth-Mingfei-Yan-Aff6)[6](#Aff6),\n* [Xitong Yang](#auth-Xitong-Yang-Aff2)[2](#Aff2),\n* [Zecheng Yu](#auth-Zecheng-Yu-Aff18)[18](#Aff18),\n* [Shengxin Cindy Zha](#auth-Shengxin_Cindy-Zha-Aff19)[19](#Aff19),\n* [Chen Zhao](#auth-Chen-Zhao-Aff22)[22](#Aff22),\n* [Ziwei Zhao](#auth-Ziwei-Zhao-Aff20)[20](#Aff20),\n* [Zhifan Zhu](#auth-Zhifan-Zhu-Aff7)[7](#Aff7),\n* [Jeff Zhuo](#auth-Jeff-Zhuo-Aff15)[15](#Aff15),\n* [Pablo Arbel\u00e1ez](#auth-Pablo-Arbel_ez-Aff13)[13](#Aff13),\n* [Gedas Bertasius](#auth-Gedas-Bertasius-Aff15)[15](#Aff15),\n* [David Crandall](#auth-David-Crandall-Aff20)[20](#Aff20),\n* [Dima Damen](#auth-Dima-Damen-Aff7)[7](#Aff7),\n* [Jakob Engel](#auth-Jakob-Engel-Aff6)[6](#Aff6),\n* [Giovanni Maria Farinella](#auth-Giovanni_Maria-Farinella-Aff16)[16](#Aff16),\n* [Antonino Furnari](#auth-Antonino-Furnari-Aff16)[16](#Aff16),\n* [Bernard Ghanem](#auth-Bernard-Ghanem-Aff22)[22](#Aff22),\n* [Judy Hoffman](#auth-Judy-Hoffman-Aff23)[23](#Aff23),\n* [C. V. Jawahar](#auth-C__V_-Jawahar-Aff8)[8](#Aff8),\n* [Richard Newcombe](#auth-Richard-Newcombe-Aff6)[6](#Aff6),\n* [Hyun Soo Park](#auth-Hyun_Soo-Park-Aff11)[11](#Aff11),\n* [James M. Rehg](#auth-James_M_-Rehg-Aff9)[9](#Aff9),\n* [Yoichi Sato](#auth-Yoichi-Sato-Aff18)[18](#Aff18),\n* [Manolis Savva](#auth-Manolis-Savva-Aff14)[14](#Aff14),\n* [Jianbo Shi](#auth-Jianbo-Shi-Aff17)[17](#Aff17),\n* [Mike Zheng Shou](#auth-Mike_Zheng-Shou-Aff12)[12](#Aff12)&amp;\n* \u2026* [Michael Wray](#auth-Michael-Wray-Aff7)[7](#Aff7)Show authors\n* 1166Accesses\n* 12Altmetric\n* [Explore all metrics](https://link.springer.com/article/10.1007/s11263-025-02557-6/metrics)\n## Abstract\nWe present Ego-Exo4D, a diverse, large-scale multimodal multiview video dataset and benchmark challenge. Ego-Exo4D centers around simultaneously-captured egocentric and exocentric video of skilled human activities (e.g., sports, music, dance, bike repair). 740 participants from 13\u00a0cities worldwide performed these activities in 123\u00a0different natural scene contexts, yielding long-form captures from 1 to 42 minutes each and 1,286\u00a0hours of video combined. ",
          "original_query": "Understanding skilled human activity from first-and third-person perspectives",
          "cleaned_query": "Understanding skilled human activity from first-and third-person perspectives"
        },
        {
          "success": true,
          "title": "Dream to Control: Learning Behaviors by Latent Imagination - arXiv",
          "url": "https://arxiv.org/abs/1912.01603",
          "content": "[1912.01603] Dream to Control: Learning Behaviors by Latent Imagination\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nIn just 5 minutes help us improve arXiv:\n[Annual Global Survey](https://cornell.ca1.qualtrics.com/jfe/form/SV_6kZEJCkEgp3yGZo)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1912.01603\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1912.01603**(cs)\n[Submitted on 3 Dec 2019 ([v1](https://arxiv.org/abs/1912.01603v1)), last revised 17 Mar 2020 (this version, v3)]\n# Title:Dream to Control: Learning Behaviors by Latent Imagination\nAuthors:[Danijar Hafner](https://arxiv.org/search/cs?searchtype=author&amp;query=Hafner,+D),[Timothy Lillicrap](https://arxiv.org/search/cs?searchtype=author&amp;query=Lillicrap,+T),[Jimmy Ba](https://arxiv.org/search/cs?searchtype=author&amp;query=Ba,+J),[Mohammad Norouzi](https://arxiv.org/search/cs?searchtype=author&amp;query=Norouzi,+M)\nView a PDF of the paper titled Dream to Control: Learning Behaviors by Latent Imagination, by Danijar Hafner and 3 other authors\n[View PDF](https://arxiv.org/pdf/1912.01603)> > Abstract:\n> Learned world models summarize an agent&#39;s experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance. Comments:|9 pages, 12 figures|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Robotics (cs.RO)|\nCite as:|[arXiv:1912.01603](https://arxiv.org/abs/1912.01603)[cs.LG]|\n|(or[arXiv:1912.01603v3](https://arxiv.org/abs/1912.01603v3)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1912.01603](https://doi.org/10.48550/arXiv.1912.01603)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Danijar Hafner [[view email](https://arxiv.org/show-email/21d2e007/1912.01603)]\n**[[v1]](https://arxiv.org/abs/1912.01603v1)**Tue, 3 Dec 2019 18:57:16 UTC (1,712 KB)\n**[[v2]](https://arxiv.org/abs/1912.01603v2)**Fri, 14 Feb 2020 17:07:58 UTC (1,724 KB)\n**[v3]**Tue, 17 Mar 2020 17:10:58 UTC (1,743 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Dream to Control: Learning Behaviors by Latent Imagination, by Danijar Hafner and 3 other authors\n* [View PDF](https://arxiv.org/pdf/1912.01603)\n* [TeX Source](https://arxiv.org/src/1912.01603)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1912.01603&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1912.01603&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2019-12](https://arxiv.org/list/cs.LG/2019-12)\nChange to browse by:\n[cs](https://arxiv.org/abs/1912.01603?context=cs)\n[cs.AI](https://arxiv.org/abs/1912.01603?context=cs.AI)\n[cs.RO](https://arxiv.org/abs/1912.01603?context=cs.RO)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1912.01603)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1912.01603)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1912.01603)\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1912.html#abs-1912-01603)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1912-01603)\n[Danijar Hafner]()\n[Timothy P. Lillicrap]()\n[Jimmy Ba]()\n[Mohammad Norouzi]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/1912.01603)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Dream to control: Learning behaviors by latent imagination",
          "cleaned_query": "Dream to control: Learning behaviors by latent imagination"
        },
        {
          "success": true,
          "title": "[2501.00103] LTX-Video: Realtime Video Latent Diffusion - arXiv",
          "url": "https://arxiv.org/abs/2501.00103",
          "content": "[2501.00103] LTX-Video: Realtime Video Latent Diffusion\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2501.00103\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Computer Vision and Pattern Recognition\n**arXiv:2501.00103**(cs)\n[Submitted on 30 Dec 2024]\n# Title:LTX-Video: Realtime Video Latent Diffusion\nAuthors:[Yoav HaCohen](https://arxiv.org/search/cs?searchtype=author&amp;query=HaCohen,+Y),[Nisan Chiprut](https://arxiv.org/search/cs?searchtype=author&amp;query=Chiprut,+N),[Benny Brazowski](https://arxiv.org/search/cs?searchtype=author&amp;query=Brazowski,+B),[Daniel Shalem](https://arxiv.org/search/cs?searchtype=author&amp;query=Shalem,+D),[Dudu Moshe](https://arxiv.org/search/cs?searchtype=author&amp;query=Moshe,+D),[Eitan Richardson](https://arxiv.org/search/cs?searchtype=author&amp;query=Richardson,+E),[Eran Levin](https://arxiv.org/search/cs?searchtype=author&amp;query=Levin,+E),[Guy Shiran](https://arxiv.org/search/cs?searchtype=author&amp;query=Shiran,+G),[Nir Zabari](https://arxiv.org/search/cs?searchtype=author&amp;query=Zabari,+N),[Ori Gordon](https://arxiv.org/search/cs?searchtype=author&amp;query=Gordon,+O),[Poriya Panet](https://arxiv.org/search/cs?searchtype=author&amp;query=Panet,+P),[Sapir Weissbuch](https://arxiv.org/search/cs?searchtype=author&amp;query=Weissbuch,+S),[Victor Kulikov](https://arxiv.org/search/cs?searchtype=author&amp;query=Kulikov,+V),[Yaki Bitterman](https://arxiv.org/search/cs?searchtype=author&amp;query=Bitterman,+Y),[Zeev Melumian](https://arxiv.org/search/cs?searchtype=author&amp;query=Melumian,+Z),[Ofir Bibi](https://arxiv.org/search/cs?searchtype=author&amp;query=Bibi,+O)\nView a PDF of the paper titled LTX-Video: Realtime Video Latent Diffusion, by Yoav HaCohen and 15 other authors\n[View PDF](https://arxiv.org/pdf/2501.00103)[HTML (experimental)](https://arxiv.org/html/2501.00103v1)> > Abstract:\n> We introduce LTX-Video, a transformer-based latent diffusion model that adopts a holistic approach to video generation by seamlessly integrating the responsibilities of the Video-VAE and the denoising transformer. Unlike existing methods, which treat these components as independent, LTX-Video aims to optimize their interaction for improved efficiency and quality. At its core is a carefully designed Video-VAE that achieves a high compression ratio of 1:192, with spatiotemporal downscaling of 32 x 32 x 8 pixels per token, enabled by relocating the patchifying operation from the transformer&#39;s input to the VAE&#39;s input. Operating in this highly compressed latent space enables the transformer to efficiently perform full spatiotemporal self-attention, which is essential for generating high-resolution videos with temporal consistency. However, the high compression inherently limits the representation of fine details. To address this, our VAE decoder is tasked with both latent-to-pixel conversion and the final denoising step, producing the clean result directly in pixel space. This approach preserves the ability to generate fine details without incurring the runtime cost of a separate upsampling module. Our model supports diverse use cases, including text-to-video and image-to-video generation, with both capabilities trained simultaneously. It achieves faster-than-real-time generation, producing 5 seconds of 24 fps video at 768x512 resolution in just 2 seconds on an Nvidia H100 GPU, outperforming all existing models of similar scale. The source code and pre-trained models are publicly available, setting a new benchmark for accessible and scalable video generation. Subjects:|Computer Vision and Pattern Recognition (cs.CV)|\nCite as:|[arXiv:2501.00103](https://arxiv.org/abs/2501.00103)[cs.CV]|\n|(or[arXiv:2501.00103v1](https://arxiv.org/abs/2501.00103v1)[cs.CV]for this version)|\n|[https://doi.org/10.48550/arXiv.2501.00103](https://doi.org/10.48550/arXiv.2501.00103)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Eitan Richardson [[view email](https://arxiv.org/show-email/a5d83eb5/2501.00103)]\n**[v1]**Mon, 30 Dec 2024 19:00:25 UTC (30,742 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled LTX-Video: Realtime Video Latent Diffusion, by Yoav HaCohen and 15 other authors\n* [View PDF](https://arxiv.org/pdf/2501.00103)\n* [HTML (experimental)](https://arxiv.org/html/2501.00103v1)\n* [TeX Source](https://arxiv.org/src/2501.00103)\n[![license icon](https://arxiv.org/icons/licenses/by-4.0.png)view license](http://creativecommons.org/licenses/by/4.0/)\nCurrent browse context:\ncs.CV\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2501.00103&amp;function=prev&amp;context=cs.CV) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2501.00103&amp;function=next&amp;context=cs.CV)\n[new](https://arxiv.org/list/cs.CV/new)|[recent](https://arxiv.org/list/cs.CV/recent)|[2025-01](https://arxiv.org/list/cs.CV/2025-01)\nChange to browse by:\n[cs](https://arxiv.org/abs/2501.00103?context=cs)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2501.00103)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2501.00103)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2501.00103)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence ",
          "original_query": "Realtime video latent diffusion",
          "cleaned_query": "Realtime video latent diffusion"
        }
      ],
      "generated_ideas": [
        "Realtime Latent-Imagination Control with Diffusion WFMs\nIntegrate a realtime latent diffusion video generator (in the style of LTX-Video) as the predictive component inside a Dreamer/DayDreamer agent, enabling high-fidelity imagined rollouts under tight compute budgets. Evaluate whether diffusion-based imagination improves long-horizon planning on vision-only manipulation and locomotion tasks, compared to RSSM-style dynamics, at equal wall-clock time.",
        "Action-Conditioned Cosmos Predict for Closed-Loop Robot Planning\nPost-train Cosmos Predict to be explicitly action-conditioned (robot proprio + commands \u2192 future video/multisensor trajectories), then plug it into Dreamer-style latent planning. The key contribution is a unified pipeline from world foundation model forecasting to policy optimization that can be deployed online on real robots and ablated for forecast horizon, control frequency, and sensor modalities.",
        "Counterfactual Data Augmentation for Policy Robustness via Cosmos Transfer\nUse Cosmos Transfer to generate structured counterfactual training sets (lighting/weather/background, distractors, object textures) while keeping task-relevant geometry consistent, then train/finetune DayDreamer policies on the augmented stream. The contribution is a principled recipe for \u201crobustness per synthetic dollar\u201d that measures gains in OOD generalization and sim-to-real transfer without changing the RL algorithm.",
        "Uncertainty-Calibrated Imagination for Safe Real-World Online Learning\nAdd calibrated uncertainty estimates to world-model rollouts (e.g., ensemble/MC dropout over Cosmos Predict or Dreamer latent dynamics) and use them to gate planning depth and real-world action execution. Demonstrate reduced catastrophic failures during DayDreamer-style online learning by switching to conservative behaviors when imagined futures have high epistemic uncertainty.",
        "Dataset Search\u2013Driven Curriculum Generation for World-Model Post-Training\nUse Cosmos Dataset Search to automatically retrieve and sequence scenarios that maximize world-model learning progress (e.g., novelty/uncertainty-driven curriculum over rare events like slips, drops, near-collisions). The contribution is an end-to-end curriculum builder that measurably improves forecast accuracy on tail events and accelerates downstream RL adaptation compared to random or chronological training.",
        "Physics-Consistent Video Forecasting via Differentiable Constraints in WFMs\nAugment Cosmos Predict/LTX-style generators with lightweight differentiable constraints (e.g., contact consistency, approximate rigid motion, no interpenetration) applied in latent space during training or sampling. Evaluate whether constrained generation yields more reliable imagined rollouts for contact-rich control, improving Dreamer planning stability and reducing \u201cphysically impossible\u201d hallucinations.",
        "Cross-Modal World Models: Joint Video\u2013LiDAR\u2013Proprio Prediction for AV/Robotics\nExtend the WFM to predict synchronized futures across camera video, LiDAR/range images, and robot/vehicle state, using Cosmos tooling for curation and multi-sensor augmentation. The key contribution is demonstrating that multi-sensor imagination enables better planning under adverse conditions (night/rain, motion blur) and improves failure recovery when a modality degrades.",
        "Language-Guided Imagination Editing for Task-Specific Data Generation\nCombine Cosmos Reason (VLM reasoning) with Cosmos Transfer/Predict to translate natural-language scenario specs (e.g., \u201cadd a pedestrian entering from the right,\u201d \u201cobject is slippery,\u201d \u201cglare on camera\u201d) into controllable synthetic rollouts used for policy training and evaluation. Contribute a benchmark and pipeline showing that language-specified edge cases systematically improve real-world robustness and enable targeted regression testing for physical AI agents."
      ],
      "generated_ideas_raw": "1. **Realtime Latent-Imagination Control with Diffusion WFMs**  \n   Integrate a realtime latent diffusion video generator (in the style of LTX-Video) as the predictive component inside a Dreamer/DayDreamer agent, enabling high-fidelity imagined rollouts under tight compute budgets. Evaluate whether diffusion-based imagination improves long-horizon planning on vision-only manipulation and locomotion tasks, compared to RSSM-style dynamics, at equal wall-clock time.\n\n2. **Action-Conditioned Cosmos Predict for Closed-Loop Robot Planning**  \n   Post-train Cosmos Predict to be explicitly action-conditioned (robot proprio + commands \u2192 future video/multisensor trajectories), then plug it into Dreamer-style latent planning. The key contribution is a unified pipeline from world foundation model forecasting to policy optimization that can be deployed online on real robots and ablated for forecast horizon, control frequency, and sensor modalities.\n\n3. **Ego-Exo Skill Priors as Latent Options for World-Model RL**  \n   Learn a library of \u201cskill tokens\u201d from Ego-Exo4D (e.g., hand-object interaction phases, subgoal transitions) and condition Dreamer\u2019s actor on these tokens as temporally extended actions (options). Test whether option-conditioned imagination reduces exploration burden and improves sparse-reward success on pick-place, tool use, and navigation tasks when only a small amount of robot data is available.\n\n4. **Counterfactual Data Augmentation for Policy Robustness via Cosmos Transfer**  \n   Use Cosmos Transfer to generate structured counterfactual training sets (lighting/weather/background, distractors, object textures) while keeping task-relevant geometry consistent, then train/finetune DayDreamer policies on the augmented stream. The contribution is a principled recipe for \u201crobustness per synthetic dollar\u201d that measures gains in OOD generalization and sim-to-real transfer without changing the RL algorithm.\n\n5. **Uncertainty-Calibrated Imagination for Safe Real-World Online Learning**  \n   Add calibrated uncertainty estimates to world-model rollouts (e.g., ensemble/MC dropout over Cosmos Predict or Dreamer latent dynamics) and use them to gate planning depth and real-world action execution. Demonstrate reduced catastrophic failures during DayDreamer-style online learning by switching to conservative behaviors when imagined futures have high epistemic uncertainty.\n\n6. **Multiview World Models that Bridge Ego-to-Exo for Better State Estimation**  \n   Train a world model that jointly predicts first-person (ego) and third-person (exo) views, leveraging Ego-Exo4D alignment signals and Cosmos-style multimodal generation. Show that planning in a shared latent space improves partial observability (occlusions, ambiguous orientations) in navigation and manipulation by \u201cimagining\u201d what an external observer would see.\n\n7. **Dataset Search\u2013Driven Curriculum Generation for World-Model Post-Training**  \n   Use Cosmos Dataset Search to automatically retrieve and sequence scenarios that maximize world-model learning progress (e.g., novelty/uncertainty-driven curriculum over rare events like slips, drops, near-collisions). The contribution is an end-to-end curriculum builder that measurably improves forecast accuracy on tail events and accelerates downstream RL adaptation compared to random or chronological training.\n\n8. **Physics-Consistent Video Forecasting via Differentiable Constraints in WFMs**  \n   Augment Cosmos Predict/LTX-style generators with lightweight differentiable constraints (e.g., contact consistency, approximate rigid motion, no interpenetration) applied in latent space during training or sampling. Evaluate whether constrained generation yields more reliable imagined rollouts for contact-rich control, improving Dreamer planning stability and reducing \u201cphysically impossible\u201d hallucinations.\n\n9. **Cross-Modal World Models: Joint Video\u2013LiDAR\u2013Proprio Prediction for AV/Robotics**  \n   Extend the WFM to predict synchronized futures across camera video, LiDAR/range images, and robot/vehicle state, using Cosmos tooling for curation and multi-sensor augmentation. The key contribution is demonstrating that multi-sensor imagination enables better planning under adverse conditions (night/rain, motion blur) and improves failure recovery when a modality degrades.\n\n10. **Language-Guided Imagination Editing for Task-Specific Data Generation**  \n   Combine Cosmos Reason (VLM reasoning) with Cosmos Transfer/Predict to translate natural-language scenario specs (e.g., \u201cadd a pedestrian entering from the right,\u201d \u201cobject is slippery,\u201d \u201cglare on camera\u201d) into controllable synthetic rollouts used for policy training and evaluation. Contribute a benchmark and pipeline showing that language-specified edge cases systematically improve real-world robustness and enable targeted regression testing for physical AI agents.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Realtime Latent-Imagination Control with Diffusion WFMs\nIntegrate a realtime latent diffusion video generator (in the style of LTX-Video) as the predictive component inside a Dreamer/DayDreamer agent,",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Action-Conditioned Cosmos Predict for Closed-Loop Robot Planning\nPost-train Cosmos Predict to be explicitly action-conditioned (robot proprio + commands \u2192 future video/multisensor trajectories), then ",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Counterfactual Data Augmentation for Policy Robustness via Cosmos Transfer\nUse Cosmos Transfer to generate structured counterfactual training sets (lighting/weather/background, distractors, object tex",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Uncertainty-Calibrated Imagination for Safe Real-World Online Learning\nAdd calibrated uncertainty estimates to world-model rollouts (e.g., ensemble/MC dropout over Cosmos Predict or Dreamer latent dyn",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Dataset Search\u2013Driven Curriculum Generation for World-Model Post-Training\nUse Cosmos Dataset Search to automatically retrieve and sequence scenarios that maximize world-model learning progress (e.g., ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Physics-Consistent Video Forecasting via Differentiable Constraints in WFMs\nAugment Cosmos Predict/LTX-style generators with lightweight differentiable constraints (e.g., contact consistency, approxim",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Cross-Modal World Models: Joint Video\u2013LiDAR\u2013Proprio Prediction for AV/Robotics\nExtend the WFM to predict synchronized futures across camera video, LiDAR/range images, and robot/vehicle state, using Co",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Language-Guided Imagination Editing for Task-Specific Data Generation\nCombine Cosmos Reason (VLM reasoning) with Cosmos Transfer/Predict to translate natural-language scenario specs (e.g., \u201cadd a pede",
          "is_match": false
        }
      ]
    },
    {
      "paper_idx": 76,
      "paper_title": "Mean Flows for One-step Generative Modeling",
      "contribution": "The MeanFlow model introduces the use of average velocity in generative modeling, providing a principled framework that enhances one-step generation without complex heuristics or pre-training.",
      "num_predecessors": 6,
      "predecessors_crawled": 6,
      "academic_sources": 6,
      "crawl_rate": 1.0,
      "ideas_generated": 8,
      "hit_at_k": false,
      "matching_idea_idx": null,
      "input_tokens": 10826,
      "output_tokens": 965,
      "predecessor_details": [
        {
          "success": true,
          "title": "[2210.02747] Flow Matching for Generative Modeling - arXiv",
          "url": "https://arxiv.org/abs/2210.02747",
          "content": "[2210.02747] Flow Matching for Generative Modeling\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2210.02747\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2210.02747**(cs)\n[Submitted on 6 Oct 2022 ([v1](https://arxiv.org/abs/2210.02747v1)), last revised 8 Feb 2023 (this version, v2)]\n# Title:Flow Matching for Generative Modeling\nAuthors:[Yaron Lipman](https://arxiv.org/search/cs?searchtype=author&amp;query=Lipman,+Y),[Ricky T. Q. Chen](https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+R+T+Q),[Heli Ben-Hamu](https://arxiv.org/search/cs?searchtype=author&amp;query=Ben-Hamu,+H),[Maximilian Nickel](https://arxiv.org/search/cs?searchtype=author&amp;query=Nickel,+M),[Matt Le](https://arxiv.org/search/cs?searchtype=author&amp;query=Le,+M)\nView a PDF of the paper titled Flow Matching for Generative Modeling, by Yaron Lipman and 4 other authors\n[View PDF](https://arxiv.org/pdf/2210.02747)> > Abstract:\n> We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers. Subjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)|\nCite as:|[arXiv:2210.02747](https://arxiv.org/abs/2210.02747)[cs.LG]|\n|(or[arXiv:2210.02747v2](https://arxiv.org/abs/2210.02747v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2210.02747](https://doi.org/10.48550/arXiv.2210.02747)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Yaron Lipman [[view email](https://arxiv.org/show-email/77e7051e/2210.02747)]\n**[[v1]](https://arxiv.org/abs/2210.02747v1)**Thu, 6 Oct 2022 08:32:20 UTC (23,746 KB)\n**[v2]**Wed, 8 Feb 2023 15:46:05 UTC (37,148 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Flow Matching for Generative Modeling, by Yaron Lipman and 4 other authors\n* [View PDF](https://arxiv.org/pdf/2210.02747)\n* [TeX Source](https://arxiv.org/src/2210.02747)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2210.02747&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2210.02747&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2022-10](https://arxiv.org/list/cs.LG/2022-10)\nChange to browse by:\n[cs](https://arxiv.org/abs/2210.02747?context=cs)\n[cs.AI](https://arxiv.org/abs/2210.02747?context=cs.AI)\n[stat](https://arxiv.org/abs/2210.02747?context=stat)\n[stat.ML](https://arxiv.org/abs/2210.02747?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2210.02747)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2210.02747)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2210.02747)\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2210.02747)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Flow matching for generative modeling",
          "cleaned_query": "Flow matching for generative modeling"
        },
        {
          "success": true,
          "title": "[2006.11239] Denoising Diffusion Probabilistic Models - arXiv",
          "url": "https://arxiv.org/abs/2006.11239",
          "content": "[2006.11239] Denoising Diffusion Probabilistic Models\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2006.11239\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2006.11239**(cs)\n[Submitted on 19 Jun 2020 ([v1](https://arxiv.org/abs/2006.11239v1)), last revised 16 Dec 2020 (this version, v2)]\n# Title:Denoising Diffusion Probabilistic Models\nAuthors:[Jonathan Ho](https://arxiv.org/search/cs?searchtype=author&amp;query=Ho,+J),[Ajay Jain](https://arxiv.org/search/cs?searchtype=author&amp;query=Jain,+A),[Pieter Abbeel](https://arxiv.org/search/cs?searchtype=author&amp;query=Abbeel,+P)\nView a PDF of the paper titled Denoising Diffusion Probabilistic Models, by Jonathan Ho and 2 other authors\n[View PDF](https://arxiv.org/pdf/2006.11239)> > Abstract:\n> We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at [> this https URL\n](https://github.com/hojonathanho/diffusion)> Subjects:|Machine Learning (cs.LG); Machine Learning (stat.ML)|\nCite as:|[arXiv:2006.11239](https://arxiv.org/abs/2006.11239)[cs.LG]|\n|(or[arXiv:2006.11239v2](https://arxiv.org/abs/2006.11239v2)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2006.11239](https://doi.org/10.48550/arXiv.2006.11239)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jonathan Ho [[view email](https://arxiv.org/show-email/c50e7925/2006.11239)]\n**[[v1]](https://arxiv.org/abs/2006.11239v1)**Fri, 19 Jun 2020 17:24:44 UTC (9,134 KB)\n**[v2]**Wed, 16 Dec 2020 21:15:05 UTC (9,137 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Denoising Diffusion Probabilistic Models, by Jonathan Ho and 2 other authors\n* [View PDF](https://arxiv.org/pdf/2006.11239)\n* [TeX Source](https://arxiv.org/src/2006.11239)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2006.11239&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2006.11239&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2020-06](https://arxiv.org/list/cs.LG/2020-06)\nChange to browse by:\n[cs](https://arxiv.org/abs/2006.11239?context=cs)\n[stat](https://arxiv.org/abs/2006.11239?context=stat)\n[stat.ML](https://arxiv.org/abs/2006.11239?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2006.11239)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2006.11239)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2006.11239)\n### [10 blog links](https://arxiv.org/tb/2006.11239)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr2006.html#abs-2006-11239)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-2006-11239)\n[Jonathan Ho]()\n[Ajay Jain]()\n[Pieter Abbeel]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2006.11239)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Denoising diffusion probabilistic models",
          "cleaned_query": "Denoising diffusion probabilistic models"
        },
        {
          "success": true,
          "title": "[2303.01469] Consistency Models",
          "url": "https://arxiv.org/abs/2303.01469",
          "content": "[We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.](https://confluence.cornell.edu/x/ALlRF)",
          "original_query": "Consistency models",
          "cleaned_query": "Consistency models"
        },
        {
          "success": true,
          "title": "[2207.12598] Classifier-Free Diffusion Guidance - arXiv",
          "url": "https://arxiv.org/abs/2207.12598",
          "content": "[2207.12598] Classifier-Free Diffusion Guidance\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:2207.12598\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:2207.12598**(cs)\n[Submitted on 26 Jul 2022]\n# Title:Classifier-Free Diffusion Guidance\nAuthors:[Jonathan Ho](https://arxiv.org/search/cs?searchtype=author&amp;query=Ho,+J),[Tim Salimans](https://arxiv.org/search/cs?searchtype=author&amp;query=Salimans,+T)\nView a PDF of the paper titled Classifier-Free Diffusion Guidance, by Jonathan Ho and 1 other authors\n[View PDF](https://arxiv.org/pdf/2207.12598)> > Abstract:\n> Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance. Comments:|A short version of this paper appeared in the NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications:[this https URL](https://openreview.net/pdf?id=qw8AKxfYbI)|\nSubjects:|Machine Learning (cs.LG); Artificial Intelligence (cs.AI)|\nCite as:|[arXiv:2207.12598](https://arxiv.org/abs/2207.12598)[cs.LG]|\n|(or[arXiv:2207.12598v1](https://arxiv.org/abs/2207.12598v1)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.2207.12598](https://doi.org/10.48550/arXiv.2207.12598)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jonathan Ho [[view email](https://arxiv.org/show-email/a41701ec/2207.12598)]\n**[v1]**Tue, 26 Jul 2022 01:42:07 UTC (3,393 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Classifier-Free Diffusion Guidance, by Jonathan Ho and 1 other authors\n* [View PDF](https://arxiv.org/pdf/2207.12598)\n* [TeX Source](https://arxiv.org/src/2207.12598)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=2207.12598&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=2207.12598&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2022-07](https://arxiv.org/list/cs.LG/2022-07)\nChange to browse by:\n[cs](https://arxiv.org/abs/2207.12598?context=cs)\n[cs.AI](https://arxiv.org/abs/2207.12598?context=cs.AI)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2207.12598)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2207.12598)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:2207.12598)\n### [1 blog link](https://arxiv.org/tb/2207.12598)\n([what is this?](https://info.arxiv.org/help/trackback.html))\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArxiv recommender toggle\nIArxiv Recommender*([What is IArxiv?](https://iarxiv.org/about))*\n* Author\n* Venue\n* Institution\n* Topic\nAbout arXivLabs\n# arXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?[**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).\n[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2207.12598)|[Disable MathJax](javascript:setMathjaxCookie())([What is MathJax?](https://info.arxiv.org/help/mathjax.html))",
          "original_query": "Classifier-free diffusion guidance",
          "cleaned_query": "Classifier-free diffusion guidance"
        },
        {
          "success": true,
          "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
          "url": "https://arxiv.org/abs/1503.03585",
          "content": "[1503.03585] Deep Unsupervised Learning using Nonequilibrium Thermodynamics\n[Skip to main content](#content)\n[![Cornell University](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nWe gratefully acknowledge support from the Simons Foundation,[member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n[](https://arxiv.org/IgnoreMe)\n[![arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)&gt;[cs](https://arxiv.org/list/cs/recent)&gt;arXiv:1503.03585\n[Help](https://info.arxiv.org/help)|[Advanced Search](https://arxiv.org/search/advanced)\nAll fieldsTitleAuthorAbstractCommentsJournal referenceACM classificationMSC classificationReport numberarXiv identifierDOIORCIDarXiv author IDHelp pagesFull text\nSearch\n[![arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n[![Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\nopen search\nGO\nopen navigation menu\n# Computer Science \\> Machine Learning\n**arXiv:1503.03585**(cs)\n[Submitted on 12 Mar 2015 ([v1](https://arxiv.org/abs/1503.03585v1)), last revised 18 Nov 2015 (this version, v8)]\n# Title:Deep Unsupervised Learning using Nonequilibrium Thermodynamics\nAuthors:[Jascha Sohl-Dickstein](https://arxiv.org/search/cs?searchtype=author&amp;query=Sohl-Dickstein,+J),[Eric A. Weiss](https://arxiv.org/search/cs?searchtype=author&amp;query=Weiss,+E+A),[Niru Maheswaranathan](https://arxiv.org/search/cs?searchtype=author&amp;query=Maheswaranathan,+N),[Surya Ganguli](https://arxiv.org/search/cs?searchtype=author&amp;query=Ganguli,+S)\nView a PDF of the paper titled Deep Unsupervised Learning using Nonequilibrium Thermodynamics, by Jascha Sohl-Dickstein and 3 other authors\n[View PDF](https://arxiv.org/pdf/1503.03585)> > Abstract:\n> A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm. Subjects:|Machine Learning (cs.LG); Disordered Systems and Neural Networks (cond-mat.dis-nn); Neurons and Cognition (q-bio.NC); Machine Learning (stat.ML)|\nCite as:|[arXiv:1503.03585](https://arxiv.org/abs/1503.03585)[cs.LG]|\n|(or[arXiv:1503.03585v8](https://arxiv.org/abs/1503.03585v8)[cs.LG]for this version)|\n|[https://doi.org/10.48550/arXiv.1503.03585](https://doi.org/10.48550/arXiv.1503.03585)\nFocus to learn more\narXiv-issued DOI via DataCite\n|\n## Submission history\nFrom: Jascha Sohl-Dickstein [[view email](https://arxiv.org/show-email/33b8482c/1503.03585)]\n**[[v1]](https://arxiv.org/abs/1503.03585v1)**Thu, 12 Mar 2015 04:51:37 UTC (5,395 KB)\n**[[v2]](https://arxiv.org/abs/1503.03585v2)**Thu, 2 Apr 2015 06:48:02 UTC (5,397 KB)\n**[[v3]](https://arxiv.org/abs/1503.03585v3)**Wed, 29 Apr 2015 06:00:20 UTC (5,403 KB)\n**[[v4]](https://arxiv.org/abs/1503.03585v4)**Wed, 13 May 2015 01:57:49 UTC (5,409 KB)\n**[[v5]](https://arxiv.org/abs/1503.03585v5)**Wed, 20 May 2015 03:19:10 UTC (4,586 KB)\n**[[v6]](https://arxiv.org/abs/1503.03585v6)**Thu, 9 Jul 2015 16:16:33 UTC (6,085 KB)\n**[[v7]](https://arxiv.org/abs/1503.03585v7)**Tue, 21 Jul 2015 19:44:20 UTC (6,092 KB)\n**[v8]**Wed, 18 Nov 2015 21:50:51 UTC (6,095 KB)\nFull-text links:## Access Paper:\nView a PDF of the paper titled Deep Unsupervised Learning using Nonequilibrium Thermodynamics, by Jascha Sohl-Dickstein and 3 other authors\n* [View PDF](https://arxiv.org/pdf/1503.03585)\n* [TeX Source](https://arxiv.org/src/1503.03585)\n[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)\nCurrent browse context:\ncs.LG\n[&lt;&lt;prev](https://arxiv.org/prevnext?id=1503.03585&amp;function=prev&amp;context=cs.LG) | [next&gt;&gt;](https://arxiv.org/prevnext?id=1503.03585&amp;function=next&amp;context=cs.LG)\n[new](https://arxiv.org/list/cs.LG/new)|[recent](https://arxiv.org/list/cs.LG/recent)|[2015-03](https://arxiv.org/list/cs.LG/2015-03)\nChange to browse by:\n[cond-mat](https://arxiv.org/abs/1503.03585?context=cond-mat)\n[cond-mat.dis-nn](https://arxiv.org/abs/1503.03585?context=cond-mat.dis-nn)\n[cs](https://arxiv.org/abs/1503.03585?context=cs)\n[q-bio](https://arxiv.org/abs/1503.03585?context=q-bio)\n[q-bio.NC](https://arxiv.org/abs/1503.03585?context=q-bio.NC)\n[stat](https://arxiv.org/abs/1503.03585?context=stat)\n[stat.ML](https://arxiv.org/abs/1503.03585?context=stat.ML)\n### References &amp; Citations\n* [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:1503.03585)\n* [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=1503.03585)\n* [Semantic Scholar](https://api.semanticscholar.org/arXiv:1503.03585)\n### [2 blog links](https://arxiv.org/tb/1503.03585)\n([what is this?](https://info.arxiv.org/help/trackback.html))\n### [DBLP](https://dblp.uni-trier.de)- CS Bibliography\n[listing](https://dblp.uni-trier.de/db/journals/corr/corr1503.html#Sohl-DicksteinW15)|[bibtex](https://dblp.uni-trier.de/rec/bibtex/journals/corr/Sohl-DicksteinW15)\n[Jascha Sohl-Dickstein]()\n[Eric A. Weiss]()\n[Niru Maheswaranathan]()\n[Surya Ganguli]()\nexport BibTeX citationLoading...\n## BibTeX formatted citation\n&times;\nloading...\nData provided by:\n### Bookmark\n[![BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)]()[![Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)]()\nBibliographic Tools\n# Bibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer*([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))*\nConnected Papers Toggle\nConnected Papers*([What is Connected Papers?](https://www.connectedpapers.com/about))*\nLitmaps Toggle\nLitmaps*([What is Litmaps?](https://www.litmaps.co/))*\nscite.ai Toggle\nscite Smart Citations*([What are Smart Citations?](https://www.scite.ai/))*\nCode, Data, Media\n# Code, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv*([What is alphaXiv?](https://alphaxiv.org/))*\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers*([What is CatalyzeX?](https://www.catalyzex.com))*\nDagsHub Toggle\nDagsHub*([What is DagsHub?](https://dagshub.com/))*\nGotitPub Toggle\nGotit.pub*([What is GotitPub?](http://gotit.pub/faq))*\nHuggingface Toggle\nHugging Face*([What is Huggingface?](https://huggingface.co/huggingface))*\nLinks to Code Toggle\nPapers with Code*([What is Papers with Code?](https://paperswithcode.com/))*\nScienceCast Toggle\nScienceCast*([What is ScienceCast?](https://sciencecast.org/welcome))*\nDemos\n# Demos\nReplicate Toggle\nReplicate*([What is Replicate?](https://replicate.com/docs/arxiv/about))*\nSpaces Toggle\nHugging Face Spaces*([What is Spaces?](https://huggingface.co/docs/hub/spaces))*\nSpaces Toggle\nTXYZ.AI*([What is TXYZ.AI?](https://txyz.ai))*\nRelated Papers\n# Recommenders and Search Tools\nLink to Influence Flower\nInfluence Flower*([What are Influence Flowers?](https://influencemap.cmlab.dev/))*\nCore recommender toggle\nCORE Recommender*([What is CORE?](https://core.ac.uk/services/recommender))*\nIArx",
          "original_query": "Deep unsupervised learning using nonequilibrium thermodynamics",
          "cleaned_query": "Deep unsupervised learning using nonequilibrium thermodynamics"
        },
        {
          "success": true,
          "title": "[PDF] FLOW STRAIGHT AND FAST: LEARNING TO GENER - OpenReview",
          "url": "https://openreview.net/pdf/910c5efa5739a5d2bef83d432da87d3096712ebe.pdf",
          "content": "Published as a conference paper at ICLR 2023\nFLOW STRAIGHT AND FAST: LEARNING TO GENER\u0002ATE AND TRANSFER DATA WITH RECTIFIED FLOW\nXingchao Liu\u2217, Chengyue Gong\u2217, Qiang Liu\nDepartment of Computer Science\nUniversity of Texas at Austin\n{xcliu, cygong, lqiang}@cs.utexas.edu\nABSTRACT\nWe present rectified flow, a simple approach to learning (neural) ordinary differen\u0002tial equation (ODE) models to transport between two empirically observed distri\u0002butions \u03c00 and \u03c01, hence providing a unified solution to generative modeling and\ndomain transfer, among various other tasks involving distribution transport. The\nidea of rectified flow is to learn the ODE to follow the straight paths connecting\nthe points drawn from \u03c00 and \u03c01 as much as possible. This is achieved by solv\u0002ing a straightforward nonlinear least squares optimization problem, which can be\neasily scaled to large models without introducing extra parameters beyond stan\u0002dard supervised learning. The straight paths are the shortest paths between two\npoints, and can be simulated exactly without time discretization and hence yield\ncomputationally efficient models. We show that, by learning a rectified flow from\ndata, we effectively turn an arbitrary coupling of \u03c00 and \u03c01 to a new deterministic\ncoupling with provably non-increasing convex transport costs. In addition, with a\n\u201creflow\u201d procedure that iteratively learns a new rectified flow from the data boot\u0002strapped from the previous one, we obtain a sequence of flows with increasingly\nstraight paths, which can be simulated accurately with coarse time discretization\nin the inference phase. In empirical studies, we show that rectified flow performs\nsuperbly on image generation and image-to-image translation. In particular, on\nimage generation and translation, our method yields nearly straight flows that give\nhigh quality results even with a single Euler discretization step. Code is available\nat https://github.com/gnobitab/RectifiedFlow.\n1 INTRODUCTION\nCompared with supervised learning, the shared difficulty of various forms of unsupervised learning\nis the lack of paired input/output data that makes standard regression or classification tasks possible.\nThe crux of many unsupervised methods is to find meaningful correspondences between points\nfrom two distributions. For example, generative models such as generative adversarial networks\n(GAN) and variational autoencoders (VAE) (e.g., Goodfellow et al., 2014; Kingma & Welling, 2013;\nDinh et al., 2016) seek to map data points to latent codes following a simple elementary (e.g.,\nGaussian) distribution with which the data can be generated and manipulated. On the other hand,\ndomain transfer methods find mappings to transfer points between two different data distributions,\nboth observed empirically, for the purpose of image-to-image translation, style transfer, and domain\nadaption (e.g., Zhu et al., 2017; Flamary et al., 2016; Trigila & Tabak, 2016; Peyre et al. \u00b4 , 2019).\nThese tasks can be framed unifiedly as finding a transport map between two distributions:\nLearning Transport Mapping Given empirical observations of two distributions \u03c00, \u03c01 on R\nd\n,\nfind a transport map T : R\nd \u2192 Rd\n, which, in the infinite data limit, gives Z1 := T(Z0) \u223c \u03c01 when\nZ0 \u223c \u03c00, that is, (Z0, Z1) is a coupling (a.k.a transport plan) of \u03c00 and \u03c01.\nWe should note that the answers of this problem are not unique because there are often infinitely\nmany transport maps between two distributions. Optimal transport (OT) (e.g., Villani, 2021; Am\u0002brosio et al., 2021; Figalli & Glaudo, 2021; Peyre et al. \u00b4 , 2019) addresses the more challenging\n\u2217Xingchao and Chengyue contributed equally.\n1\nPublished as a conference paper at ICLR 2023\nFigure 1: The trajectories of rectified flows for image generation (\u03c00: standard Gaussian noise, \u03c01: cat faces,\ntop two rows), and image transfer between human and cat faces (\u03c00: human faces, \u03c01: cat faces, bottom two\nrows), when simulated using Euler method with step size 1/N for N steps. The first rectified flow induced\nfrom the training data (1-rectified flow) yields good results with a very small number (e.g., \u2265 2) of steps; the\nstraightened reflow induced from 1-rectified flow (denoted as 2-rectified flow) has nearly straight line trajectories\nand yield good results even with one discretization step.\nproblem of finding an optimal coupling that minimizes a notion of transport cost, typically of form\nE[c(Z1 \u2212 Z0)], where c : R\nd \u2192 R is a cost function, such as c(x) = \u2225x\u2225\n2\n. However, for the gen\u0002erative and transfer modeling tasks above, the transport cost is not of direct interest, even though\nit induces a number of desirable properties. Hence, it is not necessary to accurate solve the OT\nproblems given the high difficulty of doing so. An important question is to identify relaxed notions\nof optimality that are of direct interest for ML tasks and are easier to enforce in practice.\nSeveral lines of techniques have been developed depending on how to represent and train the map\nT. In traditional generative models, T is parameterized as a neural network, and trained with either\nGAN-type minimax algorithms or (approximate) maximum likelihood estimation (MLE). However,\nGANs suffer from numerically instability and mode collapse issues, and require substantial engi\u0002neering efforts and human tuning, which tend to transfer poorly across different model architecture\nand datasets. On the other hand, MLE tends to be intractable for complex models, and hence requires\neither approximate (variational or Monte Carlo) inference techniques such as those used in VAE, or\nspecial model structures that yield tractable likelihood such as normalizing flow and auto-regressive\nmodels, which causes difficult trade-offs between expressive power and computational cost.\nRecently, advances have been made by representing the transport plan implicitly as a continuous time\nprocess, including flow models with neural ordinary differential equations (ODEs) (e.g., Chen et al.,\n2018; Papamakarios et al., 2021; Song et al., 2020a) and diffusion models by stochastic differential\nequations (SDEs) (e.g., Song et al., 2020b; Ho et al., 2020; Tzen & Raginsky, 2019; De Bortoli et al.,\n2021; Vargas et al., 2021). In these models, a neural network is trained to represent the drift force of\nthe processes and a numerical ODE/SDE solver is used to simulate the process during inference. By\nleveraging the mathematical structures of ODEs/SDEs, the continuous-time models can be trained\nefficiently without resorting to minimax or traditional approximate inference techniques. The most\nnotable examples are the score-based generative models (Song & Ermon, 2019; 2020; Song et al.,\n2020b) and denoising diffusion probabilistic models (DDPM) (Ho et al., 2020), which has achieved\nimpressive empirical results on image generation recently (e.g., Dhariwal & Nichol, 2021). How\u0002ever, compared with the traditional \u201cone-step\u201d models like GAN and VAE, continuous-times models\nare effectively \u201cinfinite-step\u201d and cast high computational cost in inference time: drawing a single\npoint (e.g., an image) requires to solve the ODE/SDE with a numerical solver that needs to repeat\u0002edly call the expensive neural force field for a large number of times.\nMoreover, in existing approaches, generative modeling and domain transfer are typically treated\nseparately. It often requires to extend techniques to solve domain transfer problems; see e.g., Cycle\n2\nPublished as a conference paper at ICLR 2023\n(a) Linear interpolation\nXt = tX1 + (1 \u2212 t)X0\n(b) Rectified flow Zt\ninduced by (X0, X1)\n(c) Linear interpolation\nZt = tZ1 + (1 \u2212 t)Z0\n(d) Rectified flow Z\n\u2032\nt\ninduced by (Z0, Z1)\nFigure 2: (a) Linear interpolation of data input (X0, X1) \u223c \u03c00 \u00d7 \u03c01. (b) The trained rectified flow Zt ; the\ntrajectories are \u201crewired\u201d at the intersection points to avoid crossing. (c) The linear interpolation of the end\npoints (Z0, Z1) of flow Zt. (d) The rectified flow induced from (Z0, Z1), which follows straight paths.\nGAN (Zhu et al., ",
          "original_query": "Flow straight and fast: Learning to generate and transfer data with rectified flow",
          "cleaned_query": "Flow straight and fast: Learning to generate and transfer data with rectified flow"
        }
      ],
      "generated_ideas": [
        "Adaptive Path Selection for Flow Matching via Learnable Probability-Path Families\nExtend Flow Matching (FM) by parameterizing the conditional probability path family (e.g., Gaussian paths spanning diffusion-like to OT-like interpolations) with a small learnable controller that selects path parameters as a function of time and data features. Train jointly with the vector field to minimize FM regression loss while regularizing for low curvature/straightness, yielding faster ODE sampling without sacrificing likelihood.",
        "Classifier-Free Guidance for CNFs Trained with Flow Matching (CFG-FM)\nGeneralize classifier-free guidance (CFG) from diffusion score models to FM-trained continuous normalizing flows by training conditional and unconditional vector fields under the same probability path. Derive a guidance rule that mixes conditional/unconditional velocity fields analogously to CFG and evaluate controllability vs. diversity using ODE solvers at very low step counts.",
        "Reflow-Enhanced Flow Matching: Iterative Straightening with Likelihood Tracking\nIntegrate rectified flow\u2019s \u201creflow\u201d bootstrapping into FM: alternate between (a) sampling trajectories from the current FM-CNF and (b) retraining FM on the induced coupling to reduce path curvature and convex transport cost proxies. Track exact/approximate likelihood through CNF change-of-variables to study how trajectory straightening trades off with density calibration.",
        "Entropy-Controlled Nonequilibrium Paths in Flow Matching\nInspired by nonequilibrium thermodynamics diffusion, introduce an explicit entropy-production or KL-to-noise schedule constraint on the FM probability path, optimizing a trade-off between exploration (diffusion-like) and efficiency (OT-like). Implement by adding a differentiable regularizer computed from path marginals (Gaussian family) and show improved stability and reduced mode dropping on large-scale datasets.",
        "Coupling-Aware Flow Matching Using Approximate Optimal Transport Pairings\nFM and rectified flow both depend on couplings between noise and data; design a training pipeline that periodically refreshes the coupling using mini-batch approximate OT (e.g., Sinkhorn) or learned matching in latent space. Quantify how coupling quality impacts sample efficiency, likelihood, and step-count at inference, and provide practical heuristics for scalable coupling updates.",
        "Solver-Aware Training Objectives for FM-CNFs (Discretization-Robust Flows)\nTrain FM models with an objective that matches not only the continuous-time vector field but also the induced discrete update of a target solver (Euler/Heun/RK) at chosen step sizes. This directly optimizes for low-NFE sampling (like rectified flow\u2019s single-step behavior) while maintaining the CNF framework, producing models that degrade gracefully under aggressive discretization.",
        "Uncertainty-Calibrated Guidance via Time-Dependent Guidance Scaling in FM/Diffusion\nDevelop a principled schedule for classifier-free guidance scaling based on predicted epistemic/aleatoric uncertainty of the conditional vs. unconditional vector fields (or scores) at each time. Implement uncertainty estimates via ensembles or dropout and show improved prompt/condition adherence without oversharpening or reduced diversity, especially in low-step sampling regimes.",
        "Hybrid OT\u2013Diffusion Probability Paths for Robustness to Distribution Shift\nConstruct probability paths that interpolate between OT displacement (fast, straight) near the data manifold and diffusion-like noise injection (robust, mixing) in regions of low data density, trained with FM. Evaluate on out-of-distribution generalization (e.g., ImageNet variants, corruptions) to test whether hybrid paths preserve FM\u2019s efficiency while inheriting diffusion\u2019s robustness and mode coverage."
      ],
      "generated_ideas_raw": "1. **Adaptive Path Selection for Flow Matching via Learnable Probability-Path Families**  \n   Extend Flow Matching (FM) by parameterizing the conditional probability path family (e.g., Gaussian paths spanning diffusion-like to OT-like interpolations) with a small learnable controller that selects path parameters as a function of time and data features. Train jointly with the vector field to minimize FM regression loss while regularizing for low curvature/straightness, yielding faster ODE sampling without sacrificing likelihood.\n\n2. **Classifier-Free Guidance for CNFs Trained with Flow Matching (CFG-FM)**  \n   Generalize classifier-free guidance (CFG) from diffusion score models to FM-trained continuous normalizing flows by training conditional and unconditional vector fields under the same probability path. Derive a guidance rule that mixes conditional/unconditional velocity fields analogously to CFG and evaluate controllability vs. diversity using ODE solvers at very low step counts.\n\n3. **Consistency-Regularized Flow Matching for One-to-Few Step Generation**  \n   Combine Consistency Models\u2019 self-consistency objective with FM by enforcing that integrating the learned ODE from any intermediate time to the endpoint yields consistent outputs across discretizations (e.g., 1-step vs. multi-step). This creates an FM-trained generator that retains CNF likelihood advantages while approaching rectified-flow-like 1\u20134 step sampling.\n\n4. **Reflow-Enhanced Flow Matching: Iterative Straightening with Likelihood Tracking**  \n   Integrate rectified flow\u2019s \u201creflow\u201d bootstrapping into FM: alternate between (a) sampling trajectories from the current FM-CNF and (b) retraining FM on the induced coupling to reduce path curvature and convex transport cost proxies. Track exact/approximate likelihood through CNF change-of-variables to study how trajectory straightening trades off with density calibration.\n\n5. **Entropy-Controlled Nonequilibrium Paths in Flow Matching**  \n   Inspired by nonequilibrium thermodynamics diffusion, introduce an explicit entropy-production or KL-to-noise schedule constraint on the FM probability path, optimizing a trade-off between exploration (diffusion-like) and efficiency (OT-like). Implement by adding a differentiable regularizer computed from path marginals (Gaussian family) and show improved stability and reduced mode dropping on large-scale datasets.\n\n6. **Coupling-Aware Flow Matching Using Approximate Optimal Transport Pairings**  \n   FM and rectified flow both depend on couplings between noise and data; design a training pipeline that periodically refreshes the coupling using mini-batch approximate OT (e.g., Sinkhorn) or learned matching in latent space. Quantify how coupling quality impacts sample efficiency, likelihood, and step-count at inference, and provide practical heuristics for scalable coupling updates.\n\n7. **Domain Transfer with Shared Vector Fields via Bidirectional Rectified Flow Matching**  \n   Build a unified model that learns two transports (A\u2192B and B\u2192A) using rectified-flow-style straight paths but trained with FM objectives for continuous-time consistency. Add cycle-consistency in continuous time (integrate forward then backward) to reduce artifacts, enabling stable unpaired image-to-image translation with coarse (1\u20132 step) solvers.\n\n8. **Solver-Aware Training Objectives for FM-CNFs (Discretization-Robust Flows)**  \n   Train FM models with an objective that matches not only the continuous-time vector field but also the induced discrete update of a target solver (Euler/Heun/RK) at chosen step sizes. This directly optimizes for low-NFE sampling (like rectified flow\u2019s single-step behavior) while maintaining the CNF framework, producing models that degrade gracefully under aggressive discretization.\n\n9. **Uncertainty-Calibrated Guidance via Time-Dependent Guidance Scaling in FM/Diffusion**  \n   Develop a principled schedule for classifier-free guidance scaling based on predicted epistemic/aleatoric uncertainty of the conditional vs. unconditional vector fields (or scores) at each time. Implement uncertainty estimates via ensembles or dropout and show improved prompt/condition adherence without oversharpening or reduced diversity, especially in low-step sampling regimes.\n\n10. **Hybrid OT\u2013Diffusion Probability Paths for Robustness to Distribution Shift**  \n   Construct probability paths that interpolate between OT displacement (fast, straight) near the data manifold and diffusion-like noise injection (robust, mixing) in regions of low data density, trained with FM. Evaluate on out-of-distribution generalization (e.g., ImageNet variants, corruptions) to test whether hybrid paths preserve FM\u2019s efficiency while inheriting diffusion\u2019s robustness and mode coverage.",
      "judgments": [
        {
          "idea_idx": 0,
          "idea_text": "Adaptive Path Selection for Flow Matching via Learnable Probability-Path Families\nExtend Flow Matching (FM) by parameterizing the conditional probability path family (e.g., Gaussian paths spanning dif",
          "is_match": false
        },
        {
          "idea_idx": 1,
          "idea_text": "Classifier-Free Guidance for CNFs Trained with Flow Matching (CFG-FM)\nGeneralize classifier-free guidance (CFG) from diffusion score models to FM-trained continuous normalizing flows by training condi",
          "is_match": false
        },
        {
          "idea_idx": 2,
          "idea_text": "Reflow-Enhanced Flow Matching: Iterative Straightening with Likelihood Tracking\nIntegrate rectified flow\u2019s \u201creflow\u201d bootstrapping into FM: alternate between (a) sampling trajectories from the current ",
          "is_match": false
        },
        {
          "idea_idx": 3,
          "idea_text": "Entropy-Controlled Nonequilibrium Paths in Flow Matching\nInspired by nonequilibrium thermodynamics diffusion, introduce an explicit entropy-production or KL-to-noise schedule constraint on the FM prob",
          "is_match": false
        },
        {
          "idea_idx": 4,
          "idea_text": "Coupling-Aware Flow Matching Using Approximate Optimal Transport Pairings\nFM and rectified flow both depend on couplings between noise and data; design a training pipeline that periodically refreshes ",
          "is_match": false
        },
        {
          "idea_idx": 5,
          "idea_text": "Solver-Aware Training Objectives for FM-CNFs (Discretization-Robust Flows)\nTrain FM models with an objective that matches not only the continuous-time vector field but also the induced discrete update",
          "is_match": false
        },
        {
          "idea_idx": 6,
          "idea_text": "Uncertainty-Calibrated Guidance via Time-Dependent Guidance Scaling in FM/Diffusion\nDevelop a principled schedule for classifier-free guidance scaling based on predicted epistemic/aleatoric uncertaint",
          "is_match": false
        },
        {
          "idea_idx": 7,
          "idea_text": "Hybrid OT\u2013Diffusion Probability Paths for Robustness to Distribution Shift\nConstruct probability paths that interpolate between OT displacement (fast, straight) near the data manifold and diffusion-li",
          "is_match": false
        }
      ]
    }
  ]
}
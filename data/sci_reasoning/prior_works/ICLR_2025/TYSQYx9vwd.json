{
  "prior_works": [
    {
      "title": "Graph Neural Ordinary Differential Equations",
      "authors": "Poli et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "LGNSDE takes GNODE\u2019s continuous-depth message-passing formulation as the base architecture and upgrades the ODE dynamics to an SDE with a learned diffusion and a Bayesian latent mechanism to deliver calibrated uncertainty."
    },
    {
      "title": "Latent ODEs for Irregularly-Sampled Time Series",
      "authors": "Yulia Rubanova et al.",
      "year": 2019,
      "arxiv_id": "1907.03907",
      "role": "Foundation",
      "relationship_sentence": "LGNSDE adopts the latent prior\u2013posterior variational framework from Latent ODEs for continuous-time representation learning, but replaces deterministic evolution with graph-conditioned stochastic dynamics to quantify uncertainty."
    },
    {
      "title": "Neural Stochastic Differential Equations: Deep Latent Gaussian Models in the Diffusion Limit",
      "authors": "Brandon Tzen et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "LGNSDE extends the neural SDE latent-variable paradigm of Tzen & Raginsky by injecting Brownian motion into graph-evolving hidden states and performing variational inference to disentangle aleatoric (diffusion) from epistemic (posterior) uncertainty."
    },
    {
      "title": "Neural SDEs",
      "authors": "Patrick Kidger et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "LGNSDE relies on Neural SDEs\u2019 differentiable SDE solvers and the accompanying existence/uniqueness and stability conditions to train end-to-end and to justify its variance-propagation and robustness guarantees."
    },
    {
      "title": "Bayesian Graph Convolutional Neural Networks for Semi-Supervised Classification",
      "authors": "Zhang et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "LGNSDE addresses BGCN\u2019s limitation of modeling only weight (epistemic) uncertainty in static GNN layers by introducing process-level stochasticity that also captures aleatoric uncertainty in continuous-depth graph dynamics."
    },
    {
      "title": "Neural ODE Processes",
      "authors": "Andrew Norcliffe et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "LGNSDE generalizes Neural ODE Processes\u2019 Bayesian treatment of the vector field to the graph setting and augments it with an explicit diffusion term to provide a principled separation of epistemic and aleatoric uncertainties."
    }
  ],
  "synthesis_narrative": "Graph Neural Ordinary Differential Equations introduced continuous-depth message passing by interpreting GNN layers as an ODE flow, enabling time-continuous representation learning on graphs. Latent ODEs contributed the key variational machinery for continuous-time latent states, using a prior\u2013posterior formulation and an ELBO to learn dynamics from data. Neural Stochastic Differential Equations established how Brownian motion can drive latent dynamics and how variational inference can be performed in SDE-driven generative models, directly tying diffusion to data uncertainty. Neural SDEs provided practical differentiable SDE solvers and formal conditions for existence, uniqueness, and stability of learned SDEs, grounding training and analysis of stochastic continuous-depth models. Bayesian Graph Convolutional Neural Networks showed that Bayesian treatment of GNN parameters yields epistemic uncertainty but remains static and lacks process-level noise modeling. Neural ODE Processes placed Bayesian priors over ODE vector fields to quantify uncertainty in continuous-time dynamics, highlighting the role of uncertainty in the drift itself.\nTogether, these works reveal a gap: continuous-depth GNNs lack principled uncertainty quantification, while Bayesian GNNs and ODE-based uncertainty methods do not model graph-driven stochastic dynamics or disentangle epistemic and aleatoric effects with guarantees. LGNSDE naturally synthesizes these threads by upgrading GNODE to an SDE whose drift is learned under a Bayesian prior\u2013posterior and whose diffusion models process noise, leveraging Neural SDE theory to prove variance bounds and robustness, and adopting latent variational training to obtain uncertainty-aware graph representations.",
  "target_paper": {
    "title": "Uncertainty Modeling in Graph Neural Networks via Stochastic Differential Equations",
    "authors": "Richard Bergna, Sergio Calvo Ordo\u00f1ez, Felix Opolka, Pietro Lio, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Graph Neural Networks, Stochastic Differential Equations, Uncertainty Quantification, Bayesian Machine Learning",
    "abstract": "We propose a novel Stochastic Differential Equation (SDE) framework to address the problem of learning uncertainty-aware representations for graph-structured data. While Graph Neural Ordinary Differential Equations (GNODEs) have shown promise in learning node representations, they lack the ability to quantify uncertainty. To address this, we introduce Latent Graph Neural Stochastic Differential Equations (LGNSDE), which enhance GNODE by embedding randomness through a Bayesian prior-posterior mechanism for epistemic uncertainty and Brownian motion for aleatoric uncertainty. By leveraging the existence and uniqueness of solutions to graph-based SDEs, we prove that the variance of the latent space bounds the variance of model outputs, thereby providing theoretically sensible guarantees for the uncertainty estimates. Furthermore, we show mathematically that LGNSDEs are robust to small perturbations in the input, maintaining stability over time. Empirical results across several benchmarks d",
    "openreview_id": "TYSQYx9vwd",
    "forum_id": "TYSQYx9vwd"
  },
  "analysis_timestamp": "2026-01-06T14:34:30.337107"
}
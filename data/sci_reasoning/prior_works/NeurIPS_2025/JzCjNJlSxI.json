{
  "prior_works": [
    {
      "title": "Relative Entropy Policy Search (REPS)",
      "authors": "Jan Peters, Katharina M\u00fclling, Yasemin Altun",
      "year": 2010,
      "role": "KL-regularized policy optimization",
      "relationship_sentence": "REPS formalized maximizing expected return subject to a KL constraint to a reference policy, providing the canonical entropy-regularized fine-tuning template that FDC generalizes beyond both the expected-reward utility and the KL divergence."
    },
    {
      "title": "Fine-Tuning Language Models from Human Preferences",
      "authors": "Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano",
      "year": 2019,
      "role": "KL-penalized fine-tuning of large generative models",
      "relationship_sentence": "This work instantiated KL-regularized reward maximization at scale for pretrained generators (LMs), establishing the practical baseline\u2014maximize reward while staying close to a reference\u2014that FDC extends to general utilities (risk/diversity) and broader divergences."
    },
    {
      "title": "The Variational Formulation of the Fokker\u2013Planck Equation (JKO Scheme)",
      "authors": "Richard Jordan, David Kinderlehrer, Felix Otto",
      "year": 1998,
      "role": "Proximal optimization in Wasserstein space via sequences of simpler subproblems",
      "relationship_sentence": "The JKO proximal scheme shows how complex distributional objectives can be solved as a sequence of simpler optimization steps under an OT metric; FDC adopts this density-control/proximal perspective to reduce general utility-regularized fine-tuning to tractable iterative updates under chosen divergences (e.g., OT, KL, R\u00e9nyi)."
    },
    {
      "title": "Diffusion Schr\u00f6dinger Bridge",
      "authors": "Valentino De Bortoli, James Thornton, Jeremy Heng, Arnaud Doucet",
      "year": 2021,
      "role": "Distribution control via entropic optimal transport for generative modeling",
      "relationship_sentence": "Schr\u00f6dinger bridge methods demonstrate controlling probability flows by balancing data/utilities with divergence to a prior path measure; FDC leverages this distribution-control viewpoint to preserve pretrained priors using OT-type distances beyond KL during utility optimization."
    },
    {
      "title": "Flow Matching for Generative Modeling",
      "authors": "Yaron Lipman, Ricky T. Q. Chen, et al.",
      "year": 2023,
      "role": "Training continuous-time flows via vector-field matching",
      "relationship_sentence": "Flow matching provides the flow/continuity-equation machinery to implement density evolution; FDC builds on this to parameterize and compute the sequential density updates required by its utility-plus-regularizer objective for flow and diffusion models."
    },
    {
      "title": "R\u00e9nyi Divergence Variational Inference",
      "authors": "Yingzhen Li, Richard E. Turner",
      "year": 2016,
      "role": "Using R\u00e9nyi divergences as tunable alternatives to KL",
      "relationship_sentence": "By introducing R\u00e9nyi-divergence objectives, this work motivates moving beyond KL to control trade-offs in approximation; FDC adopts R\u00e9nyi regularization as a flexible mechanism for prior preservation during generative optimization."
    },
    {
      "title": "Policy Gradient for Coherent Risk Measures",
      "authors": "Aviv Tamar, Shie Mannor, Huan Xu",
      "year": 2015,
      "role": "Optimization of risk-averse utilities (e.g., CVaR) in sequential decision-making",
      "relationship_sentence": "This paper formalizes gradients for coherent risk measures, directly informing FDC\u2019s extension from expected reward to risk-sensitive utilities (e.g., CVaR) when optimizing generative models."
    }
  ],
  "synthesis_narrative": "Flow Density Control (FDC) sits at the confluence of entropy-regularized fine-tuning, density control in probability spaces, and modern flow/diffusion generative training. REPS and subsequent KL-regularized fine-tuning of large models (e.g., Ziegler et al.) established the core paradigm of maximizing an objective while constraining deviation from a reference via KL. FDC generalizes this template in two axes: utilities and regularizers. On the utility side, insights from risk-sensitive RL, particularly policy gradients for coherent risk measures, motivate optimizing beyond average reward to capture risk aversion, novelty seeking, and exploration/diversity criteria. On the regularization side, moving past KL toward broader divergences is inspired by R\u00e9nyi variational inference and by optimal transport viewpoints.\nCrucially, FDC operationalizes these generalizations through a density-control/proximal lens. The JKO scheme shows that complex distributional objectives can be solved via a sequence of proximal problems under Wasserstein distance; Schr\u00f6dinger bridge methods extend this idea to entropic OT for generative modeling, balancing fidelity and prior adherence along probability flows. Flow matching provides the practical vehicle to implement these continuous-time density updates for flows and diffusions. By combining flow-based parameterizations with proximal updates under chosen divergences (KL, R\u00e9nyi, OT), FDC reduces general utility-regularized adaptation of pretrained generators to a tractable sequence of simpler subproblems, thereby unifying and extending entropy-regularized fine-tuning to richer objectives and prior-preservation metrics.",
  "analysis_timestamp": "2026-01-06T23:42:48.130816"
}
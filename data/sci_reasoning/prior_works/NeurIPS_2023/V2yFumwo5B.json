{
  "prior_works": [
    {
      "title": "Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer",
      "authors": "David Madras, Toniann Pitassi, Richard Zemel",
      "year": 2018,
      "role": "Foundational learning-to-defer/triage",
      "relationship_sentence": "Introduced the formal framework for models that abstain and defer to humans, which this paper extends by discovering and communicating when deference should occur through natural-language rules and user onboarding."
    },
    {
      "title": "Consistent Estimators for Learning to Defer to an Expert",
      "authors": "Hussein Mozannar, David Sontag",
      "year": 2020,
      "role": "Theoretical basis for triage with humans",
      "relationship_sentence": "Provided consistency guarantees and practical estimators for learning to defer, directly informing this work\u2019s goal of identifying data regions where either the human or the AI should lead."
    },
    {
      "title": "Learning to Complement Humans",
      "authors": "Bryan Wilder, Eric Horvitz, Ece Kamar",
      "year": 2020,
      "role": "Human\u2013AI complementarity optimization",
      "relationship_sentence": "Showed that optimizing for complementarity can outperform pure accuracy, motivating this paper\u2019s focus on surfacing actionable collaboration policies that improve team performance."
    },
    {
      "title": "Anchors: High-Precision Model-Agnostic Explanations",
      "authors": "Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin",
      "year": 2018,
      "role": "Local rule-style explanations",
      "relationship_sentence": "Demonstrated precise, human-readable local rules, inspiring the idea of describing discovered data regions with concise natural-language rules for guiding collaboration."
    },
    {
      "title": "Interpretable Decision Sets: A Joint Framework for Description and Prediction",
      "authors": "Hima Lakkaraju, Stephen H. Bach, Jure Leskovec",
      "year": 2016,
      "role": "Global rule-set interpretability",
      "relationship_sentence": "Introduced transparent decision sets that align with human reasoning, echoed here by assembling multiple natural-language rules as a global playbook for human\u2013AI teaming."
    },
    {
      "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
      "authors": "Been Kim et al.",
      "year": 2018,
      "role": "Concept-level, human-meaningful explanations",
      "relationship_sentence": "Established a pathway to explain models using human concepts, influencing this work\u2019s grounding of discovered regions in natural-language concepts via LLM-generated descriptions."
    },
    {
      "title": "Trust Calibration for AI-Assisted Decision-Making",
      "authors": "Z. Bu\u00e7inca et al.",
      "year": 2021,
      "role": "Onboarding and trust calibration in AI assistance",
      "relationship_sentence": "Showed that targeted onboarding and interventions improve reliance calibration, shaping this paper\u2019s onboarding stage that teaches rule-based collaboration strategies to users."
    }
  ],
  "synthesis_narrative": "The core innovation of this paper\u2014teaching humans how to collaborate with AI via learned, natural-language rules grounded in discovered data regions and an explicit onboarding phase\u2014emerges at the intersection of triage, complementarity, interpretability, and human-factors research. Foundational learning-to-defer work by Madras et al. and Mozannar & Sontag formalized algorithmic triage, establishing the need to determine when AI should act versus defer to humans. Complementarity-focused research, notably Wilder et al., showed that optimizing for team performance rather than raw model accuracy yields gains, highlighting the value of actionable collaboration policies.\nMethodologically, interpretable rule frameworks such as Lakkaraju et al.\u2019s interpretable decision sets and Ribeiro et al.\u2019s Anchors demonstrated how high-precision, human-readable rules can communicate model behavior at global and local levels. Concept-based interpretability (Kim et al.\u2019s TCAV) further emphasized grounding explanations in human-meaningful concepts, a precursor to leveraging large language models to describe discovered regions succinctly and contrastively.\nFinally, human\u2013AI interaction studies on trust calibration and onboarding (e.g., Bu\u00e7inca et al.) underscored that explanations alone are insufficient; structured training improves appropriate reliance. This paper synthesizes these threads by (1) discovering embedding-space regions where AI corrects human priors, (2) converting them into interpretable, natural-language rules via iterative, contrastive LLM prompting, and (3) operationalizing them through an onboarding curriculum. The result is a principled, end-to-end approach that turns triage theory and interpretability into practical guidance, producing measurable gains in human\u2013AI team accuracy.",
  "analysis_timestamp": "2026-01-06T23:42:49.069882"
}
{
  "prior_works": [
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "arxiv_id": "2203.02155",
      "role": "Foundation",
      "relationship_sentence": "The study adopts the two-stage pretraining + instruction-tuning paradigm formalized by InstructGPT and explicitly manipulates whether and how code appears at each stage to isolate its effect on reasoning."
    },
    {
      "title": "Finetuned Language Models Are Zero-Shot Learners",
      "authors": "Jason Wei et al.",
      "year": 2021,
      "arxiv_id": "2109.01652",
      "role": "Foundation",
      "relationship_sentence": "This work established supervised instruction tuning as a vehicle for transferring task knowledge (including reasoning) to pretrained LMs, providing the methodological basis to test how code-focused instruction data shapes task-specific reasoning."
    },
    {
      "title": "Evaluating Large Language Models Trained on Code",
      "authors": "Mark Chen et al.",
      "year": 2021,
      "arxiv_id": "2107.03374",
      "role": "Inspiration",
      "relationship_sentence": "By demonstrating that large-scale code pretraining (Codex) confers strong capabilities beyond pure NL modeling, it directly motivated the hypothesis that code exposure can endow or transfer general reasoning skills."
    },
    {
      "title": "LLaMA 2: Open Foundation and Fine-Tuned Chat Models",
      "authors": "Hugo Touvron et al.",
      "year": 2023,
      "arxiv_id": "2307.09288",
      "role": "Inspiration",
      "relationship_sentence": "LLaMA 2 reported that mixing a nontrivial fraction of code in pretraining correlates with improved mathematical/logical evaluations, inspiring a controlled, stage-wise analysis of where code helps most."
    },
    {
      "title": "Code Llama: Open Foundation Models for Code",
      "authors": "Baptiste Rozi\u00e8re et al.",
      "year": 2023,
      "arxiv_id": "2308.12950",
      "role": "Gap Identification",
      "relationship_sentence": "Code Llama showed continued pretraining on code boosts reasoning-leaning benchmarks but did not disentangle pretraining versus instruction-tuning effects, a gap this paper addresses via stage-specific interventions."
    },
    {
      "title": "WizardCoder: Empowering Large Language Models to Develop Code",
      "authors": "Ziyang Luo et al.",
      "year": 2023,
      "arxiv_id": "2306.08568",
      "role": "Related Problem",
      "relationship_sentence": "WizardCoder demonstrated that code-centric instruction tuning substantially shifts model capabilities, motivating the paper\u2019s examination of how code at the instruction-tuning stage imparts task-specific reasoning."
    },
    {
      "title": "Training Verifiers to Solve Math Word Problems",
      "authors": "Karl Cobbe et al.",
      "year": 2021,
      "arxiv_id": "2110.14168",
      "role": "Foundation",
      "relationship_sentence": "GSM8K provides a standard math reasoning benchmark used here to quantify how injecting code at pretraining versus instruction-tuning differentially affects general and task-specific reasoning."
    }
  ],
  "synthesis_narrative": "Instruction tuning emerged as a distinct stage after large-scale pretraining with InstructGPT, establishing a pipeline where stage-specific data choices can steer model behavior; FLAN showed that supervised instruction tuning transfers broad task competence, including reasoning, to pretrained LMs. Codex demonstrated that pretraining on massive code corpora yields capabilities that generalize beyond code, suggesting that structural regularities in programming data can shape reasoning skills. LLaMA 2 reported performance gains on math and logic coincident with including a meaningful fraction of code during pretraining, connecting code mixture to emergent reasoning. Code Llama further showed that continued pretraining on code after text improves reasoning-leaning metrics, but without dissecting when in the training pipeline code matters most. Complementarily, WizardCoder provided evidence that code-focused instruction tuning can substantially shift capabilities, hinting that code at the instruction phase may impart task-specific reasoning patterns. GSM8K furnished a rigorous math reasoning benchmark for measuring such effects. Together, these works indicated that code benefits reasoning but left unresolved whether benefits stem primarily from pretraining, instruction tuning, or their combination. Building on the two-stage framework, the paper systematically injects code at pretraining, instruction tuning, and both, and evaluates with reasoning tasks like GSM8K to separate general from task-specific gains. This synthesis clarifies that mixing code with text during pretraining chiefly enhances general reasoning with minimal negative transfer, while code at instruction tuning confers task-specific reasoning, making the stage-aware use of code a natural next step.",
  "target_paper": {
    "title": "At Which Training Stage Does Code Data Help LLMs Reasoning?",
    "authors": "YINGWEI MA, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, Shanshan Li",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "code data, large language models, reasoning capabilities",
    "abstract": "Large Language models (LLMs) have exhibited remarkable reasoning capabilities and become the foundation of language technologies. Inspired by the great success of code data in training LLMs, we naturally wonder at which training stage introducing code data can really help LLMs reasoning. To this end, this paper systematically explores the impact of code data on LLMs at different stages. Concretely, we introduce the code data at the pre-training stage, instruction-tuning stage, and both of them, respectively. Then, the reasoning capability of LLMs is comprehensively and fairly evaluated via six reasoning tasks. We critically analyze the experimental results and provide conclusions with insights. First, pre-training LLMs with the mixture of code and text can significantly enhance LLMs' general reasoning capability almost without negative transfer on other tasks. Besides, at the instruction-tuning stage, code data endows LLMs the task-specific reasoning capability. Moreover, the dynamic m",
    "openreview_id": "KIPJKST4gw",
    "forum_id": "KIPJKST4gw"
  },
  "analysis_timestamp": "2026-01-06T16:16:57.328611"
}
{
  "prior_works": [
    {
      "title": "Quantal Response Equilibria for Normal Form Games",
      "authors": "Richard D. McKelvey and Thomas R. Palfrey",
      "year": 1995,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "This work introduced logit quantal response equilibrium as a bounded-rational alternative to Nash, whose logit/softmax response model is directly extended here by embedding risk-averse utility to define tractable risk-averse QRE in multi-agent settings."
    },
    {
      "title": "Quantal Response Equilibria for Extensive Form Games",
      "authors": "Richard D. McKelvey and Thomas R. Palfrey",
      "year": 1998,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "By generalizing QRE to sequential (extensive-form) interactions, this paper provides the conceptual bridge that the current work leverages to carry QRE-style bounded rationality into finite-horizon Markov games."
    },
    {
      "title": "Markov Games as a Framework for Multi-Agent Reinforcement Learning",
      "authors": "Michael L. Littman",
      "year": 1994,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper established the Markov game formulation used here, enabling the definition and analysis of the proposed equilibria in finite-horizon general-sum MARL."
    },
    {
      "title": "Risk-Sensitive Markov Decision Processes",
      "authors": "Ronald A. Howard and James E. Matheson",
      "year": 1972,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "It introduced exponential (entropic) risk-sensitive objectives and their dynamic programming operators, which the present work adapts to multi-agent games to encode risk aversion in equilibrium computation."
    },
    {
      "title": "Risk-Averse Dynamic Programming for Markov Decision Processes",
      "authors": "Andrzej Ruszczy\u0144ski",
      "year": 2010,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "This paper developed time-consistent risk-averse dynamic programming with coherent/convex risk measures, directly informing the tractable value-iteration-style operators used when embedding risk aversion into Markov games."
    },
    {
      "title": "A Simple Adaptive Procedure Leading to Correlated Equilibrium",
      "authors": "Sergiu Hart and Andreu Mas-Colell",
      "year": 2000,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By showing that no-regret learning converges to an equilibrium concept, this work motivates the present result that no-regret dynamics in suitably adjusted (risk-averse, bounded-rational) games converge to the proposed RQE."
    },
    {
      "title": "The Complexity of Computing a Nash Equilibrium",
      "authors": "Constantinos Daskalakis, Paul W. Goldberg, and Christos H. Papadimitriou",
      "year": 2009,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Its PPAD-hardness results for Nash in general games provide the central tractability gap that motivates replacing Nash with a behaviorally grounded, computationally tractable equilibrium such as RQE."
    }
  ],
  "synthesis_narrative": "Quantal response equilibrium was introduced as a stochastic, bounded-rational alternative to Nash, with agents choosing according to a logit response that smooths best replies (McKelvey and Palfrey, 1995). This framework was subsequently extended beyond static normal-form games to sequential settings, demonstrating how bounded rationality can be made consistent with dynamic interaction structures (McKelvey and Palfrey, 1998). On the control side, risk-sensitive decision making in Markov processes was formalized via exponential utility and associated dynamic programming operators, establishing how risk aversion can be embedded into recursive value updates (Howard and Matheson, 1972), and later generalized to time-consistent formulations with coherent/convex risk measures that retain tractable dynamic programming structure (Ruszczy\u0144ski, 2010). The Markov game formalism created the standard multi-agent reinforcement learning setting in which such sequential equilibrium concepts are defined and computed (Littman, 1994). Complementing these modeling tools, no-regret learning was shown to converge to equilibrium concepts in repeated games, crystallizing the link between online learning dynamics and solution concepts (Hart and Mas-Colell, 2000). Finally, PPAD-hardness of computing Nash equilibria underscored the need for computationally tractable alternatives to Nash in general games (Daskalakis et al., 2009). Together, these strands suggest a natural synthesis: embed bounded rationality via logit response and risk aversion via risk-sensitive utilities into Markov games, and analyze the equilibria reached by no-regret dynamics in the adjusted games. This combination yields a behaviorally grounded equilibrium notion that preserves dynamic-programming tractability while circumventing Nash\u2019s computational barriers, making it a compelling target for general n-player matrix and finite-horizon Markov games.",
  "target_paper": {
    "title": "Tractable Multi-Agent Reinforcement Learning through Behavioral Economics",
    "authors": "Eric Mazumdar, Kishan Panaganti, Laixi Shi",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "behavioral economics, risk-aversion, multi-agent reinforcement learning, quantal response, bounded rationality",
    "abstract": "A significant roadblock to the development of principled multi-agent reinforcement learning (MARL) algorithms is the fact that desired solution concepts like Nash equilibria may be intractable to compute. We show how one can overcome this obstacle by introducing concepts from behavioral economics into MARL. To do so, we imbue agents with two key features of human decision-making: risk aversion and bounded rationality. We show that introducing these two properties into games gives rise to a class of equilibria---risk-averse quantal response equilibria (RQE)---which are tractable to compute in \\emph{all} $n$-player matrix and finite-horizon Markov games.  In particular, we show that they emerge as the endpoint of no-regret learning in suitably adjusted versions of the games. Crucially, the class of computationally tractable RQE is independent of the underlying game structure and only depends on agents' degrees of risk-aversion and bounded rationality.  To validate the expressivity of thi",
    "openreview_id": "stUKwWBuBm",
    "forum_id": "stUKwWBuBm"
  },
  "analysis_timestamp": "2026-01-06T19:04:58.138837"
}
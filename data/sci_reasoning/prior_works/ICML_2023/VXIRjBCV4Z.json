{
  "prior_works": [
    {
      "title": "Black Box Variational Inference",
      "authors": "Ranganath et al.",
      "year": 2014,
      "role": "Foundation",
      "relationship_sentence": "Introduced BBVI and the score-function estimator, framing the core gradient-variance challenge that this paper resolves by proving matching second-moment bounds for BBVI gradients under common model assumptions."
    },
    {
      "title": "Auto-Encoding Variational Bayes",
      "authors": "Kingma et al.",
      "year": 2014,
      "role": "Foundation",
      "relationship_sentence": "Established the reparameterization (pathwise) gradient for continuous-latent VI; this paper\u2019s variance bounds are derived in the BBVI setting with reparameterized Gaussians and rely on the pathwise-gradient structure."
    },
    {
      "title": "Automatic Differentiation Variational Inference",
      "authors": "Kucukelbir et al.",
      "year": 2017,
      "role": "Related Problem",
      "relationship_sentence": "Standardized practical mean-field Gaussian BBVI with nonlinear variance parameterizations (e.g., softplus) used widely in practice; this paper generalizes its variance bounds to such nonlinear covariance parameterizations and proves favorable dimensional dependence for mean-field."
    },
    {
      "title": "The Generalized Reparameterization Gradient",
      "authors": "Ruiz et al.",
      "year": 2016,
      "role": "Extension",
      "relationship_sentence": "Proposed GRG to handle non\u2013location-scale transformations in VI; the present work extends variance analysis to these nonlinear covariance parameterizations by building directly on GRG\u2019s formulation."
    },
    {
      "title": "Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference",
      "authors": "Roeder et al.",
      "year": 2017,
      "role": "Gap Identification",
      "relationship_sentence": "Diagnosed specific excess-variance components in reparameterization gradients and proposed a control-variate fix; this paper addresses the underlying gap by proving global ABC-style second-moment bounds that formalize when BBVI gradients are controlled."
    },
    {
      "title": "Stochastic First-Order Methods for Nonconvex Stochastic Programming",
      "authors": "Ghadimi et al.",
      "year": 2013,
      "role": "Foundation",
      "relationship_sentence": "Provided SGD convergence under bounded second-moment/variance assumptions; this paper shows BBVI gradients satisfy the corresponding ABC-type bound, enabling these SGD guarantees to apply to BBVI."
    },
    {
      "title": "Optimization Methods for Large-Scale Machine Learning",
      "authors": "Bottou et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Synthesized SGD theory and highlighted growth/variance conditions (A\u2013B\u2013C\u2013style bounds) used to prove convergence; the main theorem here verifies BBVI meets these conditions under smooth, quadratically-growing log-likelihoods."
    }
  ],
  "synthesis_narrative": "The core innovation of this paper is to rigorously bound the variance of BBVI gradients so that they satisfy the ABC-style second-moment conditions used to analyze SGD, and to do so for practical Gaussian parameterizations (including nonlinear covariance maps) while clarifying dimensional dependence for mean-field. This builds squarely on the BBVI paradigm introduced by Ranganath et al., where high-variance stochastic gradients were a central challenge, and on the pathwise gradient estimator of Kingma and Welling, whose structure the new bounds explicitly exploit. Practical BBVI as standardized by ADVI (Kucukelbir et al.) motivated analyzing widely used nonlinear variance parameterizations; the authors\u2019 results extend cleanly to such parameterizations and explain why mean-field can enjoy superior dimensional scaling. Methodologically, the extension to nonlinear covariance follows the formulation of generalized reparameterization gradients (Ruiz et al.), ensuring the bounds apply beyond simple location\u2013scale cases. On the optimization theory side, the work connects BBVI to SGD convergence frameworks: classical analyses such as Ghadimi and Lan\u2019s require bounded second moments, and Bottou, Curtis, and Nocedal\u2019s synthesis emphasizes growth/variance conditions (the ABC template). By proving that BBVI satisfies a matching ABC-type bound under smooth, quadratically growing log-likelihoods, the paper closes a specific gap flagged by variance-focused VI works like Roeder et al., turning heuristic practice into provable guarantees and enabling direct application of modern SGD theory to BBVI.",
  "analysis_timestamp": "2026-01-06T23:09:26.584308"
}
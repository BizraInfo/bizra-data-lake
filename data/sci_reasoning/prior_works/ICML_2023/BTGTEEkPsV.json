{
  "prior_works": [
    {
      "title": "Numerical Continuation Methods: An Introduction",
      "authors": "E. L. Allgower et al.",
      "year": 1990,
      "role": "Baseline",
      "relationship_sentence": "This classic homotopy/continuation framework is the baseline the paper departs from, replacing sequential, schedule-driven path following with a learned model that represents the entire continuation path."
    },
    {
      "title": "Visual Reconstruction",
      "authors": "Andrew Blake et al.",
      "year": 1987,
      "role": "Foundation",
      "relationship_sentence": "The graduated non-convexity idea introduced here establishes the easy-to-hard surrogate optimization principle that the paper generalizes by learning a continuous path over the homotopy parameter instead of hand-crafted schedules."
    },
    {
      "title": "Curriculum Learning",
      "authors": "Yoshua Bengio et al.",
      "year": 2009,
      "role": "Related Problem",
      "relationship_sentence": "This work popularized easy-to-hard scheduling in machine learning, whose reliance on a manually designed curriculum directly motivates the paper\u2019s goal to learn and exploit the entire continuation path without a fragile schedule."
    },
    {
      "title": "Graduated Optimization for Stochastic Non-Convex Problems",
      "authors": "Elad Hazan et al.",
      "year": 2016,
      "role": "Gap Identification",
      "relationship_sentence": "By formalizing graduated optimization and exposing its dependence on a schedule of surrogate problems, this paper highlights the sensitivity the new work addresses by optimizing all surrogate subproblems jointly via a learned path."
    },
    {
      "title": "Least Angle Regression",
      "authors": "Bradley Efron et al.",
      "year": 2004,
      "role": "Inspiration",
      "relationship_sentence": "LARS computes the entire LASSO regularization path, directly inspiring the idea that having access to a full solution path is powerful; the new paper generalizes this path-centric view beyond specific problems by learning the whole homotopy path."
    },
    {
      "title": "Fast solution of l1-norm minimization problems when the solution may be sparse",
      "authors": "David L. Donoho et al.",
      "year": 2008,
      "role": "Extension",
      "relationship_sentence": "Homotopy methods here track solutions as a regularization parameter varies; the new work extends this path-following concept by amortizing over the homotopy parameter with a model that can output any intermediate solution in real time."
    },
    {
      "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
      "authors": "Maziar Raissi et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "PINNs demonstrate learning a continuous solution function by enforcing residual conditions; analogously, the paper learns a solution map along the homotopy parameter by enforcing optimality across surrogate subproblems."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014learning a continuous model of the entire continuation path\u2014stands on the foundation of classical homotopy and graduated non-convexity. Allgower and Georg\u2019s continuation framework established path-following as a means to solve difficult problems via easier surrogates, while Blake and Zisserman\u2019s graduated non-convexity formulated the easy-to-hard surrogate principle. These works define the problem setting that the new paper retains but fundamentally rethinks: instead of unidirectional, schedule-driven traversal, learn the full path itself. Curriculum learning and graduated optimization in machine learning sharpened the motivation by showing both the promise and the brittleness of schedule design; their sensitivity to curriculum/temperature schedules constitutes the explicit gap the paper addresses. The value of entire solution paths was further crystallized by path algorithms like LARS and homotopy methods for l1 minimization, which compute full regularization trajectories; these directly inspire the paper\u2019s objective to make every intermediate solution available and useful, but now for general homotopies. Finally, physics-informed neural networks provided a concrete blueprint for representing continuous solution manifolds with a model trained by residuals across a domain. Combining these threads, the paper proposes a model-based approach that jointly optimizes the original objective and all surrogate subproblems, amortizes inference over the homotopy parameter, and enables real-time generation of any intermediate solution\u2014resolving schedule sensitivity while exploiting the entire continuation path.",
  "analysis_timestamp": "2026-01-06T23:09:26.564949"
}
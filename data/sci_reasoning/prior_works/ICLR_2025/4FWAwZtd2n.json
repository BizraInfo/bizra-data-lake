{
  "prior_works": [
    {
      "title": "Large Language Models are Zero-Shot Reasoners",
      "authors": "Takeshi Kojima et al.",
      "year": 2022,
      "arxiv_id": "2205.11916",
      "role": "Foundation",
      "relationship_sentence": "By introducing chain-of-thought prompting and stepwise reasoning traces, this work established the response format that the paper explicitly searches over and adapts during test-time compute scaling."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang et al.",
      "year": 2022,
      "arxiv_id": "2203.11171",
      "role": "Baseline",
      "relationship_sentence": "Self-consistency\u2019s multi-sample voting is the primary inference-time compute baseline that the paper analyzes and surpasses by optimizing compute allocation and adding verifier-guided search."
    },
    {
      "title": "Let\u2019s Verify Step by Step",
      "authors": "Jonathan Uesato et al.",
      "year": 2022,
      "arxiv_id": "2211.11610",
      "role": "Foundation",
      "relationship_sentence": "This paper introduced process-level verifiers that score intermediate reasoning steps, directly enabling the paper\u2019s use of dense process reward models (PRMs) to guide search at test time."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "arxiv_id": "2305.10601",
      "role": "Related Problem",
      "relationship_sentence": "By framing reasoning as search over partial thoughts with evaluators, this work informs the paper\u2019s idea of using a PRM as a dense value function to steer inference-time exploration."
    },
    {
      "title": "Competition-Level Code Generation with AlphaCode",
      "authors": "Yujia Li et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "AlphaCode\u2019s generate\u2011and\u2011test paradigm\u2014scaling samples and filtering with verifiers (unit tests)\u2014motivates the paper\u2019s core strategy of scaling test-time compute via verifier-guided selection for reasoning tasks."
    },
    {
      "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation",
      "authors": "Sumanth Dathathri et al.",
      "year": 2020,
      "arxiv_id": "1912.02164",
      "role": "Inspiration",
      "relationship_sentence": "PPLM demonstrated adjusting an LM\u2019s output distribution at inference using an external reward/critic, directly inspiring the paper\u2019s adaptive update of the response distribution under a fixed compute budget."
    }
  ],
  "synthesis_narrative": "Chain-of-thought prompting revealed that language models can expose multi-step reasoning traces that are amenable to manipulation and evaluation, establishing a structured space of intermediate steps to explore. Building on that, self-consistency showed that simply sampling multiple chains and aggregating answers can improve reasoning by exploiting variance across trajectories. In parallel, process-level verification introduced dense, stepwise signals that can score partial solutions during reasoning, providing a granular reward function rather than only outcome-level correctness. Tree of Thoughts recast reasoning as search over intermediate thoughts guided by an evaluator, suggesting a general interface for integrating such dense signals into exploration. From another direction, AlphaCode demonstrated that massive generate-and-test pipelines with verifiers can yield dramatic performance gains when test-time compute is effectively allocated to sampling and selection. Finally, PPLM established that one can steer a model\u2019s generation distribution at inference using external rewards, hinting at adaptive, on-the-fly optimization without further training.\nTogether these works suggest a gap: naive sampling or unguided self-reflection underuses test-time compute, while dense evaluators and verifier\u2011based filtering offer principled guidance. The paper synthesizes these insights by (1) searching over chains of thought using dense process reward models as value signals, and (2) adaptively updating the model\u2019s response distribution to allocate a fixed compute budget where it most increases verifier-assessed quality. This is a natural next step\u2014combining structured reasoning traces, verifier feedback, and inference-time steering\u2014to show that optimally scaled test-time compute can outperform parameter scaling for reasoning.",
  "target_paper": {
    "title": "Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning",
    "authors": "Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "test-time compute, LLMs, scaling, language models",
    "abstract": "Enabling LLMs to improve their outputs by using more test-time compute is a critical step towards building self-improving agents that can operate on open-ended natural language. In this paper, we scale up inference-time computation in LLMs, with a focus on answering: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on performance, but also on the future of LLM pretraining and how to tradeoff inference-time and pre-training compute. Little research has attempted to understand the scaling behaviors of test-time inference methods, with current work largely providing negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models (PRMs); and (2) updating the model's distribution over a response adaptively, giv",
    "openreview_id": "4FWAwZtd2n",
    "forum_id": "4FWAwZtd2n"
  },
  "analysis_timestamp": "2026-01-06T07:56:08.100534"
}
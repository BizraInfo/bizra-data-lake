{
  "prior_works": [
    {
      "title": "Neural Discrete Representation Learning",
      "authors": "van den Oord et al.",
      "year": 2017,
      "arxiv_id": "1711.00937",
      "role": "Foundation",
      "relationship_sentence": "LARP adopts discrete latent codebooks but departs by co-training with an autoregressive (AR) prior to shape the latent space for next-token prediction, directly addressing the tokenizer\u2013prior mismatch left by VQ-VAE\u2019s separate training."
    },
    {
      "title": "VideoGPT: Video Generation using VQ-VAE and Transformers",
      "authors": "Yan et al.",
      "year": 2021,
      "arxiv_id": "2104.10157",
      "role": "Foundation",
      "relationship_sentence": "VideoGPT formalized AR generation over patchwise VQ tokens for videos, which LARP retains while replacing patch tokens with holistic query-based discrete tokens to capture global semantics and shorten sequences."
    },
    {
      "title": "MAGVIT: Masked Generative Video Transformer",
      "authors": "Yu et al.",
      "year": 2023,
      "arxiv_id": "unknown",
      "role": "Baseline",
      "relationship_sentence": "MAGVIT is the primary patchwise video tokenizer baseline whose fixed grid and local code limitations LARP directly addresses via learned holistic queries and support for an arbitrary number of discrete tokens."
    },
    {
      "title": "MAGVIT-v2",
      "authors": "Yu et al.",
      "year": 2024,
      "arxiv_id": "unknown",
      "role": "Gap Identification",
      "relationship_sentence": "Despite improved compression with residual quantization, MAGVIT-v2 remains patch-based with fixed token layouts, a limitation LARP explicitly overcomes by moving to learned holistic queries and AR-prior-aligned discrete latents."
    },
    {
      "title": "Perceiver: General Perception with Iterative Attention",
      "authors": "Jaegle et al.",
      "year": 2021,
      "arxiv_id": "2103.03206",
      "role": "Inspiration",
      "relationship_sentence": "LARP borrows Perceiver\u2019s core idea of a learnable latent array that cross-attends to high-dimensional inputs to construct its holistic query set before quantization."
    },
    {
      "title": "TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?",
      "authors": "Ryoo et al.",
      "year": 2021,
      "arxiv_id": "2106.11297",
      "role": "Inspiration",
      "relationship_sentence": "LARP extends TokenLearner\u2019s adaptive spatiotemporal token selection by producing discrete, AR-aligned tokens that enable generative modeling with a variable number of tokens."
    },
    {
      "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "authors": "Alayrac et al.",
      "year": 2022,
      "arxiv_id": "2204.14198",
      "role": "Related Problem",
      "relationship_sentence": "Flamingo\u2019s Perceiver Resampler shows that a small set of learned queries can summarize visual inputs for sequence models, a mechanism LARP applies inside a tokenizer and couples with a next-token AR prior."
    }
  ],
  "synthesis_narrative": "Neural Discrete Representation Learning established vector-quantized latent codes and the use of a separate autoregressive prior over them, enabling discrete generative modeling but leaving a gap between code learning and next-token prediction. VideoGPT brought this paradigm to video, modeling sequences of patchwise VQ tokens with transformers, highlighting that locality and long token streams hinder temporal coherence and semantics. MAGVIT advanced video tokenization for AR generation but still encoded fixed-grid spatiotemporal patches, and MAGVIT\u2011v2 improved compression via residual quantization while remaining patch-based and fixed in token count. In parallel, Perceiver introduced a learnable latent array that cross-attends to inputs, demonstrating that a small set of learned queries can absorb global information efficiently. TokenLearner showed that adaptive, learned spatiotemporal tokens can summarize videos with far fewer tokens than patches. Flamingo operationalized learned-query resampling in large sequence models, validating that such holistic queries yield compact, sequence-friendly visual representations.\nTaken together, these works revealed two opportunities: replace patchwise tokens with a small, learned set of holistic queries to capture global video semantics and reduce sequence length, and align the discrete latent space with next-token prediction rather than learning it in isolation. LARP synthesizes these insights by using learned holistic queries to gather video information before quantization and by integrating a lightweight AR transformer during training so the discrete space is explicitly shaped for AR generation, while permitting an arbitrary number of tokens for adaptive efficiency.",
  "target_paper": {
    "title": "LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior",
    "authors": "Hanyu Wang, Saksham Suri, Yixuan Ren, Hao Chen, Abhinav Shrivastava",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Video Generation, Visual Tokenization",
    "abstract": "We present LARP, a novel video tokenizer designed to overcome limitations in current video tokenization methods for autoregressive (AR) generative models. Unlike traditional patchwise tokenizers that directly encode local visual patches into discrete tokens, LARP introduces a holistic tokenization scheme that gathers information from the visual content using a set of learned holistic queries. This design allows LARP to capture more global and semantic representations, rather than being limited to local patch-level information. Furthermore, it offers flexibility by supporting an arbitrary number of discrete tokens, enabling adaptive and efficient tokenization based on the specific requirements of the task. To align the discrete token space with downstream AR generation tasks, LARP integrates a lightweight AR transformer as a training-time prior model that predicts the next token on its discrete latent space. By incorporating the prior model during training, LARP learns a latent space th",
    "openreview_id": "Wr3UuEx72f",
    "forum_id": "Wr3UuEx72f"
  },
  "analysis_timestamp": "2026-01-06T18:32:31.044371"
}
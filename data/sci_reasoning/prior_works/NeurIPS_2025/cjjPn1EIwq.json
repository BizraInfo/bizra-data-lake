{
  "prior_works": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "role": "Base vision\u2013language model and promptable alignment backbone",
      "relationship_sentence": "SGCLIP is explicitly built on CLIP\u2019s contrastive text\u2013image embedding and promptability, inheriting its open-domain recognition and leveraging prompts to generate scene-graph nodes and relations."
    },
    {
      "title": "Scene Graph Generation by Iterative Message Passing",
      "authors": "Danfei Xu, Yuke Zhu, Christopher B. Choy, Li Fei-Fei",
      "year": 2017,
      "role": "Foundational scene-graph generation formulation and graph reasoning",
      "relationship_sentence": "ESCA/SGCLIP extends the core formulation of detecting entities and predicates with structured message passing by re-grounding it in a CLIP-based, open-domain and promptable framework."
    },
    {
      "title": "Neural Motifs: Scene Graph Parsing with Global Context",
      "authors": "Rowan Zellers, Mark Yatskar, Sam Thomson, Yejin Choi",
      "year": 2018,
      "role": "Context modeling and frequency-aware relation prediction for SGG",
      "relationship_sentence": "SGCLIP addresses the bias and context issues highlighted by Neural Motifs by replacing closed-set frequency priors with CLIP-driven open-vocabulary cues and prompt-conditioned context."
    },
    {
      "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
      "authors": "Ranjay Krishna et al.",
      "year": 2017,
      "role": "Canonical dataset and representation for objects\u2013attributes\u2013relations (scene graphs)",
      "relationship_sentence": "ESCA adopts the Visual Genome scene-graph schema (objects, attributes, relations) but learns it without human labels by aligning to captions, generalizing the representation to open-domain video."
    },
    {
      "title": "Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision",
      "authors": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, Jiajun Wu",
      "year": 2019,
      "role": "Neuro-symbolic alignment between language and structured scene representations",
      "relationship_sentence": "ESCA\u2019s neurosymbolic pipeline that aligns captions with self-produced scene graphs draws directly on NS-CL\u2019s principle of coupling neural perception with symbolic structure via language."
    },
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": "Junnan Li, Dongxu Li, Silvio Savarese, Steven C. H. Hoi",
      "year": 2023,
      "role": "High-quality automatic caption generation for vision\u2013language supervision",
      "relationship_sentence": "SGCLIP relies on automatically generated captions to supervise open-domain relations; BLIP-2 establishes the capability and methodology for producing rich captions that enable such weak supervision."
    },
    {
      "title": "Self-Training With Noisy Student Improves ImageNet Classification",
      "authors": "Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V. Le",
      "year": 2020,
      "role": "Pseudo-labeling/self-training paradigm for label-efficient learning",
      "relationship_sentence": "ESCA\u2019s training loop\u2014aligning captions with the model\u2019s own scene-graph predictions to avoid human labels\u2014follows the Noisy Student principle of bootstrapping from self-generated pseudo labels."
    }
  ],
  "synthesis_narrative": "ESCA\u2019s core idea is to strengthen embodied agents\u2019 grounding by generating spatio-temporal scene graphs with an open-domain, promptable foundation model (SGCLIP) trained without human annotations. This builds directly on CLIP\u2019s text\u2013image alignment and promptability, which provide the open-vocabulary scaffold SGCLIP needs to name objects, attributes, and relations from free-form prompts. The scene-graph formulation itself follows the classic SGG paradigm introduced by Iterative Message Passing and later refined by Neural Motifs, but ESCA departs from closed-set, frequency-driven relation prediction by leveraging CLIP\u2019s semantic space and prompt conditioning for robust open-domain relations. The representational schema and evaluation tradition trace to Visual Genome, whose objects\u2013attributes\u2013relations ontology ESCA generalizes from static images to videos and embodied settings. Critically, ESCA\u2019s label-free, neurosymbolic training pipeline aligns automatically generated captions with the model\u2019s own scene graphs\u2014a direct conceptual echo of NS-CL, which married neural perception with symbolic structure through language. High-quality captions from modern captioners such as BLIP-2 enable rich, open-domain textual supervision that describes entities and interactions in diverse videos. Finally, the self-training dynamic\u2014using the model\u2019s predictions as supervisory signals\u2014draws on Noisy Student\u2019s pseudo-labeling recipe to scale learning without manual annotations. Together, these lines of work converge in ESCA: a promptable, CLIP-based scene-graph generator trained via neurosymbolic alignment to captions, delivering stronger grounding for embodied agents and state-of-the-art performance in scene-graph generation and action localization.",
  "analysis_timestamp": "2026-01-07T00:21:32.267387"
}
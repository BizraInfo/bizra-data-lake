{
  "prior_works": [
    {
      "title": "Attention Is All You Need",
      "authors": "Ashish Vaswani et al.",
      "year": 2017,
      "arxiv_id": "1706.03762",
      "role": "Foundation",
      "relationship_sentence": "This paper defines the single-head self-attention computation over queries, keys, and values with softmax weights, which is exactly the module whose parameter-space Hessian this work derives and analyzes in closed form."
    },
    {
      "title": "Limitations of the Empirical Fisher Information for Curvature in Deep Learning",
      "authors": "Fredrik K\u00fcnstner et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Their formal decomposition of curvature into generalized Gauss\u2013Newton versus true Hessian terms and their matrix-calculus, layer-wise recipes provide the mathematical machinery that is adapted here to obtain an exact Hessian for self-attention and to attribute its components."
    },
    {
      "title": "BackPACK: Packing more into Backprop",
      "authors": "Felix Dangel et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "This modular second-order backpropagation framework for exact per-layer curvature motivates and is directly extended from linear/conv modules to the attention block, enabling the compact matrix-derivative expression of the attention Hessian used here."
    },
    {
      "title": "An Investigation into Neural Net Hessians",
      "authors": "Amir Ghorbani et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Their finding that network Hessians exhibit structured outliers tied to data/labels and a data-dependent bulk directly motivates the attention-layer curvature separation into data-, weight-, and attention-specific terms and the ensuing spectral interpretation."
    },
    {
      "title": "On Layer Normalization in the Transformer Architecture",
      "authors": "Ruibin Xiong et al.",
      "year": 2020,
      "arxiv_id": "2002.04745",
      "role": "Gap Identification",
      "relationship_sentence": "By documenting the instability of post-LN Transformers and the reliance on warmup and adaptive optimizers, this work identifies the concrete training pathologies that the present Hessian analysis of self-attention seeks to mechanistically explain."
    },
    {
      "title": "Tensor Programs V: Taming Transformers",
      "authors": "Greg Yang",
      "year": 2021,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "This study analyzes pre/post-LN Transformer stability via Jacobian/NTK dynamics at initialization, providing a theoretical backdrop that is complemented here by a Hessian-level characterization specific to self-attention."
    },
    {
      "title": "ReZero is All You Need: Fast Convergence at Large Depth",
      "authors": "Johannes Bachlechner et al.",
      "year": 2020,
      "arxiv_id": "2003.04887",
      "role": "Related Problem",
      "relationship_sentence": "By showing that residual scaling can stabilize deep Transformer training without normalization, this paper supplies a key empirical phenomenon that the present curvature formulas rationalize via the attenuation of sharp Hessian modes in attention."
    }
  ],
  "synthesis_narrative": "Self-attention, as introduced in Attention Is All You Need, computes softmax-normalized interactions between queries, keys, and values, establishing the exact nonlinear layer whose internal derivatives and interactions must be handled to study curvature. Work on curvature foundations showed how deep-network Hessians can be decomposed and computed layer-wise: K\u00fcnschner et al. formalized the generalized Gauss\u2013Newton versus true Hessian split and provided matrix-calculus recipes for propagating curvature through modules, and BackPACK demonstrated a modular second-order backprop framework that yields compact, Kronecker-structured expressions for common layers. Empirically and conceptually, An Investigation into Neural Net Hessians revealed that neural Hessians have structured outliers tied to data and a bulk term, suggesting interpretable decompositions of curvature into components. Transformer-specific training studies then exposed distinctive pathologies: On Layer Normalization in the Transformer Architecture documented instability of post-LN Transformers and the need for warmup/Adam, while Tensor Programs V characterized pre/post-LN stability via Jacobian/NTK analysis at initialization. Complementing these, ReZero showed that residual scaling can stabilize deep Transformers without normalization, highlighting the role of curvature/scale.\nTogether, these works suggest that a precise, layer-level Hessian for self-attention\u2014derived with modular matrix calculus\u2014could expose how attention-specific nonlinearities induce distinct curvature components, explain spectral structure (bulk versus outliers), and clarify why normalization, warmup, and adaptive methods stabilize training. Building on the curvature machinery and guided by the documented Transformer instabilities and stabilization tricks, the present work naturally synthesizes these insights into an exact Hessian characterization that isolates data, weight, and attention terms to mechanistically account for Transformers\u2019 unique optimization behavior.",
  "target_paper": {
    "title": "What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis",
    "authors": "Weronika Ormaniec, Felix Dangel, Sidak Pal Singh",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Hessian, Transformers",
    "abstract": "The Transformer architecture has inarguably revolutionized deep learning, overtaking classical architectures like multi-layer perceptions (MLPs) and convolutional neural networks (CNNs). At its core, the attention block differs in form and functionality from most other architectural components in deep learning\u2014to the extent that, in comparison to MLPs/CNNs, Transformers are more often accompanied by adaptive optimizers, layer normalization, learning rate warmup, etc. The root causes behind these outward manifestations and the precise mechanisms that govern them remain poorly understood. In this work, we bridge this gap by providing a fundamental understanding of what distinguishes the Transformer from the other architectures\u2014grounded in a theoretical comparison of the (loss) Hessian. Concretely, for a single self-attention layer, (a) we first entirely derive the Transformer\u2019s Hessian and express it in matrix derivatives; (b) we then characterize it in terms of data, weight, and attenti",
    "openreview_id": "3ddi7Uss2A",
    "forum_id": "3ddi7Uss2A"
  },
  "analysis_timestamp": "2026-01-06T12:28:30.468262"
}
{
  "prior_works": [
    {
      "title": "Highly accurate protein structure prediction with AlphaFold",
      "authors": "John Jumper et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "AlphaFold is the single-state structure predictor that AlphaFlow explicitly repurposes and fine-tunes under a flow-matching loss to become a sequence\u2011conditioned generative model of conformational ensembles."
    },
    {
      "title": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
      "authors": "Zeming Lin et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "ESMFold provides the second single-state backbone the authors adapt; ESMFlow is obtained by fine-tuning ESMFold within the same flow-matching framework to sample ensembles from a fixed sequence."
    },
    {
      "title": "Flow Matching for Generative Modeling",
      "authors": "Yaron Lipman et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "This work supplies the core objective\u2014(conditional) flow matching\u2014to learn a velocity field transporting noise to data; the paper\u2019s \u201ccustom flow matching framework\u201d is a tailored application of this idea to protein structure space."
    },
    {
      "title": "Stochastic Interpolants: A Unifying Framework for Flows and Diffusions in Generative Modeling",
      "authors": "Jonathan M. Albergo et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Stochastic interpolants motivate supervising velocities along simple data\u2013base paths, the theoretical underpinning the authors leverage to make minibatch-trainable flow objectives for protein structures."
    },
    {
      "title": "Broadly applicable and accurate protein design by integrating structure prediction networks and diffusion",
      "authors": "Brian L. H. Watson et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "RFdiffusion demonstrated that a structure prediction network can be repurposed as a generative denoiser; this paper extends that insight by repurposing AlphaFold/ESMFold under a continuous flow-matching objective to generate sequence\u2011conditioned ensembles rather than novel designs."
    },
    {
      "title": "Sampling alternative conformational states of transporters and receptors with AlphaFold2",
      "authors": "Germ\u00e1n del Alamo et al.",
      "year": 2022,
      "role": "Baseline",
      "relationship_sentence": "This work established AlphaFold2 with MSA subsampling as a practical route to diversity; the current paper uses it as the primary baseline and directly targets its shortcomings in precision\u2013diversity tradeoffs and ensemble calibration."
    },
    {
      "title": "Boltzmann Generators: Sampling Equilibrium States of Many-Body Systems with Deep Learning",
      "authors": "Frank No\u00e9 et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "Boltzmann Generators framed learning generative models of equilibrium conformational ensembles and accelerating convergence relative to MD; the present work adopts this goal when training on MD ensembles and evaluating wall\u2011clock convergence of ensemble observables."
    }
  ],
  "synthesis_narrative": "AlphaFold Meets Flow Matching hinges on two threads of prior work: highly accurate single\u2011state predictors and continuous\u2011time generative training objectives. AlphaFold2 and ESMFold provided the core, sequence\u2011to\u2011structure backbones that made high\u2011fidelity predictions feasible; the present paper\u2019s central move is to repurpose these deterministic predictors into sequence\u2011conditioned generators by fine\u2011tuning them with a flow\u2011matching loss. The flow machinery comes directly from Flow Matching, which supplies the velocity\u2011field learning objective, and from Stochastic Interpolants, which motivate supervising velocities along simple stochastic paths that make minibatch training practical. Conceptually, the feasibility of turning a structure predictor into a generative model was de\u2011risked by RFdiffusion, which showed RoseTTAFold could act as a denoiser inside a diffusion pipeline; this paper generalizes that paradigm to AlphaFold/ESMFold and swaps diffusion for a flow\u2011matching formulation tailored to protein structure space. As a baseline and foil, del Alamo et al. established AlphaFold2 with MSA subsampling as a way to obtain alternative conformations; the current work explicitly improves on its precision\u2013diversity tradeoff and addresses its lack of calibrated ensemble statistics. Finally, Boltzmann Generators articulated the objective of learning models that reproduce equilibrium conformational ensembles and accelerate convergence relative to MD; by training on MD ensembles and demonstrating faster wall\u2011clock convergence of observables, the paper situates its contribution in that tradition while delivering a practical, sequence\u2011conditioned generator.",
  "analysis_timestamp": "2026-01-06T23:09:26.458733"
}
{
  "prior_works": [
    {
      "title": "A 128\u00d7128 120 dB 15 \u00b5s Latency Asynchronous Temporal Contrast Vision Sensor (DVS)",
      "authors": "Patrick Lichtsteiner, Christoph Posch, Tobi Delbruck",
      "year": 2008,
      "role": "Foundational sensor technology",
      "relationship_sentence": "The proposed end-to-end neuromorphic navigation relies on the asynchronous, low-latency event stream pioneered by the DVS, enabling millisecond perception loops necessary for 2.3 ms closed-loop avoidance."
    },
    {
      "title": "Asynchronous event-based optical flow",
      "authors": "Ryad Benosman, Charles Clercq, Xavier Lagorce, Sio-Hoi Ieng, Chiara Bartolozzi",
      "year": 2014,
      "role": "Algorithmic precursor for motion cues and time-to-contact",
      "relationship_sentence": "Their event-based motion/optic-flow formulations directly underpin the paper\u2019s bio-inspired strategy of detecting and avoiding moving obstacles using motion cues without explicit recognition or trajectory computation."
    },
    {
      "title": "LGMD-based visual neural networks for collision detection",
      "authors": "Shigang Yue, F. C. Rind (and subsequent UAV-focused extensions by Yue et al.)",
      "year": 2006,
      "role": "Bio-inspired collision detection model",
      "relationship_sentence": "The paper\u2019s looming-based avoidance mechanism is a neuromorphic instantiation of the LGMD principle, enabling fast collision responses from local motion signals rather than object identity or explicit path prediction."
    },
    {
      "title": "ESIM: an Open Event Camera Simulator",
      "authors": "Henri Rebecq, Daniel Gehrig, Davide Scaramuzza",
      "year": 2018,
      "role": "Data generation tool for labeled event streams",
      "relationship_sentence": "ESIM\u2019s methodology for synthesizing events with ground-truth labels directly informs the creation of the paper\u2019s monocular event-based pose correction dataset with large-scale paired supervision."
    },
    {
      "title": "The Multi Vehicle Stereo Event Camera Dataset (MVSEC)",
      "authors": "Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, Kostas Daniilidis",
      "year": 2018,
      "role": "Benchmark dataset for event-based VO/pose",
      "relationship_sentence": "MVSEC\u2019s design and protocols for pairing events with accurate poses influenced the dataset curation and evaluation strategies for the paper\u2019s monocular event-based pose correction resource."
    },
    {
      "title": "Loihi: A Neuromorphic Manycore Processor with On-Chip Learning",
      "authors": "Mike Davies, Narayan Srinivasa, et al.",
      "year": 2018,
      "role": "Neuromorphic hardware enabler",
      "relationship_sentence": "Demonstrations of ultra-low-latency, low-power spiking computation on Loihi inform the paper\u2019s fully onboard neuromorphic pipeline and its stringent latency/energy budget for real-time UAV navigation."
    },
    {
      "title": "How Fast Is Too Fast? The Role of Perception-Action Latency in High-Speed Robot Navigation",
      "authors": "Davide Falanga, Kevin Kleber, Davide Scaramuzza",
      "year": 2020,
      "role": "Latency-performance tradeoff in agile flight",
      "relationship_sentence": "This work motivates the paper\u2019s emphasis on minimizing end-to-end perception-action latency, shaping the architectural choices that achieve 2.3 ms closed-loop obstacle avoidance on a quadrotor."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014fully neuromorphic, end-to-end dynamic obstacle avoidance with millisecond latency\u2014rests on three converging threads: event-based sensing, bio-inspired motion-driven collision avoidance, and neuromorphic computing with dataset support for pose correction. The Dynamic Vision Sensor introduced the asynchronous, low-latency signal modality that makes such closed-loop systems feasible. On top of this, event-based motion estimation and time-to-contact formulations established by Benosman and colleagues provided the task-relevant motion cues to trigger avoidance without explicit object recognition or trajectory prediction. In parallel, the LGMD line of work from Yue and Rind translated biological looming detection into spiking neural network models that deliver rapid collision responses from local motion patterns\u2014precisely the bio-inspired strategy leveraged here for dynamic obstacle avoidance.\nTo robustly maintain navigation state with event cameras, labeled data are essential. ESIM enabled scalable synthesis of event streams tied to ground-truth labels, while MVSEC set standards for pairing events with accurate poses; together they underpin the methodology for the paper\u2019s new monocular event-based pose correction dataset at unprecedented scale. Finally, practical deployment hinges on energy-latency constraints in agile flight. Loihi exemplified the viability of ultra-low-power, low-latency SNN execution on neuromorphic hardware, and Falanga et al. quantified how perception-action latency bounds flight performance\u2014both directly informing the system\u2019s fully onboard neuromorphic design and its 2.3 ms end-to-end loop. Integrated, these works directly enable the paper\u2019s neuromorphic navigation and dynamic obstacle avoidance breakthrough.",
  "analysis_timestamp": "2026-01-07T00:29:42.071679"
}
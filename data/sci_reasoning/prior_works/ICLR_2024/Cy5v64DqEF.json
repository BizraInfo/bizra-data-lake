{
  "prior_works": [
    {
      "title": "High-Fidelity Image Compression with Generative Adversarial Networks (HiFiC)",
      "authors": "Fabian Mentzer et al.",
      "year": 2020,
      "arxiv_id": "2006.09965",
      "role": "Baseline",
      "relationship_sentence": "HiFiC instantiated the dominant conditional generative codec for perceptual quality that this paper both analyzes (proving such codecs are idempotent) and surpasses while replacing the conditional generator with an unconditional model plus an idempotence constraint."
    },
    {
      "title": "Generative Adversarial Networks for Extreme Learned Image Compression",
      "authors": "Eirikur Agustsson et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work established the paradigm of perceptual image compression via conditional GAN decoders at very low rates, providing the specific conditional generative formulation that the current paper proves is equivalent to enforcing idempotence on an unconditional generator."
    },
    {
      "title": "The Rate-Distortion-Perception Tradeoff",
      "authors": "Yochai Blau et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "By formalizing the rate\u2013distortion\u2013perception objective and motivating perceptual metrics like FID, this work underpins the paper\u2019s focus on perceptual codecs and frames why an idempotence-based generative approach can prioritize perception over pixel distortion."
    },
    {
      "title": "Compressed Sensing using Generative Models",
      "authors": "Ashish Bora et al.",
      "year": 2017,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "This paper\u2019s blueprint\u2014recovering signals by inverting an unconditional generative model under a measurement-consistency constraint\u2014directly inspires treating a codec as the constraint and performing generator inversion under idempotence."
    },
    {
      "title": "Diffusion Posterior Sampling for General Noisy Linear Inverse Problems",
      "authors": "Hyungjin Chung et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "DPS provides an algorithmic template for enforcing data-consistency while sampling from an unconditional diffusion prior, which the current work extends to the codec setting by using idempotence as the consistency constraint during inversion."
    },
    {
      "title": "Plug-and-Play Priors for Model Based Reconstruction",
      "authors": "Sreehari Venkatakrishnan et al.",
      "year": 2013,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "PnP introduced the principle of combining a pre-trained prior with a measurement-consistency constraint without retraining, a philosophy mirrored by coupling a fixed MSE codec with a pre-trained unconditional generator under idempotence."
    }
  ],
  "synthesis_narrative": "Conditional generative codecs for perceptual image compression were first crystallized by GAN-based approaches that optimized for realism rather than pixel fidelity. HiFiC demonstrated a practical conditional generator conditioned on quantized latents to yield high perceptual quality, and earlier work on extreme learned compression with GANs formalized the low-bit-rate, generator-driven reconstruction paradigm. The theoretical groundwork for prioritizing perception came from the rate\u2013distortion\u2013perception framework, which clarified why metrics like FID are appropriate targets and legitimized abandoning strict MSE in favor of perceptual realism. In parallel, the inverse-problems community showed that one can invert an unconditional generative model by enforcing measurement consistency: compressed sensing with generative models introduced latent inversion onto a generator\u2019s range, and diffusion posterior sampling provided concrete sampling algorithms that maintain data-consistency with an unconditional diffusion prior. Plug-and-Play Priors established the general tactic of coupling fixed priors with consistency constraints without retraining.\n\nTaken together, these works suggest a path where a codec can be viewed as a forward operator and reconstruction can be obtained by inverting an unconditional generator while enforcing consistency with the compressed representation. The current paper identifies idempotence as the precise consistency condition and proves that enforcing idempotence on an unconditional generator is theoretically equivalent to using a conditional generative codec. This insight enables a new, training-free perceptual compression paradigm that pairs a pre-trained MSE codec with an unconditional generative model, achieving SOTA perceptual quality while bypassing training a conditional generator.",
  "target_paper": {
    "title": "Idempotence and Perceptual Image Compression",
    "authors": "Tongda Xu, Ziran Zhu, Dailan He, Yanghao Li, Lina Guo, Yuanyuan Wang, Zhe Wang, Hongwei Qin, Yan Wang, Jingjing Liu, Ya-Qin Zhang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "perceptual image compression, neural image compression",
    "abstract": "Idempotence is the stability of image codec to re-compression. At the first glance, it is unrelated to perceptual image compression. However, we find that theoretically: 1) Conditional generative model-based perceptual codec satisfies idempotence; 2) Unconditional generative model with idempotence constraint is equivalent to conditional generative codec. Based on this newfound equivalence, we propose a new paradigm of perceptual image codec by inverting unconditional generative model with idempotence constraints. Our codec is theoretically equivalent to conditional generative codec, and it does not require training new models. Instead, it only requires a pre-trained mean-square-error codec and unconditional generative model. Empirically, we show that our proposed approach outperforms state-of-the-art methods such as HiFiC and ILLM, in terms of Fr\u00e9chet Inception Distance (FID). The source code is provided in https://github.com/tongdaxu/Idempotence-and-Perceptual-Image-Compression.",
    "openreview_id": "Cy5v64DqEF",
    "forum_id": "Cy5v64DqEF"
  },
  "analysis_timestamp": "2026-01-06T23:31:57.592296"
}
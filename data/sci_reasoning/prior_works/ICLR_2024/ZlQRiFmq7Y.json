{
  "prior_works": [
    {
      "title": "Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations",
      "authors": "Francesco Locatello et al.",
      "year": 2019,
      "arxiv_id": "1811.12359",
      "role": "Gap Identification",
      "relationship_sentence": "This work proved that unsupervised disentanglement is impossible without inductive biases or supervision, directly motivating the use of natural language as an accessible supervisory signal to identify and separate factors of variation."
    },
    {
      "title": "SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval",
      "authors": "Thibault Formal et al.",
      "year": 2021,
      "arxiv_id": "2109.10086",
      "role": "Extension",
      "relationship_sentence": "SPLADE\u2019s core idea of representing inputs as sparse activations over a vocabulary directly informs VDR\u2019s key design of embedding both data and text into a shared vocabulary space so that individual token dimensions act as disentangled, interpretable factors."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "arxiv_id": "2103.00020",
      "role": "Inspiration",
      "relationship_sentence": "CLIP demonstrated that free-form text paired with images provides scalable supervision, inspiring the use of natural language descriptions as proxies of data-generating factors while highlighting the limitations of dense embeddings for dimension-wise disentanglement."
    },
    {
      "title": "Dense Passage Retrieval for Open-Domain Question Answering",
      "authors": "Vladimir Karpukhin et al.",
      "year": 2020,
      "arxiv_id": "2004.04906",
      "role": "Foundation",
      "relationship_sentence": "DPR popularized bi-encoder retrieval with in-batch negatives, a training paradigm that VDR adopts to align data instances with their textual counterparts within a shared (vocabulary) representation space."
    },
    {
      "title": "Concept Bottleneck Models",
      "authors": "Pang Wei Koh et al.",
      "year": 2020,
      "arxiv_id": "2007.04612",
      "role": "Inspiration",
      "relationship_sentence": "CBMs showed that supervision over human-interpretable concepts can structure representations, which VDR echoes by using vocabulary tokens as an implicit, scalable concept bottleneck without manual concept labels."
    },
    {
      "title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework",
      "authors": "Irina Higgins et al.",
      "year": 2017,
      "arxiv_id": "1611.02731",
      "role": "Baseline",
      "relationship_sentence": "beta-VAE introduced axis-aligned disentanglement via generative modeling but struggles on complex real data, providing the baseline and problem setup that VDR rethinks by replacing generative losses with retrieval in a token-aligned space."
    }
  ],
  "synthesis_narrative": "Unsupervised disentanglement was shown to be unattainable without inductive biases or supervision, as formalized by Locatello et al., crystallizing the need for auxiliary signals that reveal the underlying factors of variation. beta-VAE established axis-aligned latent factors via a constrained VAE objective, framing the disentanglement task but relying on strong generative assumptions that falter on complex, real-world data. Concept Bottleneck Models demonstrated that injecting human-defined concepts as a bottleneck can make representations interpretable and factorized, though they depend on manually curated concept labels. CLIP revealed that free-form natural language paired with images provides scalable supervision capable of aligning visual and textual semantics, yet its dense embedding space offers limited dimension-wise interpretability. SPLADE introduced sparse lexical representations where each dimension corresponds to a vocabulary token with learned importance, proving that retrieval can be driven by token-aligned, interpretable activations. DPR provided a simple and effective bi-encoder retrieval paradigm with in-batch negatives to align two modalities.\nTogether, these works suggest a path: use language as the scalable supervision source identified by CLIP, structure the representation as token-aligned sparse vectors following SPLADE, and train with DPR-style bi-encoder retrieval, thereby creating axis-level interpretability reminiscent of CBMs without manual concepts while addressing Locatello\u2019s supervision requirement and avoiding beta-VAE\u2019s generative constraints. This synthesis naturally yields a retrieval-based framework where vocabulary tokens act as proxies for factors of variation, enabling disentangled, interpretable representations on real data.",
  "target_paper": {
    "title": "Retrieval-based Disentangled Representation Learning with Natural Language Supervision",
    "authors": "Jiawei Zhou, Xiaoguang Li, Lifeng Shang, Xin Jiang, Qun Liu, Lei Chen",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Disentangled representation learning, information retriever, sparse retriever",
    "abstract": "Disentangled representation learning remains challenging as the underlying factors of variation in the data do not naturally exist. The inherent complexity of real-world data makes it unfeasible to exhaustively enumerate and encapsulate all its variations within a finite set of factors. However, it is worth noting that most real-world data have linguistic equivalents, typically in the form of textual descriptions. These linguistic counterparts can represent the data and effortlessly decomposed into distinct tokens. In light of this, we present Vocabulary Disentangled Retrieval (VDR), a retrieval-based framework that harnesses natural language as proxies of the underlying data variation to drive disentangled representation learning. Our approach employ a bi-encoder model to represent both data and natural language in a vocabulary space, enabling the model to distinguish dimensions that capture intrinsic characteristics within data through its natural language counterpart, thus facilitat",
    "openreview_id": "ZlQRiFmq7Y",
    "forum_id": "ZlQRiFmq7Y"
  },
  "analysis_timestamp": "2026-01-06T11:13:05.321715"
}
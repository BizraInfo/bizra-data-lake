{
  "prior_works": [
    {
      "title": "Efficient Noise-Tolerant Learning from Statistical Queries",
      "authors": "Michael Kearns",
      "year": 1998,
      "role": "Theoretical foundation (SQ framework and parity hardness)",
      "relationship_sentence": "This paper provides the Statistical Query model and classical hardness intuition for parity-like problems, which the NeurIPS 2023 paper leverages to cast gradient-based training as SQ and derive a computational\u2013statistical lower bound underlying their multi-resource tradeoff."
    },
    {
      "title": "A General Characterization of the Statistical Query Complexity",
      "authors": "Vitaly Feldman",
      "year": 2017,
      "role": "Analytical tool for lower bounds in SQ",
      "relationship_sentence": "Feldman\u2019s characterization supplies general machinery to formalize SQ lower bounds, enabling the authors to interpret their limits for gradient-based learning of sparse parities as a Pareto frontier over data, compute, width, and randomness."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Conceptual contrast: infinite-width kernel (lazy) regime",
      "relationship_sentence": "NTK clarifies that infinite width induces kernel-like (lazy) training, against which the paper positions its feature-learning regime and analyzes how finite-but-growing width can act as parallel search rather than degenerating to a fixed kernel."
    },
    {
      "title": "On the Lazy Training of Neural Networks: Dynamics and Generalization in Over-parameterized Systems",
      "authors": "L\u00e9na\u00efc Chizat, Francis Bach",
      "year": 2019,
      "role": "Mechanistic insight: initialization scale and feature learning vs. lazy regimes",
      "relationship_sentence": "This work shows how initialization and scaling govern whether networks learn features or stay lazy, directly informing the paper\u2019s theoretical and empirical choice of sparse initialization to enable feature discovery and improve sample efficiency."
    },
    {
      "title": "Random Features for Large-Scale Kernel Machines",
      "authors": "Ali Rahimi, Benjamin Recht",
      "year": 2007,
      "role": "Methodological precedent: width as parallel random feature search",
      "relationship_sentence": "The random features perspective underpins the paper\u2019s view of width as parallel search, where more neurons increase the chance of containing useful \u2018lottery\u2019 features that can be amplified by training."
    },
    {
      "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
      "authors": "Jonathan Frankle, Michael Carbin",
      "year": 2019,
      "role": "Empirical concept: existence of lucky sparse subnetworks at initialization",
      "relationship_sentence": "The lottery ticket hypothesis motivates the paper\u2019s \u2018lucky\u2019 dimension and its analysis showing that wider networks and sparse initializations increase the probability of containing winning neurons that drive sample-efficient learning."
    },
    {
      "title": "Scaling Laws for Neural Language Models",
      "authors": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei",
      "year": 2020,
      "role": "Motivating paradigm: compute\u2013data\u2013model tradeoffs",
      "relationship_sentence": "Empirical scaling laws inspire the paper\u2019s formalization of a Pareto frontier across resources, which it makes rigorous in a controlled sparse-parity setting while adding width and luck as explicit axes."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014a formal multi-resource Pareto frontier for feature learning and a theory of width as parallel search with \u2018luck\u2019\u2014rests on two pillars: statistical query lower bounds and a modern view of overparameterized training dynamics. Kearns\u2019 SQ framework and Feldman\u2019s general characterization supply the apparatus to show that gradient-based training on sparse parities faces intrinsic computational\u2013statistical gaps, which the authors reinterpret as a frontier trading off data, compute (iterations), model size (width), and randomness. Against the kernelized, infinite-width intuition of NTK, the work purposefully targets a feature-learning regime. Chizat and Bach\u2019s analysis of lazy versus rich regimes via initialization scale directly motivates sparse initialization as the mechanism to escape laziness, enabling neurons to specialize and discover relevant sparse features. Rahimi and Recht\u2019s random features lens provides the probabilistic intuition that wider networks offer more diverse candidate features at initialization; the authors formalize this as width acting like parallel random search that increases the odds of containing \u2018lottery-ticket\u2019 neurons. Frankle and Carbin\u2019s lottery ticket hypothesis empirically anchors the notion that lucky sparse subnetworks exist at initialization and can be harnessed for efficient learning, which this paper quantifies in terms of sample complexity gains. Finally, the empirical scaling-law literature (Kaplan et al.) frames the broader objective: mapping resource tradeoffs; here, the authors deliver a rigorous, problem-specific Pareto frontier that unifies data, compute, width, and luck, and they validate its qualitative predictions experimentally.",
  "analysis_timestamp": "2026-01-06T23:42:49.128692"
}
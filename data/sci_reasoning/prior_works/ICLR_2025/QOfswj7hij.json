{
  "prior_works": [
    {
      "title": "Classical Planning in Deep Latent Space",
      "authors": "Asai et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work showed that a planning-ready discrete representation can be learned directly from images, and the present paper generalizes that idea from propositional latent bits to learned first-order neuro-symbolic predicates with an abstract transition model for compositional generalization."
    },
    {
      "title": "From Skills to Symbols: Learning Symbolic Representations for Abstract, High-Level Planning",
      "authors": "Konidaris et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "It provided the key insight that task-specific symbols should be learned from interaction to support planning, which is extended here to relational, first-order predicate learning jointly with an abstract world model rather than assuming pre-specified skills."
    },
    {
      "title": "DreamCoder: Growing generalizable, interpretable knowledge with wake\u2013sleep program learning",
      "authors": "Ellis et al.",
      "year": 2021,
      "arxiv_id": "2006.08381",
      "role": "Inspiration",
      "relationship_sentence": "The paper adapts DreamCoder\u2019s online library-learning (wake\u2013sleep) mechanism for inventing reusable abstractions to the predicate-invention setting, consolidating a predicate vocabulary that composes into an abstract world model."
    },
    {
      "title": "Meta-Interpretive Learning of Higher-Order Logic Programs",
      "authors": "Muggleton et al.",
      "year": 2015,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "This ILP framework is a primary symbolic predicate-invention baseline whose reliance on discrete logic and search is addressed by grounding invented predicates neurally and learning them online for scalable robot planning."
    },
    {
      "title": "The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences from Natural Supervision",
      "authors": "Mao et al.",
      "year": 2019,
      "arxiv_id": "1904.12584",
      "role": "Inspiration",
      "relationship_sentence": "NS-CL\u2019s idea of neural concept detectors functioning as symbolic predicates over objects directly informs the use of neural predicates here, repurposed for model-based planning with explicit transition dynamics."
    },
    {
      "title": "An Object-Oriented Representation for Efficient Reinforcement Learning",
      "authors": "Diuk et al.",
      "year": 2008,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work formalized object- and relation-centric abstractions in RL, which is instantiated here by learning a first-order predicate language and dynamics over relational states from pixels to enable lifted planning."
    },
    {
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan)",
      "authors": "Ahn et al.",
      "year": 2022,
      "arxiv_id": "2204.01691",
      "role": "Gap Identification",
      "relationship_sentence": "SayCan highlights that VLM-based planners lack internal causal world models and struggle with OOD generalization, motivating the move to learned, task-specific neuro-symbolic predicates and abstract models."
    }
  ],
  "synthesis_narrative": "Object- and relation-centric formulations such as Object-Oriented MDPs established that first-order abstractions over entities and predicates enable efficient, lifted decision-making. LatPlan demonstrated that planning-ready discrete state can be learned directly from images, but did so at a propositional level that limits compositionality and relational generalization. NS-CL showed how neural concept detectors can be cast as symbolic predicates over objects and composed into programs, providing a recipe for grounding symbolic reasoning in perception. In symbolic learning, Meta-Interpretive Learning introduced predicate invention via logic-based search, but required fully symbolic input and struggled to scale in perceptual domains. DreamCoder advanced online abstraction invention with wake\u2013sleep library learning, making it possible to grow a reusable vocabulary of high-level concepts over time. In robotics, \u201cFrom Skills to Symbols\u201d argued that task-specific symbols should be learned from interaction to support high-level planning, but relied on pre-specified skills and largely non-relational symbols. Meanwhile, SayCan exemplified VLM planning\u2019s ability to sequence language-described skills while exposing its lack of an internal causal world model and weak OOD generalization. Taken together, these threads expose a clear opportunity: learn relational, task-specific symbols as neural predicates grounded in perception, invent them online as reusable abstractions, and couple them with an explicit abstract world model to enable lifted planning. The present work synthesizes these ideas by upgrading propositional latent planning to first-order neuro-symbolic predicates, importing online abstraction invention to grow the predicate vocabulary, and addressing VLM and ILP limitations through learned, interpretable dynamics that support sample-efficient, OOD-robust robot planning.",
  "target_paper": {
    "title": "VisualPredicator: Learning Abstract World Models with Neuro-Symbolic Predicates for Robot Planning",
    "authors": "Yichao Liang, Nishanth Kumar, Hao Tang, Adrian Weller, Joshua B. Tenenbaum, Tom Silver, Joao F. Henriques, Kevin Ellis",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "learning abstractions for planning, neuro-symbolic ai, concept learning",
    "abstract": "Broadly intelligent agents should form task-specific abstractions that selectively expose the essential elements of a task, while abstracting away the complexity of the raw sensorimotor space. In this work, we present Neuro-Symbolic Predicates, a first-order abstraction language that combines the strengths of symbolic and neural knowledge representations. We outline an online algorithm for inventing such predicates and learning abstract world models. We compare our approach to hierarchical reinforcement learning, vision-language model planning, and symbolic predicate invention approaches, on both in- and out-of-distribution tasks across five simulated robotic domains. Results show that our approach offers better sample complexity, stronger out-of-distribution generalization, and improved interpretability.",
    "openreview_id": "QOfswj7hij",
    "forum_id": "QOfswj7hij"
  },
  "analysis_timestamp": "2026-01-06T13:32:28.055184"
}
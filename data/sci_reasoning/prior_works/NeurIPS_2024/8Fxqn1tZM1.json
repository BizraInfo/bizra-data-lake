{
  "prior_works": [
    {
      "title": "HyperNetworks",
      "authors": "David Ha, Andrew Dai, Quoc V. Le",
      "year": 2016,
      "role": "Foundational higher-order models that take neural networks as inputs/targets",
      "relationship_sentence": "ScaleGMNs build on the paradigm introduced by HyperNetworks of learning functions over neural networks themselves, but advance it by enforcing explicit symmetry (scale) constraints when operating on a network\u2019s parameters and structure."
    },
    {
      "title": "Neural Message Passing for Quantum Chemistry",
      "authors": "Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl",
      "year": 2017,
      "role": "Message-passing blueprint for graph-structured computation",
      "relationship_sentence": "ScaleGMNs adapt the MPNN message-passing template to the graph-of-neurons/edges of a target NN, modifying updates and aggregations so they are equivariant to scaling transformations of weights and biases."
    },
    {
      "title": "Equivariance Through Parameter-Sharing",
      "authors": "Siamak Ravanbakhsh, Jeff Schneider, Barnab\u00e1s P\u00f3czos",
      "year": 2017,
      "role": "General recipe for building group-equivariant neural networks via weight tying",
      "relationship_sentence": "The design of ScaleGMNs leverages the principle that architectural constraints and parameter-sharing can enforce desired group equivariances, here instantiated for the multiplicative scaling group acting on NN parameters."
    },
    {
      "title": "Deep Sets",
      "authors": "Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnab\u00e1s P\u00f3czos, Ruslan R. Salakhutdinov, Alexander J. Smola",
      "year": 2017,
      "role": "Permutation-invariant function design for sets",
      "relationship_sentence": "Prior NN-as-input architectures often respected neuron permutation symmetries via Deep Sets\u2013style invariance; ScaleGMNs explicitly retain this permutation handling while extending the symmetry set to include scale equivariance."
    },
    {
      "title": "Group Equivariant Convolutional Networks",
      "authors": "Taco S. Cohen, Max Welling",
      "year": 2016,
      "role": "Conceptual foundation for encoding known symmetries through equivariance",
      "relationship_sentence": "ScaleGMNs are motivated by the G-CNN principle that respecting problem symmetries boosts sample efficiency and generalization, applying it to the scale group acting on neural parameterizations within a graph meta-network."
    },
    {
      "title": "Scale-Equivariant Convolutional Neural Networks",
      "authors": "Diego Marcos, Michele Volpi, Nikos Komodakis, Devis Tuia",
      "year": 2018,
      "role": "Explicit methods for scale equivariance in neural networks",
      "relationship_sentence": "This work demonstrates the benefits and mechanisms of scale equivariance; ScaleGMNs extend the idea from grid-based CNNs to message passing over NN graphs, designing neuron/edge representations that transform correctly under scaling."
    },
    {
      "title": "Path-SGD: Path-Normalized Optimization of Deep Neural Networks",
      "authors": "Behnam Neyshabur, Ryota Tomioka, Ruslan R. Salakhutdinov, Nathan Srebro",
      "year": 2015,
      "role": "Formalization of rescaling symmetries in positively homogeneous networks (e.g., ReLU)",
      "relationship_sentence": "By identifying rescaling invariances of weights across layers, Path-SGD provides the theoretical impetus for ScaleGMNs to treat weight/bias scalings as intrinsic symmetries to which the meta-network should be equivariant."
    }
  ],
  "synthesis_narrative": "ScaleGMNs target a central gap in the burgeoning field of networks that process other networks: going beyond permutation symmetry to explicitly encode scaling symmetries inherent in common activations and parameterizations. The higher-order modeling perspective was catalyzed by HyperNetworks, which introduced learning functions over neural networks. To operate on network structures, ScaleGMNs employ the graph-based computational template of message passing from MPNNs, viewing neurons and synapses as nodes and edges whose representations are updated via localized exchanges. Prior works handling permutation symmetries\u2014via Deep Sets\u2013style invariance and the broader framework of equivariance through parameter sharing\u2014established that respecting neuron/edge relabelings is essential when treating neural parameters as unordered objects; ScaleGMNs retain this but generalize the symmetry set. The conceptual thrust comes from the equivariance literature (G-CNNs), which argues that hard-wiring group actions into architectures yields data efficiency and better generalization. Specifically, scale-equivariant CNNs demonstrated practical mechanisms and benefits of scale symmetry on grids; ScaleGMNs transpose this insight to the metanetwork setting, engineering neuron and edge feature transformations and message/aggregation rules that are equivariant to multiplicative rescalings of weights and biases. Finally, theory on rescaling invariances in ReLU-like networks (Path-SGD) provides the parameter-space symmetry that ScaleGMNs formalize architecturally. In sum, ScaleGMNs synthesize higher-order processing, message passing on NN graphs, permutation-aware design, and explicit scale-group equivariance to produce meta-models whose internal representations transform consistently under the genuine symmetries of neural parameterizations.",
  "analysis_timestamp": "2026-01-06T23:33:36.265478"
}
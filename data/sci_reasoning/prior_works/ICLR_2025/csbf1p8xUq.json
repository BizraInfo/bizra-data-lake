{
  "prior_works": [
    {
      "title": "Parameter-Efficient Transfer Learning for NLP",
      "authors": "Neil Houlsby et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Introduced adapter modules inserted between Transformer layers, providing the architectural mechanism that X-ALMA instantiates as language-specific plug-and-play components to isolate parameters per language."
    },
    {
      "title": "MAD-X: An Adapter-based Framework for Multilingual Transfer",
      "authors": "Jonas Pfeiffer et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrated that separate, composable language and task adapters mitigate cross-lingual interference, directly inspiring X-ALMA\u2019s language-specific modularization to prevent language conflicts during training."
    },
    {
      "title": "Minimum Bayes Risk Decoding for Neural Machine Translation with Neural Evaluation Metrics",
      "authors": "Markus Freitag et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Showed that reranking n-best translations with COMET via MBR yields higher quality, which X-ALMA generalizes into training by adaptively rejecting low-scoring samples rather than only using metric-guided selection at inference."
    },
    {
      "title": "COMET-22: Unbabel\u2019s Submission to the WMT 2022 Metrics Shared Task",
      "authors": "Ricardo Rei et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Provides the neural quality estimation metric X-ALMA leverages as the automatic signal for its adaptive rejection regimen and as the primary yardstick for balanced multilingual gains."
    },
    {
      "title": "Dual Conditional Cross-Entropy Filtering of Noisy Parallel Corpora",
      "authors": "Marcin Junczys-Dowmunt",
      "year": 2018,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Established static, heuristic QE-style data filtering for MT, whose limitations (non-adaptive, model-agnostic, and not language- or stage-aware) are addressed by X-ALMA\u2019s online, model-in-the-loop adaptive rejection."
    },
    {
      "title": "FLORES-200: Evaluating Machine Translation in Low-Resource Languages",
      "authors": "Naman Goyal et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Defines the multilingual evaluation benchmark and language coverage that shape X-ALMA\u2019s target set, module granularity, and success criteria for balanced performance across 50 diverse languages."
    },
    {
      "title": "Aya: An Open-Source Instruction-Tuned Multilingual Language Model",
      "authors": "Cohere For AI et al.",
      "year": 2024,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Serves as the primary multilingual LLM baseline whose monolithic training exhibits high-resource bias that X-ALMA aims to overcome with language-specific modules and adaptive rejection."
    }
  ],
  "synthesis_narrative": "Adapter research established a way to add small, trainable modules to frozen Transformers: Houlsby et al. introduced bottleneck adapters that can be inserted between layers to enable parameter-efficient specialization. Building on this, MAD-X showed that separating language adapters from task adapters and composing them at inference time mitigates cross-lingual interference and enables plug-and-play transfer, highlighting modularity as a practical route to multilingual robustness. On the quality-selection front, Freitag et al. demonstrated that Minimum Bayes Risk decoding guided by a neural metric (e.g., COMET) consistently improves translation by choosing the best candidate among samples, revealing the power of metric-driven selection. COMET-22 provided a strong, language-agnostic learned metric that correlates with human judgments, making it suitable as an automatic signal for both evaluation and decision-making. Earlier, Junczys-Dowmunt\u2019s dual conditional cross-entropy filtering showed that data quality filtering matters, but did so statically and without model or language adaptivity. FLORES-200 defined a standardized, wide-coverage benchmark emphasizing low- and mid-resource languages, setting the stage for truly balanced multilingual evaluation. Taken together, these works suggest two converging opportunities: use modular, per-language parameterization to avoid interference, and use strong automatic metrics to prefer high-quality supervision. X-ALMA synthesizes these by instantiating language-specific, plug-and-play modules to isolate learning per language and by extending metric-guided selection from inference to training via adaptive rejection, enabling balanced, high-quality translation at scale.",
  "target_paper": {
    "title": "X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality Translation at Scale",
    "authors": "Haoran Xu, Kenton Murray, Philipp Koehn, Hieu Hoang, Akiko Eriguchi, Huda Khayrallah",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Large Language Model, Machine Translation, Multilingual",
    "abstract": "Large language models (LLMs) have achieved remarkable success across various NLP tasks with a focus on English due to English-centric pre-training and limited multilingual data. In this work, we focus on the problem of translation, and \nwhile some multilingual LLMs claim to support for hundreds of languages, models often fail to provide high-quality responses for mid- and low-resource languages, leading to imbalanced performance heavily skewed in favor of high-resource languages. We introduce **X-ALMA**, a model designed to ensure top-tier performance across 50 diverse languages, regardless of their resource levels. X-ALMA surpasses state-of-the-art open-source multilingual LLMs, such as Aya-101 and Aya-23, in every single translation direction on the FLORES-200 and WMT'23 test datasets according to COMET-22. This is achieved by plug-and-play language-specific module architecture to prevent language conflicts during training and a carefully designed training regimen with novel optimiza",
    "openreview_id": "csbf1p8xUq",
    "forum_id": "csbf1p8xUq"
  },
  "analysis_timestamp": "2026-01-06T13:52:22.259651"
}
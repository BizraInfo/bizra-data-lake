{
  "prior_works": [
    {
      "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
      "authors": "Mildenhall et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "Established the neural volumetric rendering framework that NeuralPlane builds upon to encode scene geometry/appearance and to aggregate multi-view supervision in a single 3D field."
    },
    {
      "title": "NeuS: Learning Neural Implicit Surfaces by Volume Rendering",
      "authors": "Wang et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "Provided the surface-oriented neural implicit formulation and training (SDF with volume rendering) that NeuralPlane adapts to enforce coplanarity and plane-aligned geometric constraints during optimization."
    },
    {
      "title": "PlaneNet: Piecewise Planar Reconstruction from a Single RGB Image",
      "authors": "Liu et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Introduced the single-image piecewise planar segmentation/reconstruction formulation that NeuralPlane\u2019s monocular module follows to produce 2D plane observations as inputs to its multi-view distillation."
    },
    {
      "title": "PlaneRCNN: 3D Plane Detection and Reconstruction from a Single Image",
      "authors": "Liu et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrated accurate single-image plane detection but highlighted the limitation of per-view inconsistency and reliance on annotated plane labels that NeuralPlane addresses by fusing multi-view plane observations in a unified 3D neural field without plane annotations."
    },
    {
      "title": "PlanarRecon: Real-time Planar Reconstruction from Posed Monocular Video",
      "authors": "Guo et al.",
      "year": 2022,
      "role": "Baseline",
      "relationship_sentence": "Serves as the primary plane-based multi-view reconstruction baseline; NeuralPlane improves by replacing explicit plane tracking with plane-guided neural field training that yields higher-fidelity geometry and semantics without explicit plane supervision."
    },
    {
      "title": "MonoSDF: Exploring Monocular Cues for Neural Implicit Surface Reconstruction",
      "authors": "Yu et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Showed how to supervise neural implicit surfaces using monocular predictions (depth/normals); NeuralPlane generalizes this idea from pixel-wise cues to region-level planar observations and multi-view coplanarity constraints for learning accurate 3D planes."
    },
    {
      "title": "Semantic-NeRF: Learning Semantics in Neural Radiance Fields",
      "authors": "Zhi et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "Introduced learning a semantic field jointly with a radiance field from 2D signals; NeuralPlane extends this by introducing a self-supervised Neural Coplanarity Field that lifts plane-aware semantic cues from 2D to a 3D-consistent feature field without labels."
    }
  ],
  "synthesis_narrative": "NeuralPlane sits at the intersection of plane-based scene abstraction and neural implicit reconstruction. NeRF provided the neural rendering backbone to aggregate multi-view signals in a single 3D field, while NeuS supplied a surface-oriented implicit formulation that enables precise geometry extraction\u2014both essential to turn plane cues into high-fidelity 3D structure. On the observation side, PlaneNet and PlaneRCNN established the single-image piecewise planar formulation and practical detectors, yet exposed crucial limitations: predictions are per-view, inconsistent across images, and depend on annotated plane labels. PlanarRecon extended plane reasoning to monocular video, but remained tied to explicit plane detection and tracking, limiting geometric fidelity and robustness. MonoSDF demonstrated that monocular 2D predictions could supervise neural implicit surfaces, suggesting that 2D cues can be distilled into 3D via neural fields. NeuralPlane generalizes this to region-level plane observations and introduces a plane-guided training procedure that implicitly enforces coplanarity and planar geometry across views. Finally, inspired by semantic fields in Semantic-NeRF, NeuralPlane proposes a self-supervised Neural Coplanarity Field, lifting plane-aware features from 2D to 3D without labels to jointly capture semantics and structure. Together, these works directly shaped NeuralPlane\u2019s core innovation: distilling inconsistent 2D plane observations into a unified, semantics-aware 3D neural plane representation that achieves annotation-free, high-fidelity planar reconstruction.",
  "analysis_timestamp": "2026-01-06T23:09:26.629120"
}
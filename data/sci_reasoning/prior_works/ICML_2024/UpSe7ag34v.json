{
  "prior_works": [
    {
      "title": "Prediction and Entropy of Printed English",
      "authors": "Claude E. Shannon",
      "year": 1951,
      "role": "Foundation",
      "relationship_sentence": "Shannon explicitly posed the problem of predicting language tokens and framed entropy-based limits, raising the time-direction question that this paper directly revisits and resolves at LLM scale."
    },
    {
      "title": "Deep Contextualized Word Representations",
      "authors": "Matthew E. Peters et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "ELMo operationalized forward and backward language modeling as separate autoregressive objectives, providing the concrete previous-token prediction formulation the present work uses to measure backward perplexity against forward perplexity."
    },
    {
      "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
      "authors": "Zhilin Yang et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "XLNet showed that manipulating factorization orders to exploit bidirectional context materially affects modeling, motivating the current paper\u2019s focused isolation of pure time-direction (forward vs backward) effects in autoregressive LMs."
    },
    {
      "title": "Masked Language Model Scoring",
      "authors": "Julian Salazar et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "By demonstrating that scoring and perplexity depend on objective and directional access to context, this work highlighted a comparability gap that the present paper addresses by directly contrasting next-token versus previous-token prediction in matched settings."
    },
    {
      "title": "Information-Geometric Approach to Causal Inference",
      "authors": "Dominik Janzing et al.",
      "year": 2012,
      "role": "Inspiration",
      "relationship_sentence": "IGCI formalized an asymmetry between generative mechanisms and their inverses via complexity/independence principles, inspiring the paper\u2019s theory that sparsity and computational constraints can yield directional asymmetries despite information-theoretic symmetry."
    },
    {
      "title": "Human Behavior and the Principle of Least Effort",
      "authors": "George K. Zipf",
      "year": 1949,
      "role": "Foundation",
      "relationship_sentence": "Zipf\u2019s law established heavy-tailed sparsity in language, a structural assumption the paper leverages to argue why one temporal direction can be computationally easier to approximate in practice."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014documenting and explaining a persistent forward\u2013backward asymmetry in large language models\u2014traces back to Shannon\u2019s foundational framing of language prediction and entropy, which first raised the prospect of directional symmetry in principle. That problem formulation sets the theoretical baseline the authors interrogate at LLM scale. Peters et al. (ELMo) provided the concrete machinery to operationalize time direction in neural LMs, formalizing previous\u2011token prediction as a bona fide autoregressive objective; this enabled a clean, apples\u2011to\u2011apples empirical comparison of forward versus backward log\u2011perplexity. Subsequent pretraining advances such as XLNet underscored that changing factorization orders and directional access to context substantively alters model behavior, motivating a focused decomposition of pure time-direction effects apart from bidirectional objectives. Salazar et al. showed that perplexity itself is objective\u2011dependent, exposing a methodological gap for comparing models with different directional access; the present work addresses this by training matched forward and backward LMs and quantifying the asymmetry directly. To reconcile the empirical asymmetry with information\u2011theoretic symmetry, the authors draw on ideas from causal\u2011inference asymmetry (IGCI), where mechanisms are simpler in one direction than the inverse, and on Zipfian sparsity of language. Together, these works directly shape the paper\u2019s insight: natural language\u2019s heavy\u2011tailed, sparse structure coupled with computational constraints can produce a robust arrow of time in learned predictors, even when entropy considerations alone would predict symmetry.",
  "analysis_timestamp": "2026-01-06T23:09:26.449348"
}
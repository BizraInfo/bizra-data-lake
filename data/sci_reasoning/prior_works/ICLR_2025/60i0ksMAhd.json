{
  "prior_works": [
    {
      "title": "Relational Reinforcement Learning",
      "authors": "Sa\u0161o D\u017eeroski et al.",
      "year": 2001,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "It established first-order logic policy representations for RL, providing the symbolic policy formalism that BlendRL retains while making it differentiable and co-trainable with neural networks."
    },
    {
      "title": "Towards Deep Symbolic Reinforcement Learning",
      "authors": "Marta Garnelo et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By separating neural perception from symbolic planning with hand-crafted predicates on Atari, it exposed the brittleness of disjoint pipelines that BlendRL replaces with a unified hybrid policy."
    },
    {
      "title": "Logic Tensor Networks: Deep Learning and Logical Reasoning",
      "authors": "Luciano Serafini et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "BlendRL adopts LTN-style t-norm\u2013based differentiable first-order logic to parameterize its symbolic policy head and backpropagate through logical rules during joint training."
    },
    {
      "title": "DeepProbLog: Neural Probabilistic Logic Programming",
      "authors": "Robin Manhaeve et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Its end-to-end coupling of neural perception with a differentiable logic program directly inspired BlendRL\u2019s joint optimization of neural and logical policy components."
    },
    {
      "title": "Neural Logic Machines",
      "authors": "Honghua Dong et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "NLM demonstrated that differentiable multi-step relational inference can improve control, informing BlendRL\u2019s design of an embedded reasoning module inside the policy."
    },
    {
      "title": "Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning",
      "authors": "Fernando F. Icarte et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "By showing that explicit symbolic task structure yields robustness and transfer, it motivated BlendRL to embed symbolic structure at the policy level rather than only in rewards."
    },
    {
      "title": "Verifiable Reinforcement Learning via Policy Extraction (VIPER)",
      "authors": "Osbert Bastani et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "VIPER\u2019s post-hoc decision-tree extraction from deep RL highlighted the lack of jointly learned, interpretable policies\u2014a gap BlendRL fills by learning neural and symbolic policies together."
    }
  ],
  "synthesis_narrative": "Relational Reinforcement Learning introduced first-order logic as a policy representation, showing how logical predicates and rules can drive action selection in an MDP, but relied on non-differentiable learning over predefined symbols. Deep Symbolic Reinforcement Learning separated neural perception from a symbolic planner in Atari, revealing the promise of symbolic structure for generalization while exposing brittleness from hand-crafted predicates and disjoint training. Logic Tensor Networks provided t\u2011norm\u2013based differentiable semantics for first\u2011order logic, enabling gradients to flow through weighted rules. DeepProbLog demonstrated end-to-end learning where neural modules populate symbols that a probabilistic logic program reasons over, establishing a practical recipe for co-training perception and logic. Neural Logic Machines showed that differentiable multi-step relational inference improves policy learning on tasks demanding algorithmic generalization. Reward Machines formalized high-level task structure via automata, empirically linking explicit symbolic structure to robustness and transfer in RL. VIPER extracted interpretable decision trees from deep policies but did so post\u2011hoc, underscoring the need for jointly learned, faithful symbolic control.\nTogether, these works suggested a clear opportunity: marry the interpretability and structure of first-order reasoning with the reactive flexibility of deep policies in a single, end-to-end trainable agent. Building on LTN/DeepProbLog\u2019s differentiable logic to make symbolic decisions optimizable by gradient descent, adopting NLM-style relational inference capacity, and addressing the brittleness of disjoint pipelines highlighted by deep symbolic RL and VIPER, the resulting synthesis naturally yields a blended policy architecture that inherits robustness benefits akin to Reward Machines while remaining reactive and learnable on Atari-scale environments.",
  "target_paper": {
    "title": "BlendRL: A Framework for Merging Symbolic and Neural Policy Learning",
    "authors": "Hikaru Shindo, Quentin Delfosse, Devendra Singh Dhami, Kristian Kersting",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Neuro-Symbolic AI, Differentiable Reasoning, Reinforcement Learning, Interpretable AI, First-order logic",
    "abstract": "Humans can leverage both symbolic reasoning and intuitive responses. In contrast, reinforcement learning policies are typically encoded in either opaque systems like neural networks or symbolic systems that rely on predefined symbols and rules. This disjointed approach severely limits the agents\u2019 capabilities, as they often lack either the flexible low-level reaction characteristic of neural agents or the interpretable reasoning of symbolic agents. \n\nTo overcome this challenge, we introduce *BlendRL*, a neuro-symbolic RL framework that harmoniously integrates both paradigms. \nWe empirically demonstrate that BlendRL agents outperform both neural and symbolic baselines in standard Atari environments, and showcase their robustness to environmental changes. Additionally, we analyze the interaction between neural and symbolic policies, illustrating how their hybrid use helps agents overcome each other's limitations.",
    "openreview_id": "60i0ksMAhd",
    "forum_id": "60i0ksMAhd"
  },
  "analysis_timestamp": "2026-01-06T15:04:08.560298"
}
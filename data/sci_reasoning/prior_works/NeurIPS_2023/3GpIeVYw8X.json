{
  "prior_works": [
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations (SimCLR)",
      "authors": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton",
      "year": 2020,
      "role": "Self-supervised representation learning and linear evaluation protocol",
      "relationship_sentence": "HUME\u2019s reliance on fixed pretrained representations with linear probes is grounded in SimCLR\u2019s demonstration that semantic structure is linearly extractable from self-supervised features, enabling model-agnostic separability tests."
    },
    {
      "title": "Deep Clustering for Unsupervised Learning of Visual Features (DeepCluster)",
      "authors": "Mathilde Caron, Piotr Bojanowski, Armand Joulin, Matthijs Douze",
      "year": 2018,
      "role": "Unsupervised discovery of label-like structure via clustering",
      "relationship_sentence": "DeepCluster established that unsupervised grouping of features can recover human-aligned categories, directly motivating HUME\u2019s search over labelings guided by a separability criterion rather than explicit labels."
    },
    {
      "title": "Invariant Information Clustering for Unsupervised Image Classification and Segmentation (IIC)",
      "authors": "Xu Ji, Joao F. Henriques, Andrea Vedaldi",
      "year": 2019,
      "role": "Unsupervised objective aligning clusters with semantic invariances",
      "relationship_sentence": "IIC\u2019s use of view-consistency to induce semantic partitions informs HUME\u2019s principle that true human labelings manifest consistently across representations, operationalized via linear separability across feature spaces."
    },
    {
      "title": "SCAN: Learning to Classify Images without Labels",
      "authors": "Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, Luc Van Gool",
      "year": 2020,
      "role": "Semantic clustering with simple classifiers on fixed features",
      "relationship_sentence": "SCAN showed that a lightweight classifier on frozen self-supervised features can recover human classes, reinforcing HUME\u2019s choice to train only linear heads and to evaluate labelings through classifier simplicity/separability."
    },
    {
      "title": "Emerging Properties in Self-Supervised Vision Transformers (DINO)",
      "authors": "Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, Armand Joulin",
      "year": 2021,
      "role": "General-purpose self-supervised features with linearly accessible semantics",
      "relationship_sentence": "DINO\u2019s evidence that diverse pretrained models yield linearly readable semantics supports HUME\u2019s key assumption that human-aligned classes are linearly separable across many representation spaces."
    },
    {
      "title": "Combining Labeled and Unlabeled Data with Co-Training",
      "authors": "Avrim Blum, Tom Mitchell",
      "year": 1998,
      "role": "Multi-view agreement principle for inferring labels",
      "relationship_sentence": "HUME\u2019s core idea\u2014select labelings that are simultaneously separable across multiple pretrained representations\u2014echoes co-training\u2019s multi-view agreement criterion, replacing views with independently learned feature spaces."
    },
    {
      "title": "Do Better ImageNet Models Transfer Better?",
      "authors": "Simon Kornblith, Jonathon Shlens, Quoc V. Le",
      "year": 2019,
      "role": "Linear-probe transferability across architectures and datasets",
      "relationship_sentence": "Findings that linear probes on diverse pretrained models transfer well underpin HUME\u2019s model-agnostic design and its expectation of consistent linear separability for human labelings across heterogeneous features."
    }
  ],
  "synthesis_narrative": "HUME\u2019s central contribution is an unsupervised, model-agnostic procedure that infers human labeling by searching for labelings that are linearly separable across many fixed representation spaces. This idea stands on two pillars: the robustness of semantic structure to representation choice and the use of simple linear probes to expose that structure. SimCLR and DINO established that self-supervised representations encode semantics that are reliably extractable by linear classifiers, legitimizing HUME\u2019s decision to keep representations frozen and evaluate candidate labelings purely via linear separability. Complementing this, work on transferability by Kornblith et al. showed that linear probes succeed across architectures and datasets, suggesting that human-aligned class geometry persists across diverse feature spaces\u2014an assumption HUME explicitly operationalizes.\n\nOn the unsupervised labeling side, DeepCluster, IIC, and SCAN demonstrated that, without labels, one can recover categories that closely track human annotations by leveraging clustering, mutual-information-based view consistency, and simple classifiers on frozen features. HUME synthesizes these insights but replaces instance- or view-level objectives with a global search over labelings guided by an across-representation separability score. Finally, the multi-view agreement principle from co-training provides the theoretical template: the correct labeling is the one that is simultaneously simple and consistent in multiple independent views. HUME instantiates this by treating different pretrained encoders as views and selecting the labeling that is linearly separable in all of them, thereby aligning unsupervised discovery with human labeling.",
  "analysis_timestamp": "2026-01-07T00:02:04.820458"
}
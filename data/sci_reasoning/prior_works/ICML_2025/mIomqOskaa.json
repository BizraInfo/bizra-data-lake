{
  "prior_works": [
    {
      "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
      "authors": "Frankle et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "Established that fixed sparse subnetworks can be trained to match dense performance, directly motivating this paper\u2019s use of fixed sparsity as a principled alternative to scaling dense DRL networks."
    },
    {
      "title": "SNIP: Single-shot Network Pruning based on Connection Sensitivity",
      "authors": "Lee et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Introduced pruning-once-before-training; the present work adopts this exact timing and simplifies the selection rule to random masking to isolate the effect of static sparsity in DRL."
    },
    {
      "title": "What\u2019s Hidden in a Randomly Weighted Neural Network?",
      "authors": "Ramanujan et al.",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "Showed that suitable subnetworks (supermasks) exist at random initialization, inspiring the hypothesis tested here that even random one-shot pruning can confer beneficial inductive bias for RL scaling."
    },
    {
      "title": "Rigging the Lottery: Making All Tickets Winners",
      "authors": "Evci et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrated dynamic sparse training can match dense models but at added algorithmic complexity; this paper shows a simpler static alternative (one-shot random pruning) suffices to unlock DRL scaling."
    },
    {
      "title": "Phasic Policy Gradient",
      "authors": "Cobbe et al.",
      "year": 2021,
      "role": "Baseline",
      "relationship_sentence": "Proposed decoupling policy/value updates to mitigate representation interference in PPO; the current work targets the same interference but via architecture-level sparsity and reports improvements over dense baselines."
    },
    {
      "title": "Gradient Surgery for Multi-Task Learning",
      "authors": "Yu et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "Formalized gradient conflict/interference and proposed PCGrad; this paper addresses an analogous interference between RL losses by showing fixed sparsity inherently reduces gradient conflicts without gradient surgery."
    },
    {
      "title": "On Lazy Training in Differentiable Programming",
      "authors": "Chizat et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "Characterized the lazy/NTK regime where very wide networks lose feature-learning plasticity; the present work builds on this lens to argue and empirically show that sparsity counteracts plasticity loss when scaling DRL."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core idea\u2014that static network sparsity alone can unlock the scaling potential of deep reinforcement learning\u2014stands on two converging intellectual threads: sparse subnetworks and RL-specific optimization pathologies. Foundational pruning work, especially the Lottery Ticket Hypothesis, established that fixed sparse subnetworks can train as well as dense models, legitimizing sparse connectivity as a first-class design choice rather than a post-hoc compression tool. One-shot pruning at initialization (SNIP) provided the exact operational timing adopted here; the authors deliberately simplify its sensitivity-based criterion to random masking to isolate sparsity\u2019s intrinsic benefits. Complementary evidence from Ramanujan et al. suggested that performant subnetworks can be identified within randomly initialized weights, reinforcing the plausibility of random masks. While RigL showed dynamic sparse training can match dense networks, its complexity motivated the present work\u2019s focus on static masks as a simpler path to scale.\n\nOn the RL side, Phasic Policy Gradient targeted gradient interference between policy and value updates through training-phase decoupling; this paper instead shows architecture-level sparsity naturally reduces such interference while improving scaling. The broader notion of gradient conflicts, crystallized by PCGrad, frames the optimization challenge that sparsity helps alleviate. Finally, the lazy-training perspective of Chizat et al. provides the theoretical backdrop for \u201cplasticity loss\u201d in wide networks; the authors leverage this lens to argue that sparsity preserves feature-learning dynamics as models scale. Together, these works directly shaped the paper\u2019s thesis: fixed, one-shot sparsity is a simple, principled lever for overcoming plasticity loss and gradient interference in large DRL models.",
  "analysis_timestamp": "2026-01-06T23:07:19.612745"
}
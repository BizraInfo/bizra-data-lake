{
  "prior_works": [
    {
      "title": "Optimization Algorithms on Matrix Manifolds",
      "authors": "P.-A. Absil et al.",
      "year": 2008,
      "role": "Foundation",
      "relationship_sentence": "Provides the core Riemannian optimization framework (retractions, vector transports, line-search/trust-region schemes) that the paper uses to formulate and analyze manifold stochastic methods."
    },
    {
      "title": "Stochastic Gradient Descent on Riemannian Manifolds",
      "authors": "Silv\u00e8re Bonnabel",
      "year": 2013,
      "role": "Gap Identification",
      "relationship_sentence": "Introduces Riemannian SGD but relies on hand-tuned, decaying step sizes; the new work explicitly removes this learning-rate tuning requirement while working in the same stochastic manifold setting."
    },
    {
      "title": "Riemannian SVRG: Fast Stochastic Optimization on Manifolds",
      "authors": "S. Zhang et al.",
      "year": 2016,
      "role": "Baseline",
      "relationship_sentence": "Establishes a leading stochastic manifold optimizer that still requires tuned learning rates; the proposed learning-rate-free algorithms are positioned to compete with and improve practical robustness over such tuned baselines."
    },
    {
      "title": "First-order Methods for Geodesically Convex Optimization",
      "authors": "H. Zhang et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "Formalizes geodesic convexity and first-order complexity on manifolds, providing the problem formulation and rate benchmarks that the paper\u2019s high-probability guarantees aim to match (up to logarithmic factors)."
    },
    {
      "title": "Gradient Methods for Minimizing Functionals",
      "authors": "B. T. Polyak",
      "year": 1969,
      "role": "Inspiration",
      "relationship_sentence": "Introduces the Polyak step size based on function-value decrease and gradient norm; the paper\u2019s learning-rate-free step selection adapts this principle to the Riemannian stochastic setting."
    },
    {
      "title": "A Stochastic Line Search Method with Expected Complexity Guarantees",
      "authors": "C. Paquette et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "Demonstrates learning-rate-free step-size selection under noise in Euclidean spaces via stochastic line search, directly motivating analogous learning-rate-free strategies the paper develops on manifolds."
    },
    {
      "title": "Probabilistic Line Searches for Stochastic Optimization",
      "authors": "A. Mahsereci et al.",
      "year": 2017,
      "role": "Related Problem",
      "relationship_sentence": "Proposes probabilistic line-search rules to avoid manual learning-rate tuning in stochastic optimization, an idea the paper transports conceptually to the Riemannian context."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014learning-rate-free stochastic optimization on Riemannian manifolds\u2014sits at the intersection of two lines of work: (i) the geometric machinery of manifold optimization and (ii) parameter-free/adaptive step-size selection in stochastic optimization. Absil et al. established the modern Riemannian optimization toolkit\u2014retractions, vector transports, and descent frameworks\u2014on which any principled manifold algorithm must rely. Bonnabel\u2019s seminal Riemannian SGD brought stochasticity to this setting but required hand-tuned decaying step sizes, creating a practical bottleneck that the present work directly removes. Zhang\u2013Reddi\u2013Sra\u2019s RSVRG further advanced stochastic manifold optimization but still depended on tuned learning rates, furnishing natural baselines and highlighting the gap in adaptivity the new methods fill. On the complexity side, Zhang\u2013Sra\u2019s theory for geodesically convex problems provides the problem formulation and rate benchmarks that the paper targets, achieving optimal guarantees up to logarithmic factors. The learning-rate-free idea is rooted in Polyak\u2019s classical step size based on function decrease and gradient norms, whose spirit the paper adapts to the Riemannian, stochastic regime. Finally, Euclidean stochastic line-search methods (Paquette\u2013Scheinberg) and probabilistic line searches (Mahsereci\u2013Hennig) demonstrated that robust, tuning-free step selection is possible under noise; these works inform the paper\u2019s design of manifold-specific, learning-rate-free procedures with high-probability convergence.",
  "analysis_timestamp": "2026-01-06T23:09:26.483325"
}
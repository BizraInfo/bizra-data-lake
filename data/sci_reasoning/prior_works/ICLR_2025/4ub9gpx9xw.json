{
  "prior_works": [
    {
      "title": "Towards Faithfully Interpretable NLP Systems",
      "authors": "Omer Jacovi et al.",
      "year": 2020,
      "arxiv_id": "2004.03685",
      "role": "Foundation",
      "relationship_sentence": "This work formalized the distinction between faithfulness and plausibility in NLP explanations, which is directly instantiated here as alignment between explanation-implied influential concepts and the concepts that causally influence model outputs."
    },
    {
      "title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models",
      "authors": "Jay DeYoung et al.",
      "year": 2020,
      "arxiv_id": "1911.03429",
      "role": "Baseline",
      "relationship_sentence": "ERASER introduced widely used sufficiency and comprehensiveness tests for explanation faithfulness via token removal, providing the main baseline paradigm that this paper replaces with concept-level, causally grounded counterfactual evaluation."
    },
    {
      "title": "Learning the Difference that Makes a Difference: Counterfactual Data Augmentation for Robustness",
      "authors": "Divyansh Kaushik et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "This paper\u2019s paradigm of minimally editing inputs to produce realistic counterfactuals directly motivates using controlled edits to manipulate specific concepts when testing whether an explanation\u2019s claimed influences truly affect the model."
    },
    {
      "title": "Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving NLP Models",
      "authors": "Tongshuang Wu et al.",
      "year": 2021,
      "arxiv_id": "2101.00288",
      "role": "Extension",
      "relationship_sentence": "Polyjuice\u2019s LM-driven, control-code-based counterfactual rewriting is extended here by using an auxiliary LLM to selectively toggle targeted concept values while preserving naturalness to enable causal tests of concept influence."
    },
    {
      "title": "Concept Bottleneck Models",
      "authors": "Koh et al.",
      "year": 2020,
      "arxiv_id": "2007.04612",
      "role": "Foundation",
      "relationship_sentence": "CBMs establish intervenable high-level concepts as causal variables for predictions, which this paper generalizes to free-text settings by intervening on concept values via counterfactual input edits rather than an explicit bottleneck."
    },
    {
      "title": "Language Models Don\u2019t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought",
      "authors": "William G. Turpin et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By demonstrating that LLM-generated rationales can misrepresent internal decision processes, this work motivates a principled metric that compares explanation-implied influential concepts to those with measured causal effects."
    },
    {
      "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
      "authors": "Been Kim et al.",
      "year": 2018,
      "arxiv_id": "1711.11279",
      "role": "Related Problem",
      "relationship_sentence": "TCAV introduced quantifying model sensitivity to human-defined concepts, a core insight this paper adapts to language by testing causal influence of concepts through controlled counterfactual rewrites and comparing with claimed influences."
    }
  ],
  "synthesis_narrative": "Work on explainability clarified that faithfulness and plausibility are distinct goals; in particular, Jacovi and Goldberg argued that faithful explanations must reflect the features that actually drive predictions. ERASER operationalized faithfulness tests for text models with sufficiency and comprehensiveness via token removal, establishing common baselines but relying on unrealistic ablations. Kaushik, Hovy, and Lipton showed that minimally edited, realistic counterfactuals reveal what features truly affect labels, while Wu et al.\u2019s Polyjuice demonstrated that language models can generate fluent, controlled counterfactuals for specific attributes. Concept Bottleneck Models introduced intervenable, human-understandable concept variables whose manipulation reveals their causal effect on predictions, and TCAV quantified model sensitivity to human-defined concepts, framing explanations around concept-level influence rather than raw tokens. Turpin et al. subsequently showed that LLM chain-of-thought can be unfaithful to the model\u2019s actual decision process, underscoring the need for metrics that verify whether claimed influential concepts genuinely matter. Together, these works expose two gaps: existing faithfulness tests often use brittle token ablations instead of realistic interventions, and free-text explanations frequently misstate the model\u2019s causal drivers. Building on counterfactual editing and controlled LM rewrites, and adopting the concept-as-causal-variable perspective of CBMs/TCAV, the current paper makes the natural next step: define faithfulness as agreement between explanation-implied and causally influential concepts, and estimate those causal influences via LLM-generated, concept-targeted counterfactuals aggregated with robust statistical modeling.",
  "target_paper": {
    "title": "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations",
    "authors": "Katie Matton, Robert Ness, John Guttag, Emre Kiciman",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "large language models, faithful explanations, explainability, safety, counterfactual reasoning",
    "abstract": "Large language models (LLMs) are capable of generating *plausible* explanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model's \"reasoning\" process, i.e., they can be *unfaithful*. This, in turn, can lead to over-trust and misuse. We introduce a new approach for measuring the faithfulness of LLM explanations. First, we provide a rigorous definition of faithfulness. Since LLM explanations mimic human explanations, they often reference high-level *concepts* in the input question that purportedly influenced the model. We define faithfulness in terms of the difference between the set of concepts that the LLM's *explanations imply* are influential and the set that *truly* are. Second, we present a novel method for estimating faithfulness that is based on: (1) using an auxiliary LLM to modify the values of concepts within model inputs to create realistic counterfactuals, and (2) using a hierarchical Bayesian model to quantify the causa",
    "openreview_id": "4ub9gpx9xw",
    "forum_id": "4ub9gpx9xw"
  },
  "analysis_timestamp": "2026-01-06T08:02:02.107877"
}
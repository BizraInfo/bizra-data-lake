{
  "prior_works": [
    {
      "title": "DeepCAD: A Deep Generative Network for Parametric CAD Models",
      "authors": "Luo et al.",
      "year": 2022,
      "role": "Dataset and generative baseline for parametric CAD sequences",
      "relationship_sentence": "Text2CAD builds directly on the DeepCAD representation and data, extending it with large-scale natural-language annotations and conditioning an autoregressive transformer to generate the same parametric tokens from text."
    },
    {
      "title": "Fusion 360 Gallery: A Dataset and Environment for Programmatic CAD",
      "authors": "Willis et al.",
      "year": 2021,
      "role": "Large-scale repository of parametric CAD sketches, features, and executable modeling traces",
      "relationship_sentence": "The Gallery\u2019s standardized sketch/feature primitives (e.g., circles, extrude) and evaluation conventions inform Text2CAD\u2019s program tokenization and end-to-end generation/evaluation of CAD construction sequences."
    },
    {
      "title": "SketchGraphs: A Large-Scale Dataset for Modeling Relational Geometry in CAD",
      "authors": "Seff et al.",
      "year": 2020,
      "role": "Dataset and graph formulation for constraint-aware CAD sketch modeling",
      "relationship_sentence": "SketchGraphs\u2019 representation of geometric primitives and constraints underpins Text2CAD\u2019s handling of sketch-level entities and motivates sequence modeling that respects CAD relationships when conditioned on text."
    },
    {
      "title": "CSGNet: Neural Shape Parser for Constructive Solid Geometry",
      "authors": "Sharma et al.",
      "year": 2018,
      "role": "Autoregressive program synthesis of geometric construction sequences",
      "relationship_sentence": "CSGNet\u2019s view of geometry generation as token-by-token program synthesis directly motivates Text2CAD\u2019s transformer-based autoregressive decoding of CAD operations conditioned on natural language."
    },
    {
      "title": "ShapeAssembly: Learning to Generate Programs for 3D Shape Structure",
      "authors": "Jones et al.",
      "year": 2020,
      "role": "Programmatic DSL and compositional generative modeling for parameterized 3D shapes",
      "relationship_sentence": "ShapeAssembly\u2019s parameterized, part-based program abstraction informs Text2CAD\u2019s choice to represent CAD as compositional, parameter-bearing programs that can be produced from textual instructions."
    },
    {
      "title": "LLaVA and LLaVA-NeXT: Large Language and Vision Assistant (Visual Instruction Tuning)",
      "authors": "Liu et al.",
      "year": 2023,
      "role": "Multimodal LLMs for visual instruction following and captioning",
      "relationship_sentence": "Text2CAD leverages LLaVA(-NeXT) to automatically describe CAD geometry previews, forming a core component of its annotation pipeline that scales natural-language supervision."
    },
    {
      "title": "Mistral 7B: Open Efficient Language Models",
      "authors": "Lample et al.",
      "year": 2023,
      "role": "Base LLM for high-quality, controllable text generation",
      "relationship_sentence": "Mistral is used in Text2CAD\u2019s pipeline to generate and refine beginner-to-expert-level CAD instructions aligned to parametric operations, enabling rich text supervision for the model."
    }
  ],
  "synthesis_narrative": "Text2CAD\u2019s core innovation\u2014end-to-end generation of parametric CAD programs from natural-language instructions at varying expertise levels\u2014sits at the intersection of three strands of prior work. First, foundational CAD program representations and datasets made sequence-based CAD modeling feasible. Fusion 360 Gallery systematized sketch/feature primitives and executable traces, while SketchGraphs framed sketches as relational geometric entities with constraints. Most directly, DeepCAD provided a parametric tokenization and large-scale corpus of CAD sequences that Text2CAD both adopts and augments with natural-language supervision. Second, program-synthesis approaches to geometry established autoregressive generation as the right inductive bias. CSGNet showed that geometric construction can be cast as token-by-token program decoding, and ShapeAssembly demonstrated the value of a parameterized, compositional DSL for 3D structure\u2014both ideas that Text2CAD extends by conditioning the decoding process on text to produce designer-specified parametric operations. Third, recent progress in large language and vision-language models enabled scaling language supervision in domains lacking human-written annotations. LLaVA/LLaVA-NeXT provide visual grounding to caption CAD previews, and Mistral supplies controllable language generation to produce beginner-to-expert instruction templates. By combining these, Text2CAD contributes a large text-annotated parametric CAD corpus and a transformer-based autoregressive model that translates natural language directly into CAD construction programs, closing the gap between designer-friendly instructions and executable parametric design.",
  "analysis_timestamp": "2026-01-06T23:39:42.963040"
}
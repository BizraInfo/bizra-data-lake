{
  "prior_works": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "authors": "Alexey Dosovitskiy et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "ViT-22B directly adopts the ViT patch-tokenization and transformer encoder design introduced by Dosovitskiy et al., and the paper\u2019s core contribution is scaling this exact architecture to 22B parameters."
    },
    {
      "title": "How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers",
      "authors": "Andreas Steiner et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "The 22B training recipe extends the AugReg-style ViT training setup (data augmentation, regularization, and supervised pretraining protocol) and modifies it for stability and efficiency at extreme scale."
    },
    {
      "title": "Scaling Vision Transformers",
      "authors": "Xiaohua Zhai et al.",
      "year": 2022,
      "role": "Baseline",
      "relationship_sentence": "This work established billion-parameter ViT baselines (e.g., ViT-G/14) and empirical scaling trends that ViT-22B explicitly pushes beyond, while addressing the instability/efficiency issues that appear past ~1\u20132B parameters."
    },
    {
      "title": "V-MoE: Learning Sparse Vision Models with Mixture-of-Experts",
      "authors": "Carlos Riquelme et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "V-MoE scaled ViTs via sparse experts, highlighting that scaling dense ViTs remained open; ViT-22B addresses this gap by providing a simple, stable recipe for a dense 22B ViT without MoE routing complexity."
    },
    {
      "title": "Swin Transformer V2: Scaling Up Capacity and Resolution",
      "authors": "Ze Liu et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "SwinV2 demonstrated stabilization techniques to scale hierarchical vision transformers to multi-billion parameters, and ViT-22B targets the analogous stability challenge for plain (global) ViTs while scaling an order of magnitude larger."
    },
    {
      "title": "Training Compute-Optimal Large Language Models",
      "authors": "Jordan Hoffmann et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Chinchilla-style compute\u2013data scaling laws directly informed ViT-22B\u2019s choice of model size and training duration, guiding the compute-efficient scaling recipe for a dense vision transformer."
    },
    {
      "title": "Big Transfer (BiT): General Visual Representation Learning",
      "authors": "Alexander Kolesnikov et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "BiT established the large-scale supervised pretraining and linear-probe transfer protocol on long-tailed datasets that ViT-22B follows and improves upon at unprecedented model scale."
    }
  ],
  "synthesis_narrative": "ViT-22B\u2019s core innovation\u2014an efficient, stable recipe to train a dense Vision Transformer at 22B parameters\u2014rests on a clear lineage. The architectural starting point is ViT, which introduced patch tokenization and a plain Transformer encoder for images. Practical know-how for supervised pretraining came from BiT and was specialized to transformers by the AugReg recipe in How to train your ViT?, which ViT-22B extends to the extreme-scale regime. Empirically, Scaling Vision Transformers set the first billion-parameter ViT baselines (e.g., ViT-G/14) and revealed the benefits and hurdles of going bigger; ViT-22B directly tackles those hurdles, pushing far beyond prior dense scales. In parallel, V-MoE showed that sparse experts can scale ViTs to massive parameter counts, but also underscored the open problem of achieving similar capacity in a dense model without routing overhead\u2014a gap ViT-22B closes with a simple, dense recipe. Swin Transformer V2 contributed stabilization insights for multi-billion\u2013parameter vision transformers, which informed ViT-22B\u2019s focus on robustness and training stability for the plain ViT family. Finally, Chinchilla\u2019s compute\u2013data scaling laws guided how the authors balanced model size and training duration, enabling compute-efficient training at unprecedented scale. Together, these works directly shaped the design choices and objectives that culminated in a stable, compute-efficient 22B-parameter dense ViT.",
  "analysis_timestamp": "2026-01-06T23:09:26.519881"
}
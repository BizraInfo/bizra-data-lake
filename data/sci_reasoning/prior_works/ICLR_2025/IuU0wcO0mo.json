{
  "prior_works": [
    {
      "title": "A large-scale, standardized physiological survey of mouse visual cortex",
      "authors": "B. de Vries et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work provides the Allen Brain Observatory two-photon visual coding dataset\u2014with multi-area recordings, diverse visual stimuli, and cell-type annotations\u2014that defines the multi-session, multi-task, multi-region setting and supplies the scale needed for the paper\u2019s pretraining and transfer experiments."
    },
    {
      "title": "High-precision coding in visual cortex",
      "authors": "C. Stringer et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By showing that decoding precision improves dramatically with larger neural populations and highlighting heterogeneity across neurons, this paper motivates scaling up decoding by pooling across sessions and cell types."
    },
    {
      "title": "Generalization in data-driven models of primary visual cortex",
      "authors": "L. Lurz et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "It introduced the shared-core plus neuron-specific readout paradigm to pool data across animals/sessions for visual cortex modeling, a strategy that is directly generalized here via a transformer with tokenized neuron/metadata to achieve multi-task decoding across regions and cell types."
    },
    {
      "title": "Inferring single-trial neural population dynamics using sequential auto-encoders (LFADS)",
      "authors": "V. Pandarinath et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "LFADS established a strong per-session population-dynamics model and decoder baseline, whose need for session-specific training and limited cross-session/region transfer is addressed by the paper\u2019s single multi-task transformer that learns a unified representation."
    },
    {
      "title": "Stabilization of a brain\u2013computer interface via alignment of low-dimensional representations of neural activity",
      "authors": "A. Degenhart et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "This study solved across-day drift via manifold alignment but required the same subject/area and task, directly motivating a method that obviates explicit alignment by learning a transferable representation across distinct regions and tasks."
    },
    {
      "title": "A Reduced-Dimension Approach to Shared Response Modeling",
      "authors": "P.-H. Chen et al.",
      "year": 2015,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "SRM provided the key idea of aligning heterogeneous neural recordings into a common latent space across subjects, which is adapted here by learning a shared attention-based representation across sessions, regions, and cell types."
    },
    {
      "title": "BENDR: Using transformers to learn generalizable representations of EEG",
      "authors": "D. Kostas et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "BENDR demonstrated that transformer pretraining on heterogeneous neural time series yields transferable decoders across tasks and subjects, directly inspiring a multi-task transformer and tokenization strategy for large-scale neural decoding."
    }
  ],
  "synthesis_narrative": "The Allen Brain Observatory\u2019s visual coding survey established a uniquely comprehensive two-photon resource with recordings from six areas, diverse visual stimuli, and cell-type labels, enabling population-level analyses at scale. High-precision coding work in visual cortex showed that decoding accuracy grows with the number of recorded neurons and underscored heterogeneity across populations, motivating approaches that exploit larger, pooled datasets. Data-driven V1 modeling then introduced a shared-core with neuron-specific readouts to train across animals and sessions, demonstrating that shared representations can capture common structure while accommodating per-neuron idiosyncrasies. Sequential auto-encoder models like LFADS provided powerful per-session population dynamics models and decoders, but typically required separate training and did not natively support heterogeneous, multi-session pooling. Complementarily, BCI stabilization via manifold alignment proved that consistent low-dimensional structure could be leveraged across days, though it relied on explicit alignment within the same subject, area, and task. In parallel, shared response modeling in fMRI formalized alignment into a common latent space across heterogeneous subjects, foreshadowing scalable shared representations for neural populations. Together, these works reveal both the promise and limits of pooling: shared cores and latent alignments help, but per-session training and same-area constraints impede broader transfer. The natural next step is a unified, scalable model that learns a common representational space directly from large, heterogeneous neural datasets. By combining the shared-representation insight with transformer-based pretraining proven to transfer across neural time series, one can tokenize neurons and metadata (e.g., region, cell type) and train a single multi-task decoder that transfers across sessions, stimuli, brain regions, and cell types.",
  "target_paper": {
    "title": "Multi-session, multi-task neural decoding from distinct cell-types and brain regions",
    "authors": "Mehdi Azabou, Krystal Xuejing Pan, Vinam Arora, Ian Jarratt Knight, Eva L Dyer, Blake Aaron Richards",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "neural population, multi-task, transformer, tokenization, two-photon calcium imaging, visual stimuli, cell types",
    "abstract": "Recent work has shown that scale is important for improved brain decoding, with more data leading to greater decoding accuracy. However, large-scale decoding across many different datasets is challenging because neural circuits are heterogeneous---each brain region contains a unique mix of cellular sub-types, and the responses to different stimuli are diverse across regions and sub-types. It is unknown whether it is possible to pre-train and transfer brain decoding models between distinct tasks, cellular sub-types, and brain regions. To address these questions, we developed a multi-task transformer architecture and trained it on the entirety of the Allen Institute's Brain Observatory dataset. This dataset contains responses from over 100,000 neurons in 6 areas of the brains of mice, observed with two-photon calcium imaging, recorded while the mice observed different types of visual stimuli. Our results demonstrate that transfer is indeed possible -combining data from different sources ",
    "openreview_id": "IuU0wcO0mo",
    "forum_id": "IuU0wcO0mo"
  },
  "analysis_timestamp": "2026-01-06T08:36:19.478438"
}
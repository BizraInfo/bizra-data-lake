{
  "prior_works": [
    {
      "title": "Forecasting Treatment Responses Over Time Using Recurrent Marginal Structural Networks",
      "authors": "Bryan Lim et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "RMSN established the neural-network\u2013based longitudinal HTE paradigm (RNN outcome model with IPTW for time-varying confounding), which this paper directly targets by replacing the black-box RNN counterfactual rollout with a closed-form ODE while retaining longitudinal identification ideas."
    },
    {
      "title": "Counterfactual Recurrent Networks for Predicting Individualized Treatment Effects Over Time",
      "authors": "Ioana Bica et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "CRN formalized sequence-to-sequence counterfactual trajectory prediction with representation learning for deconfounding, providing the main neural baseline and problem setup that this work re-casts as an explicit ODE system to gain interpretability and continuous-time handling."
    },
    {
      "title": "Time Series Deconfounder: Estimating Treatment Effects over Time in the Presence of Hidden Confounders",
      "authors": "Ioana Bica et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By addressing hidden confounding via latent factor models in time series, this work exposed identification fragility and black-box limitations that motivated the present paper\u2019s alternative ODE-based structural assumptions and weighting strategy for longitudinal causal inference."
    },
    {
      "title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
      "authors": "Steven L. Brunton et al.",
      "year": 2016,
      "arxiv_id": "1509.03580",
      "role": "Inspiration",
      "relationship_sentence": "SINDy provided the core idea of learning parsimonious, closed-form ODEs from data via a function library and sparse regression, which the current paper adapts to discover interpretable treatment\u2013response dynamics for counterfactual inference."
    },
    {
      "title": "Sparse identification of nonlinear dynamics with control (SINDYc)",
      "authors": "Joshua L. Proctor et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "SINDYc introduced modeling exogenous inputs within the SINDy framework, a mechanism directly extended here by treating time-varying treatments as control inputs to learn heterogeneous, state-dependent treatment effects in the discovered ODE."
    },
    {
      "title": "Marginal structural models and causal inference in epidemiology",
      "authors": "James M. Robins et al.",
      "year": 2000,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "MSMs established the longitudinal identification framework (sequential ignorability, positivity, IPTW/g-formula) that underpins how causal effects along time-varying treatment paths are defined and estimated in the ODE formulation."
    }
  ],
  "synthesis_narrative": "Recurrent Marginal Structural Networks demonstrated that counterfactual trajectories under time-varying treatments could be forecast by combining RNN outcome models with inverse-propensity weighting, thereby encoding longitudinal identification in a neural pipeline. Counterfactual Recurrent Networks refined this by casting the task as sequence-to-sequence prediction with learned balanced representations, enabling individualized rollouts but remaining a black-box, discrete-time architecture. The Time Series Deconfounder proposed a latent-factor approach to mitigate hidden confounding in temporal data, highlighting both the promise and fragility of neural latent assumptions. In parallel, sparse identification of nonlinear dynamics (SINDy) showed that one can recover concise, interpretable governing equations from data using a function library and sparse regression. SINDYc extended this to systems with exogenous inputs, formalizing how control signals enter the discovered ODEs\u2014exactly the structure needed to encode interventions. Marginal Structural Models provided the core longitudinal causal framework, specifying assumptions and weighting schemes to identify effects of treatment paths over time.\nTogether these works revealed a gap: powerful but opaque neural estimators dominate longitudinal HTE, while interpretable, continuous-time structural models remain underutilized. By marrying SINDy/SINDYc\u2019s parsimonious ODE discovery with MSM-style longitudinal identification, the present approach recasts sequential counterfactual prediction as discovering a closed-form, treatment-controlled ODE that supports irregular sampling, transparent mechanisms, and explicit state-dependent heterogeneity\u2014an evident next step given the limitations and insights of the prior literature.",
  "target_paper": {
    "title": "ODE Discovery for Longitudinal Heterogeneous Treatment Effects Inference",
    "authors": "Krzysztof Kacprzyk, Samuel Holt, Jeroen Berrevoets, Zhaozhi Qian, Mihaela van der Schaar",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Treatment Effects over Time",
    "abstract": "Inferring unbiased treatment effects has received widespread attention in the machine learning community. In recent years, our community has proposed numerous solutions in standard settings, high-dimensional treatment settings, and even longitudinal settings. While very diverse, the solution has mostly relied on neural networks for inference and simultaneous correction of assignment bias. New approaches typically build on top of previous approaches by proposing new (or refined) architectures and learning algorithms. However, the end result\u2014a neural-network-based inference machine\u2014remains unchallenged. In this paper, we introduce a different type of solution in the longitudinal setting: a closed-form ordinary differential equation (ODE). While we still rely on continuous optimization to learn an ODE, the resulting inference machine is no longer a neural network. Doing so yields several advantages such as interpretability, irregular sampling, and a different set of identification assumpt",
    "openreview_id": "pxI5IPeWgW",
    "forum_id": "pxI5IPeWgW"
  },
  "analysis_timestamp": "2026-01-06T11:59:14.711351"
}
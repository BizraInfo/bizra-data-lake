{
  "prior_works": [
    {
      "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions",
      "authors": "Tim Brooks et al.",
      "year": 2023,
      "arxiv_id": "2211.09800",
      "role": "Foundation",
      "relationship_sentence": "This work established the instruction-based image editing formulation and diffusion-based editor that MGIE directly builds upon, and MGIE explicitly addresses its failure on brief/ambiguous instructions by enriching them via an MLLM."
    },
    {
      "title": "LLaVA: Large Language and Vision Assistant",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "arxiv_id": "2304.08485",
      "role": "Inspiration",
      "relationship_sentence": "LLaVA demonstrated that visual instruction-tuned LLMs can ground on an image and generate detailed, step-by-step responses, which MGIE leverages to expand terse edit commands into explicit, actionable guidance."
    },
    {
      "title": "InstructBLIP: Towards General-Purpose Vision-Language Models with Instruction Tuning",
      "authors": "Wenliang Dai et al.",
      "year": 2023,
      "arxiv_id": "2305.06500",
      "role": "Inspiration",
      "relationship_sentence": "InstructBLIP showed that instruction-tuned VLMs produce fine-grained, visually grounded descriptions, a capability MGIE uses to derive expressive edit rationales from image\u2013instruction pairs."
    },
    {
      "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",
      "authors": "Chenfei Wu et al.",
      "year": 2023,
      "arxiv_id": "2303.04671",
      "role": "Related Problem",
      "relationship_sentence": "By decomposing high-level edit requests into explicit tool-usable steps via an LLM, Visual ChatGPT provided the key insight that textual planning can guide image manipulation, which MGIE internalizes as MLLM-generated edit guidance for end-to-end training."
    },
    {
      "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
      "authors": "Deyao Zhu et al.",
      "year": 2023,
      "arxiv_id": "2304.10592",
      "role": "Inspiration",
      "relationship_sentence": "MiniGPT-4 showed that aligning a visual encoder with an LLM enables coherent, detailed image-grounded generation, motivating MGIE\u2019s use of MLLMs to \u201cimagine\u201d and articulate precise visual attributes for editing."
    },
    {
      "title": "Emu Edit: Instruction Tuning for Image Editing",
      "authors": "X Sun et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Emu Edit demonstrated that instruction-following image editors benefit from precise, granular edit descriptions but rely on massive curated data, a limitation MGIE addresses by eliciting such granularity from an MLLM instead of collecting it explicitly."
    }
  ],
  "synthesis_narrative": "Instruction-based image editing was concretely formulated by InstructPix2Pix, which fine-tunes a diffusion model to follow natural-language edit commands but struggles when requests are brief or ambiguous. LLaVA established that visual instruction tuning yields multimodal LLMs capable of conditioning on images and producing step-by-step, grounded responses, offering a mechanism to elaborate under-specified commands. InstructBLIP further showed that instruction-tuned vision-language models can generate fine-grained, visually grounded descriptions, indicating that such models can articulate actionable attributes and operations tied to image content. Visual ChatGPT revealed that large language models can decompose high-level editing intents into explicit tool-usable plans, highlighting the value of textual planning as guidance for manipulation. MiniGPT-4 demonstrated that aligning vision encoders with powerful LLMs enables coherent, detailed image-grounded generation, suggesting these models can \u201cimagine\u201d target edits and verbalize them precisely. Emu Edit underscored that instruction-based editors benefit from precise, granular directions but typically require extensive curated training pairs to obtain them. Together, these works reveal a gap: existing editors need explicit, detailed guidance, while MLLMs can produce such guidance but are not integrated into the editing pipeline. The natural next step is to couple an MLLM\u2019s grounded, stepwise elaboration with an editor trained end-to-end to execute it, transforming terse user instructions into expressive, actionable guidance that robustly drives Photoshop-style, global, and local edits.",
  "target_paper": {
    "title": "Guiding Instruction-based Image Editing via Multimodal Large Language Models",
    "authors": "Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, Zhe Gan",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "image editing, multimodal large language model",
    "abstract": "Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. Multimodal large language models (MLLMs) show promising capabilities in cross-modal understanding and visual-aware response generation via LMs. We investigate how MLLMs facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and our MGIE can lead to a notable improvement in automatic metrics and ",
    "openreview_id": "S1RKWSyZ2Y",
    "forum_id": "S1RKWSyZ2Y"
  },
  "analysis_timestamp": "2026-01-07T00:08:31.004546"
}
{
  "prior_works": [
    {
      "title": "Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics",
      "authors": "J. B. Kruskal",
      "year": 1977,
      "role": "Foundational uniqueness theorem for CP decompositions",
      "relationship_sentence": "The paper\u2019s identifiability proofs hinge on Kruskal-type k-rank conditions to guarantee uniqueness of the latent factors obtained from the tensor representations of polynomial layers, yielding degree\u2013width tradeoffs for generic identifiability."
    },
    {
      "title": "On the uniqueness of multilinear decomposition of N-way arrays",
      "authors": "N. D. Sidiropoulos, R. Bro",
      "year": 2000,
      "role": "Generalization of Kruskal\u2019s uniqueness to higher-order tensors",
      "relationship_sentence": "Extending Kruskal\u2019s criterion to N-way arrays underpins the authors\u2019 treatment of higher-degree polynomial activations (higher-order/symmetric tensors) and multi-layer compositions, enabling identifiability conditions beyond the trilinear case."
    },
    {
      "title": "On the uniqueness of the Canonical Polyadic Decomposition of third-order tensors\u2014Part I/II",
      "authors": "A. Domanov, L. De Lathauwer",
      "year": 2013,
      "role": "Sharper CPD uniqueness conditions and tools",
      "relationship_sentence": "Refined uniqueness criteria and constructive arguments from these works are leveraged to obtain layerwise and global identifiability in deep PNNs under mild genericity, especially when widths are non-increasing."
    },
    {
      "title": "On generic identifiability of symmetric tensors",
      "authors": "L. Chiantini, G. Ottaviani",
      "year": 2012,
      "role": "Generic identifiability for Waring (symmetric CP) decompositions",
      "relationship_sentence": "Since polynomial neurons with homogeneous activations correspond to sums of powers (Waring decompositions), these results directly inform when neuron parameters are generically recoverable from symmetric tensor representations."
    },
    {
      "title": "Polynomial interpolation in several variables",
      "authors": "J. Alexander, A. Hirschowitz",
      "year": 1995,
      "role": "Dimension/rank of secant varieties to Veronese (Alexander\u2013Hirschowitz theorem)",
      "relationship_sentence": "The dimension formulas for secant varieties of the Veronese variety are used to compute neurovariety dimensions and to settle the conjecture on the dimension of PNN neurovarieties, informing minimum activation degrees for identifiability."
    },
    {
      "title": "Convolutional rectifier networks as generalized tensor decompositions",
      "authors": "N. Cohen, A. Shashua",
      "year": 2016,
      "role": "Conceptual bridge between deep networks and tensor factorizations",
      "relationship_sentence": "This work motivates the constructive mapping from deep network computations to low-rank tensor (and tensor network) structures, a central proof device in deriving identifiability conditions for deep PNNs."
    },
    {
      "title": "Tensor Decompositions for Learning Latent Variable Models",
      "authors": "A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, M. Telgarsky",
      "year": 2014,
      "role": "Moment/tensor methods yielding identifiability in layered latent models",
      "relationship_sentence": "The strategy of proving identifiability via low-order moments and CP/Waring uniqueness informs the encoder\u2013decoder analysis, where controlled decoder width versus activation degree ensures recoverability."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014generic identifiability of deep polynomial neural networks (PNNs) with precise degree\u2013width tradeoffs and constructive proofs\u2014rests on recasting network layers as low-rank tensor factorizations whose uniqueness is well understood. Kruskal\u2019s classic k-rank theorem and its N-way generalizations by Sidiropoulos and Bro supply the primary uniqueness mechanism for canonical polyadic (CP) decompositions, letting the authors certify when factor matrices (weights) are uniquely determined from tensorized layer outputs. Domanov and De Lathauwer\u2019s sharper CPD conditions and constructive arguments further enable layerwise recovery under mild genericity, which the paper exploits to prove identifiability of architectures with non-increasing widths.\nFor polynomial activations, neurons correspond to symmetric tensors (sums of powers), so Waring identifiability results by Chiantini and Ottaviani directly specify when such decompositions are generically unique, tying activation degree to permissible layer width. The Alexander\u2013Hirschowitz theorem on secant varieties of the Veronese provides the dimension counts needed to analyze neurovarieties; these algebraic-geometry tools allow the authors to settle an open conjecture on their dimension and to bound degrees required for identifiability.\nCohen and Shashua\u2019s view of deep networks as generalized tensor decompositions supplies the conceptual bridge to compose layer-wise tensor maps into deep architectures. Finally, tensor-moment methods for latent variable models (Anandkumar et al.) inform the encoder\u2013decoder analysis: by controlling decoder widths relative to activation degrees, the authors obtain Kruskal-type uniqueness across layers, establishing identifiability for this important class of PNNs.",
  "analysis_timestamp": "2026-01-07T00:02:04.966375"
}
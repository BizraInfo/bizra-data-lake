{
  "prior_works": [
    {
      "title": "Visual Instruction Tuning",
      "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee",
      "year": 2023,
      "role": "MLLM backbone and training paradigm",
      "relationship_sentence": "LLaVA\u2019s recipe for turning vision-language models into instruction-following agents directly underpins Q-Insight\u2019s shift from numeric IQA scoring to explanatory, content-aware, degradation-aware quality reasoning."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, et al.",
      "year": 2022,
      "role": "RLHF framework for aligning models with human judgments",
      "relationship_sentence": "Q-Insight adopts the core RLHF principle from this work\u2014optimizing a policy against human-derived quality signals\u2014to align a multimodal policy with perceptual quality preferences instead of relying solely on supervised labels."
    },
    {
      "title": "Proximal Policy Optimization Algorithms",
      "authors": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov",
      "year": 2017,
      "role": "Algorithmic foundation for stable policy optimization",
      "relationship_sentence": "Q-Insight\u2019s GRPO training is a group-relative variant rooted in PPO\u2019s clipped policy optimization, leveraging its stability to optimize a vision-language policy for IQA reasoning."
    },
    {
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "authors": "Rafael Rafailov, Arman A. Askari, Esin Durmus, Dan Jurafsky, Stefano Ermon, Tatsunori B. Hashimoto",
      "year": 2023,
      "role": "Preference-based alignment without explicit RL",
      "relationship_sentence": "DPO showed the effectiveness of pairwise preference signals for alignment; Q-Insight similarly capitalizes on relative/comparative quality judgments but optimizes them within an RL (GRPO) framework to improve visual reasoning."
    },
    {
      "title": "PieAPP: Perceptual Image-Error Assessment Through Pairwise Preference",
      "authors": "Yash Prashnani, Hong Cai, Yasamin Mostofi, Pradeep Sen",
      "year": 2018,
      "role": "Pairwise human preference training for IQA",
      "relationship_sentence": "PieAPP established that pairwise human comparisons yield robust perceptual supervision; Q-Insight extends this insight to multimodal reasoning and RL optimization, using relative judgments to train with limited rating data."
    },
    {
      "title": "MUSIQ: Multi-scale Image Quality Transformer",
      "authors": "Junjie Ke, Hossein Talebi, et al.",
      "year": 2021,
      "role": "Strong NR-IQA baseline for MOS regression",
      "relationship_sentence": "MUSIQ exemplifies state-of-the-art score regression yet limited interpretability; Q-Insight targets this gap by replacing pure MOS regression with RL-trained, explanation-rich quality understanding across content, degradations, and comparisons."
    }
  ],
  "synthesis_narrative": "Q-Insight\u2019s core contribution\u2014training a multimodal model to perform explanation-rich, comparative image quality understanding with limited supervision via reinforcement learning\u2014emerges from the confluence of advances in multimodal instruction tuning, preference-based alignment, and perceptual IQA supervision. LLaVA demonstrated that visual-language models can be instruction-following agents, enabling not just answers but stepwise, content-aware reasoning\u2014exactly the expressivity Q-Insight needs to explain degradations and comparisons rather than output only MOS values.\n\nOn the alignment side, Ouyang et al.\u2019s RLHF showed how to optimize policies to match human preferences, while PPO provided the stable policy-gradient machinery on which Q-Insight\u2019s GRPO variant relies. DPO further emphasized that pairwise preferences are a powerful supervision signal, inspiring Q-Insight\u2019s use of relative/contrastive judgments to reduce annotation burdens and encourage comparison reasoning. From the IQA community, PieAPP established that pairwise human preference data can learn perceptually aligned quality metrics more robustly than absolute scores, a principle Q-Insight lifts into a multimodal RL setting. Meanwhile, MUSIQ represents the high-water mark of NR-IQA score regression, highlighting the remaining gap in interpretability and general reasoning that Q-Insight aims to fill. Together, these works directly shaped Q-Insight\u2019s design: a vision-language policy trained with group-relative policy optimization on preference-like quality signals to deliver interpretable, content- and degradation-aware IQA reasoning with minimal labeled scores.",
  "analysis_timestamp": "2026-01-07T00:21:33.177242"
}
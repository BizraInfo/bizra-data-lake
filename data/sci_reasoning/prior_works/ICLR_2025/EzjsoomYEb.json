{
  "prior_works": [
    {
      "title": "Weisfeiler and Lehman Go Topological: Message Passing Simplicial Networks",
      "authors": "Cristian Bodnar et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "This work instantiated higher-order message passing on simplicial complexes and connected it to WL expressivity, providing the core HOMP paradigm that the paper formalizes and then proves has topological blindspots (e.g., inability to capture homology and orientability)."
    },
    {
      "title": "Weisfeiler and Lehman Go Cellular: CW Networks",
      "authors": "Cristian Bodnar et al.",
      "year": 2021,
      "role": "Baseline",
      "relationship_sentence": "CW Networks are a principal HOMP-based baseline operating on cell complexes; the paper analyzes their expressivity limitations and introduces MCN/SMCN to enable multi-cell interactions that overcome these constraints."
    },
    {
      "title": "How Powerful Are Graph Neural Networks?",
      "authors": "Keyulu Xu et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "By establishing the WL-based lens for GNN expressivity, this paper provides the theoretical template that the authors extend to topological domains, showing analogous WL-style indistinguishability for HOMP on complexes (e.g., failure to detect diameter, planarity)."
    },
    {
      "title": "Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks",
      "authors": "Christopher Morris et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Higher-order (k-tuple) message passing via WL serves as the direct methodological precursor to HOMP; the paper analyzes why such lifting remains insufficient for key topological invariants and motivates the multi-cellular coupling introduced in MCN/SMCN."
    },
    {
      "title": "Provably Powerful Graph Networks",
      "authors": "Haggai Maron et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "This work shows how higher-order tensor operations and invariant pooling can surpass 1-WL; the new MCN/SMCN architectures borrow this design ethos to go beyond standard HOMP by coupling signals across cells and scales to capture global/topological properties."
    },
    {
      "title": "Can Graph Neural Networks Count Substructures?",
      "authors": "Zhengdao Chen et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "By demonstrating concrete counting limitations of message passing (e.g., cycles), this paper highlights structural blindspots that the authors generalize to topological invariants in HOMP and explicitly address with multi-cellular architectures."
    },
    {
      "title": "Topological Deep Learning",
      "authors": "Michael M. Bronstein et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "This unifying survey formalized TDL and the common HOMP perspective across simplicial, cellular, and hypergraph domains, directly setting the stage for the paper\u2019s expressivity analysis and the design criteria for MCN/SMCN."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014exposing expressivity blindspots of higher-order message passing (HOMP) in topological deep learning and introducing multi-cellular networks (MCN/SMCN) to overcome them\u2014sits squarely on a lineage that unified higher-order architectures and WL-style expressivity. Bodnar et al.\u2019s MPSN and CW Networks established HOMP on simplicial and cell complexes and tied these architectures to Weisfeiler\u2013Leman analysis, providing the precise targets this work interrogates. Xu et al.\u2019s WL-based framework for GNN expressivity supplied the methodological lens, which the authors extend from graphs to topological domains, proving failures to capture global/topological invariants such as diameter, planarity, orientability, and homology. Morris et al.\u2019s higher-order (k-tuple) GNNs showed how lifting enhances expressivity, but their residual limitations directly motivate the paper\u2019s result that HOMP cannot fully exploit lifting/pooling to recover key invariants. Maron et al.\u2019s provably powerful tensor-based architectures demonstrated that coupling higher-order interactions with invariant pooling can break WL barriers; this inspires the MCN/SMCN design, which couples signals across multiple cells and scales to capture global topology. Finally, Chen et al.\u2019s substructure counting limits sharpen the gap: message passing misses certain global/structural properties even with motif encodings, a deficiency the authors generalize to TDL and explicitly address. Bronstein et al.\u2019s survey codified TDL under a common HOMP umbrella, directly enabling the paper\u2019s unified expressivity analysis and guiding the new architectures\u2019 scope.",
  "analysis_timestamp": "2026-01-06T23:09:26.594534"
}
{
  "prior_works": [
    {
      "title": "Language Models are Few-Shot Learners",
      "authors": "Tom B. Brown et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "Introduced the modern in-context learning (ICL) phenomenon and few-shot prompting formulation that this paper mechanistically explains, providing the behavioral target whose rise and disappearance our analysis studies."
    },
    {
      "title": "In-Context Learning and Induction Heads",
      "authors": "Catherine Olsson et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Identified concrete ICL subcircuits (induction heads) in transformers; we build directly on this mechanistic lens to show that the same subcircuits are shared by both ICL and our discovered context-constrained in-weights learning (CIWL), enabling the coopetitive dynamics we analyze."
    },
    {
      "title": "Transformers Learn In-Context by Gradient Descent",
      "authors": "Moritz von Oswald et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "Showed that transformers can implement an in-context optimization algorithm, motivating our framing of ICL as a competing strategy whose training dynamics can be contrasted with an in-weights learner (CIWL) to explain ICL\u2019s transience."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Datasets",
      "authors": "Alethea Power et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Revealed phase-like training dynamics and strategy switching between memorization and algorithmic solutions; we adapt this perspective to the ICL-to-CIWL transition and provide a circuit-level mechanism for the switch."
    },
    {
      "title": "Progress Measures for Grokking via Mechanistic Interpretability",
      "authors": "Neel Nanda et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "Developed circuit-level diagnostics to track competing solutions during training; we extend this methodology to simultaneously track ICL and CIWL, showing shared subcircuits and coopetitive dynamics that drive ICL\u2019s emergence and eventual demise."
    },
    {
      "title": "Toy Models of Superposition",
      "authors": "Nelson Elhage et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "Demonstrated how feature sharing and interference arise under capacity constraints; we use this idea to explain why ICL and CIWL share subcircuits and how such superposition enables both cooperation and competition between the two strategies."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": "Andrew M. Saxe et al.",
      "year": 2014,
      "role": "Foundation",
      "relationship_sentence": "Provided a theoretical framework for multi-timescale learning dynamics and competition between representational modes; we apply this lens to interpret how slowly developing CIWL both enables the initial emergence of ICL and ultimately outcompetes it."
    }
  ],
  "synthesis_narrative": "The core contribution of this work\u2014a mechanistic account of why in-context learning (ICL) emerges transiently and then fades\u2014rests on three intertwined intellectual threads. First, Brown et al. defined the ICL phenomenon and few-shot problem setting we seek to explain, while Olsson et al. established concrete mechanistic circuits (induction heads) that implement ICL, giving us the subcircuit substrate to analyze. Second, prior views of ICL as an algorithm learned in-context (von Oswald et al.) motivated framing ICL as one of multiple competing strategies learned during training, inviting explicit comparison with an in-weights alternative. Third, the grokking literature (Power et al.; Nanda et al.) demonstrated phase-like training dynamics and provided circuit-level tools to track competing solutions, which we adapt to reveal that a previously uncharacterized hybrid, context-constrained in-weights learning (CIWL), both enables and eventually replaces ICL. Our explanation for this coopetition draws on superposition (Elhage et al.), which predicts subcircuit sharing and interference under capacity constraints, and on classic learning-dynamics theory (Saxe et al.), which anticipates multi-timescale emergence and competition between representational modes. Together, these works directly shape our findings: we identify CIWL as the asymptotic in-weights strategy, show that it shares induction-style subcircuits with ICL, and demonstrate that their shared circuitry induces cooperative early dynamics that enable ICL, followed by competitive dynamics that drive its transience.",
  "analysis_timestamp": "2026-01-06T23:07:19.589148"
}
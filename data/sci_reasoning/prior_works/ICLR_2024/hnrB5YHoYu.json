{
  "prior_works": [
    {
      "title": "Fairness GAN",
      "authors": "Sattigeri et al.",
      "year": 2018,
      "arxiv_id": "unknown",
      "role": "Foundation",
      "relationship_sentence": "It framed fairness as matching the distribution of sensitive attributes to a specified target, a formulation the paper directly adopts by replacing the GAN adversary with an explicit attribute-distribution loss over generated images."
    },
    {
      "title": "DreamFusion: Text-to-3D using 2D diffusion",
      "authors": "Poole et al.",
      "year": 2022,
      "arxiv_id": "2209.14988",
      "role": "Inspiration",
      "relationship_sentence": "By introducing score distillation sampling (SDS), it established the recipe of taking gradients of image-level objectives through a diffusion model\u2019s sampling process\u2014an idea the paper leverages to optimize fairness losses defined on generated images."
    },
    {
      "title": "ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation via 2D Diffusion Priors",
      "authors": "Wang et al.",
      "year": 2023,
      "arxiv_id": "unknown",
      "role": "Inspiration",
      "relationship_sentence": "It identified bias/instability in SDS gradients and proposed Variational Score Distillation (VSD) as a corrected gradient, directly motivating the paper\u2019s adjusted DFT that modifies the gradient signal for stable optimization of external (fairness) losses."
    },
    {
      "title": "ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation",
      "authors": "Xu et al.",
      "year": 2023,
      "arxiv_id": "2304.05977",
      "role": "Related Problem",
      "relationship_sentence": "It demonstrated practical pipelines for aligning T2I models by optimizing differentiable, image-level reward objectives on generated samples, which the paper generalizes from scalar preference rewards to a distributional fairness objective."
    },
    {
      "title": "Pick-a-Pic: An Open Dataset of Human Preferences for Text-to-Image Generation",
      "authors": "Kirstain et al.",
      "year": 2023,
      "arxiv_id": "2306.13955",
      "role": "Foundation",
      "relationship_sentence": "By establishing the preference-alignment setting for T2I (optimizing generated images to match target human-judged distributions), it directly motivates casting fairness as an explicit, target distribution alignment problem."
    },
    {
      "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation",
      "authors": "Ruiz et al.",
      "year": 2023,
      "arxiv_id": "2208.12242",
      "role": "Baseline",
      "relationship_sentence": "It established direct UNet fine-tuning of text-to-image diffusion models as a strong adaptation baseline, against which the paper positions its loss-driven (fairness) finetuning and gradient adjustment mechanism."
    },
    {
      "title": "An Image is Worth One Word: Personalizing Text-to-Image Generation via Textual Inversion",
      "authors": "Gal et al.",
      "year": 2022,
      "arxiv_id": "2208.01618",
      "role": "Foundation",
      "relationship_sentence": "It showed that optimizing a few learnable soft tokens can effectively steer diffusion outputs, which the paper uses to realize low-parameter fairness control by finetuning only a handful of tokens."
    }
  ],
  "synthesis_narrative": "Fairness GAN formalized fairness as a distributional constraint, enforcing outputs to match a desired sensitive-attribute distribution rather than optimizing instance-wise corrections. DreamFusion introduced score distillation sampling to take gradients of objectives defined on images generated by a diffusion model, thereby enabling direct optimization of external goals through the sampling process. ProlificDreamer later exposed biases and instability in the SDS gradient and provided a corrected alternative (VSD) for more stable optimization. ImageReward demonstrated that text-to-image diffusion models can be aligned by optimizing a differentiable, learned reward applied to generated images, validating the feasibility of direct loss-based finetuning. Pick-a-Pic established the preference-alignment paradigm\u2014optimizing generated image distributions to match human-judged targets\u2014by providing data and framing that emphasized distributional outcomes. DreamBooth proved that directly finetuning the diffusion UNet is an effective and practical adaptation mechanism. Textual Inversion showed that optimizing a few soft tokens can steer generation efficiently, offering a lightweight alternative to full-model finetuning.\nTogether, these works reveal a path to training-time alignment that matches attribute distributions to user-specified targets, realized by differentiating image-level objectives through the diffusion sampler. The instability cautions from SDS/VSD motivate adjusting the gradient to stably optimize such external losses. Preference alignment results suggest that moving from scalar rewards to explicit distributional targets is natural, while DreamBooth and Textual Inversion provide complementary finetuning routes: full-model updates or soft-token adaptation. Synthesizing these insights leads to a fairness-as-distributional-alignment loss and an adjusted direct finetuning procedure that reliably steers text-to-image diffusion outputs toward user-defined demographic distributions.",
  "target_paper": {
    "title": "Finetuning Text-to-Image Diffusion Models for Fairness",
    "authors": "Xudong Shen, Chao Du, Tianyu Pang, Min Lin, Yongkang Wong, Mohan Kankanhalli",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "Fairness, Alignment, Diffusion Models, Text-to-Image Generation",
    "abstract": "The rapid adoption of text-to-image diffusion models in society underscores an urgent need to address their biases. Without interventions, these biases could propagate a skewed worldview and restrict opportunities for minority groups. In this work, we frame fairness as a distributional alignment problem. Our solution consists of two main technical contributions: (1) a distributional alignment loss that steers specific characteristics of the generated images towards a user-defined target distribution, and (2) adjusted direct finetuning of diffusion model's sampling process (adjusted DFT), which leverages an adjusted gradient to directly optimize losses defined on the generated images. Empirically, our method markedly reduces gender, racial, and their intersectional biases for occupational prompts. Gender bias is significantly reduced even when finetuning just five soft tokens. Crucially, our method supports diverse perspectives of fairness beyond absolute equality, which is demonstrated",
    "openreview_id": "hnrB5YHoYu",
    "forum_id": "hnrB5YHoYu"
  },
  "analysis_timestamp": "2026-01-06T12:39:22.940414"
}
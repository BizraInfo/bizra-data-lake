{
  "prior_works": [
    {
      "title": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport",
      "authors": "L\u00e9na\u00efc Chizat; Francis Bach",
      "year": 2018,
      "role": "Foundational mean-field and Wasserstein gradient-flow formulation for two-layer networks",
      "relationship_sentence": "Introduced the optimal-transport/Wasserstein gradient-flow dynamics for distributions of neural parameters that this paper extends to symmetry-constrained (WI/SI) laws and to symmetry-leveraging training schemes."
    },
    {
      "title": "A Mean Field View of the Landscape of Two-Layer Neural Networks",
      "authors": "Song Mei; Andrea Montanari; Phan-Minh Nguyen",
      "year": 2018,
      "role": "Rigorous mean-field limit and McKean\u2013Vlasov PDE dynamics for overparameterized networks trained by (stochastic) gradient methods",
      "relationship_sentence": "Provides the empirical-measure and PDE framework that underpins the paper\u2019s mean-field limit, which the authors adapt to datasets with compact-group symmetries."
    },
    {
      "title": "Neural Networks as Interacting Particle Systems: Asymptotic Behavior and Global Convergence",
      "authors": "Jeffrey Rotskoff; Eric Vanden-Eijnden",
      "year": 2018,
      "role": "Interacting-particle/mean-field formulation of training and gradient flows over probability measures",
      "relationship_sentence": "Motivates modeling units as particles whose distribution evolves, a perspective the paper enriches by imposing group-invariant (WI) and fixed-point (SI) constraints on the particle law."
    },
    {
      "title": "Mean Field Analysis of Neural Networks: A Law of Large Numbers",
      "authors": "Justin Sirignano; Konstantinos Spiliopoulos",
      "year": 2020,
      "role": "Convergence of SGD to McKean\u2013Vlasov dynamics and propagation of chaos for wide networks",
      "relationship_sentence": "Supplies stochastic-approximation foundations justifying the SGD-to-mean-field limit the paper uses to derive Wasserstein gradient flows for DA, FA, and EA regimes."
    },
    {
      "title": "Group Equivariant Convolutional Networks",
      "authors": "Taco S. Cohen; Max Welling",
      "year": 2016,
      "role": "Canonical formulation of equivariant architectures via compact-group actions and group convolutions/pooling",
      "relationship_sentence": "Establishes the architectural enforcement of symmetry that this work captures measure-theoretically as strongly invariant (SI) parameter laws supported on group-fixed parameters."
    },
    {
      "title": "On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups",
      "authors": "Risi Kondor; Shubhendu Trivedi",
      "year": 2018,
      "role": "Representation-theoretic framework for equivariance over compact groups and homogeneous spaces",
      "relationship_sentence": "Informs the paper\u2019s general compact-group setting and formalizes how group actions act on parameters/features, enabling precise WI/SI definitions and analysis."
    },
    {
      "title": "Gradient Flows in Metric Spaces and in the Space of Probability Measures",
      "authors": "Luigi Ambrosio; Nicola Gigli; Giuseppe Savar\u00e9",
      "year": 2008,
      "role": "Mathematical foundation of Wasserstein gradient flows and variational formulations",
      "relationship_sentence": "Provides the variational and metric-space tools used to interpret the mean-field limits of DA, FA, and EA as Wasserstein gradient flows on parameter distributions."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014casting symmetry-leveraging training (data augmentation, feature averaging, and equivariant architectures) as Wasserstein gradient flows in a mean-field regime\u2014rests on fusing two mature lines of work: mean-field training dynamics and group-theoretic symmetry in deep learning. On the dynamics side, Chizat and Bach\u2019s optimal-transport perspective established that wide neural networks can be analyzed via gradient flows of probability measures over parameters, while Mei, Montanari, and Nguyen, together with the interacting-particle formulations of Rotskoff and Vanden-Eijnden and the SGD-to-McKean\u2013Vlasov convergence results of Sirignano and Spiliopoulos, furnished a rigorous PDE/particle foundation for measure evolution under SGD. This paper builds directly on that toolkit but introduces symmetry constraints on the evolving measure, formalized as weakly invariant (G-invariant) and strongly invariant (supported on group-fixed parameters) laws. On the symmetry side, Cohen and Welling\u2019s G-CNNs and Kondor and Trivedi\u2019s representation-theoretic treatment of compact-group actions provide the exact architectural and mathematical constructs the authors encode in the measure space (strong invariance capturing equivariant architectures; weak invariance capturing data-augmentation/feature-averaging effects). Ambrosio\u2013Gigli\u2013Savar\u00e9\u2019s general theory of Wasserstein gradient flows then enables a unified variational description: in the N\u2192\u221e limit, SGD with SL techniques becomes a gradient flow constrained by group symmetry, clarifying how DA, FA, and EA bias the learning dynamics and equilibria via invariant measures.",
  "analysis_timestamp": "2026-01-06T23:33:35.545879"
}
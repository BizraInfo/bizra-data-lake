{
  "prior_works": [
    {
      "title": "Deferred Neural Rendering: Image Synthesis using Neural Textures",
      "authors": "Justus Thies et al.",
      "year": 2019,
      "role": "Introduced editable, view-dependent neural texture (atlas) representations mapped from 2D to 3D surfaces.",
      "relationship_sentence": "NAGs generalize the neural-texture/atlas idea by making each scene-graph node a view-dependent neural atlas, retaining 2D editability while embedding nodes in a 3D, occlusion-aware composition."
    },
    {
      "title": "Neural Scene Flow Fields for Space-Time View Synthesis",
      "authors": "Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang",
      "year": 2020,
      "role": "Pioneered dynamic scene modeling with layered, occlusion-aware compositing via scene flow between frames.",
      "relationship_sentence": "NAGs inherit the insight that dynamic view synthesis benefits from layered decompositions, but move from a global foreground/background layering to per-object atlas nodes with explicit 3D ordering."
    },
    {
      "title": "DynIBaR: Neural Dynamic Image-Based Rendering",
      "authors": "Zhengqi Li, Richard Tucker, Noah Snavely",
      "year": 2023,
      "role": "Advanced dynamic IBR with a background canonicalization plus dynamic foreground handling and occlusion reasoning.",
      "relationship_sentence": "NAGs address DynIBaR\u2019s two-layer limitation by allowing many interacting, occluding elements as graph nodes, each represented as an editable neural atlas."
    },
    {
      "title": "GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields",
      "authors": "Michael Niemeyer, Andreas Geiger",
      "year": 2021,
      "role": "Popularized object-centric, compositional scene representations enabling 3D object placement and manipulation.",
      "relationship_sentence": "NAGs adopt the compositional, object-centric paradigm but replace volumetric feature fields with atlas-based nodes to make appearance edits simple and view-consistent."
    },
    {
      "title": "Neural Scene Graphs",
      "authors": "Sebastian Ost et al.",
      "year": 2021,
      "role": "Learned object-level scene graphs (poses, relations) from annotated driving datasets using implicit volumetric nodes.",
      "relationship_sentence": "Directly informing NAGs\u2019 graph structure and supervision regime, this work motivates object-centric nodes with 3D ordering; NAGs improve editability by making each node a neural atlas rather than an implicit volume."
    },
    {
      "title": "Object-NeRF: Towards Compositional Neural Radiance Fields",
      "authors": "Zhenpei Yang et al.",
      "year": 2021,
      "role": "Decomposed scenes into per-object radiance fields to enable object-level manipulation and occlusion handling.",
      "relationship_sentence": "NAGs extend object-wise decomposition to dynamic scenes and swap per-object volumetric radiance fields for view-dependent atlases to support high-resolution, 2D-friendly editing."
    },
    {
      "title": "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (Instant-NGP)",
      "authors": "Thomas M\u00fcller et al.",
      "year": 2022,
      "role": "Enabled fast test-time fitting of neural fields via efficient encodings, making high-res optimization practical.",
      "relationship_sentence": "NAGs\u2019 test-time fitting of multiple atlas-nodes is made feasible by the efficiency principles popularized by Instant-NGP, supporting high-resolution, rapid optimization in practice."
    }
  ],
  "synthesis_narrative": "Neural Atlas Graphs (NAGs) fuse two historically distinct lines of work: 2D-editable neural atlases and 3D object-centric scene graphs. On the atlas side, Deferred Neural Rendering (Thies et al., 2019) established neural textures as view-dependent 2D atlases that are easy to author and edit. Dynamic view-synthesis methods such as NSFF (Li et al., 2020) and DynIBaR (Li et al., 2023) demonstrated that dynamic scenes benefit from layered decompositions and occlusion-aware compositing, but they typically rely on a coarse two-layer (foreground/background) model that struggles with multiple occluding objects.\nOn the compositional side, GIRAFFE (Niemeyer & Geiger, 2021) and related object-centric NeRFs (e.g., Object-NeRF, 2021) showed that representing scenes as sets of transformable object nodes supports manipulation and 3D ordering. Neural Scene Graphs (Ost et al., 2021) further grounded this idea in the autonomous-driving domain, leveraging masks and bounding boxes to learn object nodes as implicit volumes\u2014powerful but difficult to edit consistently.\nNAGs combine these threads by making every scene-graph node a view-dependent neural atlas. This preserves the 2D editability and high resolution of atlas-based methods while enabling true multi-object composition with explicit 3D ordering and interactions from scene-graph models. Finally, the practicality of fitting many high-resolution nodes at test time is underpinned by efficiency advances such as Instant-NGP (M\u00fcller et al., 2022), which popularized fast encodings for neural fields. Together, these works directly motivate NAGs\u2019 core contribution: a hybrid scene-graph-of-atlases representation that is both editable and capable of modeling complex dynamic scenes.",
  "analysis_timestamp": "2026-01-07T00:21:32.336965"
}
{
  "prior_works": [
    {
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "STAIR\u2019s safety objective explicitly inherits the helpfulness\u2013harmlessness formulation introduced in this work and designs a reward that balances these two axes rather than defaulting to broad refusals."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "Constitutional AI is a primary safety-alignment baseline that relies on critique/revision and principled refusals, which STAIR replaces with safety-aware introspective reasoning and step-level preference optimization to mitigate over-refusal and jailbreak susceptibility."
    },
    {
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "authors": "Rafael Rafailov et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "STAIR builds its iterative preference optimization on DPO\u2019s preference-based objective and extends it to step-level (reasoning-step) preferences derived from SI-MCTS."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "STAIR adopts the idea of structured, stepwise reasoning over a search tree and adapts it by introducing safety-aware evaluation to guide the exploration of reasoning paths."
    },
    {
      "title": "Reasoning via Planning with Language Models (RAP)",
      "authors": "Shibo Hao et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "STAIR extends the MCTS-style search over chains of thought popularized by RAP by injecting a safety-informed reward to steer planning toward solutions that are both helpful and harmless."
    },
    {
      "title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
      "authors": "Noah Shinn et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "STAIR\u2019s introspective, self-improvement loop draws on Reflexion\u2019s core idea of models analyzing and critiquing their own reasoning to iteratively refine behavior."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "authors": "Andy Zou et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "This work exposes jailbreak vulnerabilities in refusal-based safety methods, a concrete gap STAIR addresses by training safety-aware chains of thought via SI-MCTS to resist such attacks."
    }
  ],
  "synthesis_narrative": "STAIR emerges at the intersection of safety alignment and structured reasoning. The problem framing and objective\u2014simultaneously optimizing helpfulness and harmlessness\u2014trace directly to Anthropic\u2019s HHH formulation, which STAIR operationalizes with a theoretically grounded reward that explicitly balances the two aims. Constitutional AI provides the leading baseline for safety via critique and principles, but its reliance on refusals can degrade task performance and remains jailbreak-prone; STAIR targets this limitation by replacing refusal-centric strategies with introspective analysis inside the reasoning process. On the optimization side, DPO supplies the core preference-learning paradigm that STAIR extends to reasoned, step-level supervision, letting the model internalize safety judgments at each stage rather than only at outcomes. For generating those stepwise signals, STAIR draws from the line of work that treats reasoning as search: Tree of Thoughts introduced tree-structured deliberation, while RAP demonstrated MCTS-style planning over chains of thought. STAIR modifies this machinery with a safety-informed evaluation\u2014SI-MCTS\u2014to prioritize paths that are both safe and useful. Finally, Reflexion\u2019s self-critique loop inspires STAIR\u2019s introspective reasoning, enabling self-improvement of safety-aware CoT. The need for robustness is underscored by jailbreak studies showing transferable adversarial prompts, a gap STAIR aims to close by aligning the reasoning steps themselves, not just the final responses.",
  "analysis_timestamp": "2026-01-06T23:07:19.636922"
}
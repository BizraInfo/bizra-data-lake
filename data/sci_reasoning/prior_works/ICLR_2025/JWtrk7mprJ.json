{
  "prior_works": [
    {
      "title": "Mat\u00e9rn Gaussian Processes on Riemannian Manifolds",
      "authors": "Viacheslav Borovitskiy et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "This work provides the practical shallow GP priors on manifolds (via Laplace\u2013Beltrami/SPDE constructions) that the new model explicitly builds upon and surpasses, addressing their stationarity-driven limitations by stacking residual manifold-aware layers."
    },
    {
      "title": "Deep Gaussian Processes",
      "authors": "Andreas Damianou et al.",
      "year": 2013,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "It introduced the core idea of composing GPs into multi-layer structures, which the present paper generalizes from Euclidean spaces to manifold-to-manifold layers."
    },
    {
      "title": "Doubly Stochastic Variational Inference for Deep Gaussian Processes",
      "authors": "Hugh Salimbeni et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "The paper delivers the scalable variational inference machinery for DGPs that is adapted here to manifold-valued layers, enabling practical training of deep GP compositions beyond Euclidean inputs."
    },
    {
      "title": "A Framework for Interdomain and Inducing Variables for Gaussian Processes",
      "authors": "Mark van der Wilk et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Its interdomain inducing-variable framework allows defining inducing features through linear operators, which is leveraged here for manifold SPDE/Laplace-Beltrami-based priors to make manifold deep layers scalable."
    },
    {
      "title": "Mat\u00e9rn Gaussian Processes for Vector Fields on Riemannian Manifolds (tangent-bundle GPs)",
      "authors": "Viacheslav Borovitskiy et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "By constructing GP priors over tangent-vector fields on manifolds, this work provides the geometric building block the new residual layers use to add tangent updates and map back to the manifold."
    },
    {
      "title": "Deep Residual Learning for Image Recognition",
      "authors": "Kaiming He et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "The residual connection principle directly inspires the layer design here, where small GP-predicted manifold perturbations allow deep models to remain stable and revert to near-identity (i.e., shallow) behavior when extra depth is unnecessary."
    }
  ],
  "synthesis_narrative": "Deep Gaussian Processes were introduced as compositions of Gaussian-process mappings, offering rich hierarchical function priors beyond shallow kernels. Doubly stochastic variational inference then made such deep models trainable at scale by sampling through layers with variationally learned inducing variables. A general framework for interdomain inducing variables further enabled inducing features defined via linear operators, crucial when kernels arise from differential operators rather than closed-form covariance functions. On the geometric side, Mat\u00e9rn Gaussian processes on Riemannian manifolds established practical, Laplace\u2013Beltrami/SPDE-based priors that respect manifold geometry and work well for scalar fields on curved spaces, while making clear the stationarity-driven limits of shallow models on complex, nonstationary data. Complementarily, Mat\u00e9rn-type constructions for vector fields on manifolds provided GP priors on tangent bundles, yielding geometrically consistent stochastic vector fields that can drive intrinsic updates on the manifold. Meanwhile, residual networks demonstrated that additive, small-step updates stabilize deep compositions and permit identity mappings that guard against overfitting.\nTaken together, these works suggest composing manifold-aware layers where each layer predicts a tangent-vector update with a Mat\u00e9rn manifold GP and then maps back to the manifold, i.e., an intrinsic residual step. Interdomain inducing variables and doubly stochastic VI make these manifold layers scalable, while residual design brings stability and a natural fallback to shallow behavior. Building on manifold Mat\u00e9rn priors for both scalar fields and vector fields yields layers that handle manifold-to-manifold mappings and support scalar or vector-field outputs, making deep GPs practical and well-calibrated for complex nonstationary phenomena on curved spaces.",
  "target_paper": {
    "title": "Residual Deep Gaussian Processes on Manifolds",
    "authors": "Kacper Wyrwal, Andreas Krause, Viacheslav Borovitskiy",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Gaussian processes, manifolds, deep Gaussian processes, probabilistic methods, variational inference, uncertainty quantification, geometric learning",
    "abstract": "We propose practical deep Gaussian process models on Riemannian manifolds, similar in spirit to residual neural networks.\nWith manifold-to-manifold hidden layers and an arbitrary last layer, they can model manifold- and scalar-valued functions, as well as vector fields.\nWe target data inherently supported on manifolds, which is too complex for shallow Gaussian processes thereon.\nFor example, while the latter perform well on high-altitude wind data, they struggle with the more intricate, nonstationary patterns at low altitudes.\nOur models significantly improve performance in these settings, enhancing prediction quality and uncertainty calibration, and remain robust to overfitting, reverting to shallow models when additional complexity is unneeded.\nWe further showcase our models on Bayesian optimisation problems on manifolds, using stylised examples motivated by robotics, and obtain substantial improvements in later stages of the optimisation process.\nFinally, we show our models to have ",
    "openreview_id": "JWtrk7mprJ",
    "forum_id": "JWtrk7mprJ"
  },
  "analysis_timestamp": "2026-01-06T11:39:49.270959"
}
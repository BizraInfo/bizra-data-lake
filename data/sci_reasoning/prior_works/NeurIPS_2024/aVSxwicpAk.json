{
  "prior_works": [
    {
      "title": "Scaling Laws for Neural Language Models",
      "authors": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei",
      "year": "2020",
      "role": "Empirical scaling-law baseline and compute tradeoff",
      "relationship_sentence": "Provided the canonical empirical power-law relations and compute\u2013data\u2013parameters tradeoffs that this paper seeks to theoretically explain and refine via a solvable model and phased compute-optimal analysis."
    },
    {
      "title": "Training Compute-Optimal Large Language Models",
      "authors": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, et al.",
      "year": "2022",
      "role": "Compute-optimal parameter\u2013data allocation",
      "relationship_sentence": "Identified the empirical compute-optimal frontier (Chinchilla) and revised exponents, directly motivating the paper\u2019s rigorous derivation of compute-optimal model size and scaling exponents across regimes."
    },
    {
      "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
      "authors": "Matthew D. Hoffman, Andrew Gelman, Tamara Broderick, Martin J. Wainwright, Dustin Tran; commonly cited as Mandt, Hoffman, and Blei",
      "year": "2017",
      "role": "Optimizer-noise theory",
      "relationship_sentence": "Linked SGD dynamics to a temperature/noise scale, informing how optimizer noise sets phase boundaries; the new paper leverages this to delineate regimes where optimization noise versus capacity dominate scaling."
    },
    {
      "title": "An Empirical Model of Large-Batch Training",
      "authors": "Sam McCandlish, Jared Kaplan, Dario Amodei, OpenAI Dota team",
      "year": "2018",
      "role": "Gradient noise scale and compute accounting",
      "relationship_sentence": "Introduced the gradient noise scale and practical FLOPs-based training cost, foundational for this paper\u2019s compute-limited analysis and the dependence of optimal parameter count on training compute."
    },
    {
      "title": "Surprises in High-Dimensional Ridgeless Least Squares: Double Descent",
      "authors": "Trevor Hastie, Andrea Montanari, Saharon Rosset, Ryan J. Tibshirani",
      "year": "2019",
      "role": "Capacity/interpolation effects",
      "relationship_sentence": "Characterized risk behavior across under- to over-parameterization, clarifying capacity-driven phases; the present work builds on these insights to position phase boundaries tied to model capacity."
    },
    {
      "title": "Generalization error of random features and kernel regression in high dimensions",
      "authors": "Song Mei, Andrea Montanari",
      "year": "2019",
      "role": "Random features and spectral structure",
      "relationship_sentence": "Provided precise high-dimensional learning-curve predictions for random-feature models, grounding the paper\u2019s solvable model that separates data/target complexity and feature embedding effects."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": "Andrew M. Saxe, James L. McClelland, Surya Ganguli",
      "year": "2013",
      "role": "Closed-form learning dynamics",
      "relationship_sentence": "Developed analytic learning curves via mode-wise dynamics, a methodological precursor to the paper\u2019s all-iteration loss-curve representation under one-pass SGD enabling phase-wise scaling analysis."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014a mathematically grounded, solvable neural scaling model that yields compute-optimal exponents and a 4+3 phase diagram\u2014sits at the intersection of empirical scaling laws and rigorous analyses of SGD and high-dimensional generalization. Empirically, Kaplan et al. established power-law loss trends and compute tradeoffs, while Hoffmann et al. refined the compute-optimal frontier (Chinchilla), motivating a theory that can predict when and why optimal model size shifts with compute. The present work supplies that theory by combining insights about optimizer noise and compute from Mandt et al. and McCandlish et al.: SGD\u2019s effective temperature and gradient-noise scale determine when optimization noise, rather than model capacity, limits performance\u2014demarcating key phase boundaries.\n\nOn the generalization side, Hastie et al.\u2019s double-descent results isolate capacity/interpolation effects, and Mei & Montanari\u2019s random-features analysis ties performance to spectral structure of data and targets. These foundations enable the authors\u2019 three-parameter model (data complexity, target complexity, parameter count) and their identification of phases governed by capacity, optimizer noise, and feature embedding. Finally, Saxe et al.\u2019s mode-wise, closed-form dynamics of gradient-based learning underpin the paper\u2019s exact representation of loss curves over iterations under one-pass SGD. Together, these works directly inform the new theory, allowing Paquette et al. to derive provable scaling exponents and compute-optimal parameter counts across regimes, unifying and extending empirical scaling observations with a principled, phase-dependent framework.",
  "analysis_timestamp": "2026-01-06T23:33:36.284730"
}
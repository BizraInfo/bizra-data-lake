{
  "prior_works": [
    {
      "title": "Black Box Variational Inference",
      "authors": "Rajesh Ranganath, Sean Gerrish, David M. Blei",
      "year": 2014,
      "role": "Foundational BBVI baseline using the ELBO with stochastic gradients",
      "relationship_sentence": "BaM is motivated as an alternative to ELBO-based BBVI whose high-variance gradient estimators and sensitivity to hyperparameters were identified in this work."
    },
    {
      "title": "Estimation of Non-Normalized Statistical Models by Score Matching",
      "authors": "Aapo Hyv\u00e4rinen",
      "year": 2005,
      "role": "Objective/score-based divergence (Fisher divergence) that compares distributions via their scores",
      "relationship_sentence": "BaM\u2019s core objective is a score-based divergence essentially rooted in Hyv\u00e4rinen\u2019s score matching, enabling reliance on target scores rather than normalization constants and yielding tractable updates for Gaussian families."
    },
    {
      "title": "Operator Variational Inference",
      "authors": "Rajesh Ranganath, Dustin Tran, David M. Blei",
      "year": 2016,
      "role": "General framework for VI objectives defined by operators (including score/Stein-based discrepancies)",
      "relationship_sentence": "BaM can be viewed as instantiating an operator-based VI objective centered on the score operator, aligning with OPVI\u2019s blueprint for black-box divergences beyond the ELBO."
    },
    {
      "title": "Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm",
      "authors": "Qiang Liu, Dilin Wang",
      "year": 2016,
      "role": "Score-based discrepancy and gradient flow for variational inference",
      "relationship_sentence": "SVGD demonstrated the practical power of score-based divergences for black-box inference, directly informing BaM\u2019s turn away from ELBOs toward score-driven objectives for improved optimization behavior."
    },
    {
      "title": "Conjugate-Computation Variational Inference",
      "authors": "Mohammad Emtiyaz Khan, Wu Lin",
      "year": 2017,
      "role": "Closed-form natural-gradient/mirror-descent style updates for exponential-family variational distributions",
      "relationship_sentence": "BaM\u2019s closed-form proximal update for full-covariance Gaussians echoes CVI\u2019s insight that structured updates in exponential families can yield stable, batch-wise moment/parameter matching."
    },
    {
      "title": "Expectation Propagation for Approximate Bayesian Inference",
      "authors": "Thomas P. Minka",
      "year": 2001,
      "role": "Moment matching updates via local divergence minimization",
      "relationship_sentence": "BaM\u2019s batch-and-match philosophy parallels EP\u2019s moment matching ethos, inspiring the idea that well-chosen divergences can admit closed-form Gaussian updates that stably track target moments."
    }
  ],
  "synthesis_narrative": "BaM\u2019s key contribution\u2014replacing ELBO optimization with a score-based divergence that admits closed-form proximal updates for Gaussian variational families\u2014sits at the intersection of three lines of work. First, Black Box Variational Inference established ELBO-centric stochastic optimization as the default paradigm but exposed practical issues of gradient variance and hyperparameter sensitivity that BaM explicitly targets. Second, the theoretical backbone for BaM\u2019s objective comes from score matching: Hyv\u00e4rinen\u2019s Fisher divergence formulates learning via score alignment, bypassing normalization constants and providing a natural, black-box route when only target scores are available. Operator Variational Inference and Stein-based methods (e.g., SVGD) broadened the VI toolkit to operator-defined objectives that leverage target scores, demonstrating both feasibility and advantages of score-driven discrepancies over ELBOs in practice. Third, BaM\u2019s algorithmic shape\u2014its batch-wise, closed-form parameter updates for Gaussian families\u2014draws on traditions of structured variational updates exemplified by CVI and Expectation Propagation, where mirror-descent/natural-gradient or moment-matching perspectives yield stable, analytical updates in exponential families. BaM synthesizes these threads by choosing a score-based divergence within an operator-VI lens and designing a proximal update that, for Gaussians, exactly matches target score conditions and provably yields exponential convergence under Gaussian targets. This combination directly addresses BBVI\u2019s variance/sensitivity while retaining black-box applicability across hierarchical and deep generative models.",
  "analysis_timestamp": "2026-01-07T00:02:04.890889"
}
{
  "prior_works": [
    {
      "title": "Attention Is All You Need",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, et al.",
      "year": 2017,
      "role": "Foundational architecture establishing full self-attention and KV caching",
      "relationship_sentence": "The paper\u2019s recall objective is benchmarked against full softmax attention, whose KV-cache drives the memory bottleneck that BASED seeks to alleviate while preserving recall."
    },
    {
      "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
      "authors": "Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Fran\u00e7ois Fleuret",
      "year": 2020,
      "role": "Core linear attention formulation with feature maps and fixed-size recurrent state",
      "relationship_sentence": "BASED\u2019s linear-attention branch directly leverages the insight that linearized attention maintains a constant-size state determined by feature dimension, enabling explicit control of the recall\u2013memory tradeoff."
    },
    {
      "title": "Performer: Lightweight, Efficient and Fast Training of Deep Neural Networks with FAVOR+",
      "authors": "Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, et al.",
      "year": 2020,
      "role": "Kernel-based softmax approximation for linear-time attention",
      "relationship_sentence": "By framing softmax attention via random feature kernels, Performer informs BASED\u2019s choice of linear-attention feature mappings and the notion that feature dimension parameterizes state size and global aggregation capacity."
    },
    {
      "title": "Longformer: The Long-Document Transformer",
      "authors": "Iz Beltagy, Matthew E. Peters, Arman Cohan",
      "year": 2020,
      "role": "Sliding-window (local) attention for long sequences",
      "relationship_sentence": "BASED\u2019s sliding-window component builds on Longformer\u2019s local attention pattern to preserve high-fidelity short-range recall while keeping per-token compute and memory manageable."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "authors": "Albert Gu, Tri Dao",
      "year": 2024,
      "role": "State-space model alternative with fixed-size recurrent state",
      "relationship_sentence": "Mamba exemplifies efficient fixed-state sequence models that BASED theoretically and empirically contrasts, motivating the identified tradeoff wherein fixed state impairs long-range recall."
    },
    {
      "title": "RWKV: Reinventing RNNs for the Transformer Era",
      "authors": "Bo Peng (BlinkDL) et al.",
      "year": 2023,
      "role": "RNN/attention hybrid with constant-size state for autoregressive LMs",
      "relationship_sentence": "RWKV serves as a key comparison point showing that constant-state recurrent approaches achieve throughput but degrade on recall tasks, sharpening BASED\u2019s recall\u2013throughput Pareto framing."
    },
    {
      "title": "Hungry Hungry Hippos: Towards Language Modeling with State Space Models (H3)",
      "authors": "Daniel Y. Fu, Tri Dao, et al.",
      "year": 2023,
      "role": "Early SSM-based LM exploring long-context modeling with compact states",
      "relationship_sentence": "H3 provides the SSM paradigm that BASED contrasts; its fixed-size state highlights the limitations in retrieving arbitrary distant tokens that BASED overcomes by coupling linear and local attention."
    }
  ],
  "synthesis_narrative": "The core contribution of BASED is to expose and exploit a precise recall\u2013throughput (memory) tradeoff governed by a model\u2019s recurrent state size, and to realize a simple, tunable architecture that rides the Pareto frontier by combining linear attention with sliding-window attention. This builds on the strengths and weaknesses of prior paradigms. Full softmax attention (Vaswani et al., 2017) sets the high-recall gold standard but incurs prohibitive KV-cache memory at inference. Linear attention works (Katharopoulos et al., 2020; Choromanski et al., 2020) recast attention as a constant-state recurrence whose capacity scales with the feature dimension, directly inspiring BASED\u2019s \u201cdial\u201d for global aggregation with predictable memory cost. Meanwhile, sparse/local patterns such as Longformer (Beltagy et al., 2020) demonstrate that sliding-window attention can retain strong short-range recall at low compute, motivating BASED\u2019s local branch to capture precise nearby dependencies cheaply.\nIn contrast, state-space and RNN-like alternatives\u2014H3, Mamba, and RWKV\u2014show that fixed-size recurrent states yield excellent throughput but struggle to recall arbitrary distant tokens, empirically motivating the tradeoff BASED formalizes. By explicitly decomposing recall into a local, high-fidelity channel (sliding window) and a global, linear kernel channel whose state dimension is user-controlled, BASED inherits the efficiency of fixed-state models while recovering the strong recall behavior of attention. These prior works collectively shaped BASED\u2019s insight that recall capacity is essentially a function of state size, and its design that cleanly interpolates between memory footprint and recall fidelity.",
  "analysis_timestamp": "2026-01-07T00:02:04.872452"
}
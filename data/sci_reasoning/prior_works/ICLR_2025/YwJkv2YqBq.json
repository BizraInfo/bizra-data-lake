{
  "prior_works": [
    {
      "title": "Introductory Lectures on Convex Optimization: A Basic Course",
      "authors": "Yurii Nesterov",
      "year": 2004,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This monograph formalized Nesterov\u2019s accelerated gradient (NAG) and its optimal rates under convex/strongly convex assumptions\u2014the exact algorithm and guarantees that the current work extends to benignly non-convex settings."
    },
    {
      "title": "A Differential Equation for Modeling Nesterov\u2019s Accelerated Gradient: Theory and Insights",
      "authors": "Weijie Su et al.",
      "year": 2016,
      "arxiv_id": "1503.01243",
      "role": "Extension",
      "relationship_sentence": "Their continuous-time ODE model for NAG is the template the current paper adapts to analyze accelerated dynamics under benign non-convex geometry and in the presence of stochastic noise."
    },
    {
      "title": "A Variational Perspective on Accelerated Methods",
      "authors": "Wilson et al.",
      "year": 2016,
      "arxiv_id": "1603.04245",
      "role": "Inspiration",
      "relationship_sentence": "The Bregman-Lagrangian/Lyapunov framework provided here informs the energy-based analyses that the current work tailors to non-convex but PL/benign landscapes for both continuous and discrete NAG."
    },
    {
      "title": "Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak\u2013\u0141ojasiewicz Condition",
      "authors": "Hamed Karimi et al.",
      "year": 2016,
      "arxiv_id": "1608.04636",
      "role": "Foundation",
      "relationship_sentence": "This paper established the PL (gradient dominance) condition as a benign non-convex geometry yielding strong convergence guarantees, which the current work leverages to obtain accelerated-style rates for NAG."
    },
    {
      "title": "Accelerated Gradient Methods for Nonconvex Nonlinear and Stochastic Programming",
      "authors": "Saeed Ghadimi et al.",
      "year": 2016,
      "arxiv_id": "1607.00321",
      "role": "Gap Identification",
      "relationship_sentence": "By highlighting that classical acceleration results largely require convexity and offering limited nonconvex guarantees, this work frames the gap the current paper closes by proving NAG guarantees under structured nonconvexity."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot et al.",
      "year": 2018,
      "arxiv_id": "1806.07572",
      "role": "Foundation",
      "relationship_sentence": "The NTK regime\u2019s local linearization implies PL/strongly-convex-like behavior in overparameterized networks, directly supporting the paper\u2019s claim that benign non-convex assumptions hold in deep learning."
    },
    {
      "title": "Gradient Descent Finds Global Minima of Deep Neural Networks",
      "authors": "Simon S. Du et al.",
      "year": 2019,
      "arxiv_id": "1811.03804",
      "role": "Foundation",
      "relationship_sentence": "By proving global convergence of gradient methods in overparameterized nets via favorable curvature/error-bound properties, this work justifies the practical relevance of the benign landscape assumptions used to analyze NAG."
    }
  ],
  "synthesis_narrative": "Nesterov introduced the accelerated gradient method and its optimal rates for convex and strongly convex objectives, establishing the algorithmic template and guarantees that define acceleration. Su, Boyd, and Cand\u00e8s recast Nesterov\u2019s method as a second-order differential equation, enabling Lyapunov- and energy-based analyses of acceleration in continuous time. Wibisono, Wilson, and Jordan broadened this picture through a Bregman-Lagrangian framework, systematizing Lyapunov constructions that can be tailored to different geometries. Karimi, Nutini, and Schmidt identified the Polyak\u2013\u0141ojasiewicz (PL) condition as a benign non-convex structure under which gradient methods enjoy linear convergence without convexity\u2014pinpointing a precise weakening of convexity that still yields strong rates. Ghadimi and Lan cataloged what acceleration can and cannot guarantee beyond convexity, underscoring that classical proofs largely rely on convex geometry and leaving a gap for nonconvex acceleration theory. In parallel, Jacot, Gabriel, and Hongler\u2019s NTK analysis and Du et al.\u2019s overparameterization results showed that deep networks operate in regimes with locally linearized or curvature-controlled dynamics, implying PL-like behavior and error bounds. Together, these works suggested a path: import the ODE/Lyapunov machinery of accelerated methods into a PL/benignly non-convex setting that is empirically relevant for deep learning, and extend the analysis to discrete iterations and stochastic gradients. The current paper follows this trajectory, proving NAG-style guarantees under benign non-convexity and validating the assumptions via overparameterized neural network theory, thereby closing the convexity-to-practice gap.",
  "target_paper": {
    "title": "Nesterov acceleration in benignly non-convex landscapes",
    "authors": "Kanan Gupta, Stephan Wojtowytsch",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Nonconvex optimization, stochastic optimization, stochastic acceleration, smooth convex optimization, deep learning, accelerated gradient descent",
    "abstract": "While momentum-based optimization algorithms are commonly used in the notoriously non-convex optimization problems of deep learning, their analysis has historically been restricted to the convex and strongly convex setting. In this article, we partially close this gap between theory and practice and demonstrate that virtually identical guarantees can be obtained in optimization problems with a 'benign' non-convexity. We show that these weaker geometric assumptions are well justified in overparametrized deep learning, at least locally. Variations of this result are obtained for a continuous time model of Nesterov's accelerated gradient descent algorithm (NAG), the classical discrete time version of NAG, and versions of NAG with stochastic gradient estimates with purely additive noise and with noise that exhibits both additive and multiplicative scaling.",
    "openreview_id": "YwJkv2YqBq",
    "forum_id": "YwJkv2YqBq"
  },
  "analysis_timestamp": "2026-01-06T09:29:39.007916"
}
{
  "prior_works": [
    {
      "title": "Language Models as Knowledge Bases?",
      "authors": "Fabio Petroni et al.",
      "year": 2019,
      "arxiv_id": "1909.01066",
      "role": "Foundation",
      "relationship_sentence": "This work formalized cloze-style probing of factual relations and released LAMA/T-REx relations, providing the exact problem formulation and evaluation setting that the current paper uses to test linear relation decoding."
    },
    {
      "title": "Linguistic Regularities in Continuous Space Word Representations",
      "authors": "Tomas Mikolov et al.",
      "year": 2013,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By showing that many semantic relations manifest as linear offsets in word embeddings, this paper directly motivates the hypothesis that relational knowledge might be linearly decodable from language model representations."
    },
    {
      "title": "Translating Embeddings for Modeling Multi-relational Data",
      "authors": "Antoine Bordes et al.",
      "year": 2013,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "This work models knowledge-graph relations as additive translations between entity embeddings, inspiring the idea that a single linear operator could map subject representations to object predictions for specific relations."
    },
    {
      "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
      "authors": "Mor Geva et al.",
      "year": 2021,
      "arxiv_id": "2012.14913",
      "role": "Inspiration",
      "relationship_sentence": "By demonstrating that MLP layers act as key-value stores that insert relation-relevant value vectors into the residual stream, this paper motivated approximating relation decoding with a simple (near-linear) transformation on the subject representation."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT",
      "authors": "Kevin Meng et al.",
      "year": 2022,
      "arxiv_id": "2202.05262",
      "role": "Gap Identification",
      "relationship_sentence": "This paper showed factual associations are causally localized and editable via rank-one updates but did not test whether a single linear map can generalize across subjects for a relation, a gap the current work addresses with first-order linearization."
    },
    {
      "title": "A Structural Probe for Finding Syntax in Word Representations",
      "authors": "John Hewitt and Christopher D. Manning",
      "year": 2019,
      "arxiv_id": "1906.04249",
      "role": "Inspiration",
      "relationship_sentence": "Introducing linear probes that recover structured relations (e.g., tree distances) from contextualized representations provided the methodological precedent for using linear maps to decode relational information from LM hidden states."
    }
  ],
  "synthesis_narrative": "Cloze probing established that pretrained language models encode factual relations that can be elicited with minimal context, with LAMA/T-REx defining a standardized suite of subject\u2013relation\u2013object queries and evaluation protocols. Earlier, linear regularities in static word embeddings revealed that many semantic relations correspond to simple vector arithmetic, suggesting that relations may have linear signatures in representation spaces. In knowledge-graph embeddings, relations were explicitly modeled as additive translations between entities, providing a concrete template for linear relational operators. Mechanistic studies of transformers then showed that MLP layers behave like key\u2013value memories, injecting relation-relevant value vectors into the residual stream through near-linear composition, while causal editing work localized factual associations and modified them with rank-one interventions. Parallelly, structural probing demonstrated that complex relational structure can be decoded from contextual representations using learned linear mappings.\nThese threads jointly highlight a tantalizing possibility: relational knowledge in language models may be readable through simple linear maps, yet prior approaches either relied on trained probes across many examples or focused on static embeddings or causal edits. The natural next step is to analytically derive a relation-specific linear operator from a single prompt via a first-order approximation, then test whether this operator generalizes across subjects and relations\u2014thereby directly assessing when and how relation decoding is truly linear in transformer language models.",
  "target_paper": {
    "title": "Linearity of Relation Decoding in Transformer Language Models",
    "authors": "Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, Yonatan Belinkov, David Bau",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Natural language processing, interpretability, language models",
    "abstract": "Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in transformer LMs.",
    "openreview_id": "w7LU2s14kE",
    "forum_id": "w7LU2s14kE"
  },
  "analysis_timestamp": "2026-01-06T10:54:59.644537"
}
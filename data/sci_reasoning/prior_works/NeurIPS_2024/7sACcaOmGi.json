{
  "prior_works": [
    {
      "title": "Near-Optimal Reinforcement Learning in Polynomial Time",
      "authors": [
        "Michael Kearns",
        "Satinder Singh"
      ],
      "year": 2002,
      "role": "Simulator/generative-model oracle baseline",
      "relationship_sentence": "Established the algorithmic and statistical power of simulator (generative model) access in RL, providing a conceptual benchmark that the present paper refines by showing strong guarantees with a much weaker local-reset simulator."
    },
    {
      "title": "Contextual Decision Processes with Low Bellman Rank are PAC-Learnable",
      "authors": [
        "Nan Jiang",
        "Akshay Krishnamurthy",
        "Alekh Agarwal",
        "John Langford",
        "Robert E. Schapire"
      ],
      "year": 2017,
      "role": "General structural conditions for rich-observation RL",
      "relationship_sentence": "Introduced a unifying framework and structural measures (e.g., Bellman rank) for sample-efficient RL with function approximation, against which the new results position themselves by achieving guarantees under weaker assumptions using local planning."
    },
    {
      "title": "Provably Efficient Reinforcement Learning with Rich Observations via Latent State Decoding",
      "authors": [
        "Wen Sun",
        "Nan Jiang",
        "Akshay Krishnamurthy",
        "Alekh Agarwal"
      ],
      "year": 2019,
      "role": "Block MDPs: positive results under representation/decoding assumptions",
      "relationship_sentence": "Provided the canonical learnability results for Block MDPs via latent-state decoding, a subclass subsumed by coverability; the new paper attains guarantees for this family using only Q*-realizability with local resets."
    },
    {
      "title": "Model-Based Reinforcement Learning in Contextual Decision Processes (Witness Rank framework)",
      "authors": [
        "Wen Sun",
        "Akshay Krishnamurthy",
        "Alekh Agarwal",
        "Nan Jiang",
        "Sham M. Kakade"
      ],
      "year": 2019,
      "role": "Model-based RL with general function approximation under a generative model",
      "relationship_sentence": "Showed that with a strong simulator (generative model) one can learn broad classes characterized by witness rank; the present work narrows the oracle to local simulator access while still obtaining strong guarantees under coverability and Q*-realizability."
    },
    {
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation",
      "authors": [
        "Chi Jin",
        "Zhuoran Yang",
        "Zhaoran Wang"
      ],
      "year": 2020,
      "role": "Low-rank/linear MDP baseline with stronger representation assumptions",
      "relationship_sentence": "Delivered sample-efficient online RL under linear/low-rank structure via strong Bellman completeness-type assumptions, which the new paper relaxes by leveraging resets and requiring only Q*-realizability."
    },
    {
      "title": "PAC Reinforcement Learning with Rich Observations",
      "authors": [
        "Akshay Krishnamurthy",
        "Alekh Agarwal",
        "John Langford",
        "Lihong Li"
      ],
      "year": 2016,
      "role": "Hardness and motivation in rich-observation settings",
      "relationship_sentence": "Highlighted fundamental difficulties of online RL with rich observations without additional structure, motivating both structured classes (e.g., Block/low-rank) and the search for stronger interaction models such as local resets used here."
    },
    {
      "title": "Policy Coverability in Reinforcement Learning",
      "authors": [
        "Tengyang Xie et al."
      ],
      "year": 2023,
      "role": "Coverability: the structural condition targeted by this paper",
      "relationship_sentence": "Introduced and analyzed policy coverability, a broad condition subsuming Block and Low-Rank MDPs; the present work leverages local simulator access to obtain sample-efficient learning under coverability with only Q*-realizability."
    }
  ],
  "synthesis_narrative": "This paper\u2019s key contribution\u2014showing that local simulator access (resets to previously seen states for local planning) enables sample-efficient online RL under the broad coverability condition with only Q*-realizability\u2014builds on three intertwined lines of work. First, classic results on simulator power (Kearns & Singh, 2002) and subsequent model-based advances under strong generative-model oracles (e.g., witness-rank frameworks) demonstrated that richer interaction models can dramatically improve statistical efficiency in RL with function approximation. The present paper sharpens this insight by identifying a substantially weaker oracle\u2014local resets\u2014that still suffices for strong guarantees. Second, structural program developments for rich-observation RL\u2014CDPs and Bellman rank (Jiang et al., 2017), Block MDPs and latent-state decoding (Sun et al., 2019), and linear/low-rank MDPs (Jin et al., 2020)\u2014provided positive results but typically required strong representation assumptions (e.g., Bellman completeness or decoding). By targeting policy coverability (Xie et al., 2023), which subsumes Block and Low-Rank MDPs, the paper advances this line by proving learnability with only Q*-realizability when local resets are available. Third, hardness results in rich-observation settings (Krishnamurthy et al., 2016) underscored the limits of standard online interaction without additional structure, justifying the search for enhanced protocols. The synthesis here is a principled demonstration that modest simulator capabilities\u2014local planning via resets\u2014close a longstanding gap: they unlock guarantees previously thought to require stronger oracles or stronger representational assumptions, and they resolve challenging instances (e.g., exogenous Block MDPs) within the coverability regime.",
  "analysis_timestamp": "2026-01-06T23:33:35.524302"
}
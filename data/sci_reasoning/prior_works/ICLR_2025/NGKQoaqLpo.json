{
  "prior_works": [
    {
      "title": "Language Models as Knowledge Bases?",
      "authors": "Fabio Petroni et al.",
      "year": 2019,
      "arxiv_id": "1909.01066",
      "role": "Foundation",
      "relationship_sentence": "This work established using cloze-style token probabilities to probe factual associations in LMs, which directly underpins the paper\u2019s predictor that pre-update keyword probabilities forecast how newly learned facts will permeate and prime unrelated generations."
    },
    {
      "title": "Language Models (Mostly) Know What They Know",
      "authors": "Saurav Kadavath et al.",
      "year": 2022,
      "arxiv_id": "2207.05221",
      "role": "Inspiration",
      "relationship_sentence": "By showing model confidence/log-probabilities correlate with factual correctness and awareness, it motivated the paper\u2019s key insight that pre-training token probabilities of key terms can quantitatively predict the strength of post-training priming."
    },
    {
      "title": "Editing Factual Knowledge in Language Models",
      "authors": "Nicola De Cao et al.",
      "year": 2021,
      "arxiv_id": "2104.08164",
      "role": "Foundation",
      "relationship_sentence": "This paper introduced the targeted knowledge-editing problem and locality/generalization evaluation protocols that the current work adapts to the standard gradient-updating setting to study how new information spreads."
    },
    {
      "title": "Fast Model Editing at Scale (MEND)",
      "authors": "Eric Mitchell et al.",
      "year": 2022,
      "arxiv_id": "2110.11309",
      "role": "Related Problem",
      "relationship_sentence": "MEND\u2019s goal of making localized updates with minimal side-effects and its locality metrics directly inform the present paper\u2019s measurement of unintended spread (\u201cpriming\u201d) and the need for techniques that constrain it during fine-tuning."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT (ROME)",
      "authors": "Kevin Meng et al.",
      "year": 2022,
      "arxiv_id": "2202.05262",
      "role": "Gap Identification",
      "relationship_sentence": "ROME documented that parametric fact edits can leak broadly (imperfect locality) and introduced CounterFact to test this, a limitation that motivates the new Outlandish probes and the paper\u2019s analysis of broader priming under standard training."
    },
    {
      "title": "Mass-Editing Memory in a Transformer (MEMIT)",
      "authors": "Kevin Meng et al.",
      "year": 2023,
      "arxiv_id": "2210.07229",
      "role": "Related Problem",
      "relationship_sentence": "By showing how multiple edits propagate across layers and contexts and formalizing generalization/locality trade-offs, MEMIT provides the immediate backdrop for analyzing how a single newly learned fact can permeate and trigger undesired priming."
    },
    {
      "title": "Overcoming catastrophic forgetting in neural networks (EWC)",
      "authors": "James Kirkpatrick et al.",
      "year": 2017,
      "arxiv_id": "1612.00796",
      "role": "Baseline",
      "relationship_sentence": "As the canonical regularization baseline for controlling interference in continual learning, EWC serves as the primary comparator the paper contrasts against when proposing dilution strategies that reduce priming without incurring forgetting."
    }
  ],
  "synthesis_narrative": "Cloze-probing established that token probabilities can reveal stored factual associations in language models, grounding a quantitative way to read out latent knowledge before any update. Subsequent evidence showed that models\u2019 own log-probabilities correlate with factual correctness and self-knowledge, suggesting that simple likelihood signals can predict when a model will be confident\u2014and potentially overconfident\u2014about applying information. Targeted editing work then formalized the task of inserting or changing facts while evaluating locality versus generalization, defining concrete metrics and setups for assessing how an update should and should not spread. MEND operationalized localized edits at scale while emphasizing side-effect measurement, and ROME pinpointed where factual associations live and demonstrated that even carefully localized edits can leak, with CounterFact giving a standardized probe of overreach. MEMIT extended these ideas to many edits, revealing how updates propagate across layers and contexts and sharpening notions of desired versus undesired generalization. In parallel, elastic weight consolidation provided the standard regularization approach to suppress interference during sequential learning, though it is agnostic to the semantic structure of the new information.\nTogether, these strands exposed a gap: we lacked a systematic understanding of how ordinary gradient-based learning causes new facts to permeate unrelated contexts, and a principled, predictive handle on when that would happen. Building on likelihood-as-knowledge readouts and edit-locality diagnostics, the paper introduces diverse probes to observe this priming phenomenon, shows that pre-update keyword probabilities predict its strength across models, and proposes dilution techniques that surpass generic interference regularizers by specifically targeting the mechanisms that drive undesired spread.",
  "target_paper": {
    "title": "How new data permeates LLM knowledge and how to dilute it",
    "authors": "Chen Sun, Renat Aksitov, Andrey Zhmoginov, Nolan Andrew Miller, Max Vladymyrov, Ulrich Rueckert, Been Kim, Mark Sandler",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "fine-tuning, hallucinations, knowledge injection, memory, LLMs",
    "abstract": "Large language models continually learn through the accumulation of gradient-based updates, but how individual pieces of new information affect existing knowledge, leading to both beneficial generalization and problematic hallucination, remains poorly understood. We demonstrate that when learning new information, LLMs exhibit a \"priming\" effect: learning a new fact can cause the model to inappropriately apply that knowledge in unrelated contexts.\nTo systematically study this phenomenon, we introduce \"Outlandish,\" a carefully curated dataset of 1320 diverse text samples designed to probe how new knowledge permeates through an LLM's existing knowledge base. Using this dataset, we show that the degree of priming after learning new information can be predicted by measuring the token probability of key words before training. This relationship holds robustly across different model architectures (PALM-2, Gemma, Llama), sizes, and training stages.\nFinally, we develop two novel techniques to mo",
    "openreview_id": "NGKQoaqLpo",
    "forum_id": "NGKQoaqLpo"
  },
  "analysis_timestamp": "2026-01-06T05:53:18.290693"
}
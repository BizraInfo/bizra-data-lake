{
  "prior_works": [
    {
      "title": "Bayesian Learning for Neural Networks",
      "authors": "Radford M. Neal",
      "year": 1996,
      "role": "NN\u2013GP correspondence foundation",
      "relationship_sentence": "SNEFY\u2019s tractable normalizers hinge on treating infinitely wide neural networks as Gaussian processes; Neal\u2019s result is the seminal link enabling the kernel-based analytical treatment used to normalize squared network outputs."
    },
    {
      "title": "Gaussian Processes for Machine Learning",
      "authors": "Carl E. Rasmussen, Christopher K. I. Williams",
      "year": 2006,
      "role": "GP theory and kernel calculus",
      "relationship_sentence": "Provides the GP/kernels toolkit (Mercer expansions, integrals, conditioning/marginals) that SNEFY leverages to compute closed-form normalizing constants and to derive conditioning and marginalization properties."
    },
    {
      "title": "Kernel Methods for Deep Learning",
      "authors": "Youngmin Cho, Lawrence K. Saul",
      "year": 2009,
      "role": "Analytic NNGP kernels for common activations",
      "relationship_sentence": "Arc-cosine kernels give closed-form expressions for infinite-width ReLU-like networks, which SNEFY uses to obtain analytic integrals of squared network outputs under common base measures."
    },
    {
      "title": "Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View",
      "authors": "Amit Daniely, Roy Frostig, Yoram Singer",
      "year": 2016,
      "role": "Activation\u2013kernel duality under Gaussian inputs",
      "relationship_sentence": "Their Hermite-based dual-activation framework yields explicit kernel/eigensystems under Gaussian measures, directly enabling SNEFY\u2019s closed-form normalizers and tractable marginals for Gaussian base measures."
    },
    {
      "title": "Deep Neural Networks as Gaussian Processes",
      "authors": "Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein",
      "year": 2018,
      "role": "Modern NNGP for deep architectures",
      "relationship_sentence": "Extends NN\u2013GP correspondence to deep nets with practical kernel recursions; SNEFY relies on these depth-recursive kernels to instantiate flexible yet analyzable squared-network densities."
    },
    {
      "title": "Kernel Mean Embedding of Distributions: A Review and Beyond",
      "authors": "Krikamol Muandet, Kenji Fukumizu, Bharath K. Sriperumbudur, Bernhard Sch\u00f6lkopf",
      "year": 2017,
      "role": "Kernel expectations and operators",
      "relationship_sentence": "Kernel mean/covariance operator identities underpin SNEFY\u2019s computation of normalizers (integrals of k(x,x\u2032)) and facilitate closure under conditioning/marginalization via operator calculus with respect to base measures."
    },
    {
      "title": "Graphical Models, Exponential Families, and Variational Inference",
      "authors": "Martin J. Wainwright, Michael I. Jordan",
      "year": 2008,
      "role": "Exponential-family structure and properties",
      "relationship_sentence": "SNEFY positions itself as a strict generalization of exponential families and inherits EF-like closure properties; this work provides the canonical formalism that guided SNEFY\u2019s design and theoretical guarantees."
    }
  ],
  "synthesis_narrative": "Squared Neural Families (SNEFY) achieve tractable, flexible density modeling by defining densities as the squared 2-norm of a neural network, normalized with respect to a base measure. The central technical lever is the infinite-width neural network\u2013Gaussian process correspondence. Neal (1996) established this bridge, while Rasmussen and Williams (2006) provided the GP toolkit\u2014Mercer expansions, conditioning and marginalization rules, and kernel integral identities\u2014that SNEFY exploits to compute normalizing constants and to prove closure under conditioning and tractable marginals.\n\nCrucially, SNEFY\u2019s closed-form normalizers depend on having analytic neural kernels. Cho and Saul (2009) derived arc-cosine kernels for ReLU-like activations, and Daniely et al. (2016) gave a dual-activation/Hermite framework under Gaussian inputs, yielding explicit eigensystems and kernel recursions. These results make integrals of squared network outputs with respect to Gaussian (and related) base measures computable in closed form. Lee et al. (2018) extended NN\u2013GP correspondence to deep architectures with practical kernel recurrences, allowing SNEFY to retain tractability while scaling depth and expressivity.\n\nFinally, kernel mean embedding theory (Muandet et al., 2017) supplies operator identities for expectations of kernels and their products, directly used to evaluate normalizers and to derive conditional and marginal densities. Framing SNEFY as a strict generalization of exponential families invokes the classical EF structure and closure properties as codified by Wainwright and Jordan (2008), clarifying why SNEFY inherits EF-like tractability while offering a richer, kernel/NN-driven function class.",
  "analysis_timestamp": "2026-01-07T00:02:04.837604"
}
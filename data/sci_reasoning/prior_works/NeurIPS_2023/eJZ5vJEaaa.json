{
  "prior_works": [
    {
      "title": "STRIPS: A new approach to the application of theorem proving to problem solving",
      "authors": "Richard E. Fikes, Nils J. Nilsson",
      "year": 1971,
      "role": "Foundational planning formalism and regression",
      "relationship_sentence": "The paper\u2019s serialized goal regression search (S-GRS) builds directly on STRIPS-style goal regression, inheriting its representation of actions, preconditions, and back-chaining from goals to subgoals."
    },
    {
      "title": "On reasonable and forced goal orderings and their use in an incremental planning algorithm",
      "authors": "Jana Koehler, J\u00f6rg Hoffmann",
      "year": 2000,
      "role": "Goal serializability and ordering theory",
      "relationship_sentence": "The classification of planning problems by how subgoals can be serialized connects to formal results on goal orderings and serializability, which underpin the paper\u2019s notion that certain domains admit efficient serialized regression (and thus bounded circuit resources)."
    },
    {
      "title": "Universal Value Function Approximators",
      "authors": "Tom Schaul, Daniel Horgan, Karol Gregor, David Silver",
      "year": 2015,
      "role": "Goal-conditioned policy/value formulation",
      "relationship_sentence": "The work formalizes goal-conditioned policies\u2014the object of analysis in this paper\u2014providing the standard feed-forward mapping from state and goal to action/value that the circuit-complexity results target."
    },
    {
      "title": "Attention Is All You Need",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin",
      "year": 2017,
      "role": "Relational neural architecture (Transformers)",
      "relationship_sentence": "Transformers are a prime class of relational neural networks studied here, and their attention-based depth/width trade-offs are directly analyzed under the S-GRS lens."
    },
    {
      "title": "Relational inductive biases, deep learning, and graph networks",
      "authors": "Peter W. Battaglia et al.",
      "year": 2018,
      "role": "Relational neural network framework (GNNs/graph networks)",
      "relationship_sentence": "This work motivates viewing GNNs/transformers as relational circuits operating over objects and relations, which the paper formalizes to derive circuit-complexity bounds for planning policies."
    },
    {
      "title": "How Powerful are Graph Neural Networks?",
      "authors": "Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka",
      "year": 2019,
      "role": "Expressivity and depth of GNNs",
      "relationship_sentence": "Results linking message-passing depth to the radius of information and WL expressivity inform the paper\u2019s constructive proofs about required network depth/width as functions of horizon and object count."
    },
    {
      "title": "Width and structure in classical planning",
      "authors": "Nir Lipovetzky, Hector Geffner",
      "year": 2012,
      "role": "Structural classification of planning problems",
      "relationship_sentence": "The idea that planning domains can be categorized by a notion of width aligns with the paper\u2019s classification into classes with different circuit-width/depth growth, guiding its structural analysis of solvability by relational networks."
    }
  ],
  "synthesis_narrative": "This paper\u2019s core innovation is a circuit-complexity account of when relational neural networks (GNNs and transformers) can implement goal-conditioned policies for planning via a correspondence to serialized goal regression search (S-GRS). The foundation for S-GRS is classical goal regression in STRIPS (Fikes & Nilsson), which formalizes regressing from a goal to supporting subgoals through action preconditions. Building on this, theoretical work on goal serializability and ordering (Koehler & Hoffmann) clarifies when subgoals can be pursued in sequence without destructive interactions, directly motivating the paper\u2019s three classes of planning problems that imply different resource growth for a serialized policy circuit.\n\nOn the neural side, the focus on goal-conditioned policies follows Universal Value Function Approximators (Schaul et al.), which frame policies as feed-forward mappings from state and goal\u2014the exact object whose width/depth complexity is analyzed. Relational neural network architectures provide the computational substrate: Transformers (Vaswani et al.) and graph networks (Battaglia et al.) supply object- and relation-centric computation with shared parameters, enabling generalization across varying numbers of objects. The paper\u2019s constructive proofs leverage insights from GNN expressivity (Xu et al.), where required message-passing depth reflects the radius of necessary information aggregation, to tie planning horizon and interaction structure to circuit depth and width. Finally, the structural perspective on planning complexity from width-based analyses (Lipovetzky & Geffner) informs the idea that domains fall into qualitatively distinct classes, here reframed as distinct scaling regimes for relational network circuits implementing S-GRS policies.",
  "analysis_timestamp": "2026-01-06T23:42:49.081082"
}
{
  "prior_works": [
    {
      "title": "Overcoming catastrophic forgetting in neural networks (Elastic Weight Consolidation)",
      "authors": "James Kirkpatrick et al.",
      "year": 2017,
      "role": "Foundational continual-learning regularization",
      "relationship_sentence": "Selective Amnesia adapts EWC\u2019s Fisher information\u2013based parameter-importance regularization to control which parameters are preserved or altered, turning a retention-centric tool into a targeted forgetting mechanism for specific concepts."
    },
    {
      "title": "Synaptic Intelligence: Continual Learning in Deep Neural Networks",
      "authors": "Friedemann Zenke, Ben Poole, Surya Ganguli",
      "year": 2017,
      "role": "Alternative importance-weighting in continual learning",
      "relationship_sentence": "The idea of estimating parameter importance from learning dynamics (as in SI) directly informs Selective Amnesia\u2019s design choice of selectively constraining important parameters while permitting change where it most effectively induces forgetting."
    },
    {
      "title": "Auto-Encoding Variational Bayes",
      "authors": "Diederik P. Kingma, Max Welling",
      "year": 2013,
      "role": "Core variational likelihood framework (VAEs)",
      "relationship_sentence": "Selective Amnesia is derived for conditional variational likelihood models; the VAE objective (ELBO) provides the variational foundation that the method leverages to define and optimize concept-conditional forgetting."
    },
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho, Ajay Jain, Pieter Abbeel",
      "year": 2020,
      "role": "Base diffusion modeling framework",
      "relationship_sentence": "Because Selective Amnesia targets modern text-to-image systems, DDPM supplies the training objective and generative process onto which the continual-learning\u2013inspired forgetting regularizer is applied."
    },
    {
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "authors": "Robin Rombach, Andreas Blattermann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer",
      "year": 2022,
      "role": "Practical text-conditioned diffusion backbone",
      "relationship_sentence": "Selective Amnesia demonstrates controllable concept forgetting on large-scale text-to-image models like LDM/Stable Diffusion, making this architecture a direct application target and empirical testbed."
    },
    {
      "title": "Variational Continual Learning",
      "authors": "Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, Richard E. Turner",
      "year": 2018,
      "role": "Bayesian/variational view of continual learning",
      "relationship_sentence": "Selective Amnesia\u2019s formulation over conditional variational likelihoods echoes VCL\u2019s variational treatment of task transitions, but repurposes it to explicitly forget a designated concept while preserving others."
    },
    {
      "title": "Erasing Concepts from Diffusion Models",
      "authors": "Ramya Gandikota et al.",
      "year": 2023,
      "role": "Direct prior on concept unlearning in diffusion",
      "relationship_sentence": "This work established fine-tuning-based concept erasure for diffusion models; Selective Amnesia builds on the same goal but contributes a principled continual-learning\u2013derived regularization that enables controllable forgetting across VAEs and diffusion."
    }
  ],
  "synthesis_narrative": "Selective Amnesia\u2019s core idea\u2014targeted forgetting in deep generative models\u2014sits at the intersection of continual learning regularization and modern variational generative modeling. The continual learning lineage (EWC and Synaptic Intelligence) provides the key mechanism: estimate parameter importance and regularize updates to avoid unwanted drift. Selective Amnesia inverts this retention-centric objective, constraining parameters critical to non-target knowledge while allowing modifications that specifically degrade a chosen concept. Variational Continual Learning contributes a complementary perspective by treating learning (and here, unlearning) through a variational lens, aligning naturally with the paper\u2019s derivation for conditional variational likelihood models.\n\nOn the generative side, Auto-Encoding Variational Bayes supplies the ELBO-based foundation that enables a principled formulation of forgetting as operating on conditional likelihoods. Denoising Diffusion Probabilistic Models and Latent Diffusion Models ground the method in the dominant text-to-image architectures and objectives, making it practical to apply selective forgetting to prompts such as nudity and celebrity names. Finally, recent attempts at diffusion concept erasure, exemplified by Erasing Concepts from Diffusion Models, crystallize the problem setting and baseline strategies (fine-tuning to suppress specific concepts). Against this backdrop, Selective Amnesia\u2019s contribution is to replace ad hoc erasure procedures with a continual-learning\u2013inspired, controllable regularization that generalizes across VAEs and diffusion while maintaining non-target capabilities.",
  "analysis_timestamp": "2026-01-07T00:02:04.832739"
}
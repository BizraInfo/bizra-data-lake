{
  "prior_works": [
    {
      "title": "Convex Multi-task Feature Learning",
      "authors": "Argyriou et al.",
      "year": 2008,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "The paper directly extends Argyriou et al.\u2019s trace-norm low-rank parameter sharing from static multi-task regression to a stack of per-sequence autoregressive parameter matrices to encode the common-domain subspace across sequences."
    },
    {
      "title": "Estimation of (approximately) low-rank matrices with nuclear-norm regularization",
      "authors": "Negahban and Wainwright",
      "year": 2011,
      "arxiv_id": "arXiv:1009.2118",
      "role": "Foundation",
      "relationship_sentence": "The recovery guarantees rely on the nuclear-norm M-estimation framework and restricted strong convexity conditions developed by Negahban and Wainwright to bound error for the low-rank shared parameter matrix."
    },
    {
      "title": "Prox-method with rate O(1/t) for variational inequalities with monotone operators and smooth convex-concave saddle point problems",
      "authors": "Nemirovski",
      "year": 2004,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Casting the convex parameter recovery as a monotone variational inequality and analyzing convergence/error uses Nemirovski\u2019s VI framework and Mirror-Prox methodology for monotone operators."
    },
    {
      "title": "Regularized estimation in high-dimensional vector autoregressive models",
      "authors": "Basu and Michailidis",
      "year": 2015,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "The per-sequence autoregressive modeling follows the high-dimensional VAR/AR estimation setup in Basu and Michailidis, but the present work departs by coupling many AR models via a low-rank constraint instead of sparsity."
    },
    {
      "title": "Deep learning for universal linear embeddings of nonlinear dynamics",
      "authors": "Lusch et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "This work demonstrated that nonlinear dynamics can be embedded into low-dimensional linear coordinates via nonconvex deep Koopman models, whose lack of convexity and guarantees motivated a provable, convex VI-based alternative."
    },
    {
      "title": "Representation Learning with Contrastive Predictive Coding",
      "authors": "van den Oord et al.",
      "year": 2018,
      "arxiv_id": "arXiv:1807.03748",
      "role": "Gap Identification",
      "relationship_sentence": "As a dominant unsupervised sequence representation baseline without explicit generative structure or recovery guarantees, CPC\u2019s limitations motivate a model-based, low-rank coupled approach with provable reconstruction."
    }
  ],
  "synthesis_narrative": "Low-rank parameter sharing across related problems was crystallized by Argyriou et al., who used a trace-norm penalty to couple task parameter matrices and recover a shared low-dimensional subspace; their formulation established that a convex surrogate can encode cross-task commonality. Negahban and Wainwright provided the statistical backbone for such programs, giving finite-sample recovery guarantees for nuclear-norm\u2013regularized estimators under restricted strong convexity, thus connecting convex low-rank modeling with provable parameter recovery. Nemirovski\u2019s framework cast monotone variational inequalities as a unifying lens for convex equilibrium problems and supplied Mirror-Prox analysis to obtain convergence and error rates for monotone operators. In time series, Basu and Michailidis formalized high-dimensional AR/VAR estimation with convex regularization, illustrating how structural priors (e.g., sparsity) enable consistent recovery of autoregressive dynamics. In parallel, Lusch et al. showed nonlinear dynamics can be embedded into low-dimensional linear coordinates via deep Koopman networks, though training is nonconvex and lacks guarantees. Contrastive Predictive Coding became a workhorse for unsupervised sequence representations, yet it forgoes explicit dynamical modeling and offers no identifiability or recovery assurances. Together, these strands reveal an opportunity: combine per-sequence autoregressive modeling with convex low-rank coupling to capture a common domain, analyze recovery through nuclear-norm theory, and operationalize the estimator via a monotone variational inequality that admits provable convergence. The present work synthesizes these ingredients\u2014multi-task low-rank sharing, high-dimensional AR parameterization, and VI-based analysis\u2014to produce an unsupervised, convex sequence embedding method that targets nonlinear dynamics while delivering recoverability and practical downstream utility.",
  "target_paper": {
    "title": "Nonlinear Sequence Embedding by Monotone Variational Inequality",
    "authors": "Jonathan Yuyang Zhou, Yao Xie",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Monotone Variational Inequality, Convex Optimization, Sequence Data, Time Series, Representation Learning",
    "abstract": "In the wild, we often encounter collections of sequential data such as electrocardiograms, motion capture, genomes, and natural language, and sequences may be multichannel or symbolic with nonlinear dynamics. We introduce a method to learn low-dimensional representations of nonlinear sequence and time-series data without supervision which has provable recovery guarantees. The learned representation can be used for downstream machine-learning tasks such as clustering and classification. The method assumes that the observed sequences arise from a common domain, with each sequence following its own autoregressive model, and these models are related through low-rank regularization. We cast the problem as a convex matrix parameter recovery problem using monotone variational inequalities (VIs) and encode the common domain assumption via low-rank constraint across the learned representations, which can learn a subspace approximately spanning the entire domain as well as faithful representatio",
    "openreview_id": "U834XHJuqk",
    "forum_id": "U834XHJuqk"
  },
  "analysis_timestamp": "2026-01-06T13:47:54.398951"
}
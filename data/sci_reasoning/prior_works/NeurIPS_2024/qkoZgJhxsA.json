{
  "prior_works": [
    {
      "title": "AutoTutor: A tutor with dialogue in natural language",
      "authors": "Arthur C. Graesser, Natalie K. Person, Jack P. Magliano",
      "year": 2001,
      "role": "Foundational Socratic tutoring paradigm",
      "relationship_sentence": "AutoTutor operationalized Socratic, mixed-initiative tutoring with prompts, hints, and metacognitive probes\u2014directly informing SocraticLM\u2019s shift from answer-giving to thought\u2011provoking, multi-turn pedagogical dialogue."
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei et al.",
      "year": 2022,
      "role": "Reasoning scaffolding via step-by-step explanations",
      "relationship_sentence": "CoT established that eliciting intermediate reasoning improves problem solving, underpinning SocraticLM\u2019s prompt and training design to guide students through structured reasoning rather than providing final answers."
    },
    {
      "title": "Self-Ask: A Simple Approach to Compositional Question Answering",
      "authors": "Ofir Press et al.",
      "year": 2022,
      "role": "Socratic self-questioning and decomposition",
      "relationship_sentence": "Self-Ask showed LMs can improve by generating and answering their own sub-questions, directly inspiring SocraticLM\u2019s teacher agent to drive inquiry through targeted, incremental questions."
    },
    {
      "title": "CAMEL: Communicative Agents for \u201cMind\u201d Exploration",
      "authors": "Y. Li et al.",
      "year": 2023,
      "role": "Multi-agent role-play framework",
      "relationship_sentence": "CAMEL demonstrated that role-playing agents can synthesize high-quality, task-oriented dialogues, shaping SocraticLM\u2019s Dean\u2013Teacher\u2013Student multi-agent pipeline for data generation and interaction control."
    },
    {
      "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
      "authors": "Yizhong Wang et al.",
      "year": 2023,
      "role": "LLM-driven data bootstrapping",
      "relationship_sentence": "Self-Instruct provided a practical recipe for using LLMs to create and filter instruction data, influencing SocraticLM\u2019s construction of the SocraTeach dataset via iterative generation and curation."
    },
    {
      "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
      "authors": "Jiacheng Liu et al.",
      "year": 2023,
      "role": "LLM-as-a-judge for automated quality control",
      "relationship_sentence": "G-Eval validated using LLMs as evaluators, motivating SocraticLM\u2019s Dean role to assess and refine teacher\u2013student dialogues for Socratic quality, correctness, and pedagogical effectiveness."
    },
    {
      "title": "Training Verifiers to Solve Math Word Problems (GSM8K)",
      "authors": "Karl Cobbe et al.",
      "year": 2021,
      "role": "Benchmark and source of foundational math problems",
      "relationship_sentence": "GSM8K supplied canonical, stepwise math word problems and evaluation norms that SocraticLM used to ground Socratic dialogues and to measure gains in reasoning and tutoring efficacy."
    }
  ],
  "synthesis_narrative": "SocraticLM\u2019s core contribution\u2014operationalizing a Socratic, thought\u2011provoking teaching paradigm with a Dean\u2013Teacher\u2013Student multi\u2011agent pipeline and a large SocraTeach dataset\u2014sits at the intersection of classic intelligent tutoring systems and modern LLM prompting and data-generation methods. AutoTutor established the pedagogical blueprint for Socratic, mixed\u2011initiative tutoring, showing that carefully sequenced prompts and probes can elicit deeper reasoning. Modern LLM reasoning techniques then supplied the mechanism to implement this pedagogy at scale: Chain\u2011of\u2011Thought prompting provides the scaffolding for stepwise explanations, while Self\u2011Ask concretizes the Socratic tactic of decomposing problems into sub\u2011questions, shaping the Teacher agent\u2019s dialogic strategy. \nOn the data creation side, CAMEL demonstrated that structured, role\u2011based multi\u2011agent interactions can reliably generate high\u2011quality dialogues, directly inspiring SocraticLM\u2019s Dean\u2013Teacher\u2013Student roles for controllable synthesis. Self\u2011Instruct offered a recipe for bootstrapping diverse instruction data with the model itself, which SocraticLM adapts to curate multi\u2011turn, Socratic teaching exchanges. To maintain quality, G\u2011Eval\u2019s LLM\u2011as\u2011a\u2011judge paradigm motivates the Dean\u2019s automated evaluation and refinement loop, aligning dialogues with pedagogical goals. Finally, GSM8K anchors the effort in well\u2011studied math word problems, providing both content and evaluation practices for reasoning\u2011centric tutoring. Together, these works converge to enable SocraticLM\u2019s key innovation: a scalable, high\u2011quality, Socratic tutoring system that actively engages learners in multi\u2011round reasoning rather than passive answer reception.",
  "analysis_timestamp": "2026-01-06T23:33:35.572167"
}
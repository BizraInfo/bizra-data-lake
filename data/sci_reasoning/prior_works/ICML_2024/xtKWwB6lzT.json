{
  "prior_works": [
    {
      "title": "Optimal Dynamic Treatment Regimes",
      "authors": "Susan A. Murphy",
      "year": 2003,
      "role": "Conceptual foundation for DTRs and the RL framing of sequential clinical decisions",
      "relationship_sentence": "The position paper\u2019s call to reassess RL in DTRs is grounded in Murphy\u2019s formal framework, which underpins how treatment policies, states, and rewards should be rigorously defined in longitudinal care."
    },
    {
      "title": "The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis",
      "authors": "Matthieu Komorowski, Leo Anthony Celi, Omar Badawi, Anthony C. Gordon, Abdulaziz A. Faisal",
      "year": 2018,
      "role": "Seminal RL-in-healthcare application and evaluation template on the sepsis setting",
      "relationship_sentence": "This high-profile study popularized offline RL for sepsis with specific MDP and OPE choices, directly motivating the paper\u2019s systematic reexamination of how such choices drive conclusions."
    },
    {
      "title": "Deep Reinforcement Learning for Sepsis Treatment",
      "authors": "Aniruddh Raghu, Matthieu Komorowski, Leo A. Celi, Peter Szolovits, Finale Doshi-Velez",
      "year": 2017,
      "role": "Early deep RL approach to sepsis emphasizing design choices (state, action, reward) in DTRs",
      "relationship_sentence": "By showing how discretization and reward design affect learned policies, this work foreshadowed and directly informs the paper\u2019s large-scale analysis of sensitivity to MDP formulation."
    },
    {
      "title": "Doubly Robust Off-policy Value Evaluation for Reinforcement Learning",
      "authors": "Nan Jiang, Lihong Li",
      "year": 2016,
      "role": "Core off-policy evaluation methodology (DR) widely used in offline RL",
      "relationship_sentence": "The paper evaluates how DR and related estimators can yield inconsistent rankings, reinforcing its argument that OPE choice critically shapes reported performance in DTR studies."
    },
    {
      "title": "Eligibility Traces for Off-Policy Policy Evaluation",
      "authors": "Doina Precup, Richard S. Sutton, Satinder Singh",
      "year": 2000,
      "role": "Foundational importance-sampling based OPE (including per-decision/weighted variants)",
      "relationship_sentence": "The dependence of offline RL conclusions on IS/WIS/PDIS stems from these foundations, which the paper scrutinizes by showing that different IS variants can even favor random baselines."
    },
    {
      "title": "Conservative Q-Learning for Offline Reinforcement Learning",
      "authors": "Aviral Kumar, Aurick Zhou, George Tucker, Sergey Levine",
      "year": 2020,
      "role": "Canonical offline RL algorithm and benchmark for policy learning from fixed datasets",
      "relationship_sentence": "As a representative strong offline RL method, CQL anchors the paper\u2019s empirical comparisons, where outcomes shift with evaluation metrics and reward design despite algorithmic advances."
    },
    {
      "title": "Guidelines for Reinforcement Learning in Healthcare",
      "authors": "Omer Gottesman, Fredrik Johansson, Uri Shalit, David Sontag, Finale Doshi-Velez, Leo Anthony Celi",
      "year": 2019,
      "role": "Prior critique and best-practice recommendations for RL in clinical settings",
      "relationship_sentence": "The paper operationalizes these cautions\u2014calling for baselines, transparent metrics, and careful MDP design\u2014by demonstrating at scale how current practices yield inconclusive or misleading evaluations."
    }
  ],
  "synthesis_narrative": "The position paper\u2019s central claim\u2014that offline reinforcement learning for dynamic treatment regimes is highly sensitive to evaluation metrics and modeling choices\u2014builds on two intertwined threads: the DTR/RL formalism and the healthcare RL practice that popularized sepsis as a testbed. Murphy\u2019s formulation of optimal dynamic treatment regimes provides the theoretical scaffolding for specifying states, actions, and rewards in longitudinal care, making clear that modeling choices are consequential. Early sepsis RL efforts, notably Komorowski et al.\u2019s AI Clinician and Raghu et al.\u2019s deep RL study, catalyzed interest and established de facto templates for MDP construction, discretization, and retrospective off-policy evaluation; their heterogeneity and strong claims motivate a systematic reexamination.\nOn the evaluation side, foundational OPE methods\u2014importance sampling variants (Precup et al.) and doubly robust estimation (Jiang & Li)\u2014enable value estimation from logged data but are known to be variance- and support-sensitive. The paper leverages and stress-tests these estimators, showing that policy rankings (and even apparent superiority over random baselines) can flip under different OPE choices and reward designs. Meanwhile, advances in offline RL such as Conservative Q-Learning offer stronger learning algorithms, yet the paper demonstrates that algorithmic strength does not immunize results against evaluation fragility. Finally, echoing the cautions and best practices articulated by Gottesman et al., the study\u2019s 17,000-experiment case analysis concretely substantiates the need for standardized evaluation protocols, inclusion of naive and supervised baselines, and transparent MDP/reward specifications in RL for healthcare.",
  "analysis_timestamp": "2026-01-06T23:42:48.075798"
}
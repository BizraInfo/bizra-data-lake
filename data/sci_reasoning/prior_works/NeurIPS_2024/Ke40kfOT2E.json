{
  "prior_works": [
    {
      "title": "Sum-Product Networks: A New Deep Architecture",
      "authors": "Hoifung Poon, Pedro Domingos",
      "year": 2011,
      "role": "Foundational tractable probabilistic circuits (PCs)",
      "relationship_sentence": "PICs inherit the summed-and-multiplied computational graph with decomposability/smoothness from SPNs, and this paper extends that circuit calculus to continuous latent-variable integration at scale."
    },
    {
      "title": "A Differential Approach to Inference in Bayesian Networks",
      "authors": "Adnan Darwiche",
      "year": 2003,
      "role": "Arithmetic circuits and circuit-based inference",
      "relationship_sentence": "Established compiling probabilistic inference into DAG arithmetic circuits and exploiting derivatives; the new work applies this circuit view to integrals over continuous LVs, enabling DAG-shaped PICs and gradient-based training."
    },
    {
      "title": "Probabilistic Sentential Decision Diagrams",
      "authors": "Avi Pfeffer Kisa, Guy Van den Broeck, Arthur Choi, Adnan Darwiche",
      "year": 2014,
      "role": "Structured decompositions (vtrees) for DAG PCs with sharing",
      "relationship_sentence": "The paper\u2019s pipeline for building DAG-shaped PICs from arbitrary variable decompositions mirrors the PSDD/vtree principle of mapping variable partitioning into a shared DAG while preserving decomposability."
    },
    {
      "title": "Learning the Structure of Sum-Product Networks",
      "authors": "Romain Gens, Pedro Domingos",
      "year": 2013,
      "role": "Variable decompositions and region-graph templates for SPNs",
      "relationship_sentence": "Their variable partitioning/region-graph methodology directly informs the proposed construction of DAG PIC topologies from generic variable decompositions."
    },
    {
      "title": "Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Models",
      "authors": "Robert Peharz, Antonio Vergari, Kristian Kersting, et al.",
      "year": 2020,
      "role": "Tensorized PC architectures and neural weight sharing",
      "relationship_sentence": "Provides the tensorized, GPU-friendly circuit layers and parameter sharing mechanisms that this paper adapts to train PICs/QPCs at scale."
    },
    {
      "title": "Probabilistic Integral Circuits",
      "authors": "Gennaro Gala, Cassio de Campos, Antonio Vergari, Erik Quaeghebeur",
      "year": 2023,
      "role": "Introduction of PICs and their QPC approximations",
      "relationship_sentence": "Introduced PICs with tree-shaped structures and QPCs for numerical quadrature; the current paper addresses their scalability limits by generalizing to DAGs and tensorized training."
    },
    {
      "title": "Probabilistic Circuits: A Unifying Framework for Tractable Probabilistic Models",
      "authors": "Arthur Choi, Antonio Vergari, Guy Van den Broeck",
      "year": 2020,
      "role": "Formal properties and design principles for PCs",
      "relationship_sentence": "Clarifies decomposability/smoothness and sharing in DAG PCs, guiding how the authors enforce tractability while converting PICs into QPCs and enabling DAG-shaped designs."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core advance\u2014scaling continuous latent-variable models as probabilistic integral circuits (PICs) via DAG structures, tensorized training, and neural functional sharing\u2014sits squarely on the tractable circuit lineage and its neuralized variants. SPNs introduced the basic algebra of sums and products under decomposability and smoothness, which PICs extend by integrating over continuous latents. Darwiche\u2019s arithmetic circuits established compiling probabilistic inference into DAGs and leveraging derivatives, a perspective directly repurposed here for continuous integrals and gradient-based learning. The move from trees to DAGs in PICs is enabled by the structured decomposition paradigm of PSDDs and SPN structure learning: vtrees/region-graphs operationalize variable decompositions into shared DAGs while preserving tractability, which this paper generalizes to arbitrary decompositions for PIC construction. On the optimization side, Einsum Networks provided tensorized circuit layers and parameter-tying/sharing patterns that make large PCs trainable on GPUs; this work adapts those ideas to PICs/QPCs, mitigating the memory footprint of quadrature-based training. Finally, the prior PICs work introduced the model class and the QPC approximation via hierarchical numerical quadrature; the present paper tackles the identified bottlenecks\u2014tree-only structures and memory-intensive training\u2014by marrying DAG-based sharing from the probabilistic circuits literature with tensorized, neural circuit implementations to deliver scalable continuous LV modeling.",
  "analysis_timestamp": "2026-01-07T00:02:04.746252"
}
{
  "prior_works": [
    {
      "title": "Online SGD for Single-Index Models under Gaussian Features",
      "authors": "Gerard Ben Arous, Andrea Montanari, Song Mei, others",
      "year": 2021,
      "role": "Baseline upper bound and limitations of vanilla online SGD",
      "relationship_sentence": "Established that learning SIMs with information exponent k* via online SGD requires n \u2273 d^{k*\u22121} samples and argued this rate is tight for standard (unsmoothed) online SGD, creating the upper/lower bound gap this paper closes."
    },
    {
      "title": "Distribution-specific hardness of learning neural networks",
      "authors": "Ohad Shamir",
      "year": 2016,
      "role": "CSQ lower bound for gradient-based/SQ methods",
      "relationship_sentence": "Provided statistical query lower bounds for gradient-based learning under Gaussian inputs that translate to a necessary n \u2273 d^{k*/2} for SIMs, serving as the fundamental benchmark the present work matches with a smoothed loss."
    },
    {
      "title": "A Statistical Model for Tensor PCA",
      "authors": "Emile Richard, Andrea Montanari",
      "year": 2014,
      "role": "Theoretical foundation linking degree-k signals and sample thresholds",
      "relationship_sentence": "Analyzed detection/estimation thresholds in tensor PCA, where signal appears in order-k moments with optimal scaling ~ d^{k/2}; this mirrors the SIM Hermite-k* signal and informs the target statistical rate achieved via smoothing."
    },
    {
      "title": "Tensor Decompositions for Learning Latent Variable Models",
      "authors": "Anima Anandkumar, Rong Ge, Daniel Hsu, Sham Kakade, Matus Telgarsky",
      "year": 2014,
      "role": "Algorithmic paradigm for extracting higher-order moment signal",
      "relationship_sentence": "Showed how higher-order moment/tensor methods recover low-degree structure efficiently, motivating the idea that shaping the objective (here, smoothing) can surface the Hermite-k* signal analogous to tensor methods."
    },
    {
      "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
      "authors": "Matthew D. Hoffman, Andrew Gordon Wilson, Stephan Mandt",
      "year": 2017,
      "role": "Implicit regularization/smoothing perspective on SGD noise",
      "relationship_sentence": "Interpreted minibatch SGD noise as inducing a Gaussian-like smoothing of the loss, underpinning this paper\u2019s connection between explicit smoothing and the implicit regularization of minibatch SGD."
    },
    {
      "title": "Fluctuation-Dissipation Relations for Stochastic Gradient Descent",
      "authors": "Sho Yaida",
      "year": 2018,
      "role": "Quantitative link between SGD noise and effective landscape",
      "relationship_sentence": "Characterized how SGD noise shapes the effective optimization landscape, supporting the paper\u2019s claim that smoothing boosts the learnable signal and explaining why minibatching can emulate the proposed smoothed-loss behavior."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core advance\u2014showing that smoothing the loss enables online SGD to learn single index models (SIMs) at the statistically optimal n \u2273 d^{k*/2}\u2014is tightly anchored to three prior threads. First, Ben Arous et al. (2021) analyzed online SGD on the unsmoothed population loss for SIMs and proved that n \u2273 d^{k*\u22121} samples suffice (and are effectively necessary for vanilla online SGD). This result highlighted a gap with the centered statistical query (CSQ) lower bounds for gradient-based methods, such as those developed by Shamir (2016), which imply that only n \u2273 d^{k*/2} samples are information-theoretically necessary. Closing this gap requires amplifying the learnable Hermite-k* signal in gradients without invoking higher-complexity tensor methods.\nA second thread comes from tensor PCA. Richard and Montanari (2014) and the broader tensor-decomposition literature (e.g., Anandkumar et al., 2014) show that degree-k signals naturally exhibit optimal sample scaling \u2248 d^{k/2}, mirroring the SIM information exponent k*. This paper leverages that insight by designing a smoothed objective whose gradient selectively enhances the k*th Hermite component, allowing first-order SGD to exploit the same low-degree signal that tensor methods target.\nFinally, the connection to minibatch SGD\u2019s implicit regularization is grounded in works like Mandt et al. (2017) and Yaida (2018), which model SGD noise as inducing an effective smoothing of the loss landscape. These results conceptually justify why explicit smoothing should boost signal-to-noise in gradients and explain the paper\u2019s observed parallels between smoothed-loss SGD and practical minibatch SGD behavior.",
  "analysis_timestamp": "2026-01-06T23:42:49.102993"
}
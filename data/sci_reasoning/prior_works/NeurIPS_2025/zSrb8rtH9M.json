{
  "prior_works": [
    {
      "title": "Adaptive Mixtures of Local Experts",
      "authors": "Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, Geoffrey E. Hinton",
      "year": 1991,
      "role": "Foundational MoE architecture introducing gating with expert subnetworks",
      "relationship_sentence": "The paper\u2019s expressivity analysis relies on the original MoE concept of a learned gating function routing inputs to experts, and it dissects how this gating-expert decomposition controls approximation accuracy and piecewise structure."
    },
    {
      "title": "Hierarchical Mixtures of Experts and the EM Algorithm",
      "authors": "Michael I. Jordan, Robert A. Jacobs",
      "year": 1994,
      "role": "Introduced deep/hierarchical MoE compositions",
      "relationship_sentence": "The result that L-layer MoEs with E experts realize E^L piecewise structures builds on the hierarchical gating idea, formalizing how layered expert compositions induce exponentially many regions/tasks."
    },
    {
      "title": "Hierarchical Mixtures-of-Experts for Exponential Family Regression Models: Approximation and Maximum Likelihood Estimation",
      "authors": "Jiang, Tanner",
      "year": 1999,
      "role": "Early approximation theory for (hierarchical) MoE",
      "relationship_sentence": "Their approximation guarantees for HME provide the theoretical precursor that MoEs can approximate complex targets; the present work sharpens this by quantifying piecewise expressivity and structured sparsity across depth and experts."
    },
    {
      "title": "On the Number of Linear Regions of Deep Neural Networks",
      "authors": "Guido F. Mont\u00fafar, Razvan Pascanu, Kyunghyun Cho, Yoshua Bengio",
      "year": 2014,
      "role": "Depth-driven exponential growth of piecewise regions in ReLU nets",
      "relationship_sentence": "The E^L-piece result for deep MoEs parallels the region-count analysis for ReLU networks, translating depth-induced combinatorial expressivity to gated MoE partitions governed by the number of experts per layer."
    },
    {
      "title": "Deep vs. Shallow Networks: An Approximation Theory Perspective",
      "authors": "Hrushikesh N. Mhaskar, Tomaso Poggio",
      "year": 2016,
      "role": "Approximation benefits from compositional structure",
      "relationship_sentence": "The notion of compositional sparsity directly connects to their theory that deep models efficiently capture hierarchical structure; this work adapts that lens to MoEs, showing how layered gating realizes exponentially many structured tasks."
    },
    {
      "title": "Provable approximation properties for deep neural networks",
      "authors": "Clara L. H. Shaham, Alexander Cloninger, Ronald R. Coifman",
      "year": 2015,
      "role": "Approximation on low-dimensional manifolds to mitigate the curse of dimensionality",
      "relationship_sentence": "The shallow-MoE manifold approximation result extends manifold-based approximation theory by demonstrating that gating plus local experts can achieve ambient-dimension\u2013free rates when the target is supported on a low-dimensional manifold."
    },
    {
      "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
      "authors": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, Jeff Dean",
      "year": 2017,
      "role": "Modern sparse MoE architecture emphasizing gating, number of experts, and depth",
      "relationship_sentence": "Motivates the paper\u2019s focus on how the number of experts, layers, and sparse gating affect expressivity, providing the architectural substrate whose components are dissected in the theoretical results."
    }
  ],
  "synthesis_narrative": "This paper\u2019s core contribution\u2014a principled characterization of MoE expressivity for structured tasks\u2014sits at the intersection of classic MoE design, deep-network expressivity, and structure-exploiting approximation theory. The original MoE formulation (Jacobs et al., 1991) and its hierarchical extension (Jordan & Jacobs, 1994) establish gating-driven partitions and layered compositions of experts. This conceptual scaffold is essential for the paper\u2019s E^L-piece result: by cascading gating decisions across L layers, deep MoEs inherit a combinatorial partitioning akin to decision trees, but implemented by learnable soft gates.\nParallel advances in deep expressivity theory inform how depth yields exponential capacity. Mont\u00fafar et al. (2014) show ReLU networks generate exponentially many linear regions, a template this work adapts to MoE partitions controlled by the number of experts per layer. Mhaskar & Poggio (2016) further argue that deep architectures excel on compositional functions; the paper leverages this perspective to formalize \u201ccompositional sparsity\u201d in MoEs and quantify when depth is beneficial.\nOn the approximation-theoretic side specific to MoEs, Jiang & Tanner (1999) validate that (hierarchical) MoEs can approximate broad function classes, which the present work refines by pinpointing rates and counts under sparsity and depth constraints. For manifold-structured targets, Shaham\u2013Cloninger\u2013Coifman (2015) provide dimension-sensitive bounds for deep nets; here, shallow MoEs are shown to achieve similar ambient-dimension\u2013free approximation by using gating to localize experts to manifold charts. Finally, modern sparse MoE practice (Shazeer et al., 2017) motivates analyzing how gating design, number of experts, and depth jointly govern expressivity\u2014the very hyperparameters dissected in the new theory.",
  "analysis_timestamp": "2026-01-07T00:02:04.958823"
}
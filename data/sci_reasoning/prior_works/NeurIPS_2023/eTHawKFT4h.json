{
  "prior_works": [
    {
      "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
      "authors": "Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell",
      "year": 2017,
      "role": "Empirical foundation for ensembling-based uncertainty in deep learning",
      "relationship_sentence": "The paper\u2019s central contribution\u2014casting deep ensembles as particles following a variational/Wasserstein flow\u2014provides the first rigorous theoretical bridge to the widely used deep ensembles of Lakshminarayanan et al., explaining their strong empirical performance relative to variational Bayes."
    },
    {
      "title": "Weight Uncertainty in Neural Networks",
      "authors": "Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra",
      "year": 2015,
      "role": "Canonical parametric variational Bayes baseline for neural networks",
      "relationship_sentence": "By contrasting parametric VI (as in Bayes by Backprop) with optimization over probability measures, the paper clarifies when and why measure-space convexification and particle/ensemble methods can outperform parameterized VI objectives."
    },
    {
      "title": "Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm",
      "authors": "Qiang Liu, Dilin Wang",
      "year": 2016,
      "role": "Particle-based variational inference linking Bayesian updates to deterministic interacting particle dynamics",
      "relationship_sentence": "SVGD\u2019s view of posterior approximation via interacting particles directly informs the paper\u2019s interpretation of deep ensembles as interacting particle systems and its design of new ensemble dynamics with convergence guarantees."
    },
    {
      "title": "The Variational Formulation of the Fokker\u2013Planck Equation",
      "authors": "Richard Jordan, David Kinderlehrer, Felix Otto",
      "year": 1998,
      "role": "Foundational Wasserstein gradient flow (JKO) framework for probability measures",
      "relationship_sentence": "The paper\u2019s core technical step\u2014studying generalized variational inference objectives as Wasserstein gradient flows\u2014builds on the JKO framework that interprets KL-driven dynamics as gradient flows in the space of probability measures."
    },
    {
      "title": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport",
      "authors": "L\u00e9na\u00efc Chizat, Francis Bach",
      "year": 2018,
      "role": "Optimal-transport/measure-space perspective on neural network training",
      "relationship_sentence": "Chizat and Bach\u2019s convexification in the space of parameter measures and OT-based analysis underpins the paper\u2019s reformulation of nonconvex neural training as convex optimization over distributions, a key enabler of the ensemble\u2013Bayes unification."
    },
    {
      "title": "Mean Field Theory of Two-Layer Neural Networks: Dimension-Free Bounds and Kernel Limit",
      "authors": "Song Mei, Andrea Montanari, Phan-Minh Nguyen",
      "year": 2018,
      "role": "Mean-field limit and PDE/gradient-flow view of neural network parameter distributions",
      "relationship_sentence": "The mean-field PDE perspective for parameter distributions provides the technical backbone for viewing ensembles as particles evolving under distributional gradient flows of variational objectives."
    },
    {
      "title": "R\u00e9nyi Divergence Variational Inference",
      "authors": "Yingzhen Li, Richard E. Turner",
      "year": 2016,
      "role": "Generalized VI objectives beyond KL (\u03b1-divergences)",
      "relationship_sentence": "By embracing generalized VI (e.g., \u03b1-divergences), the paper extends measure-space gradient-flow analysis beyond standard KL, and Li & Turner\u2019s formulation delineates the objective family the authors study under Wasserstein flows."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014establishing a rigorous link between deep ensembles and (variational) Bayesian methods by reformulating neural training as convex optimization over probability measures and analyzing it via Wasserstein gradient flows\u2014rests on three intellectual pillars. First, empirical and methodological motivation comes from deep ensembles (Lakshminarayanan et al.), whose strong uncertainty estimates needed a principled Bayesian explanation, and from parametric variational Bayes for neural nets (Blundell et al.), which provides the contrasting baseline that the present work seeks to subsume and, in some regimes, surpass.\nSecond, the mathematical machinery arises from the optimal transport and gradient-flow literature. The JKO framework (Jordan\u2013Kinderlehrer\u2013Otto) establishes KL-driven dynamics as Wasserstein gradient flows, enabling the authors to cast generalized VI objectives as flows on probability measures. The optimal-transport viewpoint on neural network training (Chizat & Bach) and the mean-field PDE perspective for parameter distributions (Mei\u2013Montanari\u2013Nguyen) supply the precise measure-space representation and convexification tools that make the nonconvex parameter problem tractable in distribution space.\nThird, particle-based variational inference links these ideas to practical ensemble procedures. SVGD (Liu & Wang) shows how interacting particles can deterministically follow a variational objective, directly inspiring the paper\u2019s interpretation of deep ensembles as interacting particle systems and its derivation of new ensemble dynamics with convergence guarantees. Finally, generalized VI objectives (Li & Turner\u2019s R\u00e9nyi VI) situate the analysis beyond KL, allowing the authors to unify ensembles with a broad class of Bayesian and variational methods within a single Wasserstein-flow framework.",
  "analysis_timestamp": "2026-01-06T23:42:49.134835"
}
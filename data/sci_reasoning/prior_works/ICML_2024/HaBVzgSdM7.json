{
  "prior_works": [
    {
      "title": "End-to-End Object Detection with Transformers",
      "authors": "Nicolas Carion et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "DAT inherits the DETR-style query-based detector/decoder and directly extends it by introducing multiple query groups for word/line/paragraph/page and an across-granularity interactive attention among these queries."
    },
    {
      "title": "Relation Networks for Object Detection",
      "authors": "Han Hu et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "The idea of explicitly modeling interactions among object instances via attention in Relation Networks directly inspires DAT\u2019s across-granularity interactive attention to correlate and refine representations across different text queries."
    },
    {
      "title": "HierText: A Hierarchical Text Dataset and Strong Baselines",
      "authors": "Shuyang Sun et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "HierText formalized hierarchical text structure (word\u2013line\u2013paragraph) and motivated DAT\u2019s unified multi-granularity formulation that jointly reasons over these levels rather than training separate detectors."
    },
    {
      "title": "Real-time Scene Text Detection with Differentiable Binarization",
      "authors": "Minghui Liao et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "DBNet\u2019s segmentation-based pipeline is typically trained per scenario/granularity; DAT explicitly addresses this limitation by replacing separate models with a single unified detector whose queries interact across granularities."
    },
    {
      "title": "Shape Robust Text Detection with Progressive Scale Expansion Network",
      "authors": "Wenhai Wang et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "PSENet is a strong text detector but remains granularity-specific, motivating DAT\u2019s core contribution of cross-granularity interaction so improvements at one level benefit others."
    },
    {
      "title": "Segment Anything",
      "authors": "Alexander Kirillov et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "DAT\u2019s prompt-based segmentation module directly builds on the SAM-style promptable segmentation paradigm by using detection queries (e.g., boxes/points) as prompts to refine text instance masks."
    },
    {
      "title": "DocBank: A Benchmark Dataset for Document Layout Analysis",
      "authors": "Minghao Li et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "DocBank codified document layout detection as a detection task; DAT incorporates layout analysis as one granularity and unifies it with scene text detection within a single model."
    }
  ],
  "synthesis_narrative": "DAT\u2019s core innovation\u2014unifying scene text detection, document layout analysis, and page-level detection across word/line/paragraph/page via an across-granularity interactive attention\u2014emerges by marrying query-based detection with explicit inter-instance reasoning. DETR established the query\u2013decoder paradigm that DAT adopts and extends, allocating dedicated query sets for each granularity and enabling end-to-end learning without NMS. Building on the notion of relational reasoning from Relation Networks, DAT introduces an interactive attention mechanism that lets queries at different granularities exchange structural cues (e.g., words informing lines, lines informing paragraphs), turning hierarchical structure into mutual supervision. The need for such unification is driven by the limitations of leading segmentation-style text detectors like DBNet and PSENet, which are typically trained separately for each scenario or granularity and thus fail to transfer improvements across levels. HierText provided the problem formulation and evidence that hierarchical text structure (word\u2013line\u2013paragraph) is beneficial, directly motivating DAT to learn these levels jointly within one decoder. To sharpen boundaries without specialized per-task post-processing, DAT leverages the promptable segmentation principle popularized by Segment Anything, feeding detection outputs as prompts to refine text masks. Finally, document layout datasets such as DocBank formalized layout detection as a detection problem, which DAT subsumes under its unified query set, demonstrating that a single interactive attention framework can provide cross-granularity gains across scene text and document domains.",
  "analysis_timestamp": "2026-01-06T23:09:26.505980"
}
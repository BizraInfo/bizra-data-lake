{
  "prior_works": [
    {
      "title": "Tensor Product Variable Binding and the Representation of Symbolic Structure in Connectionist Systems",
      "authors": "Paul Smolensky",
      "year": 1990,
      "role": "Foundation",
      "relationship_sentence": "Propositional probes explicitly operationalize role\u2013filler binding\u2014combining entities and predicates into propositions\u2014by adapting TPR-style variable binding to a learned binding subspace over LM activations."
    },
    {
      "title": "Understanding intermediate layers using linear classifier probes",
      "authors": "Guillaume Alain et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "The method relies on linear probes over token activations; propositional probes are a structured variant that decodes compositional, proposition-level information rather than simple labels."
    },
    {
      "title": "A Structural Probe for Finding Syntax in Word Representations",
      "authors": "John Hewitt et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "This work extends structural probing\u2014linear projections that recover symbolic structure (e.g., parse trees)\u2014to decode logical propositions and bindings directly from hidden states."
    },
    {
      "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
      "authors": "Been Kim et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "The idea of extracting \u2018lexical concepts\u2019 as directions in representation space mirrors TCAV\u2019s concept vectors, which propositional probes then compose and bind into predicate\u2013argument structures."
    },
    {
      "title": "Toy Models of Superposition in Neural Networks",
      "authors": "Nelson Elhage et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "Superposition highlights interference between features in shared subspaces; propositional probes directly address this by learning a binding subspace where only bound token pairs exhibit high similarity, suppressing unbound crosstalk."
    },
    {
      "title": "In-Context Learning and Induction Heads",
      "authors": "Catherine Olsson et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Findings on induction/name-mover heads show transformers form token\u2013token associations that implement role binding, motivating the paper\u2019s explicit readout of bound pairs via similarity within a binding subspace."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT",
      "authors": "Kevin Meng et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "By localizing subject\u2192object relational subspaces and editing them, this work demonstrates that relational bindings are represented in specific layers; propositional probes leverage this insight to decode context-induced propositions rather than edit parametric memory."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014a propositional probe that decodes lexical concepts and binds them into explicit predicates from LM activations\u2014sits at the intersection of symbolic binding theory and modern representation probing. Smolensky\u2019s Tensor Product Representations supply the foundational idea that propositions can be encoded through role\u2013filler binding; this work adapts that principle to learned neural subspaces. From the interpretability side, linear probes (Alain & Bengio) and structural probes (Hewitt & Manning) established that linear projections can recover structured symbolic information from hidden states; propositional probes extend this lineage from trees to logical predicates. TCAV further shaped the approach by framing concepts as linear directions, a notion directly reused to extract lexical concepts that can be composed and bound.\nAnthropic\u2019s line of work provides both motivation and mechanism. Toy Models of Superposition identified interference among co-encoded features, a limitation addressed here by isolating a dedicated binding subspace that raises similarity only for bound pairs. In parallel, the induction-heads analysis showed transformers form token\u2013token associations implementing role binding, suggesting the specific geometric criterion\u2014high similarity for bound entities but not for unbound ones\u2014that underlies the probe. Finally, knowledge-editing results (ROME) demonstrated that relational bindings (subject\u2192object) are localized and manipulable, reinforcing the premise that proposition-level relations live in tractable subspaces. Together, these works directly enable and motivate a probing method that monitors latent world states as compositional logical propositions.",
  "analysis_timestamp": "2026-01-06T23:09:26.603293"
}
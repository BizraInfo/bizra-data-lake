{
  "prior_works": [
    {
      "title": "High-performance brain-to-text communication via handwriting",
      "authors": "Francis R. Willett, Donald T. Avansino, Daniel Trivedi, Brian C. Rezaii, Leigh R. Hochberg, Jaimie M. Henderson, Krishna V. Shenoy",
      "year": 2021,
      "role": "Baseline system and LM-augmented decoding for intracortical communication",
      "relationship_sentence": "Established the intracortical brain-to-text paradigm and integrated language-model\u2013based autocorrection, which CORP leverages by turning LM-corrected outputs into pseudo-labels for continual self-recalibration."
    },
    {
      "title": "Stabilizing brain\u2013computer interfaces through alignment of low-dimensional neural activity spaces",
      "authors": "Andrew D. Degenhart et al.",
      "year": 2020,
      "role": "Problem framing and methodological precursor for long-term iBCI stability",
      "relationship_sentence": "Identified the core challenge of day-to-day neural nonstationarities and proposed unsupervised manifold alignment; CORP addresses the same stability goal but replaces alignment with LM-driven pseudo-label self-training for plug-and-play operation."
    },
    {
      "title": "Closed-loop decoder adaptation shapes neural plasticity for skillful neuroprosthetic control",
      "authors": "Alex C. Orsborn, Cemal K. Dangi, Stephen M. Moorman, Jose M. Carmena",
      "year": 2014,
      "role": "Algorithmic precursor for continual online adaptation",
      "relationship_sentence": "Pioneered continuous closed-loop decoder adaptation; CORP inherits the continual update paradigm but supplies labels via LM-corrected outputs rather than supervised calibration."
    },
    {
      "title": "High performance communication by people with paralysis using an intracortical brain\u2013computer interface",
      "authors": "Chethan Pandarinath et al.",
      "year": 2017,
      "role": "BCI communication foundation with predictive text support",
      "relationship_sentence": "Demonstrated practical intracortical communication aided by language modeling, foreshadowing CORP\u2019s use of linguistic priors to improve and supervise decoding without interruptions."
    },
    {
      "title": "Pseudo-Label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks",
      "authors": "Dong-Hyun Lee",
      "year": 2013,
      "role": "Core semi-supervised learning principle",
      "relationship_sentence": "Introduced pseudo-labeling as self-training; CORP adapts this idea by treating LM-corrected iBCI outputs as pseudo-labels to drive continual online recalibration."
    },
    {
      "title": "Neuroprosthesis for Decoding Speech in a Paralyzed Person with Anarthria",
      "authors": "David A. Moses et al.",
      "year": 2021,
      "role": "LM-integrated neural decoding for natural language output",
      "relationship_sentence": "Showed that powerful language models can correct and constrain neural decoding of text/speech; CORP extends this LM-in-the-loop concept to generate supervisory signals for self-recalibration."
    }
  ],
  "synthesis_narrative": "The CORP framework\u2019s core innovation\u2014plug-and-play stability via self-recalibration using language-model\u2013generated pseudo-labels\u2014emerges at the intersection of three lines of prior work. First, intracortical communication systems established the feasibility and mechanics of brain-to-text decoding with LM support. Willett et al. (2021) created the high-performance handwriting-based iBCI and integrated language-model autocorrection, while Pandarinath et al. (2017) showed that predictive text can substantially boost communication rates. These works supply CORP\u2019s operational substrate and the insight that linguistic priors can systematically clean decoder outputs.\nSecond, long-term stability in iBCIs has been pursued through decoder adaptation and alignment. Orsborn et al. (2014) introduced closed-loop decoder adaptation to maintain performance as neural signals drift, and Degenhart et al. (2020) framed day-to-day nonstationarity as a manifold alignment problem. CORP aligns with the continual-update philosophy but eliminates explicit calibration or alignment steps by converting online, LM-corrected text into training supervision.\nThird, the methodological backbone is semi-supervised learning via pseudo-labeling (Lee, 2013). CORP operationalizes pseudo-labels in a neuroprosthetic context: LM-corrected outputs become self-generated labels that enable continual online retraining. Complementing this, Moses et al. (2021) reinforced that strong language models can robustly constrain neural-to-text decoding, validating LM-based correction as a reliable supervisory signal. Together, these works directly informed CORP\u2019s design: an LM-in-the-loop, self-training decoder that sustains year-long, interruption-free brain-to-text communication.",
  "analysis_timestamp": "2026-01-07T00:02:04.790875"
}
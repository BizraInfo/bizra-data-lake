{
  "prior_works": [
    {
      "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?",
      "authors": "et al.",
      "year": 2023,
      "role": "Benchmark/infrastructure",
      "relationship_sentence": "SWE-bench operationalized repository-level evaluation by standardizing Dockerized environments per repo, exposing that scalable, automated environment setup is the bottleneck for running tests at scale\u2014precisely the gap Repo2Run targets."
    },
    {
      "title": "BugSwarm: Mining and Maintaining a Dataset of Reproducible Failures and Fixes",
      "authors": "et al.",
      "year": 2019,
      "role": "Dataset/infrastructure for reproducible builds",
      "relationship_sentence": "BugSwarm demonstrated the feasibility and value of capturing CI contexts as Docker images to reliably reproduce builds, motivating Repo2Run\u2019s automated synthesis of Dockerfiles to scale reproducible execution across arbitrary repositories."
    },
    {
      "title": "Repairnator: An Automatic Program Repair Bot for Open-Source Projects",
      "authors": "Matias Martinez et al.",
      "year": 2018,
      "role": "System precedent in automated build/test orchestration",
      "relationship_sentence": "Repairnator automated reproducing CI failures and iterating through build/test loops on real repositories, foreshadowing Repo2Run\u2019s core loop of build \u2192 test \u2192 analyze logs \u2192 fix environment specifications."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "Agentic method foundation",
      "relationship_sentence": "ReAct\u2019s interleaving of reasoning with tool use underpins Repo2Run\u2019s design as an agent that plans, calls external tools (docker build/test), and uses intermediate feedback to guide the next action."
    },
    {
      "title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
      "authors": "Noah Shinn et al.",
      "year": 2023,
      "role": "Iterative self-correction via feedback",
      "relationship_sentence": "Reflexion showed that leveraging execution feedback to iteratively revise solutions boosts success; Repo2Run embodies this by reading build/test logs to iteratively refine Dockerfiles until the pipeline succeeds."
    },
    {
      "title": "SWE-agent: Agentic Coding with Tools for Software Engineering",
      "authors": "et al.",
      "year": 2024,
      "role": "LLM agent for repo-level software tasks",
      "relationship_sentence": "SWE-agent established that LLM agents can operate shells, package managers, and tests inside containers; Repo2Run specializes this paradigm to the environment-building subproblem, turning transient actions into reusable Dockerfiles."
    },
    {
      "title": "OpenDevin: An Open Platform for AI Software Developers",
      "authors": "OpenDevin Community",
      "year": 2024,
      "role": "End-to-end coding agent platform",
      "relationship_sentence": "OpenDevin highlighted the centrality and fragility of environment setup for autonomous coding agents; Repo2Run directly addresses this by automating robust container environment synthesis that downstream agents can reuse."
    }
  ],
  "synthesis_narrative": "Repo2Run\u2019s core contribution\u2014an LLM agent that automatically synthesizes and iteratively refines Dockerfiles to create executable environments for arbitrary repositories\u2014sits at the intersection of reproducible software infrastructure and agentic LLM methodologies. On the infrastructure side, SWE-bench made repository-level evaluation mainstream but relied on curated, per-repo Docker images, revealing that automated environment construction is the key bottleneck to scaling execution-based data. BugSwarm previously demonstrated that Docker-based encapsulation enables reproducible builds at scale by capturing CI contexts, while Repairnator showed that automating build-and-test orchestration on real repositories is feasible and impactful. Repo2Run extends these ideas by not just replaying environments but synthesizing them: it uses build and test feedback to produce reusable Dockerfiles for previously unprepared repos.\nMethodologically, ReAct provides the blueprint for interleaving reasoning with tool use\u2014exactly the control loop Repo2Run uses to call docker build and test tools, interpret logs, and plan next steps. Reflexion supplies the principle that iteratively leveraging execution feedback improves task success; Repo2Run operationalizes this via log-driven Dockerfile refinement until the pipeline passes. Finally, modern software-engineering agents such as SWE-agent and OpenDevin established that LLMs can manipulate shells, package managers, and test runners inside containers but also surfaced environment setup as a persistent pain point. Repo2Run directly addresses that gap, transforming brittle, ad hoc setup into an automated, scalable, and reusable Dockerfile synthesis process that unlocks large-scale executable code data.",
  "analysis_timestamp": "2026-01-07T00:21:32.325526"
}
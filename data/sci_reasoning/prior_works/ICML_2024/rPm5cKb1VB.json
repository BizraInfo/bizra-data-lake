{
  "prior_works": [
    {
      "title": "Weisfeiler-Lehman Graph Kernels",
      "authors": "Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, Karsten M. Borgwardt",
      "year": 2011,
      "role": "foundational algorithm/theoretical lens",
      "relationship_sentence": "Introduced subtree-based WL relabeling and an implicit fragment feature space, which Fragment-WL explicitly generalizes to analyze fragment-biased GNNs theoretically."
    },
    {
      "title": "How Powerful are Graph Neural Networks?",
      "authors": "Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka",
      "year": 2019,
      "role": "expressivity theory baseline",
      "relationship_sentence": "Established the 1-WL expressivity ceiling for message-passing GNNs (e.g., GIN), providing the formal baseline that Fragment-WL seeks to surpass by incorporating fragment information."
    },
    {
      "title": "Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks",
      "authors": "Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, Martin Grohe",
      "year": 2019,
      "role": "higher-order GNN framework",
      "relationship_sentence": "Connected k-WL to higher-order GNNs and improved expressivity, directly motivating the paper\u2019s comparison that fragment-biased models can outperform these despite their theoretical power and inspiring a WL-style extension for fragments."
    },
    {
      "title": "Provably Powerful Graph Networks",
      "authors": "Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, Yaron Lipman",
      "year": 2019,
      "role": "theoretical comparator (beyond 1-WL)",
      "relationship_sentence": "Provided 3-WL-level expressive architectures that serve as state-of-the-art higher-order baselines against which the paper positions Fragment-WL and its fragment-augmented GNN."
    },
    {
      "title": "Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting (Graph Substructure Networks)",
      "authors": "Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, Michael M. Bronstein",
      "year": 2020,
      "role": "substructure/fragment inductive bias",
      "relationship_sentence": "Demonstrated that injecting explicit substructure counts boosts GNN expressivity beyond 1-WL, directly informing the paper\u2019s focus on fragment-biased architectures and the need for a WL-style theory to analyze them."
    },
    {
      "title": "Extended-Connectivity Fingerprints (ECFP)",
      "authors": "David Rogers, Mathew Hahn",
      "year": 2010,
      "role": "molecular fragment representation",
      "relationship_sentence": "Pioneered circular, fragment-based molecular descriptors with effectively unbounded vocabularies, inspiring the paper\u2019s infinite-vocabulary fragmentation scheme within a learnable GNN."
    },
    {
      "title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints",
      "authors": "David K. Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael G\u00f3mez-Bombarelli, Timothy Hirzel, Al\u00e1n Aspuru-Guzik, Ryan P. Adams",
      "year": 2015,
      "role": "neural fragment learning for molecules",
      "relationship_sentence": "Showed that learnable, ECFP-like neural fingerprints (fragment aggregations) yield strong molecular performance, directly motivating fragment-centric inductive biases that Fragment-WL formalizes and extends."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014Fragment-WL and a fragment-augmented GNN with an infinite vocabulary\u2014is rooted in two converging lines of work: WL-based expressivity theory and fragment-centric representations for molecules. On the theory side, Shervashidze et al.\u2019s WL graph kernels operationalized subtree relabeling as an implicit fragment enumeration, establishing a canonical mechanism for generating discriminative substructures. Xu et al. formalized that standard message-passing GNNs are at most as powerful as 1-WL, while Morris et al. and Maron et al. advanced higher-order GNNs aligned with k-WL to surpass these limits. Despite their theoretical strength, such higher-order models often underperform in molecular prediction, indicating a gap between expressivity and the inductive biases needed for chemistry.\nOn the representation side, Rogers and Hahn\u2019s ECFP cemented fragment-based descriptors as a dominant paradigm, effectively leveraging an enormous fragment vocabulary. Duvenaud et al. bridged ECFP and learning by introducing neural fingerprints that learn fragment aggregations end-to-end. Bouritsas et al. then demonstrated that explicit substructure signals (via isomorphism counting) can lift GNN expressivity beyond 1-WL. Building on these insights, the present paper extends the WL framework itself to fragments (Fragment-WL), providing the missing theory for fragment-biased GNNs and motivating an architecture that couples WL-style updates with an infinite fragment vocabulary. This unifies expressivity gains with domain-relevant inductive bias, explaining the observed performance and generalization improvements on molecular benchmarks.",
  "analysis_timestamp": "2026-01-07T00:02:04.892915"
}
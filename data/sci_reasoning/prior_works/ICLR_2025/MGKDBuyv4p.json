{
  "prior_works": [
    {
      "title": "The Secret Sharer: Measuring Unintended Memorization in Neural Networks",
      "authors": "Nicholas Carlini et al.",
      "year": 2019,
      "arxiv_id": "1802.08232",
      "role": "Foundation",
      "relationship_sentence": "This work introduced the canary/exposure formulation and concrete extraction metrics that the current paper adopts to define, measure, and directly target memorization in language models."
    },
    {
      "title": "Extracting Training Data from Large Language Models",
      "authors": "Nicholas Carlini et al.",
      "year": 2021,
      "arxiv_id": "2012.07805",
      "role": "Foundation",
      "relationship_sentence": "By demonstrating large-scale verbatim regurgitation and practical extraction attacks, this paper established the real-world risk the present work aims to mitigate and provides the primary evaluation protocol the authors build upon."
    },
    {
      "title": "Deduplicating Training Data Makes Language Models Better",
      "authors": "Katherine Lee et al.",
      "year": 2022,
      "arxiv_id": "2107.06499",
      "role": "Gap Identification",
      "relationship_sentence": "This study showed that corpus deduplication reduces memorization but requires re-pretraining, motivating the current paper\u2019s focus on post-hoc regularization, fine-tuning, and unlearning methods that do not need rebuilding models from scratch."
    },
    {
      "title": "Machine Unlearning",
      "authors": "Lucas Bourtoule et al.",
      "year": 2021,
      "arxiv_id": "1912.03817",
      "role": "Foundation",
      "relationship_sentence": "This work formalized unlearning (e.g., SISA) and practical deletion procedures that the current paper adapts and extends to generative LMs, serving as the conceptual and algorithmic basis for several of its unlearning baselines and new variants."
    },
    {
      "title": "Mass-Editing Memory in a Transformer (MEMIT)",
      "authors": "Kevin Meng et al.",
      "year": 2023,
      "arxiv_id": "2210.07229",
      "role": "Extension",
      "relationship_sentence": "MEMIT\u2019s fast, localized weight-editing mechanism directly inspires the paper\u2019s unlearning-style interventions that erase specific memorized spans, which the authors extend from factual edits to systematic memorization mitigation."
    },
    {
      "title": "TinyStories: How Small Language Models Can Write Stories",
      "authors": "Ronen Eldan and Yuanzhi Li",
      "year": 2023,
      "arxiv_id": "2305.07759",
      "role": "Inspiration",
      "relationship_sentence": "The idea of purpose-built, tiny LMs for rapid, faithful experimentation motivates the paper\u2019s TinyMem suite as a small-scale testbed for developing and validating memorization-mitigation methods before transferring to production LMs."
    },
    {
      "title": "Pythia: A Suite for Analyzing Large Language Models",
      "authors": "Stella Biderman et al.",
      "year": 2023,
      "arxiv_id": "2304.01373",
      "role": "Inspiration",
      "relationship_sentence": "Pythia\u2019s controlled, analysis-ready model suite informs the design principle behind TinyMem\u2014curating reproducible, size-scaled models to study behaviors (here, memorization) that transfer to larger systems."
    }
  ],
  "synthesis_narrative": "Unintended memorization in neural networks was concretely formalized by The Secret Sharer through canary insertion and exposure metrics, establishing a precise way to quantify extraction risk from model outputs. Subsequent work on Extracting Training Data from Large Language Models demonstrated practical, large-scale recovery of verbatim training snippets, moving the concern from theoretical to operational, and supplied attack-style evaluations that became standard for assessing mitigation. Deduplicating Training Data Makes Language Models Better showed that corpus-level dedup can materially reduce regurgitation, yet it requires re-pretraining and thus does not address already-deployed models. In parallel, Machine Unlearning articulated principled deletion objectives and practical schemes (e.g., SISA) that make removal of specific data feasible without full retraining, offering a blueprint for LM-focused unlearning. Complementing these, MEMIT introduced fast, localized weight edits to modify or erase stored associations, suggesting a mechanism to target memorized content directly in transformer weights. Finally, TinyStories and Pythia established the value of curated, small model suites for rapid, reproducible experimentation whose findings transfer to larger models.\nTogether, these works revealed a pressing need for efficient, post-hoc mitigation of LM memorization, provided metrics and attack protocols to evaluate it, and suggested two promising levers\u2014unlearning frameworks and direct weight edits. Building on these ideas, it becomes natural to develop a compact model suite for rapid iteration (TinyMem) and to design LM-specific unlearning variants that adapt SISA-like principles and editing-style interventions, then validate that these mitigations discovered at small scale reliably transfer to production-grade LMs.",
  "target_paper": {
    "title": "Mitigating Memorization in Language Models",
    "authors": "Mansi Sakarvadia, Aswathy Ajith, Arham Mushtaq Khan, Nathaniel C Hudson, Caleb Geniesse, Kyle Chard, Yaoqing Yang, Ian Foster, Michael W. Mahoney",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "language models, memorization, machine unlearning, regularization, fine-tuning, natural language processing",
    "abstract": "Language models (LMs) can \u201cmemorize\u201d information, i.e., encode training data in their weights in such a way that inference-time queries can lead to verbatim regurgitation of that data. This ability to extract training data can be problematic, for example, when data are private or sensitive. In this work, we investigate methods to mitigate memorization: three regularizer-based, three fine-tuning-based, and eleven machine unlearning-based methods, with five of the latter being new methods that we introduce. We also introduce TinyMem, a suite of small, computationally-efficient LMs for the rapid development and evaluation of memorization-mitigation methods. We demonstrate that the mitigation methods that we develop using TinyMem can successfully be applied to production-grade LMs, and we determine via experiment that: regularizer-based mitigation methods are slow and ineffective at curbing memorization; fine-tuning-based methods\nare effective at curbing memorization, but overly expensive,",
    "openreview_id": "MGKDBuyv4p",
    "forum_id": "MGKDBuyv4p"
  },
  "analysis_timestamp": "2026-01-06T07:39:21.248481"
}
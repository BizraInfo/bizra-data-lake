{
  "prior_works": [
    {
      "title": "Auto-Encoding Variational Bayes",
      "authors": [
        "Diederik P. Kingma",
        "Max Welling"
      ],
      "year": 2014,
      "role": "foundational algorithm",
      "relationship_sentence": "SPAGD\u2019s reconstruction-driven training and residual-based signals build on VAE-style latent reconstruction, using reconstruction residuals as informative cues to drive self-perturbation and anomaly-aware updates."
    },
    {
      "title": "Graph Attention Networks",
      "authors": [
        "Petar Veli\u010dkovi\u0107",
        "Guillem Cucurull",
        "Arantxa Casanova",
        "Adriana Romero",
        "Pietro Li\u00f2",
        "Yoshua Bengio"
      ],
      "year": 2018,
      "role": "architectural building block",
      "relationship_sentence": "SPAGD\u2019s dynamic inter-variable dependency modeling leverages attention-based message passing in graphs, conceptually grounded in GAT to flexibly weight variable relationships over time."
    },
    {
      "title": "Deep Anomaly Detection with Outlier Exposure",
      "authors": [
        "Dan Hendrycks",
        "Mantas Mazeika",
        "Thomas Dietterich"
      ],
      "year": 2019,
      "role": "inspiration",
      "relationship_sentence": "SPAGD\u2019s self-perturbation module echoes the OE idea of training with auxiliary atypical samples to mitigate class imbalance, but generates pseudo-anomalies from normal reconstructions instead of using external outliers."
    },
    {
      "title": "OmniAnomaly: Detecting Anomalies in Multivariate Time Series via Stochastic Recurrent Networks",
      "authors": [
        "Hansheng Ren et al."
      ],
      "year": 2019,
      "role": "methodological predecessor",
      "relationship_sentence": "SPAGD extends reconstruction-based multivariate TSAD exemplified by OmniAnomaly by explicitly using reconstruction residuals both to create self-perturbations and to guide graph adaptation, addressing overfitting to normal patterns."
    },
    {
      "title": "MTAD-GAT: Multivariate Time-Series Anomaly Detection via Graph Attention Network",
      "authors": [
        "Bing Zhou et al."
      ],
      "year": 2020,
      "role": "methodological predecessor",
      "relationship_sentence": "SPAGD advances MTAD-GAT\u2019s idea of modeling variable dependencies with attention by making the graph structure anomaly-aware and dynamically adjusted using residuals from self-perturbed series."
    },
    {
      "title": "Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy",
      "authors": [
        "Yixin Liu et al."
      ],
      "year": 2021,
      "role": "inspiration",
      "relationship_sentence": "SPAGD shares Anomaly Transformer\u2019s insight that contrasting normal associations helps surface subtle anomalies, operationalizing it via residual-driven, dynamically reweighted graphs rather than attention association discrepancy."
    }
  ],
  "synthesis_narrative": "SPAGD sits at the intersection of reconstruction-based TS anomaly detection, graph-based dependency modeling, and data imbalance mitigation through auxiliary signals. Variational autoencoders (Kingma & Welling) underpin SPAGD\u2019s reconstruction stage, whose residuals become a first-class signal. OmniAnomaly demonstrated the efficacy and limits of reconstruction-centered multivariate TSAD: it captures normal dynamics but can overfit and miss subtle deviations. SPAGD tackles this by converting reconstruction residuals into two mechanisms\u2014self-perturbations that generate pseudo-anomalies to relieve class imbalance, and anomaly-aware cues that adapt the dependency graph. The graph component traces to GAT, enabling learnable, time-varying inter-variable weights; MTAD-GAT brought this idea to TSAD, showing that modeling sensor relations boosts detection. SPAGD pushes further by making the graph explicitly responsive to residuals from self-perturbed data, steering edges toward correlations most informative for anomalies rather than purely normal dynamics. Complementing this, Outlier Exposure\u2019s principle\u2014training with auxiliary atypical samples to improve boundary awareness\u2014motivates SPAGD\u2019s internal pseudo-anomaly generation without requiring external outliers. Finally, Anomaly Transformer\u2019s association-discrepancy perspective highlights that subtle anomalies often manifest as shifts in relational structure; SPAGD embraces this by using residuals to drive dynamic, anomaly-aware graph reweighting. Together, these strands yield a framework that balances scarce anomalies, adapts to evolving inter-variable correlations, and resists overfitting to normal reconstructions.",
  "analysis_timestamp": "2026-01-07T00:21:32.304737"
}
{
  "prior_works": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Foundational baseline and limitation (lazy training/NTK regime)",
      "relationship_sentence": "The paper\u2019s central claim\u2014that three-layer nets provably learn nonlinear hierarchical features beyond kernel behavior\u2014explicitly departs from the NTK/lazy regime formalized by Jacot et al., using it as a foil to motivate and frame guarantees that require genuine feature evolution."
    },
    {
      "title": "A Mean Field View of the Landscape of Two-Layer Neural Networks",
      "authors": "Song Mei, Andrea Montanari, Phan-Minh Nguyen",
      "year": 2018,
      "role": "Technical precedent for provable feature learning in two-layer networks",
      "relationship_sentence": "This mean-field analysis established that two-layer networks can provably learn features (beyond NTK) under small-initialization gradient dynamics; the present work builds directly on this line by extending provable feature learning guarantees to three layers and contrasting their richer capabilities with two-layer limits."
    },
    {
      "title": "The Power of Depth for Feedforward Neural Networks",
      "authors": "Ronen Eldan, Ohad Shamir",
      "year": 2016,
      "role": "Conceptual foundation: depth separation/representational benefits of three layers",
      "relationship_sentence": "Eldan\u2013Shamir\u2019s depth-separation result identifies functions efficiently representable by three-layer networks but not by two-layer ones, providing the representational backdrop that this paper upgrades to learnability and sample-complexity guarantees via layer-wise gradient descent."
    },
    {
      "title": "Benefits of depth in neural networks",
      "authors": "Mikhail Telgarsky",
      "year": 2016,
      "role": "Conceptual foundation: hierarchical/compositional function classes favoring depth",
      "relationship_sentence": "Telgarsky\u2019s constructions showing exponential advantages of depth for certain compositional functions directly motivate the hierarchical targets (e.g., functions of quadratics) for which this paper proves efficient three-layer feature learning."
    },
    {
      "title": "When Do Neural Networks Outperform Kernel Methods?",
      "authors": "Amir Ghorbani, Song Mei, Andrea Montanari",
      "year": 2021,
      "role": "Comparative theory: feature-learning advantage over kernels on structured tasks",
      "relationship_sentence": "By pinpointing regimes where learned features yield statistical gains over kernel methods\u2014especially for hierarchical structure\u2014this work informs the present paper\u2019s focus on sample-complexity advantages arising from nonlinear feature learning in deeper (three-layer) architectures."
    },
    {
      "title": "A Convergence Theory for Deep Learning via Over-Parameterization",
      "authors": "Zeyuan Allen-Zhu, Yuanzhi Li, Yingyu Liang (Song)",
      "year": 2019,
      "role": "Optimization toolkit for deep nets under gradient descent",
      "relationship_sentence": "Techniques for controlling gradient descent in over-parameterized deep networks underpin the layer-wise training analysis here, helping justify that the proposed procedure reliably reaches regimes where hierarchical features are learned rather than remaining in a lazy kernel state."
    },
    {
      "title": "Why and when can deep\u2014but not shallow\u2014networks avoid the curse of dimensionality",
      "authors": "Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, Qianli Liao",
      "year": 2017,
      "role": "Statistical motivation: compositional structure and sample complexity",
      "relationship_sentence": "This work\u2019s theory that hierarchical compositional structure yields superior sample complexity for deep models directly motivates the current paper\u2019s general theorem and its instantiations (e.g., single-index and quadratic-function settings) demonstrating provable sample/width efficiency."
    }
  ],
  "synthesis_narrative": "Nichani, Damian, and Lee address a core gap between representation and learnability: although depth-separation works (Eldan\u2013Shamir; Telgarsky) show three-layer networks can represent hierarchical functions far more efficiently than two-layer models, prior guarantees largely operated in the lazy/NTK regime (Jacot et al.) or were confined to two-layer mean-field analyses (Mei\u2013Montanari\u2013Nguyen). Their paper advances this frontier by proving that a three-layer network trained with layer-wise gradient descent can nonlinearly learn hierarchical features and achieve favorable sample and width complexity on structured targets (e.g., single-index and functions of quadratics).\n\nThe conceptual scaffolding comes from depth-separation and compositional-statistics perspectives (Poggio et al.), which articulate why hierarchical structure should benefit deeper models. Empirically and theoretically, comparisons of neural networks against kernels (Ghorbani\u2013Mei\u2013Montanari) clarified that learned features can yield statistical gains precisely when structure is hierarchical\u2014motivating a proof framework that must escape NTK behavior. Technically, mean-field analyses of two-layer nets (Mei\u2013Montanari\u2013Nguyen) provided the first rigorous foothold for feature evolution under small-initialization gradient dynamics; the present work extends these ideas to an additional hidden layer and shows strictly richer learnable features. Finally, optimization insights for over-parameterized deep nets (Allen-Zhu\u2013Li\u2013Song) inform the control of gradient dynamics necessary for the layer-wise scheme to succeed without collapsing to a kernel limit. Together, these strands enable the paper\u2019s main contribution: general-purpose, provable guarantees for nonlinear hierarchical feature learning in three-layer networks with concrete sample-complexity and width bounds.",
  "analysis_timestamp": "2026-01-06T23:42:49.114656"
}
{
  "prior_works": [
    {
      "title": "Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold",
      "authors": "Xingang Pan et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "It established the drag-style handle\u2013target formulation and energy-based point alignment that DragonDiffusion adapts from GAN latent optimization to diffusion sampling via correspondence-driven gradient guidance."
    },
    {
      "title": "DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing",
      "authors": "Shi et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "As the primary diffusion-based drag baseline, it motivates DragonDiffusion\u2019s shift from pixel/latent constraints to diffusion-feature correspondence and multi-scale guidance to mitigate geometry distortions and identity drift observed in DragDiffusion."
    },
    {
      "title": "DIFT: Diffusion Features for Dense Visual Correspondence",
      "authors": "Chen et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "It showed that multi-scale Stable Diffusion features provide robust dense correspondences, directly enabling DragonDiffusion\u2019s correspondence-centric energy functions and its semantic\u2013geometric multi-scale guidance design."
    },
    {
      "title": "Prompt-to-Prompt Image Editing with Cross-Attention Control",
      "authors": "Amir Hertz et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "Its cross-attention control mechanism for preserving layout/content is extended by DragonDiffusion into a visual cross-attention memory bank that enforces consistency with the source image during drag edits."
    },
    {
      "title": "Pix2Pix-Zero: Zero-shot Image-to-Image Translation",
      "authors": "Huang et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "Its training-free attention injection to preserve subject/layout informs DragonDiffusion\u2019s strategy of caching and reusing attention information to maintain fidelity while applying drag guidance."
    },
    {
      "title": "Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models",
      "authors": "Hila Chefer et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "By demonstrating that manipulating attention can steer diffusion toward desired semantics, it inspires DragonDiffusion\u2019s use of a memory-backed cross-attention mechanism to couple semantic consistency with drag-driven geometric changes."
    }
  ],
  "synthesis_narrative": "Drag-style interactive manipulation was crystallized by Drag Your GAN, which framed editing as moving user-specified handles to targets by minimizing an energy defined on internal features and updating the generator with gradients. DragDiffusion transferred this idea to diffusion models, but its point constraints in pixel or latent space often yielded identity drift and geometric artifacts under large or semantic motions. In parallel, DIFT established that Stable Diffusion\u2019s internal features encode strong dense correspondences at multiple layers\u2014early layers capturing geometry and later layers capturing semantics\u2014suggesting a principled space for alignment-based control. Prompt-to-Prompt revealed that reusing or constraining cross\u2011attention maps can preserve scene layout and content during editing, while Pix2Pix\u2011Zero generalized this training\u2011free attention injection to maintain identity and structure across denoising steps. Attend\u2011and\u2011Excite further showed that targeted attention modulation can enforce semantic focus, underscoring attention as a lightweight, effective control knob.\n\nTogether, these works expose a clear opportunity: marry drag-style energy minimization with diffusion\u2019s rich, multi-scale correspondences, and stabilize edits by explicitly preserving attention patterns tied to the source image. DragonDiffusion synthesizes this by defining correspondence-driven energy functions over diffusion features and injecting their gradients into sampling, scaling guidance from geometric to semantic layers, and introducing a memory\u2011banked visual cross\u2011attention to anchor content fidelity during drag operations\u2014achieving precise, training\u2011free, drag-style editing on diffusion models.",
  "target_paper": {
    "title": "DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models",
    "authors": "Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, Jian Zhang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Diffusion model, Image editing, Image generation",
    "abstract": "Despite the ability of text-to-image (T2I) diffusion models to generate high-quality images, transferring this ability to accurate image editing remains a challenge. In this paper, we propose a novel image editing method, DragonDiffusion, enabling Drag-style manipulation on Diffusion models. Specifically, we treat image editing as the change of feature correspondence in a pre-trained diffusion model. By leveraging feature correspondence, we develop energy functions that align with the editing target, transforming image editing operations into gradient guidance. Based on this guidance approach, we also construct multi-scale guidance that considers both semantic and geometric alignment. Furthermore, we incorporate a visual cross-attention strategy based on a memory bank design to ensure consistency between the edited result and original image. Benefiting from these efficient designs, all content editing and consistency operations come from the feature correspondence without extra model f",
    "openreview_id": "OEL4FJMg1b",
    "forum_id": "OEL4FJMg1b"
  },
  "analysis_timestamp": "2026-01-06T22:34:19.894735"
}
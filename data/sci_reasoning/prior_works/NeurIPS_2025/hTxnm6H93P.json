{
  "prior_works": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Foundational infinite-width theory establishing linearized (kernel) training dynamics",
      "relationship_sentence": "This work provides the canonical infinite-width NTK framework whose prediction of lazy, linearized training (and hence vanishing feature learning under stable scaling) forms the theoretical baseline that the present paper re-examines for SP at large learning rates."
    },
    {
      "title": "Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent",
      "authors": "Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, Jeffrey Pennington",
      "year": 2019,
      "role": "Rigorous demonstration of linearized dynamics in the infinite-width limit",
      "relationship_sentence": "By formalizing when and how wide networks behave as linear models during training, this paper sharpened the expectation that feature learning should vanish at width\u2014an expectation the new work shows can be violated under SP with cross-entropy and large learning rates."
    },
    {
      "title": "Tensor Programs IV: Feature Learning in Neural Networks",
      "authors": "Greg Yang",
      "year": 2020,
      "role": "Theory identifying parametrizations/regimes that preserve feature learning at infinite width",
      "relationship_sentence": "TP-IV delineates conditions for nontrivial feature learning at infinite width, highlighting limitations of SP predictions and motivating the present paper\u2019s search for feature-learning behavior within SP via a finer analysis of the ostensibly unstable regime."
    },
    {
      "title": "Tensor Programs V: Tuning Large Neural Networks via \u03bc-Parametrization",
      "authors": "Greg Yang, Edward J. Hu",
      "year": 2021,
      "role": "Prescriptive scaling (\u03bcP) that stabilizes training and feature learning across widths",
      "relationship_sentence": "\u03bcP argues that SP with width-independent learning rates should be unstable or lazy, so the current paper\u2019s finding that SP can train stably and learn features at large widths with large learning rates stands in direct dialogue with \u03bcP\u2019s prescriptions."
    },
    {
      "title": "The Implicit Bias of Gradient Descent on Separable Data",
      "authors": "Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, Nathan Srebro",
      "year": 2018,
      "role": "Core analysis of cross-entropy dynamics and implicit bias",
      "relationship_sentence": "This result\u2014that under cross-entropy, gradient descent drives norms to infinity while converging in direction to a max-margin solution\u2014underpins the paper\u2019s CE-specific refinement of the \u2018unstable\u2019 regime into subregimes with distinct, stable feature-learning behavior."
    },
    {
      "title": "A Mean Field View of the Landscape of Two-Layer Neural Networks",
      "authors": "Song Mei, Andrea Montanari, Phan-Minh Nguyen",
      "year": 2018,
      "role": "Mean-field training dynamics showing feature learning at infinite width under alternative scaling",
      "relationship_sentence": "Mean-field analyses demonstrate that infinite-width feature learning is possible under non-NTK scalings, providing a conceptual contrast that motivates the paper\u2019s claim that even SP can exhibit nontrivial feature learning when the large\u2013learning-rate regime is analyzed carefully."
    },
    {
      "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
      "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
      "year": 2015,
      "role": "Establishes He initialization that defines the standard parameterization used in practice",
      "relationship_sentence": "He initialization under a global learning rate is the practical SP baseline whose surprising large\u2013learning-rate effectiveness the present paper explains theoretically."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014showing that, under cross-entropy loss, standard parameterization (He initialization with a global learning rate) admits a finer structure within the traditionally \u201cunstable\u201d large\u2013learning-rate regime that yields both stable training and nontrivial feature learning at large width\u2014builds directly on and challenges canonical width-limit theory. NTK theory (Jacot et al., Lee et al.) formalized the infinite-width, linearized (lazy) regime, leading to the prevailing belief that stable training at width necessarily suppresses feature learning, and that SP with width-independent learning rates is either unstable or lazy. In contrast, the Tensor Programs line (Yang; Yang & Hu) characterized parametrizations and scalings (notably \u03bcP) that preserve feature learning at width and prescribe learning-rate rules, implicitly suggesting SP\u2019s limitations and providing a foil for the present analysis. Crucially, the new results hinge on the specific dynamics of cross-entropy: Soudry et al.\u2019s implicit-bias analysis shows that CE drives norms to infinity while stabilizing prediction directions, a mechanism that can alter stability thresholds and learning dynamics compared to squared loss. Mean-field analyses (Mei, Montanari, Nguyen) further demonstrate that infinite-width feature learning is achievable under alternative scalings, motivating the search for analogous behavior within SP. By integrating these strands, the paper revisits the ostensibly unstable SP regime and reveals CE-induced subregimes that reconcile practice with theory: learning rates can be larger than predicted by NTK/\u03bcP prescriptions while maintaining stability and enabling feature evolution at large widths.",
  "analysis_timestamp": "2026-01-07T00:21:33.147558"
}
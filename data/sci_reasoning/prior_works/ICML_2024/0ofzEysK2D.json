{
  "prior_works": [
    {
      "title": "Universal Intelligence: A Definition of Machine Intelligence",
      "authors": "Legg et al.",
      "year": 2007,
      "role": "Foundation",
      "relationship_sentence": "Provides a formal, task-agnostic definition of intelligence as performance averaged over a wide distribution of environments; Levels of AGI operationalizes this into explicit depth (performance) and breadth (generality) axes."
    },
    {
      "title": "Measuring Universal Intelligence: Towards an Anytime Intelligence Test",
      "authors": "Hern\u00e1ndez-Orallo et al.",
      "year": 2010,
      "role": "Foundation",
      "relationship_sentence": "Introduces universal, species- and system-agnostic testing and anytime evaluation, directly shaping the paper\u2019s principles for an ontology and its call for benchmarks that assess generality across diverse tasks."
    },
    {
      "title": "On the Measure of Intelligence",
      "authors": "Chollet et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Argues that intelligence should be measured by efficiency of skill acquisition and generalization across tasks (exemplified by ARC), inspiring the paper\u2019s explicit separation of performance depth from generality breadth and its critique of static, narrow benchmarks."
    },
    {
      "title": "A Model for Types and Levels of Human Interaction with Automation",
      "authors": "Parasuraman et al.",
      "year": 2000,
      "role": "Inspiration",
      "relationship_sentence": "Establishes a levels-of-automation framework and HAI considerations that directly inform the paper\u2019s autonomy dimension and its emphasis on matching interaction paradigms to capability levels for responsible deployment."
    },
    {
      "title": "Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles (J3016)",
      "authors": "SAE International et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrates how a domain-wide, \u2018levels\u2019 taxonomy can operationalize progress and safety in a complex technology; the paper adapts this template to AGI by proposing capability levels tied to performance, generality, and autonomy."
    },
    {
      "title": "Holistic Evaluation of Language Models (HELM)",
      "authors": "Liang et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "Shows that fragmented, single-metric leaderboards obscure real capabilities and risks; the paper addresses this gap by proposing an ontology that unifies breadth, depth, and autonomy, and by outlining requirements for future, level-aligned benchmarks."
    },
    {
      "title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
      "authors": "Bubeck et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "The ambiguous \u2018AGI\u2019 claims based on ad-hoc evaluations highlight the absence of a common language for capability levels; the paper directly responds with a principled, level-based framework to standardize assessments and risk discussions."
    }
  ],
  "synthesis_narrative": "The Levels of AGI framework crystallizes long-standing ideas about intelligence measurement while responding to recent evaluation shortcomings and deployment realities. Legg and Hutter\u2019s universal intelligence formalized intelligence as expected performance over broad environment distributions, a foundation that this paper operationalizes into orthogonal axes of depth (performance) and breadth (generality). Hern\u00e1ndez-Orallo and Dowe\u2019s universal, anytime testing further grounded the principle that evaluations must be system-agnostic and span diverse tasks, directly informing the paper\u2019s six ontology principles and its call for future benchmarks aligned to levels. Chollet\u2019s measure reframed intelligence as the efficiency of acquiring new skills and generalizing across tasks; this insight motivates the paper\u2019s explicit separation of performance from generality and its critique of overfitting to narrow benchmarks.\n\nTo connect capabilities to real-world deployment, the authors draw on established autonomy taxonomies. Parasuraman, Sheridan, and Wickens\u2019 levels-of-automation and human\u2013automation interaction guidance shape the autonomy dimension and the emphasis on aligning interaction paradigms with capability levels. SAE\u2019s J3016 demonstrates how a community can coordinate around levels to track progress and manage safety\u2014a template that the paper adapts for AGI. Finally, HELM\u2019s holistic evaluation reveals fragmentation and metric myopia, while GPT-4\u2019s \u2018sparks of AGI\u2019 claims expose the lack of a shared language for progress. Together, these works directly motivate and enable the paper\u2019s core innovation: a principled, level-based ontology that integrates performance, generality, and autonomy to compare models, assess risks, and guide benchmark development.",
  "analysis_timestamp": "2026-01-06T23:09:26.509714"
}
{
  "prior_works": [
    {
      "title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks",
      "authors": "Dan Hendrycks, Kevin Gimpel",
      "year": 2017,
      "role": "Foundational post-hoc OOD baseline (Maximum Softmax Probability)",
      "relationship_sentence": "MultiOOD evaluates and extends classic post-hoc scoring like MSP across multiple modalities, showing that aggregating modality-wise confidence signals substantially boosts OOD detection."
    },
    {
      "title": "Enhancing the Reliability of Out-of-distribution Detection in Neural Networks",
      "authors": "Shiyu Liang, Yixuan Li, R. Srikant",
      "year": 2018,
      "role": "Temperature-scaled, perturbed post-hoc OOD method (ODIN)",
      "relationship_sentence": "ODIN serves as a strong unimodal baseline whose gains are amplified in MultiOOD when scores are computed per-modality and combined, reinforcing the paper\u2019s claim that multi-modality improves OOD detection."
    },
    {
      "title": "A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks",
      "authors": "Kimin Lee, Kibok Lee, Honglak Lee, Jinwoo Shin",
      "year": 2018,
      "role": "Feature-distance OOD scoring (Mahalanobis)",
      "relationship_sentence": "MultiOOD leverages feature-space OOD scores for each modality and demonstrates that cross-modal aggregation of Mahalanobis-like signals reduces false positives versus unimodal use."
    },
    {
      "title": "Energy-based Out-of-distribution Detection",
      "authors": "Weitang Liu, Xiaoyun Wang, John D. Owens, Yixuan Li",
      "year": 2020,
      "role": "Unified energy score for OOD detection",
      "relationship_sentence": "Energy scores are used as strong, architecture-agnostic baselines in MultiOOD; computing and fusing energies across modalities directly underpins the paper\u2019s empirical finding that more modalities yield better OOD detection."
    },
    {
      "title": "Deep Anomaly Detection with Outlier Exposure",
      "authors": "Dan Hendrycks, Mantas Mazeika, Thomas Dietterich",
      "year": 2019,
      "role": "Training-time regularization for OOD (Outlier Exposure)",
      "relationship_sentence": "MultiOOD investigates OE-style training in multimodal settings, showing that exposure benefits persist and often strengthen when the model can align or cross-check signals from multiple modalities."
    },
    {
      "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
      "authors": "Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell",
      "year": 2017,
      "role": "Uncertainty via predictive disagreement",
      "relationship_sentence": "MultiOOD\u2019s Modality Prediction Discrepancy operationalizes the ensemble-disagreement principle across modalities rather than models, using cross-modal predictive inconsistency as a direct OOD signal."
    },
    {
      "title": "Combining Labeled and Unlabeled Data with Co-Training",
      "authors": "Avrim Blum, Tom Mitchell",
      "year": 1998,
      "role": "Multi-view learning principle (agreement/disagreement across views)",
      "relationship_sentence": "The paper\u2019s core insight\u2014that ID data exhibits cross-view (modality) agreement while OOD induces disagreement\u2014echoes co-training\u2019s agreement principle, now repurposed as a detection cue in a modern deep, multimodal context."
    }
  ],
  "synthesis_narrative": "MultiOOD\u2019s key contribution\u2014establishing a scalable multimodal OOD benchmark and leveraging Modality Prediction Discrepancy\u2014rests on two intertwined lines of prior work: post-hoc OOD scoring from single modalities and theory/heuristics that link disagreement to uncertainty. Foundational unimodal detectors such as MSP (Hendrycks & Gimpel), ODIN (Liang et al.), Mahalanobis distance (Lee et al.), and the energy score (Liu et al.) provided simple, effective, and broadly applicable scoring functions. These methods defined the de facto evaluation protocol for OOD detection and revealed practical levers\u2014confidence calibration, feature distances, and energy landscapes\u2014that MultiOOD could port to each modality and then aggregate. Outlier Exposure (Hendrycks et al.) added a complementary training-time lever, which MultiOOD tests in a multimodal regime, showing regularization benefits can compound when multiple sensing channels are available.\nCrucially, the paper\u2019s Modality Prediction Discrepancy connects to the long-standing insight that predictive disagreement signals epistemic uncertainty. Deep Ensembles (Lakshminarayanan et al.) demonstrated this in the model-space; MultiOOD recasts it in the modality-space, using cross-modal inconsistencies as an OOD indicator. This echoes the co-training principle (Blum & Mitchell): in-distribution examples tend to yield agreement across independent views, whereas atypical or shifted inputs induce disagreement. By synthesizing these strands, MultiOOD both broadens the evaluation canvas\u2014standard OOD scores applied across modalities\u2014and crystallizes a generalizable detection cue rooted in cross-modal agreement, demonstrating that simply adding modalities and measuring their predictive consistency is a robust path forward for safety-critical deployment.",
  "analysis_timestamp": "2026-01-06T23:33:35.575791"
}
{
  "prior_works": [
    {
      "title": "RNNs Provably Solve Parity with Chain-of-Thought (CoT) Supervision",
      "authors": "Wies et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "This paper\u2019s transformer results explicitly extend Wies (2023)\u2019s RNN analysis on k-parity with step-by-step (CoT-like) intermediate supervision, translating their parity-learning mechanism and sample/iteration guarantees from RNNs to attention architectures."
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei et al.",
      "year": 2022,
      "arxiv_id": "2201.11903",
      "role": "Foundation",
      "relationship_sentence": "It introduced the central idea of supervising or eliciting intermediate reasoning chains, which this work formalizes as intermediate parities for transformers and analyzes theoretically (including under teacher forcing)."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang et al.",
      "year": 2022,
      "arxiv_id": "2203.11171",
      "role": "Inspiration",
      "relationship_sentence": "Their insight that multiple, internally consistent chains can verify reasoning motivates this paper\u2019s augmented-data mechanism that internally checks intermediate steps to enable efficient end-to-end learning without teacher forcing."
    },
    {
      "title": "STaR: Bootstrapping Reasoning With Reasoning",
      "authors": "Eric Zelikman et al.",
      "year": 2022,
      "arxiv_id": "2203.14465",
      "role": "Inspiration",
      "relationship_sentence": "STaR\u2019s idea of augmenting training with self-generated, correctness-verified rationales directly informs the paper\u2019s theoretical construction where augmented data is used to verify intermediate parities and drive efficient learning."
    },
    {
      "title": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks",
      "authors": "Ronald J. Williams and David Zipser",
      "year": 1989,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This classic work introduced teacher forcing, the exact training regime analyzed here when ground-truth intermediate labels are fed at each generation step to yield one-step learning of parity."
    },
    {
      "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks",
      "authors": "Samy Bengio et al.",
      "year": 2015,
      "arxiv_id": "1506.03099",
      "role": "Gap Identification",
      "relationship_sentence": "By highlighting exposure bias and the teacher-forcing vs. free-running mismatch, this work motivates the paper\u2019s second setting\u2014efficient end-to-end CoT learning without teacher forcing via internal verification using augmented data."
    }
  ],
  "synthesis_narrative": "Wies (2023) analyzed how RNNs can learn k-parity efficiently when trained with chain-of-thought-style intermediate supervision, showing that supervising sub-computations circumvents difficulties faced by end-to-end training. Chain-of-Thought Prompting established that explicitly modeling intermediate reasoning steps boosts performance, crystallizing the notion of supervising a chain of partial results rather than only the final answer. Self-Consistency demonstrated that sampling multiple reasoning paths and aggregating them can serve as an internal correctness check, suggesting a route to verify intermediate steps without external labels. STaR operationalized this idea in training by augmenting data with self-generated rationales filtered by correctness, providing a concrete mechanism to improve models using verified intermediate chains. The canonical teacher forcing framework of Williams and Zipser provided the training protocol where ground-truth intermediate outputs are fed back during sequence generation, and Scheduled Sampling identified the shortcomings of teacher forcing for test-time generation, spotlighting the gap between training with supervision and end-to-end inference.\n\nBuilding on these pieces, the present work generalizes Wies\u2019s RNN parity analysis to transformers and formalizes CoT as intermediate parity supervision. It shows that teacher forcing yields one-step learning with a single gradient update, directly grounded in the teacher-forcing paradigm. Addressing the exposure-bias gap identified by Scheduled Sampling, it leverages self-consistency- and STaR-style augmentation to internally verify intermediate computations, proving efficient end-to-end learning without teacher forcing. Together, these ideas justify a provable pathway from supervised chains to self-verified chains within transformers on the k-parity problem.",
  "target_paper": {
    "title": "Transformers Provably Solve Parity Efficiently with Chain of Thought",
    "authors": "Juno Kim, Taiji Suzuki",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "transformers, chain of thought, parity, self-consistency",
    "abstract": "This work provides the first theoretical analysis of training transformers to solve complex problems by recursively generating intermediate states, analogous to fine-tuning for chain-of-thought (CoT) reasoning. We consider training a one-layer transformer to solve the fundamental $k$-parity problem, extending the work on RNNs by \\citet{Wies23}. We establish three key results: (1) any finite-precision gradient-based algorithm, without intermediate supervision, requires substantial iterations to solve parity with finite samples. (2) In contrast, when intermediate parities are incorporated into the loss function, our model can learn parity in one gradient update when aided by \\emph{teacher forcing}, where ground-truth labels of the reasoning chain are provided at each generation step. (3) Even without teacher forcing, where the model must generate CoT chains end-to-end, parity can be learned efficiently if augmented data is employed to internally verify the soundness of intermediate steps",
    "openreview_id": "n2NidsYDop",
    "forum_id": "n2NidsYDop"
  },
  "analysis_timestamp": "2026-01-06T10:44:56.986205"
}
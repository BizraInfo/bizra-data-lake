{
  "prior_works": [
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Ed H. Chi",
        "Quoc V. Le",
        "Denny Zhou"
      ],
      "year": 2022,
      "role": "Empirical catalyst",
      "relationship_sentence": "This work established the CoT phenomenon and its large gains on math/reasoning tasks, posing the central question that Feng et al. address: why does generating intermediate reasoning steps dramatically help?"
    },
    {
      "title": "Large Language Models are Zero-Shot Reasoners",
      "authors": [
        "Takeshi Kojima",
        "Shixiang Shane Gu",
        "Mostafa Dehghani",
        "Payal Bajaj",
        "Yutaka Matsuo",
        "Eiji Aramaki"
      ],
      "year": 2022,
      "role": "Empirical catalyst",
      "relationship_sentence": "By showing that a simple 'Let\u2019s think step by step' prompt unlocks zero-shot reasoning, this paper sharpened the contrast between direct-answer prompting and CoT that the NeurIPS 2023 paper formalizes."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": [
        "Xuezhi Wang",
        "Jason Wei",
        "Dale Schuurmans",
        "Sharan Narang",
        "Quoc V. Le",
        "Ed H. Chi",
        "Denny Zhou"
      ],
      "year": 2022,
      "role": "Empirical refinement",
      "relationship_sentence": "Demonstrating robust, broad CoT gains via self-consistency strengthened the need for a principled explanation of why generating reasoning traces is computationally advantageous."
    },
    {
      "title": "Theoretical Limitations of Self-Attention in Neural Sequence Models",
      "authors": [
        "Michael Hahn"
      ],
      "year": 2020,
      "role": "Theoretical foundation (limitations of attention)",
      "relationship_sentence": "Provided formal tools and precedents for proving expressivity limits of bounded-depth self-attention, informing the impossibility side of the analysis for direct-answer Transformers."
    },
    {
      "title": "Transformers are Universal Approximators of Sequence-to-Sequence Functions",
      "authors": [
        "Chulhee Yun",
        "Srinadh Bhojanapalli",
        "Ankit Singh Rawat",
        "Sashank Reddi",
        "Sanjiv Kumar"
      ],
      "year": 2020,
      "role": "Theoretical foundation (upper bounds)",
      "relationship_sentence": "Established that sufficiently large/deep Transformers can compute arbitrary sequence mappings, a baseline the paper contrasts with by identifying sharp limits at bounded depth and the role of autoregressive CoT."
    },
    {
      "title": "Computational Limitations for Small-Depth Circuits",
      "authors": [
        "Johan H\u00e5stad"
      ],
      "year": 1987,
      "role": "Core circuit complexity tool",
      "relationship_sentence": "Classic AC0 lower bounds (e.g., for parity) underpin the super-polynomial size requirements the authors derive for bounded-depth Transformers attempting direct arithmetic/equation solving."
    },
    {
      "title": "Benefits of Depth in Neural Networks",
      "authors": [
        "Matus Telgarsky"
      ],
      "year": 2016,
      "role": "Conceptual foundation (depth\u2013size tradeoffs)",
      "relationship_sentence": "Depth\u2013size separation results motivate the paper\u2019s central insight that CoT introduces effective computational depth over time, enabling small models to solve tasks infeasible for shallow direct mappings."
    }
  ],
  "synthesis_narrative": "Feng et al. ground their theory in the striking empirical observation that prompting large language models to produce intermediate reasoning steps substantially boosts performance. The CoT line of work\u2014especially Wei et al. and Kojima et al., further bolstered by Wang et al.\u2019s self-consistency\u2014clearly establishes that generated derivations matter, motivating a formal account of when and why they help. To analyze this, the paper situates Transformers within established theoretical frameworks on model expressivity. Hahn\u2019s limitations of self-attention provide a template for proving lower bounds under architectural constraints, while Yun et al. show that Transformers are, in principle, universally expressive when depth/size are unconstrained\u2014highlighting that any observed limitations must stem from bounded computational depth.\n\nThe key technical move is to import circuit complexity insights to the Transformer setting. H\u00e5stad\u2019s small-depth circuit lower bounds anchor the impossibility results: tasks with parity-like structure inherent to arithmetic and equation solving are provably hard for constant-depth, polynomial-size direct-answer mappings. Complementing these lower bounds, Telgarsky\u2019s depth\u2013size separations elucidate how additional computational depth dramatically reduces size requirements. Feng et al. recast CoT as a mechanism that creates depth temporally via autoregressive generation, effectively transforming a shallow direct mapping into a multi-step computation. This yields their constructive result: constant-size Transformers can solve arithmetic/equation tasks by emitting step-by-step derivations, sidestepping the direct-answer bottleneck. Together, these works converge to a crisp theory\u2014CoT augments effective depth, explaining its empirical power and delineating when small LLMs can reason successfully.",
  "analysis_timestamp": "2026-01-06T23:42:49.128108"
}
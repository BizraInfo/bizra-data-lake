{
  "prior_works": [
    {
      "title": "Foundations of Bilevel Programming",
      "authors": "S. Dempe",
      "year": 2002,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This monograph formalized the value-function reformulation for bilevel programs, providing the exact modeling device that the paper extends by constructing a smooth proximal Lagrangian value function for constrained lower-level problems with coupling."
    },
    {
      "title": "Optimality conditions for bilevel programming problems",
      "authors": "J. Ye and D. Zhu",
      "year": 1995,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "It established key optimality and single-level reformulation principles for bilevel programs with constrained lower levels, which the paper leverages and refines by enforcing smooth constraints via its proximal Lagrangian value function."
    },
    {
      "title": "Augmented Lagrangians and applications of the proximal point algorithm in convex programming",
      "authors": "R. T. Rockafellar",
      "year": 1976,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "The augmented/proximal Lagrangian framework directly inspires the paper\u2019s construction of a proximal Lagrangian value function that smooths the constrained lower-level problem while preserving equivalence."
    },
    {
      "title": "Hyperparameter Optimization with Approximate Gradient (HOAG)",
      "authors": "F. Pedregosa",
      "year": 2016,
      "arxiv_id": "1602.02355",
      "role": "Baseline",
      "relationship_sentence": "HOAG typifies implicit-differentiation hypergradient methods that require Hessian-related computations, serving as a primary baseline whose computational burden the paper removes via a Hessian-free reformulation."
    },
    {
      "title": "Bilevel Programming for Hyperparameter Optimization and Meta-Learning",
      "authors": "L. Franceschi et al.",
      "year": 2018,
      "arxiv_id": "1806.04910",
      "role": "Foundation",
      "relationship_sentence": "This work popularized bilevel formulations in machine learning and hypergradients, motivating the need to handle constrained/coupled lower-level problems that the paper tackles through a value-function-based smoothing reformulation."
    },
    {
      "title": "Efficient and Modular Implicit Differentiation",
      "authors": "M. Blondel et al.",
      "year": 2021,
      "arxiv_id": "2105.15183",
      "role": "Gap Identification",
      "relationship_sentence": "While providing a general recipe for implicit differentiation of constrained optimization layers, it still relies on solving linear systems involving Hessians/KKT matrices, a limitation the paper addresses by deriving Hessian-free hypergradients from a smooth value-function reformulation."
    },
    {
      "title": "Lower Complexity Bounds and Optimal Algorithms for Bilevel Optimization",
      "authors": "K. Ji, J. Yang, and Y. Liang",
      "year": 2021,
      "arxiv_id": "2107.01791",
      "role": "Inspiration",
      "relationship_sentence": "Its single-loop gradient-based perspective for bilevel optimization motivates the paper\u2019s single-loop design, which is enabled here by the smooth proximal Lagrangian value function that circumvents second-order computations."
    }
  ],
  "synthesis_narrative": "Dempe\u2019s foundational treatment of bilevel programming articulated the value-function reformulation, making explicit how a lower-level program can be encoded as an upper-level constraint via its optimal value. Ye and Zhu provided optimality conditions and single-level reformulations for constrained lower-level problems, clarifying when such value-function encodings are valid even under coupling constraints. Rockafellar\u2019s augmented/proximal Lagrangian framework supplied the core idea that Lagrangian regularization can stabilize and smooth constrained programs while preserving their essential structure. In machine learning, Pedregosa introduced practical hypergradient computation through implicit differentiation, and Franceschi established bilevel programming as a central abstraction for hyperparameter optimization and meta-learning, cementing gradient-based approaches as the workhorse. Blondel and colleagues then offered a modular implicit-differentiation toolkit for optimization layers, including constrained ones, but at the cost of solving linear systems involving Hessians or KKT matrices. Parallelly, Ji, Yang, and Liang advanced single-loop bilevel methods, highlighting the algorithmic appeal of avoiding outer\u2013inner nesting if one can secure suitable reformulations.\nTogether, these works reveal a gap: while value-function reformulations and implicit differentiation enable gradients for constrained bilevel problems, they typically incur Hessian/KKT solves, and single-loop practicality hinges on smooth, tractable constraints. The paper synthesizes Dempe/Ye\u2019s value-function modeling with Rockafellar-style proximal Lagrangians to craft a smooth proximal Lagrangian value function, yielding an equivalent single-level problem with smooth constraints. This in turn enables a single-loop, Hessian-free gradient algorithm that retains theoretical guarantees while directly addressing the computational limitations identified in implicit-differentiation baselines.",
  "target_paper": {
    "title": "Constrained Bi-Level Optimization: Proximal Lagrangian Value Function Approach and Hessian-free Algorithm",
    "authors": "Wei Yao, Chengming Yu, Shangzhi Zeng, Jin Zhang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Bi-level Optimization, Constrained Optimization, Hessian-free, Single-loop, Value Function, Convergence Analysis",
    "abstract": "This paper presents a new approach and algorithm for solving a class of constrained Bi-Level Optimization (BLO) problems in which the lower-level problem involves constraints coupling both upper-level and lower-level variables. Such problems have recently gained significant attention due to their broad applicability in machine learning. However, conventional gradient-based methods unavoidably rely on computationally intensive calculations related to the Hessian matrix. To address this challenge, we devise a smooth proximal Lagrangian value function to handle the constrained lower-level problem. Utilizing this construct, we introduce a single-level reformulation for constrained BLOs that transforms the original BLO problem into an equivalent optimization problem with smooth constraints. Enabled by this reformulation, we develop a Hessian-free gradient-based algorithm\u2014termed proximal Lagrangian Value function-based Hessian-free Bi-level Algorithm (LV-HBA)\u2014that is straightforward to imple",
    "openreview_id": "xJ5N8qrEPl",
    "forum_id": "xJ5N8qrEPl"
  },
  "analysis_timestamp": "2026-01-06T15:29:20.942094"
}
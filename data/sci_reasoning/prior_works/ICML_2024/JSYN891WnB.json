{
  "prior_works": [
    {
      "title": "Contrastive Clustering",
      "authors": "Yunfan Li et al.",
      "year": 2021,
      "role": "Baseline",
      "relationship_sentence": "TAC directly builds on and improves the authors\u2019 prior contrastive clustering pipeline by replacing its purely internal self-supervision with external text-guided signals, addressing the core limitation that contrastive clustering ignores external knowledge."
    },
    {
      "title": "Deep Clustering for Unsupervised Learning of Visual Features",
      "authors": "Mathilde Caron et al.",
      "year": 2018,
      "role": "Related Problem",
      "relationship_sentence": "DeepCluster\u2019s pseudo-labeling formulation for unsupervised image clustering motivates TAC\u2019s move from internally mined supervision to externally sourced signals, which explicitly tackles DeepCluster\u2019s reliance on self-generated labels."
    },
    {
      "title": "Constrained K-means Clustering with Background Knowledge (COP-KMeans)",
      "authors": "Kiri Wagstaff et al.",
      "year": 2001,
      "role": "Foundation",
      "relationship_sentence": "COP-KMeans established the principle that external side information can guide clustering; TAC generalizes this foundational idea from pairwise constraints to semantic textual guidance drawn from a knowledge base."
    },
    {
      "title": "WordNet: A Lexical Database for English",
      "authors": "George A. Miller",
      "year": 1995,
      "role": "Foundation",
      "relationship_sentence": "TAC directly relies on WordNet\u2019s noun inventory and hierarchy as the external knowledge source from which discriminative semantic cues are retrieved to supervise image clustering."
    },
    {
      "title": "DeViSE: A Deep Visual-Semantic Embedding Model",
      "authors": "Andrea Frome et al.",
      "year": 2013,
      "role": "Inspiration",
      "relationship_sentence": "DeViSE introduced aligning images with linguistic semantics for zero-shot recognition; TAC adopts this insight by using label semantics to guide grouping without ground-truth labels."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "TAC leverages CLIP\u2019s image\u2013text alignment to score image\u2013noun affinities and select discriminative WordNet nouns, effectively extending CLIP from zero-shot classification to generating external supervision for clustering."
    },
    {
      "title": "Learning to Detect Unseen Object Classes by Attributes",
      "authors": "Christoph H. Lampert et al.",
      "year": 2009,
      "role": "Inspiration",
      "relationship_sentence": "Attribute-based zero-shot learning demonstrated that semantic descriptions can supervise recognition without labels; TAC transfers this principle to the clustering setting by using textual semantics as guidance."
    }
  ],
  "synthesis_narrative": "The core innovation of TAC\u2014using external textual knowledge to supervise image clustering\u2014emerges from two converging threads. First, deep clustering matured around internal supervision: DeepCluster\u2019s pseudo-labeling and the authors\u2019 own Contrastive Clustering formalized how to mine supervisory signals from data itself, yet both leave untapped any external semantics. Second, zero-shot recognition established that linguistic information can stand in for labels. DeViSE showed that mapping images into a semantic text space enables categorization without annotated classes, while attribute-based zero-shot learning proved that declarative semantics can act as supervision. TAC fuses these insights: rather than deriving signals only from the data distribution, it injects semantics from a curated knowledge base\u2014WordNet\u2014to guide the formation of clusters. WordNet provides the noun inventory and hierarchical relations that TAC queries to discover concepts that best partition the image set. Practically, TAC operationalizes this by exploiting CLIP\u2019s image\u2013text alignment to score image\u2013noun affinities and select discriminative WordNet terms, turning natural-language knowledge into concrete clustering supervision. Conceptually, this generalizes the classic constrained-clustering paradigm (e.g., COP-KMeans), moving from pairwise human constraints to rich, reusable semantic guidance. In doing so, TAC directly addresses the central gap in contrastive and deep clustering methods\u2014exclusive reliance on internal signals\u2014by showing that external textual semantics can be systematically harnessed to produce more discriminative clusters.",
  "analysis_timestamp": "2026-01-06T23:09:26.481582"
}
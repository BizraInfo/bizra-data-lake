{
  "prior_works": [
    {
      "title": "Online Convex Programming and Generalized Infinitesimal Gradient Ascent",
      "authors": "Martin Zinkevich",
      "year": 2003,
      "role": "Foundational OCO baseline and benchmark rates",
      "relationship_sentence": "This work established the standard O(\u221aT) regret guarantees for OLO/OCO against adversarial sequences, providing the baseline and lower-bound intuition that the present paper overturns under alternation."
    },
    {
      "title": "Prediction, Learning, and Games",
      "authors": "Nicolo Cesa-Bianchi, G\u00e1bor Lugosi",
      "year": 2006,
      "role": "Minimax regret framework and lower bounds in adversarial learning",
      "relationship_sentence": "The classical minimax analysis and \u03a9(\u221aT) lower bounds for adversarial experts/OLO in this monograph serve as the canonical impossibility results that the paper shows can be bypassed when the loss couples consecutive adversary moves via alternation."
    },
    {
      "title": "Online Learning with Predictable Sequences",
      "authors": "Alexander Rakhlin, Karthik Sridharan",
      "year": 2013,
      "role": "Optimistic OCO methodology for leveraging one-step predictions",
      "relationship_sentence": "The paper\u2019s algorithms and analyses directly build on optimistic mirror descent/FTRL ideas\u2014using c^{t\u22121} as a predictor for c^t\u2014so the alternating loss (c^t + c^{t\u22121})^\u22a4x^t yields improved regret akin to predictable-sequence bounds, enabling the log T guarantee on Euclidean balls."
    },
    {
      "title": "Prox-method with rate of convergence O(1/T) for variational inequalities and saddle point problems",
      "authors": "Arkadi Nemirovski",
      "year": 2004,
      "role": "Lookahead/extra-gradient principle reducing adversarial oscillations",
      "relationship_sentence": "The insight that a lookahead (extra-gradient/optimistic) step stabilizes saddle-point dynamics underlies the paper\u2019s thesis that alternation weakens the adversary; the analysis ethos echoes Mirror-Prox\u2019s use of prior-gradient information to gain faster rates."
    },
    {
      "title": "Fast convergence of regularized learning in games",
      "authors": "Elias Syrgkanis, Alekh Agarwal, Haipeng Luo, Robert E. Schapire",
      "year": 2015,
      "role": "Optimistic no-regret dynamics in games with accelerated convergence",
      "relationship_sentence": "By showing that optimism yields faster convergence in two-player games, this work motivates modeling alternating game-play as providing predictability, a perspective the paper formalizes via alternating OLO to derive sub-\u221aT regret."
    },
    {
      "title": "Training GANs with Optimism",
      "authors": "Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, Haoyang Zeng",
      "year": 2018,
      "role": "Empirical and theoretical evidence that optimism/alternation aids adversarial training",
      "relationship_sentence": "This paper\u2019s success of optimistic updates in adversarial (min\u2013max) training directly informs the present work\u2019s claim and design that alternating interactions reduce adversarial hardness, leading to T^{1/3} and log T regrets."
    }
  ],
  "synthesis_narrative": "The core contribution of \u201cAlternation makes the adversary weaker in two-player games\u201d is to show that a slight change in the interaction protocol\u2014alternation that couples consecutive adversary moves in the loss\u2014collapses the classical \u03a9(\u221aT) regret barrier of Online Linear Optimization. The standard OCO picture (Zinkevich, 2003; Cesa-Bianchi & Lugosi, 2006) posits a fully adversarial sequence, for which optimal rates are \u0398(\u221aT). The paper recasts alternating game-play as an online learning problem where the incurred loss involves c^{t}+c^{t\u22121}, effectively injecting a one-step predictable structure into the feedback. This aligns precisely with the optimistic OCO paradigm of Rakhlin & Sridharan (2013), where using the previous gradient as a predictor yields bounds in terms of prediction error; in the alternating model, the built-in predictor is natural and powerful, enabling O(log T) regret on Euclidean balls and improved O(T^{1/3}) on the simplex.\n\nConceptually, the result resonates with the lookahead/extra-gradient principle of Nemirovski (2004), which stabilizes adversarial saddle-point dynamics via a predictive step, and with the games literature showing that optimistic dynamics enjoy accelerated convergence (Syrgkanis et al., 2015) and practical benefits in adversarial training (Daskalakis et al., 2018). The present work distills these game-theoretic insights into a clean OLO formulation, proving that alternation itself weakens the adversary\u2019s power. Algorithmically, it blends optimism with appropriate regularization (entropic on the simplex, Euclidean on balls) to extract the improved rates, thereby formalizing when and how alternating play can beat worst-case \u221aT barriers.",
  "analysis_timestamp": "2026-01-06T23:42:49.104609"
}
{
  "prior_works": [
    {
      "title": "Linear Convergence of Gradient and Proximal-Gradient Methods under the Polyak\u2013\u0141ojasiewicz Condition",
      "authors": "Hadi Karimi et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized the PL condition as a problem class yielding linear rates, providing the exact theoretical framework (PL with parameter \u03bc) that the present work adopts to state and prove IFO lower bounds in terms of \u03ba = L/\u03bc and log(1/\u03b5)."
    },
    {
      "title": "On the Oracle Complexity of Optimization Methods for Finite Sum Problems",
      "authors": "Yossi Arjevani et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "Introduced core resisting-oracle techniques and established tight \u03a9((n + \u221a(n\u03ba)) log(1/\u03b5)) lower bounds for strongly convex finite-sum problems, which this paper extends to the PL setting with mean-squared smoothness to derive \u03a9(n + \u03ba\u221an log(1/\u03b5)) bounds."
    },
    {
      "title": "Tight Complexity Bounds for Optimizing Composite Objectives",
      "authors": "Blake Woodworth et al.",
      "year": 2016,
      "role": "Inspiration",
      "relationship_sentence": "Provided tight finite-sum lower bounds and proof templates for first-order methods that directly inspire the present lower-bound construction; the current work addresses their gap by moving from strong convexity/composite settings to PL with L-mean-squared smoothness."
    },
    {
      "title": "Katyusha: The First Direct Accelerated Stochastic Gradient Method for Finite Sum Optimization",
      "authors": "Zeyuan Allen-Zhu",
      "year": 2017,
      "role": "Baseline",
      "relationship_sentence": "Established near-optimal O((n + \u221a(n\u03ba)) log(1/\u03b5)) upper bounds for smooth strongly convex finite sums, serving as a key algorithmic benchmark the present lower bounds are compared against when discussing optimality under PL."
    },
    {
      "title": "SPIDER: Near-Optimal Nonconvex Optimization via Stochastic Path-Integrated Differential Estimator",
      "authors": "Cong Fang et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Provides state-of-the-art variance-reduced upper bounds for finite-sum problems (including PL cases via mini-batching/recursive gradients), yielding O(n + \u03ba\u221an) log(1/\u03b5) IFO complexity that the new \u03a9(n + \u03ba\u221an log(1/\u03b5)) lower bound is designed to nearly match."
    },
    {
      "title": "Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks",
      "authors": "Kevin Scaman et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Established network-dependent lower bounds in decentralized optimization with spectral gap \u03b3 and communication delays, furnishing the communication-complexity framework (\u03b3, \u03c4 dependencies) that this paper extends to PL finite-sum objectives."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014tight incremental first-order oracle lower bounds for finite-sum optimization under the PL condition, along with network-dependent lower bounds in decentralized settings\u2014rests on two intertwined lines of prior work. On the modeling side, Karimi et al. introduced the PL framework that permits linear convergence without strong convexity, defining the exact problem class this paper analyzes. On the lower-bound methodology side, Arjevani & Shamir and Woodworth & Srebro developed the finite-sum resisting-oracle machinery and tight oracle-complexity bounds (classically yielding \u03a9((n + \u221a(n\u03ba)) log(1/\u03b5)) under strong convexity). The present work directly adapts and extends these techniques to the PL setting with L-mean-squared smoothness, arriving at \u03a9(n + \u03ba\u221an log(1/\u03b5)) IFO complexity.\n\nThe motivation to pin down the \u03ba\u221an term comes from best-known variance-reduced upper bounds. Katyusha set the standard in the strongly convex regime, while SPIDER-type recursive estimators, when specialized to PL finite sums and appropriate batching, achieve O((n + \u03ba\u221an) log(1/\u03b5)) IFO complexity. The new lower bound nearly matches these results, certifying their near-optimality under the weaker smoothness model. Finally, for decentralized optimization, Scaman et al.\u2019s network-information framework (spectral gap \u03b3 and communication delay \u03c4) provides the blueprint the authors extend to PL objectives, yielding matching \u03b3- and \u03c4-dependent lower bounds that clarify the fundamental limits of distributed PL finite-sum optimization.",
  "analysis_timestamp": "2026-01-06T23:09:26.428884"
}
{
  "prior_works": [
    {
      "title": "A Practical Bayesian Framework for Backpropagation Networks",
      "authors": "David J. C. MacKay",
      "year": 1992,
      "role": "Foundational Laplace approximation for neural networks",
      "relationship_sentence": "This work established the Laplace approximation and evidence framework for neural networks, forming the methodological basis whose reparameterization shortcomings the present paper analyzes and seeks to rectify."
    },
    {
      "title": "A Scalable Laplace Approximation for Neural Networks",
      "authors": "Hippolyt Ritter, Aleksandar Botev, David Barber",
      "year": 2018,
      "role": "Modern scalable Laplace for deep nets via Gauss\u2013Newton/KFAC",
      "relationship_sentence": "By making Laplace practical for deep networks yet exhibiting underfitting and parameterization sensitivity, this paper set the empirical stage for explaining why linearized predictives work and why standard Laplace lacks reparameterization invariance."
    },
    {
      "title": "Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks",
      "authors": "Arianna Kristiadi, Matthias Hein, Philipp Hennig",
      "year": 2020,
      "role": "Last-layer/linearized Laplace predictive to improve calibration",
      "relationship_sentence": "This paper demonstrated that linearized/last-layer Laplace improves predictive uncertainty, a phenomenon the current work explains through a reparameterization-invariant geometric lens and then generalizes beyond the linearized regime."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Linearization of neural networks around a reference point",
      "relationship_sentence": "The NTK formalizes network linearization and its Gaussian predictive behavior, directly underpinning the paper\u2019s analysis of why linearized predictives exhibit superior reparameterization properties."
    },
    {
      "title": "Natural Gradient Works Efficiently in Learning",
      "authors": "Shun-ichi Amari",
      "year": 1998,
      "role": "Information geometry and invariance via Fisher\u2013Rao metric",
      "relationship_sentence": "Amari\u2019s information-geometric view supplies the core notion that learning and inference should be invariant to reparameterization, which the paper adopts to construct a geometric account of approximate Bayesian inference."
    },
    {
      "title": "Riemannian Manifold Hamiltonian Monte Carlo",
      "authors": "Mark Girolami, Ben Calderhead",
      "year": 2011,
      "role": "Riemannian MCMC with metric-induced, reparameterization-invariant dynamics",
      "relationship_sentence": "This work shows how Riemannian dynamics yield coordinate-invariant sampling, directly informing the paper\u2019s use of Riemannian diffusion processes to extend invariance to full-network predictives."
    },
    {
      "title": "Stochastic Gradient Riemannian Langevin Dynamics",
      "authors": "Sam Patterson, Yee Whye Teh",
      "year": 2013,
      "role": "Scalable Riemannian diffusion for Bayesian inference",
      "relationship_sentence": "By introducing stochastic-gradient Riemannian Langevin dynamics, this paper provides the practical diffusion machinery the authors leverage to achieve reparameterization-invariant predictions for the original (nonlinear) network."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014explaining and achieving reparameterization invariance in approximate Bayesian inference for neural networks\u2014rests on three intertwined lines of prior work. First, the Laplace approximation lineage (MacKay, 1992) and its deep-learning-era refinements (Ritter et al., 2018) furnished both the method and its known deficiencies: underfitting and sensitivity to parameterization. Empirical fixes via linearized or last-layer Laplace (Kristiadi et al., 2020) demonstrated markedly improved predictive behavior, motivating a theoretical account of why linearized predictives work.\nSecond, the linearization perspective from the Neural Tangent Kernel (Jacot et al., 2018) established that networks locally behave like linear models with Gaussian predictive structure, offering a natural bridge from weight-space posteriors to function-space predictives. This connects directly to the observation that linearized predictives partially sidestep parameterization artifacts.\nThird, information geometry supplies the invariance principle and tools: Amari (1998) argues learning rules should be invariant under reparameterization via the Fisher\u2013Rao geometry. Building on this, Riemannian Monte Carlo and Langevin methods (Girolami & Calderhead, 2011; Patterson & Teh, 2013) show how metric-induced dynamics yield coordinate-invariant sampling on parameter manifolds. The present paper synthesizes these strands: it provides a geometric explanation for the success of linearized Laplace through reparameterization invariance and then uses Riemannian diffusion to transport these invariance properties back to the original nonlinear network\u2019s predictive distribution, thereby aligning Bayesian uncertainty over parameters with uncertainty over functions.",
  "analysis_timestamp": "2026-01-06T23:33:35.529826"
}
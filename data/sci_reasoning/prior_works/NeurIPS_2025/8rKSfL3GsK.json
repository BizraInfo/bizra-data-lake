{
  "prior_works": [
    {
      "title": "Learning to Recognize Daily Actions using Gaze",
      "authors": "Fathi et al.",
      "year": 2012,
      "role": "Foundation",
      "relationship_sentence": "Established egocentric gaze as a direct signal of intention in first-person activities, providing the foundational link our paper leverages when forecasting the next locus of visual perception."
    },
    {
      "title": "EGTEA Gaze+: A Large-Scale Dataset for Gaze Tracking in Egocentric Video",
      "authors": "Li et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Standardized 2D, image-plane gaze prediction for egocentric video, whose lack of 3D scene grounding directly motivates our shift to forecasting gaze as volumetric regions in 3D."
    },
    {
      "title": "Predicting Gaze in Egocentric Video by Learning Task-Dependent Attention Transition",
      "authors": "Huang et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Introduced temporal modeling of gaze transitions in egocentric videos; we adopt this forecasting formulation but replace 2D heatmaps with 3D visual span volumes fused by a unidirectional transformer."
    },
    {
      "title": "ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras",
      "authors": "Mur-Artal et al.",
      "year": 2017,
      "role": "Extension",
      "relationship_sentence": "Provides the SLAM keypoints and map points that our method explicitly converts into gaze-compatible 3D geometry, enabling extraction of volumetric visual span regions."
    },
    {
      "title": "Gaze360: Physically Unconstrained Gaze Estimation in the Wild",
      "authors": "Kellnhofer et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "Framed gaze as 3D rays in world coordinates; we adopt this 3D geometric perspective but advance it to forecast future volumetric spans anchored in reconstructed scenes rather than static 3D gaze estimation."
    },
    {
      "title": "A Simple Yet Effective Baseline for 3D Human Pose Estimation",
      "authors": "Martinez et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "Popularized learning a 2D-to-3D \u2018lifting\u2019 function; EgoSpanLift adapts this lifting paradigm from 2D image-plane attention cues to 3D volumetric visual span forecasting."
    },
    {
      "title": "Ego4D: Around the World in 3,000 Hours of Egocentric Video",
      "authors": "Grauman et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Defined large-scale egocentric forecasting tasks and provided head/gaze signals; our work targets the same predictive setting but grounds forecasts in reconstructed 3D environments."
    }
  ],
  "synthesis_narrative": "EgoSpanLift\u2019s core innovation\u2014forecasting egocentric visual span as volumetric regions grounded in a reconstructed 3D scene\u2014rests on a clear lineage. Foundationally, Fathi et al. demonstrated that egocentric gaze is tightly coupled with intention, motivating prediction of where attention will move next. Large-scale egocentric benchmarks such as EGTEA Gaze+ and Ego4D then codified 2D gaze prediction/forecasting protocols and data, but their inherently image-plane formulations exposed a key gap: forecasts lack 3D scene grounding. On the temporal modeling side, Huang et al. established a now-standard baseline for egocentric gaze forecasting via attention transition in 2D, which our method retains conceptually while lifting the target to 3D and fusing spatio-temporal context with a unidirectional transformer. Enabling this lift, ORB-SLAM2 supplies the geometric substrate\u2014sparse keypoints and map points\u2014that EgoSpanLift explicitly converts into gaze-compatible 3D primitives, from which we extract volumetric visual span regions. Two additional threads directly inspire our design choices: the 2D-to-3D \u2018lifting\u2019 paradigm of Martinez et al. provides the methodological blueprint for mapping image-plane signals into structured 3D representations, while Gaze360\u2019s 3D gaze ray formulation anchors our representation of gaze in world coordinates. Together, these works define the problem, expose the 2D limitation our paper addresses, and provide the geometric and methodological tools that make 3D visual span forecasting feasible.",
  "analysis_timestamp": "2026-01-06T23:08:23.949610"
}
{
  "prior_works": [
    {
      "title": "Schedule-Free Learning",
      "authors": "Aaron Defazio et al.",
      "year": 2024,
      "role": "Baseline",
      "relationship_sentence": "Introduced the schedule-free family of optimizers that remove learning-rate schedules; the present paper\u2019s new online-to-nonconvex conversion maps directly onto schedule-free SGD, filling the nonconvex theory gap and proving its optimality."
    },
    {
      "title": "Coin Betting and Parameter-Free Online Learning",
      "authors": "Francesco Orabona et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "Established parameter-free (learning-rate-free) online learning via coin-betting, the core online framework that schedule-free methods build upon and that this paper converts into nonconvex optimization guarantees."
    },
    {
      "title": "Training Deep Networks without Learning Rates through Coin Betting",
      "authors": "Nicol\u00f2 Orabona et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrated practical schedule-free training based on coin-betting; this success directly motivates analyzing schedule-free SGD theoretically in the harder nonsmooth, nonconvex regime."
    },
    {
      "title": "Online Nonconvex Learning and Local Regret",
      "authors": "Elad Hazan et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Introduced the online-to-nonconvex paradigm via local-regret/gradient-norm connections; the new framework in this paper explicitly recovers such conversions and generalizes them to yield schedule-free SGD as a special case."
    },
    {
      "title": "Stochastic First- and Zeroth-Order Methods for Nonconvex Stochastic Programming",
      "authors": "Saeed Ghadimi et al.",
      "year": 2013,
      "role": "Foundation",
      "relationship_sentence": "Provided the seminal iteration-complexity analysis for (stochastic) gradient methods in nonconvex optimization, setting the rate targets that this paper matches with schedule-free SGD via its conversion framework."
    },
    {
      "title": "Stochastic Subgradient Method Converges on Weakly Convex Functions",
      "authors": "Damek Davis et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "Formalized nonsmooth nonconvex (weakly convex) optimization and stationarity measures (e.g., Moreau-envelope gradient), furnishing the problem setup and optimal-rate benchmarks that the paper achieves with schedule-free SGD."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014a general online-to-nonconvex conversion that yields optimal, schedule-free SGD for nonsmooth nonconvex objectives\u2014stands on two intertwined pillars: parameter-free online learning and online-to-nonconvex reductions. Defazio et al. (2024) provided the concrete schedule-free optimizers that remove learning-rate schedules and showed striking empirical gains; this work identifies a new conversion under which schedule-free SGD emerges naturally and proves its optimality, addressing Defazio\u2019s open theoretical gap in nonconvex settings. The parameter-free foundation originates in Orabona and P\u00e1l\u2019s coin-betting framework (2016), which formalized learning-rate-free guarantees in online convex optimization, and in Orabona and Tommasi\u2019s demonstration (2017) that such ideas can train deep networks without manual schedules\u2014directly motivating the schedule-free perspective analyzed here. On the nonconvex side, Hazan et al. (2017) established the online-to-nonconvex paradigm by connecting online guarantees to measures of stationarity (local regret/gradient norms); the present framework explicitly recovers and generalizes these conversions. Finally, classical nonconvex optimization results by Ghadimi and Lan (2013) and by Davis and Drusvyatskiy (2019) define the rate targets and the nonsmooth/weakly convex setting (and stationarity metrics) that this paper adopts. By unifying coin-betting\u2013style, parameter-free online learning with online-to-nonconvex reductions, the authors both explain and theoretically ground the effectiveness of schedule-free SGD in nonsmooth, nonconvex optimization.",
  "analysis_timestamp": "2026-01-06T23:07:19.600321"
}
{
  "prior_works": [
    {
      "title": "Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling",
      "authors": "Stella Biderman et al.",
      "year": 2023,
      "role": "Methodological blueprint for transparent training-dynamics analysis",
      "relationship_sentence": "EvoLM generalizes Pythia\u2019s idea of releasing a controlled model family and checkpoints by extending the analysis beyond pre-training to include continued pre-training, supervised fine-tuning, and reinforcement learning, enabling end-to-end dynamics and forgetting studies."
    },
    {
      "title": "Training Compute-Optimal Large Language Models (Chinchilla)",
      "authors": "Jordan Hoffmann et al.",
      "year": 2022,
      "role": "Scaling laws and compute\u2013data tradeoff for pre-training",
      "relationship_sentence": "EvoLM explicitly tests and extends Chinchilla-style compute-optimality and diminishing-returns claims, showing how marginal gains evolve not only during pre-training but also through post-training stages."
    },
    {
      "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks",
      "authors": "Suchin Gururangan et al.",
      "year": 2020,
      "role": "Introduced domain/task-adaptive continued pre-training and highlighted forgetting",
      "relationship_sentence": "EvoLM builds directly on DAPT/TAPT by systematizing continued pre-training as a bridge between pre-training and post-training, rigorously quantifying in-/out-of-domain generalization and proposing practices to mitigate catastrophic forgetting."
    },
    {
      "title": "Training Language Models to Follow Instructions with Human Feedback (InstructGPT)",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "role": "Established the modern SFT + RLHF post-training pipeline",
      "relationship_sentence": "EvoLM dissects the InstructGPT pipeline across scales, measuring the incremental gains and trade-offs of SFT and RLHF under varied data mixtures and sizes to map post-training diminishing returns."
    },
    {
      "title": "Deep Reinforcement Learning from Human Preferences",
      "authors": "Paul F. Christiano et al.",
      "year": 2017,
      "role": "Foundational RLHF method via reward modeling and preference optimization",
      "relationship_sentence": "EvoLM leverages this RLHF foundation to systematically evaluate how reinforcement learning stages affect reasoning, stability, and generalization, and where returns taper off."
    },
    {
      "title": "Finetuned Language Models Are Zero-Shot Learners (FLAN)",
      "authors": "Jason Wei et al.",
      "year": 2021,
      "role": "Instruction tuning as a bridge from pre-training to broad generalization",
      "relationship_sentence": "EvoLM quantifies how instruction tuning scale and mixture shape zero-/few-shot reasoning and how continued pre-training can better couple pre-training representations to effective post-training."
    },
    {
      "title": "LIMA: Less Is More for Alignment",
      "authors": "Haotian Zhou et al.",
      "year": 2023,
      "role": "Showed strong alignment with small, high-quality SFT datasets",
      "relationship_sentence": "EvoLM probes LIMA-style efficiency across model sizes and domains, charting where compact SFT suffices and where additional SFT or RL yields diminishing or domain-specific gains."
    }
  ],
  "synthesis_narrative": "EvoLM\u2019s core contribution\u2014a transparent, from-scratch model suite for end-to-end analysis of language model training dynamics\u2014sits at the intersection of scaling, domain adaptation, and alignment. Chinchilla\u2019s compute-optimal scaling laws motivated EvoLM\u2019s rigorous mapping of diminishing returns during pre-training and, crucially, extending that lens into post-training. Pythia provided the methodological precedent for releasing controlled model families and checkpoints to study training dynamics; EvoLM amplifies this approach across all stages: pre-training, continued pre-training, SFT, and RL.\n\nGururangan et al.\u2019s \u201cDon\u2019t Stop Pretraining\u201d established continued pre-training (DAPT/TAPT) and surfaced catastrophic forgetting\u2014directly informing EvoLM\u2019s emphasis on continued pre-training as a bridge and its protocols for mitigating forgetting while preserving generalization. The post-training pipeline crystallized by InstructGPT, grounded in the RLHF framework of Christiano et al., enabled EvoLM to systematically vary SFT and RL stages, quantify their incremental benefits, and reveal where returns taper or trade-offs emerge.\n\nFinally, FLAN and LIMA shaped EvoLM\u2019s investigation of how instruction-tuning data scale, mixture, and quality impact zero-/few-shot reasoning and alignment efficiency. Together, these works provided the conceptual and methodological scaffolding that EvoLM unifies into a single, reproducible suite\u2014allowing precise, cross-stage attribution of gains and a holistic view of training dynamics, including in-domain versus out-of-domain generalization and the practical limits of additional pre- and post-training.",
  "analysis_timestamp": "2026-01-07T00:02:04.984532"
}
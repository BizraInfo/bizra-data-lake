{
  "prior_works": [
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "The immunization\u2019s diffusion-aware objective explicitly differentiates through the denoising process introduced by DDPM to disrupt sampling across timesteps and force unrealistic edits."
    },
    {
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "authors": "Robin Rombach et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "The paper targets Stable Diffusion\u2013style latent diffusion pipelines; its \u2018encoder/latent-path\u2019 attack is designed around LDM\u2019s VAE encoding and text-guided latent denoising, corrupting image inversion and subsequent editing."
    },
    {
      "title": "SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations",
      "authors": "Chenlin Meng et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "SDEdit formalizes diffusion-based image editing (add noise then denoise with guidance), which this work explicitly treats as the malicious editing mechanism it seeks to break by making such edits yield unrealistic outputs."
    },
    {
      "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
      "authors": "Aleksander Madry et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "The immunization procedures extend iterative PGD-style optimization under small-norm constraints to the full diffusion pipeline, crafting imperceptible perturbations that reliably derail editing."
    },
    {
      "title": "Synthesizing Robust Adversarial Examples",
      "authors": "Anish Athalye et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "Because diffusion sampling is stochastic, the method adopts an Expectation-over-Transformation\u2013style optimization over noise seeds and prompts to yield perturbations robust to sampling randomness."
    },
    {
      "title": "Fawkes: Protecting Privacy against Unauthorized Deep Learning Models",
      "authors": "Shawn Shan et al.",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "Fawkes introduced the idea of proactively \u2018cloaking\u2019 images with imperceptible perturbations; this paper directly generalizes that protective concept from recognition systems to text-guided diffusion editing."
    },
    {
      "title": "Glaze: Protecting Artists from Style Mimicry by Text-to-Image Models",
      "authors": "Shawn Shan et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "Glaze shows adversarial perturbations can protect against style mimicry but does not address inference-time image editing; this limitation motivates targeting diffusion-based editing specifically with tailored immunization."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014immunizing images so diffusion models fail to edit them realistically\u2014arises from fusing adversarial optimization with the mechanics of modern diffusion pipelines. Foundationally, DDPM established the denoising process and objective that this paper differentiates through to sabotage sampling, while Latent Diffusion Models operationalized text-guided image-to-image, inpainting, and editing with a VAE latent encoder that the paper\u2019s encoder-focused attack intentionally corrupts. SDEdit formulated a general editing protocol (noise then denoise with guidance), concretely defining the malicious editing mechanism the defense is engineered to break.\nMethodologically, the work extends PGD-style adversarial optimization to a multi-step, stochastic generative pipeline, leveraging robust iterative updates under small-norm constraints to craft imperceptible yet highly effective perturbations. Crucially, Expectation-over-Transformation principles are adopted to handle diffusion\u2019s inherent randomness (noise seeds and guidance variability), ensuring the perturbations remain effective across different samples and prompts.\nStrategically, the paper is inspired by protective cloaking lines such as Fawkes, which showed that preemptive, imperceptible perturbations can raise the cost of model misuse. At the same time, Glaze identified a gap: while style-mimicry can be impeded, inference-time image editing remained unaddressed. This paper fills that gap by tailoring adversarial immunization to the specific structure of diffusion-based editing, proposing encoder- and denoising-centric objectives that directly exploit the weaknesses of LDM/DDPM pipelines to degrade malicious edits.",
  "analysis_timestamp": "2026-01-06T23:09:26.526335"
}
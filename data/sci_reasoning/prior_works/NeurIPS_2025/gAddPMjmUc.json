{
  "prior_works": [
    {
      "title": "Parametric bandits: The generalized linear case",
      "authors": "A. Filippi et al.",
      "year": 2010,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized generalized linear contextual bandits and introduced GLM-UCB; the present work adopts this GLM setting and targets the same near-optimal O~(d\u221aT) regret while proposing a different (randomized) exploration mechanism via feature perturbation."
    },
    {
      "title": "Improved Algorithms for Linear Stochastic Bandits",
      "authors": "Y. Abbasi-Yadkori et al.",
      "year": 2011,
      "role": "Foundation",
      "relationship_sentence": "OFUL established confidence-set\u2013based analysis and the canonical O~(d\u221aT) worst-case regret for linear bandits; our analysis leverages this confidence-geometry to prove O~(d\u221aT) regret for a randomized alternative that perturbs features instead of using optimism or parameter sampling."
    },
    {
      "title": "Thompson Sampling for Contextual Bandits with Linear Payoffs",
      "authors": "S. Agrawal et al.",
      "year": 2013,
      "role": "Baseline",
      "relationship_sentence": "Linear Thompson Sampling is the primary randomized baseline that explores by sampling parameters; our method replaces parameter sampling with feature perturbation and is shown to avoid the typical O~(d^{3/2}\u221aT) scaling associated with such randomized approaches."
    },
    {
      "title": "Linear Thompson Sampling Revisited",
      "authors": "M. Abeille et al.",
      "year": 2017,
      "role": "Gap Identification",
      "relationship_sentence": "This work tightened LinTS analyses but still relies on parameter posterior sampling and does not deliver a simple randomized strategy with worst-case O~(d\u221aT) guarantees; we directly address this gap by proving such a bound for randomized exploration via feature perturbation."
    },
    {
      "title": "Perturbed-History Exploration in Stochastic Multi-Armed Bandits",
      "authors": "B. Kveton et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "PHE demonstrated that injecting randomness into data (pseudo-rewards) can stand in for Bayesian sampling; we build on this perturbation principle but move the randomness from rewards to features, yielding stronger worst-case guarantees in contextual/GLM settings."
    },
    {
      "title": "An Information-Theoretic Analysis of Thompson Sampling",
      "authors": "D. Russo et al.",
      "year": 2016,
      "role": "Gap Identification",
      "relationship_sentence": "This analysis clarified that Thompson Sampling\u2019s strongest guarantees are Bayes-regret under priors; our work responds by designing a randomized scheme (feature perturbation) with worst-case O~(d\u221aT) regret, avoiding reliance on posterior sampling."
    },
    {
      "title": "Neural Contextual Bandits with UCB-based Exploration",
      "authors": "Y. Zhou et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "NeuralUCB extends optimism to neural models but requires explicit uncertainty estimation; by randomizing inputs instead of parameters, our feature-perturbation approach offers a computationally simpler path to exploration that naturally extends to neural/nonparametric models."
    }
  ],
  "synthesis_narrative": "The core of this paper\u2014randomized exploration via feature perturbation with O~(d\u221aT) worst-case regret\u2014stands on the problem and analysis frameworks introduced for contextual and generalized linear bandits. Filippi et al. (2010) defined the GLM bandit setting and GLM-UCB, while Abbasi-Yadkori et al. (2011) established the confidence-set geometry and near-optimal O~(d\u221aT) rates for linear bandits that our proof technique mirrors to analyze a randomized policy. On the randomized-exploration side, Agrawal and Goyal (2013) popularized Thompson Sampling for linear payoffs, and Abeille and Lazaric (2017) refined its analysis; however, these methods hinge on parameter posterior sampling and typically do not deliver simple randomized strategies with tight worst-case O~(d\u221aT) guarantees, motivating our shift away from parameter sampling. Russo and Van Roy (2016) further highlighted that TS\u2019s strongest theoretical guarantees are Bayes-regret, underscoring the gap this work closes by proving worst-case guarantees for a randomized method. Methodologically, Kveton et al. (2019) showed that perturbing the data (via pseudo-rewards) can approximate posterior-driven exploration; our contribution transposes this perturbation to the feature side, which both simplifies computation and improves dimension dependence in GLM contextual bandits. Finally, Zhou, Li, and Gu (2020) demonstrated exploration for neural contextual bandits via UCB, but at the cost of explicit uncertainty estimation; our feature perturbation offers a practical, scalable alternative that naturally extends beyond parametric models without sampling parameters or rewards.",
  "analysis_timestamp": "2026-01-06T23:08:23.972201"
}
{
  "prior_works": [
    {
      "title": "Neural Controlled Differential Equations for Irregular Time Series",
      "authors": "Patrick Kidger et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "SLiCEs are formulated explicitly as controlled differential equations, directly building on the Neural CDE framework to cast sequence modeling as input-driven continuous-time dynamics."
    },
    {
      "title": "HiPPO: Recurrent Memory with Optimal Polynomial Projections",
      "authors": "Albert Gu et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "HiPPO introduced the linear state-space perspective and memory projection tools that underpin SSM/CDE-style continuous-time sequence models that SLiCE unifies and extends."
    },
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "authors": "Albert Gu et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "S4 established the parallel-in-time SSM pathway for long-range modeling that SLiCE retains while generalizing the form of the input-dependent state-transition beyond S4-style structures."
    },
    {
      "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
      "authors": "Albert Gu et al.",
      "year": 2024,
      "role": "Gap Identification",
      "relationship_sentence": "Mamba\u2019s selective SSM uses input-dependent but diagonal state-transition matrices; SLiCE explicitly targets this diagonal expressivity limitation by proving structured (block-diagonal/sparse/Hadamard) transitions match dense expressivity while staying parallel-in-time."
    },
    {
      "title": "Resurrecting Recurrent Neural Networks for Long Sequences",
      "authors": "Alessio Orvieto et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "LRU demonstrated the practical value of structured linear recurrences, informing SLiCE\u2019s design space and its unification of linear RNNs with input-structured state-transition matrices."
    },
    {
      "title": "Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations",
      "authors": "Tri Dao et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Butterfly/fast transform factorisations motivate SLiCE\u2019s Walsh\u2013Hadamard structured transitions by showing how compositions of sparse/Hadamard-like operators can approximate dense linear maps efficiently."
    },
    {
      "title": "Fastfood \u2014 Approximating Kernel Expansions in Loglinear Time",
      "authors": "Quoc V. Le et al.",
      "year": 2013,
      "role": "Inspiration",
      "relationship_sentence": "Fastfood\u2019s use of Hadamard-diagonal factorizations to mimic dense projections directly inspires SLiCE\u2019s Hadamard-based variant and its expressivity-with-efficiency guarantees."
    }
  ],
  "synthesis_narrative": "SLiCE sits at the intersection of continuous-time sequence modeling and structured linear algebra. The Neural CDE formulation (Kidger et al., 2020) provides the core mathematical language: hidden states evolve via controlled dynamics driven by the input, which SLiCE keeps but specializes to linear dynamics with input-dependent, structured transition matrices. HiPPO (Gu et al., 2020) and S4 (Gu et al., 2022) established that linear state-space views can be both expressive and parallel-in-time, seeding the path for SLiCE to maintain convolution/scan-style parallelism while moving beyond standard parametrizations. Mamba (Gu et al., 2024) pinpointed a compelling direction\u2014input-dependent (selective) SSMs\u2014but with a diagonal state matrix that empirically works yet theoretically restricts expressivity; SLiCE\u2019s central advance directly addresses this gap by proving block-diagonal, sparse, and Walsh\u2013Hadamard structures recover dense-matrix expressivity. Practical evidence that structured linear recurrences can be strong sequence learners came from LRU (Orvieto et al., 2023), which informed SLiCE\u2019s unifying perspective over linear RNN-like modules. Finally, SLiCE\u2019s Hadamard variant and expressivity results are inspired by structured fast transforms: butterfly factorizations (Dao et al., 2019) and Fastfood (Le et al., 2013) show how compositions of diagonal and Hadamard-like operators can emulate dense linear maps at lower cost. Together, these works directly shaped SLiCE\u2019s formulation, theory, and efficient structured designs.",
  "analysis_timestamp": "2026-01-06T23:08:23.968317"
}
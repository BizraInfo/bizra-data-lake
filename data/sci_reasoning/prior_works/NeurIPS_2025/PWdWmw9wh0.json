{
  "prior_works": [
    {
      "title": "Look, Listen and Learn",
      "authors": "Relja Arandjelovi\u0107, Andrew Zisserman",
      "year": 2017,
      "role": "Foundational self-supervised audiovisual correspondence learning",
      "relationship_sentence": "Established the audio-visual co-occurrence objective that many SSL models build on, motivating this paper\u2019s probe into whether correspondence-trained systems develop systematic modality bias when cues conflict."
    },
    {
      "title": "Audio-Visual Scene Analysis with Self-Supervised Multisensory Features",
      "authors": "Andrew Owens et al.",
      "year": 2018,
      "role": "Self-supervised multisensory features and conflict-style evaluations",
      "relationship_sentence": "Introduced alignment-based pretexts and analyzed cross-modal interactions, inspiring the paper\u2019s psychophysics-style conflict paradigms and model diagnostics for audiovisual reliability and dominance."
    },
    {
      "title": "Learning to Localize Sound Sources in Visual Scenes",
      "authors": "Hyeonjin Senocak, Tae-Hyun Oh, Junsik Kim, In So Kweon",
      "year": 2018,
      "role": "Early sound source localization (SSL) formulation and evaluation",
      "relationship_sentence": "Pioneered explicit SSL objectives and metrics, providing canonical models and evaluation practices that the current work stress-tests under congruent and conflicting audio-visual conditions."
    },
    {
      "title": "The Sound of Pixels",
      "authors": "Hang Zhao, Chuang Gan, et al.",
      "year": 2018,
      "role": "Pixel-level audiovisual localization and separation",
      "relationship_sentence": "Demonstrated fine-grained spatial grounding of sound in images, directly informing the paper\u2019s choice of model families and the spatial precision criteria used to reveal modality-driven localization errors."
    },
    {
      "title": "The ventriloquist effect results from near-optimal bimodal integration",
      "authors": "David Alais, David Burr",
      "year": 2004,
      "role": "Psychophysics theory of cross-modal reliability weighting",
      "relationship_sentence": "Provided the normative account of human conflict resolution that this paper uses as a benchmark to test whether AI models, unlike humans, over-weight vision rather than reliability-weight auditory cues in SSL."
    },
    {
      "title": "What you see is what you hear",
      "authors": "Ladan Shams, Yukiyasu Kamitani, Shinsuke Shimojo",
      "year": 2000,
      "role": "Foundational audiovisual conflict illusion (sound-induced flash)",
      "relationship_sentence": "Established controlled audiovisual conflict paradigms, directly inspiring the paper\u2019s six-condition psychophysics-style design to elicit and measure modality bias and conflict resolution in models versus humans."
    },
    {
      "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias in CNNs using Stylized ImageNet",
      "authors": "Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, Wieland Brendel",
      "year": 2019,
      "role": "Methodology for quantifying inductive biases via controlled tests",
      "relationship_sentence": "Introduced rigorous, human-referenced diagnostics for model bias, a methodology the paper adapts to the multimodal domain to quantify visual versus auditory dominance in SSL."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014a psychophysics-inspired evaluation that exposes modality bias and conflict resolution behavior in sound source localization (SSL) models\u2014rests on two intertwined lines of prior work. First, foundational SSL and audiovisual self-supervision research (Arandjelovi\u0107 & Zisserman\u2019s Look, Listen and Learn; Owens et al.\u2019s self-supervised multisensory features; Senocak et al.\u2019s SSL formulation; and Zhao et al.\u2019s Sound of Pixels) established how to learn and assess spatial grounding of sound from natural video. These works defined the model families, training paradigms, and spatial metrics that the present paper systematically probes under carefully controlled cue configurations, revealing when models over-rely on visual correlates instead of true auditory localization.\n\nSecond, classic human psychophysics on cross-modal conflict (Shams et al.\u2019s sound-induced flash illusion and Alais & Burr\u2019s ventriloquism reliability-weighting account) provides the conceptual and methodological scaffolding for the paper\u2019s six audiovisual conditions spanning congruent, conflicting, and absent cues. This grounding enables principled human\u2013model comparisons and hypotheses about when an auditory cue should dominate. Finally, Geirhos et al.\u2019s paradigm for quantifying inductive biases with controlled stimuli directly informs the paper\u2019s bias diagnostics, adapted here to the multimodal setting. Together, these strands yield a rigorous, human-referenced benchmark that isolates modality dominance and robustness in modern SSL systems, explaining why humans outperform current models and how visual bias emerges under conflict.",
  "analysis_timestamp": "2026-01-07T00:21:32.355981"
}
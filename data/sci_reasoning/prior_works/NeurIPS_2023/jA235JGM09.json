{
  "prior_works": [
    {
      "title": "Deep Reinforcement Learning from Human Preferences",
      "authors": "Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei",
      "year": 2017,
      "role": "Foundational method (RLHF)",
      "relationship_sentence": "Introduced the preference-learning framework underlying modern RLHF safety training, which Jailbroken interrogates by showing how RLHF-aligned models can still be coerced into harmful behavior."
    },
    {
      "title": "Training Language Models to Follow Instructions with Human Feedback",
      "authors": "Long Ouyang, Jeff Wu, Xu Jiang, et al.",
      "year": 2022,
      "role": "Core alignment/safety training for LLMs",
      "relationship_sentence": "Established instruction-following via RLHF (InstructGPT) and documented early safety improvements, providing the concrete safety-training paradigm whose limitations (under adversarial prompts) Jailbroken systematically analyzes."
    },
    {
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
      "authors": "Yuntao Bai, Andy Jones, Kamal Ndousse, et al.",
      "year": 2022,
      "role": "Safety objective design and trade-offs (HHH)",
      "relationship_sentence": "Explicitly framed the helpfulness\u2013harmlessness trade-off and used red teaming to reduce harms, directly informing Jailbroken\u2019s \"competing objectives\" failure mode and evaluation focus on assistants like Claude."
    },
    {
      "title": "Red Teaming Language Models",
      "authors": "Ethan Perez, Sam Ringer, Kamil Ciosek, et al.",
      "year": 2022,
      "role": "Adversarial evaluation methodology",
      "relationship_sentence": "Pioneered systematic red teaming to elicit failure behaviors in aligned LMs, which Jailbroken builds upon by proposing theory-driven (failure-mode\u2013guided) jailbreak designs and broader evaluations."
    },
    {
      "title": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
      "authors": "Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, Noah A. Smith",
      "year": 2020,
      "role": "Safety benchmarking for harmful outputs",
      "relationship_sentence": "Provided widely used prompts and metrics for assessing toxic/harmful generations, a foundation for evaluating whether safety training generalizes\u2014central to Jailbroken\u2019s \"mismatched generalization\" framing."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "authors": "Andy Zou, Zifan Wang, Ethan Perez, Dan Hendrycks",
      "year": 2023,
      "role": "Optimization-based jailbreaks",
      "relationship_sentence": "Demonstrated automatic adversarial suffixes that reliably circumvent aligned models\u2019 safeguards, directly motivating Jailbroken\u2019s analysis of why such jailbreaks succeed and inspiring attack construction."
    },
    {
      "title": "Goal Misgeneralization in Deep Reinforcement Learning",
      "authors": "Rohin Shah, Matthew Rahtz, Jess Smith, Adam Gleave, et al.",
      "year": 2022,
      "role": "Alignment theory (misgeneralization)",
      "relationship_sentence": "Formalized how capabilities can generalize while intended goals do not, providing the conceptual precursor to Jailbroken\u2019s \"mismatched generalization\" failure mode in the LLM safety context."
    }
  ],
  "synthesis_narrative": "Jailbroken situates its core contribution\u2014identifying competing objectives and mismatched generalization as the root causes of jailbreak vulnerabilities\u2014within the evolution of LLM alignment and adversarial evaluation. The RLHF paradigm introduced by Christiano et al. and operationalized at scale by Ouyang et al. (InstructGPT) established how instruction-following and basic safety can be trained from human preferences, while Anthropic\u2019s HHH work made explicit the tension between helpfulness and harmlessness, a trade-off Jailbroken dubs the \"competing objectives\" failure mode. In parallel, red-teaming methodologies (Perez et al.) and safety benchmarks like RealToxicityPrompts enabled systematic discovery and measurement of harmful behaviors, revealing safety gaps despite alignment efforts. Theoretical advances on goal misgeneralization (Shah et al.) clarified how models\u2019 capabilities can generalize to new domains while safety objectives fail to transfer\u2014a direct precursor to Jailbroken\u2019s \"mismatched generalization\" framing. Finally, optimization-based jailbreaks (Zou et al.) demonstrated that aligned models remain brittle under adversarial prompting, indicating the existence of exploitable failure modes rather than isolated glitches. Building on these strands, Jailbroken articulates a principled taxonomy of why safety training fails and uses it to design new jailbreaks that target the capability\u2013safety conflict and generalization gaps, then validates the persistence of these vulnerabilities in state-of-the-art models like GPT-4 and Claude. This synthesis advances the field from cataloging failures to understanding and predicting them.",
  "analysis_timestamp": "2026-01-07T00:02:04.856064"
}
{
  "prior_works": [
    {
      "title": "Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming (Dyna)",
      "authors": "Richard S. Sutton",
      "year": 1990,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "MAD-TD adopts Dyna\u2019s core principle of interleaving real experience with model-generated transitions to improve TD learning, using a learned world model to supply additional updates that stabilize value learning."
    },
    {
      "title": "Model-Based Policy Optimization (MBPO)",
      "authors": "Michael Janner et al.",
      "year": 2019,
      "arxiv_id": "1906.08253",
      "role": "Extension",
      "relationship_sentence": "MBPO\u2019s demonstration that short-horizon rollouts from a learned dynamics model can safely augment off-policy RL is adapted in MAD-TD to generate a small amount of on-policy action data precisely where the buffer lacks coverage."
    },
    {
      "title": "Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning",
      "authors": "Elliot A. Feinberg et al.",
      "year": 2018,
      "arxiv_id": "1803.00101",
      "role": "Inspiration",
      "relationship_sentence": "MVE\u2019s idea of using limited model rollouts to refine TD targets directly inspires MAD-TD\u2019s use of short model-generated trajectories to supervise Q-values at on-policy actions absent from the dataset."
    },
    {
      "title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model (REDQ)",
      "authors": "Xinyue Chen et al.",
      "year": 2021,
      "arxiv_id": "2101.05982",
      "role": "Baseline",
      "relationship_sentence": "REDQ explicitly pushes high update-to-data ratios for sample efficiency but exhibits instability at large UTD, forming the primary high-UTD baseline that MAD-TD stabilizes via model-augmented data."
    },
    {
      "title": "Conservative Q-Learning for Offline Reinforcement Learning",
      "authors": "Aviral Kumar et al.",
      "year": 2020,
      "arxiv_id": "2006.04779",
      "role": "Gap Identification",
      "relationship_sentence": "CQL diagnoses Q-function misgeneralization on unseen actions and combats it with conservative penalties, a failure mode MAD-TD addresses by instead supplying model-generated on-policy action supervision."
    },
    {
      "title": "Deep Reinforcement Learning and the Deadly Triad",
      "authors": "Hado van Hasselt et al.",
      "year": 2018,
      "arxiv_id": "1812.02648",
      "role": "Foundation",
      "relationship_sentence": "The deadly triad analysis formalizes the instability of off-policy bootstrapping with function approximation\u2014amplified under high UTD\u2014which MAD-TD mitigates by injecting model-generated targets on the on-policy action manifold."
    }
  ],
  "synthesis_narrative": "Dyna introduced the central idea of improving temporal-difference learning by augmenting real experience with simulated transitions from a learned model, turning imagination into an additional supervision stream. Building on this, Model-Based Value Expansion showed that short, limited-horizon model rollouts can refine TD targets and reduce bias/variance without requiring long, error-prone simulations. MBPO extended this principle to modern deep RL, demonstrating that brief model-generated rollouts interleaved with off-policy updates can safely and effectively boost sample efficiency when the model is imperfect. In parallel, REDQ established that pushing the update-to-data ratio high can substantially improve data efficiency but also increases instability, highlighting a delicate tradeoff between learning speed and reliability. Conservative Q-Learning pinpointed a key mechanism behind instability\u2014Q-function misgeneralization on actions not covered by the data\u2014and countered it with conservative penalties on unseen actions. Complementing these empirical insights, the deadly triad framework formalized why off-policy bootstrapping with function approximation can diverge, a risk exacerbated as updates per sample grow.\nTaken together, these works suggest a targeted remedy: use a learned model for brief, judicious rollouts to supply supervision exactly where off-policy buffers lack coverage\u2014on the current policy\u2019s actions\u2014thereby addressing misgeneralization without heavy conservatism. By injecting a small stream of model-generated, on-policy action data into high-UTD training, one can retain the efficiency of aggressive updating while counteracting the deadly triad\u2019s instability mechanism identified by REDQ and CQL, following Dyna/MBPO/MVE\u2019s safe-short-rollout blueprint.",
  "target_paper": {
    "title": "MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL",
    "authors": "Claas A Voelcker, Marcel Hussing, Eric Eaton, Amir-massoud Farahmand, Igor Gilitschenski",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "reinforcement learning, model based reinforcement learning, data augmentation, high update ratios",
    "abstract": "Building deep reinforcement learning (RL) agents that find a good policy with few samples has proven notoriously challenging. To achieve sample efficiency, recent work has explored updating neural networks with large numbers of gradient steps for every new sample. While such high update-to-data (UTD) ratios have shown strong empirical performance, they also introduce instability to the training process.  Previous approaches need to rely on periodic neural network parameter resets to address this instability, but restarting the training process is infeasible in many real-world applications and requires tuning the resetting interval. In this paper, we focus on one of the core difficulties of stable training with limited samples: the inability of learned value functions to generalize to unobserved on-policy actions. We mitigate this issue directly by augmenting the off-policy RL training process with a small amount of data generated from a learned world model. Our method, Model-Augmented ",
    "openreview_id": "6RtRsg8ZV1",
    "forum_id": "6RtRsg8ZV1"
  },
  "analysis_timestamp": "2026-01-06T18:48:15.254381"
}
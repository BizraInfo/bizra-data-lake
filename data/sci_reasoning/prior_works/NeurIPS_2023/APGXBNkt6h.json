{
  "prior_works": [
    {
      "title": "Attention Is All You Need",
      "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, Illia Polosukhin",
      "year": 2017,
      "role": "Foundational architecture enabling long-range dependency modeling via self-attention.",
      "relationship_sentence": "Provides the core mechanism whose purported long-term dependency benefits in RL are interrogated here, underpinning the paper\u2019s analysis that Transformers help memory but not temporal credit assignment."
    },
    {
      "title": "Stabilizing Transformers for Reinforcement Learning (GTrXL)",
      "authors": "Emilio Parisotto et al.",
      "year": 2020,
      "role": "Seminal application of Transformers to RL with architectural/stability modifications showing strong empirical gains on POMDP-like tasks.",
      "relationship_sentence": "Motivated the question of why Transformers excel in RL; this paper builds on such evidence to disentangle whether gains arise from enhanced memory versus improved credit assignment."
    },
    {
      "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
      "authors": "Lili Chen et al.",
      "year": 2021,
      "role": "Reframed RL as sequence modeling with Transformers, demonstrating competitive performance on long-horizon tasks.",
      "relationship_sentence": "Serves as a high-profile example of Transformer success in RL that this work seeks to mechanistically explain by isolating memory length from credit assignment length."
    },
    {
      "title": "bsuite: A collection of open-source reinforcement learning experiments",
      "authors": "Ian Osband et al.",
      "year": 2019,
      "role": "Introduced diagnostic RL probes explicitly targeting phenomena like memory and credit assignment.",
      "relationship_sentence": "Directly inspires the paper\u2019s methodological stance; the authors formalize \u2018memory length\u2019 and \u2018credit assignment length\u2019 and design configurable tasks that extend bsuite-style probes into precise, scalable measurements."
    },
    {
      "title": "RUDDER: Return Decomposition for Delayed Rewards",
      "authors": "A. M. Arjona-Medina, J. Gillhofer, M. Widrich, T. Unterthiner, S. Hochreiter",
      "year": 2019,
      "role": "Method for long-term credit assignment via reward redistribution to shorten effective delays.",
      "relationship_sentence": "Provides a contrastive approach focused on credit assignment; the paper\u2019s finding that Transformers do not improve long-term credit assignment aligns with RUDDER\u2019s thesis that specialized mechanisms are required."
    },
    {
      "title": "Deep Recurrent Q-Learning for Partially Observable MDPs (DRQN)",
      "authors": "Matthew Hausknecht, Peter Stone",
      "year": 2015,
      "role": "Established recurrent networks as a means to handle partial observability and memory in RL.",
      "relationship_sentence": "Forms the recurrent-memory baseline paradigm that the paper contrasts with Transformers, showing Transformers scale memory length far beyond typical RNN capabilities while not fixing credit assignment."
    },
    {
      "title": "Recurrent Experience Replay in Distributed Reinforcement Learning (R2D2)",
      "authors": "Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, Will Dabney",
      "year": 2019,
      "role": "State-of-the-art recurrent RL with experience replay enabling long-context learning in practice.",
      "relationship_sentence": "Provides a strong RNN-based reference for memory and training regime; the paper demonstrates that Transformers extend memory horizons relative to such recurrent baselines without improving credit assignment length."
    }
  ],
  "synthesis_narrative": "Ni et al. interrogate a central puzzle raised by the recent success of Transformers in RL: do gains stem from superior memory or better temporal credit assignment? Foundationally, the Transformer architecture (Vaswani et al.) enables long-range dependency modeling via attention, and its tailored adoption for RL in GTrXL (Parisotto et al.) established strong empirical improvements on partially observable and long-context tasks. Sequence-modeling approaches like Decision Transformer (Chen et al.) further amplified evidence that Transformers can tackle long-horizon RL problems, but they did not clarify which capability\u2014memory or credit assignment\u2014drives the performance.\nTo answer this, the authors adopt a diagnostic ethos akin to bsuite (Osband et al.), which introduced targeted probes for memory and credit assignment. Building on that idea, they formalize precise notions of memory length and credit assignment length and craft configurable tasks to independently stress each dimension. Against recurrent baselines that historically address memory in POMDPs (DRQN) and strong modern recurrent agents (R2D2), they show that Transformers substantially extend memory horizons\u2014successfully recalling observations up to 1500 steps back\u2014yet do not enhance long-term credit assignment. This outcome aligns with specialized credit-assignment methods like RUDDER (Arjona-Medina et al.), which argue that targeted mechanisms (e.g., reward redistribution) are needed to handle delayed rewards. Together, these prior works shaped both the hypothesis and experimental design, enabling the paper\u2019s core contribution: decoupling and measuring memory versus credit assignment, and demonstrating Transformers primarily benefit the former.",
  "analysis_timestamp": "2026-01-06T23:42:48.050680"
}
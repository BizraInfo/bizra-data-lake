{
  "prior_works": [
    {
      "title": "Deep Learning with Differential Privacy",
      "authors": "Martin Abadi et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "Abadi et al. formalized DP training for deep learning via DP-SGD, establishing the dominant problem formulation and mechanism that contemporary \"public pretraining + private fine-tuning\" workflows are meant to augment\u2014precisely the setup this position paper interrogates."
    },
    {
      "title": "Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data",
      "authors": "Nicolas Papernot et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "PATE introduced the paradigm of leveraging public (non-private) data to improve utility under DP guarantees, a direct antecedent of the modern strategy of using large-scale public pretraining that this paper critically re-evaluates."
    },
    {
      "title": "Evaluating Differentially Private Machine Learning in Practice",
      "authors": "Bargav Jayaraman et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "By documenting severe utility degradations from DP training on standard benchmarks, this work motivated the community\u2019s turn to public pretraining to recover accuracy\u2014a reliance that the position paper questions on privacy and evaluation grounds."
    },
    {
      "title": "Extracting Training Data from Large Language Models",
      "authors": "Nicholas Carlini et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "This paper showed that LMs pretrained on web-scraped corpora can memorize and reveal verbatim training data, directly motivating the position paper\u2019s claim that large-scale public pretraining should not be presumed privacy-preserving."
    },
    {
      "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
      "authors": "Pang Wei Koh et al.",
      "year": 2021,
      "role": "Related Problem",
      "relationship_sentence": "WILDS formalized evaluation under real distribution shifts, informing the position paper\u2019s critique that standard benchmarks are ill-suited to measure transfer from public pretraining to sensitive domains."
    },
    {
      "title": "LAION-5B: An open large-scale dataset for CLIP",
      "authors": "Christoph Schuhmann et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "As a flagship web-scraped \"public\" dataset enabling massive pretraining, LAION-5B exemplifies the data sources whose privacy status this paper argues must be scrutinized in DP workflows."
    },
    {
      "title": "Stealing Machine Learning Models via Prediction APIs",
      "authors": "Florian Tram\u00e8r et al.",
      "year": 2016,
      "role": "Related Problem",
      "relationship_sentence": "By demonstrating model extraction from third-party services, this work grounds the position paper\u2019s observation that outsourcing compute for large pretrained models introduces additional privacy risks beyond differential privacy."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core argument emerges from three intertwined lines of prior work. First, Abadi et al. established the modern formulation of differentially private deep learning with DP-SGD, while Papernot et al. (PATE) introduced a concrete mechanism to combine DP with public data. Together, these works laid the foundation for today\u2019s widely adopted recipe: leverage non-private public data to boost utility, then apply DP to the sensitive component. Second, Jayaraman and Evans quantified the sharp utility costs of DP in practice, catalyzing the field\u2019s pivot to large-scale public pretraining as a remedy. The position paper\u2019s central critique directly targets this solution, interrogating whether public pretraining genuinely preserves privacy. This concern is substantiated by evidence that web-scale models memorize and can reveal sensitive content: Carlini et al. demonstrated extraction of verbatim training data from large language models, showing that the very public corpora used for pretraining can embed privacy risks. Third, the paper questions how we measure generalization to sensitive domains. WILDS shaped thinking about out-of-distribution evaluation, highlighting that standard benchmarks often fail to capture real-world shifts\u2014precisely the mismatch the paper emphasizes for sensitive-domain transfer. Finally, the reliance on large pretrained models often forces outsourcing to powerful third parties; Tram\u00e8r et al.\u2019s model-stealing results underscore that such dependence introduces additional privacy threats, beyond DP guarantees. LAION-5B exemplifies the web-scraped datasets at the heart of these debates.",
  "analysis_timestamp": "2026-01-06T23:09:26.475787"
}
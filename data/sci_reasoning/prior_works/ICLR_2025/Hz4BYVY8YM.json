{
  "prior_works": [
    {
      "title": "Audio Visual Scene-aware Dialog (AVSD): A Dataset for Multi-turn Video-grounded Dialogue",
      "authors": "Chiori Hori et al.",
      "year": 2019,
      "arxiv_id": "1806.00525",
      "role": "Foundation",
      "relationship_sentence": "AVSD established the multi-turn dialog formulation grounded in videos, which SVBench directly adopts and extends by enforcing temporally linked QA chains along a streaming timeline."
    },
    {
      "title": "TVQA: Localized, Compositional Video Question Answering",
      "authors": "Jie Lei et al.",
      "year": 2018,
      "arxiv_id": "1809.01696",
      "role": "Foundation",
      "relationship_sentence": "TVQA introduced temporally localized QA over video segments, and SVBench builds on this idea by organizing questions into consecutive multi-turn chains that maintain temporal coherence across the full stream."
    },
    {
      "title": "TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering",
      "authors": "Yunseok Jang et al.",
      "year": 2017,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "TGIF-QA framed video QA tasks that probe temporal skills (e.g., repetition, state transition), whose limitations to short, single-turn clips SVBench explicitly addresses with sustained multi-turn temporal reasoning over long streams."
    },
    {
      "title": "MovieQA: Understanding Stories in Movies through Question-Answering",
      "authors": "Makarand Tapaswi et al.",
      "year": 2016,
      "arxiv_id": "1512.02902",
      "role": "Foundation",
      "relationship_sentence": "MovieQA highlighted narrative-level, long-range reasoning needs in videos, motivating SVBench\u2019s design to evaluate persistent reasoning across entire streaming narratives rather than isolated moments."
    },
    {
      "title": "Ego4D: Around the World in 3,000 Hours of Egocentric Video",
      "authors": "Kristen Grauman et al.",
      "year": 2022,
      "arxiv_id": "2110.07058",
      "role": "Related Problem",
      "relationship_sentence": "Ego4D foregrounded continuous, long-duration video tasks requiring memory across time, directly inspiring SVBench\u2019s streaming evaluation setup that tests temporal persistence in LVLMs."
    },
    {
      "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision-Language Models",
      "authors": "Hassan Abbas Maaz et al.",
      "year": 2023,
      "arxiv_id": "2306.05424",
      "role": "Inspiration",
      "relationship_sentence": "Video-ChatGPT demonstrated LLM-driven, semi-automated generation of video-grounded instruction/QA data and multi-turn interactions, which SVBench adapts and systematizes into a scalable pipeline that enforces temporal linkage across turns."
    },
    {
      "title": "Video-MME: A Comprehensive Evaluation Benchmark for Multimodal LLMs on Video Understanding",
      "authors": "Yue Fu et al.",
      "year": 2024,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "Video-MME exposed limitations of current LVLMs under predominantly short, single-turn evaluations, a gap SVBench fills by evaluating long-context, temporally chained multi-turn dialogues over streaming videos."
    }
  ],
  "synthesis_narrative": "Audio Visual Scene-aware Dialog (AVSD) crystallized the notion that video understanding could be evaluated via multi-turn, dialog-style question answering, grounding each turn in visual and audio content. TVQA contributed a complementary insight by requiring temporal localization for each question, pushing models to link questions to specific segments rather than entire clips. TGIF-QA went further in probing temporal skills like counting and state transitions but did so on short, single-turn interactions. MovieQA underscored the importance of narrative-scale reasoning in feature-length videos, showing that long-range dependencies and story structure matter. Ego4D reframed video understanding as continuous, streaming perception, where persistence and memory across long durations are central. Video-ChatGPT demonstrated that large language models can generate high-quality, multi-turn, video-grounded instructions and QAs semi-automatically, offering a practical recipe to scale dialog-style supervision. Finally, Video-MME highlighted that many contemporary LVLM evaluations remain short and single-turn, revealing a systematic mismatch with real-world, long-context video use cases.\n\nCollectively, these works point to a missing evaluation target: sustained, temporally coherent, multi-turn reasoning over entire video streams, produced at scale. SVBench emerges as a natural synthesis\u2014marrying AVSD\u2019s dialog format with TVQA\u2019s temporal grounding, extending TGIF-QA\u2019s temporal probes to long narratives emphasized by MovieQA and Ego4D, and operationalizing Video-ChatGPT\u2019s semi-automated data creation to build temporally linked QA chains. By addressing the gap surfaced by Video-MME, SVBench formalizes a rigorous benchmark for streaming video understanding that stresses persistence, temporal linkage across turns, and dialogue continuity.",
  "target_paper": {
    "title": "SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding",
    "authors": "Zhenyu Yang, Yuhang Hu, Zemin Du, Dizhan Xue, Shengsheng Qian, Jiahong Wu, Fan Yang, Weiming Dong, Changsheng Xu",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Multimodal large language model, Streaming video analysis, Video understanding",
    "abstract": "Despite the significant advancements of Large Vision-Language Models (LVLMs) on established benchmarks, there remains a notable gap in suitable evaluation regarding their applicability in the emerging domain of long-context streaming video understanding. Current benchmarks for video understanding typically emphasize isolated single-instance text inputs and fail to evaluate the capacity to sustain temporal reasoning throughout the entire duration of video streams. To address these limitations, we introduce SVBench, a pioneering benchmark with temporal multi-turn question-answering chains specifically designed to thoroughly assess the capabilities of streaming video understanding of current LVLMs. We design a semi-automated annotation pipeline to obtain 49,979 Question-Answer (QA) pairs of 1,353 streaming videos, which includes generating QA chains that represent a series of consecutive multi-turn dialogues over video segments and constructing temporal linkages between successive QA chai",
    "openreview_id": "Hz4BYVY8YM",
    "forum_id": "Hz4BYVY8YM"
  },
  "analysis_timestamp": "2026-01-06T15:20:16.508291"
}
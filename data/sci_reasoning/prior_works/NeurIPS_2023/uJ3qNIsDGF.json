{
  "prior_works": [
    {
      "title": "Intriguing properties of neural networks",
      "authors": [
        "Christian Szegedy",
        "Wojciech Zaremba",
        "Ilya Sutskever",
        "Joan Bruna",
        "Dumitru Erhan",
        "Ian Goodfellow",
        "Rob Fergus"
      ],
      "year": 2014,
      "role": "Foundational discovery of adversarial sensitivity and geometric framing",
      "relationship_sentence": "By revealing extreme sensitivity to small perturbations and highlighting geometric aspects of decision boundaries, this work set the stage for studying the complementary phenomenon of under-sensitivity and motivated geometric analyses like equi-confidence level sets."
    },
    {
      "title": "Explaining and Harnessing Adversarial Examples",
      "authors": [
        "Ian J. Goodfellow",
        "Jonathon Shlens",
        "Christian Szegedy"
      ],
      "year": 2015,
      "role": "Methodological precedent: gradient-based view of classifier confidence",
      "relationship_sentence": "Their linearity hypothesis and use of input gradients to control class confidence directly inform Level Set Traversal, which deliberately moves in directions orthogonal to the local gradient (tangent to level sets) to maintain confidence while exploring input space."
    },
    {
      "title": "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images",
      "authors": [
        "Anh Nguyen",
        "Jason Yosinski",
        "Jeff Clune"
      ],
      "year": 2015,
      "role": "Empirical motivation for high-confidence regions far from natural images",
      "relationship_sentence": "By producing unrecognizable yet high-confidence images, this work evidenced large, poorly understood high-confidence regions that the proposed level-set exploration method systematically maps and characterizes."
    },
    {
      "title": "DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks",
      "authors": [
        "Seyed-Mohsen Moosavi-Dezfooli",
        "Alhussein Fawzi",
        "Pascal Frossard"
      ],
      "year": 2016,
      "role": "Geometric and gradient-based local linearization precedent",
      "relationship_sentence": "DeepFool\u2019s use of local linearization and gradient normals to find minimal boundary crossings inspires the present paper\u2019s reliance on local gradients; here, the authors instead exploit directions orthogonal to the gradient to stay on equi-confidence level sets."
    },
    {
      "title": "Decision-Based Adversarial Attacks: Reliable attacks against black-box machine learning models (Boundary Attack)",
      "authors": [
        "Wieland Brendel",
        "Jonas Rauber",
        "Matthias Bethge"
      ],
      "year": 2018,
      "role": "Algorithmic precedent for boundary-constrained exploration using orthogonal steps",
      "relationship_sentence": "Boundary Attack\u2019s random walks with orthogonal components to remain on a decision boundary conceptually parallel this paper\u2019s controlled traversal along constant-confidence surfaces in the interior of a class region."
    },
    {
      "title": "Excessive Invariance Causes Adversarial Vulnerability",
      "authors": [
        "J\u00f6rn-Henrik Jacobsen",
        "Jens Behrmann",
        "Richard Zemel",
        "Matthias Bethge"
      ],
      "year": 2019,
      "role": "Conceptual foundation for under-sensitivity/invariance-based adversarial examples",
      "relationship_sentence": "This work formalized the notion of excessive invariance and undersensitivity, directly motivating the paper\u2019s focus on equi-confidence level sets as a tool to expose and quantify such blind spots in CNNs and Transformers."
    },
    {
      "title": "Universal Adversarial Perturbations",
      "authors": [
        "Seyed-Mohsen Moosavi-Dezfooli",
        "Alhussein Fawzi",
        "Omar Fawzi",
        "Pascal Frossard"
      ],
      "year": 2017,
      "role": "Global geometric perspective via aggregation of local normals",
      "relationship_sentence": "By linking local decision boundary geometry across data points to reveal global structure, this work motivates exploring the connectedness and extent of high-confidence regions through level-set traversal."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014a Level Set Traversal (LST) algorithm to map equi-confidence regions and expose blind spots (under-sensitivity)\u2014is grounded in a sequence of works that progressively shaped a geometric, gradient-centric view of neural predictions. Szegedy et al. and Goodfellow et al. established adversarial sensitivity and a gradient-based account of confidence, positioning the input gradient as the local normal to a classifier\u2019s confidence surface. LST leverages this exact insight, but inverts the usual goal: rather than stepping along the gradient to change confidence, it steps in directions orthogonal to the gradient to remain on the same confidence level while exploring the input space.\n\nNguyen et al. demonstrated that high-confidence can persist in seemingly arbitrary images, motivating a systematic way to traverse and characterize such regions beyond isolated examples. DeepFool and Universal Adversarial Perturbations provided local-to-global geometric tools\u2014linearization via gradient normals and aggregation of normals across samples\u2014that inform LST\u2019s reliance on local geometry and its interest in the connectedness and extent of confidence sets. Boundary Attack offered an algorithmic template for constrained exploration using orthogonal moves on decision boundaries; LST adapts this principle away from the boundary to interior equi-confidence surfaces. Finally, Jacobsen et al.\u2019s theory of excessive invariance crystallized under-sensitivity as a primary failure mode; LST operationalizes this concept by directly mapping where confidence remains invariant under large input changes, thereby quantifying and visualizing blind spots in CNNs and Transformers.",
  "analysis_timestamp": "2026-01-06T23:33:35.585492"
}
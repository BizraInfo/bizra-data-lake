{
  "prior_works": [
    {
      "title": "Language Models (Mostly) Know What They Know",
      "authors": "Saurabh Kadavath et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrated that LMs possess internal signals about their own knowledge and uncertainty, directly motivating this paper\u2019s search for a concrete, mechanistic \u201cknowledge awareness\u201d representation that predicts refusal vs. hallucination."
    },
    {
      "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
      "authors": "Mor Geva et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Established that factual associations about entities are stored in MLP layers as key\u2013value memories, grounding this paper\u2019s hypothesis that an entity-recognition feature gates access to those memories and thus controls hallucination/refusal."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT (ROME)",
      "authors": "Kevin Meng et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "Introduced causal intervention techniques to locate and manipulate factual knowledge in specific layers, which this paper adapts by steering along discovered SAE feature directions to causally toggle refusal and hallucination."
    },
    {
      "title": "Toy Models of Superposition in Neural Networks",
      "authors": "Nelson Elhage et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Showed that neural features are superposed and that sparse methods can disentangle them, providing the theoretical basis for using sparse autoencoders to isolate an interpretable \u201centity-knownness\u201d direction."
    },
    {
      "title": "Sparse Feature Circuits: Discovering Interpretable Features in Language Models",
      "authors": "Chris Olah et al.",
      "year": 2024,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Demonstrated that SAEs trained on LM activations yield monosemantic features that support causal analysis, directly enabling this paper\u2019s extraction of entity-recognition features and their causal manipulation."
    },
    {
      "title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 with Sparse Autoencoders",
      "authors": "Will Bricken et al.",
      "year": 2024,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Provided scalable SAE training recipes and evidence of robust, transferable features, which this paper leverages to reliably discover and transfer \u201cknowledge awareness\u201d directions across models."
    }
  ],
  "synthesis_narrative": "Prior work established three pillars that are directly relevant here. First, research on LM self-knowledge showed that models can internally signal when they know or don\u2019t know answers, revealing usable epistemic information in hidden states. Second, mechanistic studies of factual storage found that feed-forward layers act as key\u2013value memories, where entity representations cue retrieval of attributes; this implies that detecting whether an entity has a retrievable memory should predict answerability. Third, causal editing methods like ROME demonstrated that intervening on specific internal representations can predictably alter factual outputs, validating that targeted activation-level manipulations can control behavior. Complementing these, theoretical and empirical advances on superposition argued that many features are entangled and that sparse approaches can disentangle them, while sparse autoencoder work showed that SAEs trained on LM activations yield monosemantic, causally meaningful features and provided scalable training protocols that make such features robust and transferable. Together, these works suggest that an interpretable, sparse feature could encode whether an entity is \u201cknown,\u201d and that causal steering along this feature should modulate answering behavior. The current paper synthesizes these insights by using SAEs to discover entity-recognition directions that reflect self-knowledge, empirically tying them to refusal versus hallucination, and demonstrating causal control\u2014thus providing a mechanistic account and an actionable handle on hallucinations that naturally follows from the prior landscape.",
  "target_paper": {
    "title": "Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models",
    "authors": "Javier Ferrando, Oscar Balcells Obeso, Senthooran Rajamanoharan, Neel Nanda",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Mechanistic Interpretability, Hallucinations, Language Models",
    "abstract": "Hallucinations in large language models are a widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting our ability to solve this problem. Using sparse autoencoders as an interpretability tool, we discover that a key part of these mechanisms is entity recognition, where the model detects if an entity is one it can recall facts about. Sparse autoencoders uncover meaningful directions in the representation space, these detect whether the model recognizes an entity, e.g. detecting it doesn't know about an athlete or a movie. This shows that models can have self-knowledge: internal representations about their own capabilities. These directions are causally relevant: capable of steering the model to refuse to answer questions about known entities, or to hallucinate attributes of unknown entities when it would otherwise refuse. We demonstrate that despite the sparse autoencoders being trained on the base model, these directions have a causa",
    "openreview_id": "WCRQFlji2q",
    "forum_id": "WCRQFlji2q"
  },
  "analysis_timestamp": "2026-01-06T13:43:11.857386"
}
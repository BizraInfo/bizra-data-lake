{
  "prior_works": [
    {
      "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
      "authors": "R. Thomas McCoy, Ellie Pavlick, Tal Linzen",
      "year": 2019,
      "role": "Empirical foundation on syntactic shortcuts",
      "relationship_sentence": "This work showed neural models latch onto shallow syntactic heuristics (HANS), directly motivating the paper\u2019s focus on syntax overriding semantics and inspiring controlled tests for syntax-driven behavior."
    },
    {
      "title": "Annotation Artifacts in Natural Language Inference Data",
      "authors": "Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R. Bowman, Noah A. Smith",
      "year": 2018,
      "role": "Dataset artifact analysis",
      "relationship_sentence": "Established that spurious dataset artifacts can drive model predictions, informing this paper\u2019s framing of syntax\u2013domain correlations as a dataset-induced artifact to be detected and measured."
    },
    {
      "title": "Shortcut Learning in Deep Neural Networks",
      "authors": "Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A. Wichmann",
      "year": 2020,
      "role": "Conceptual framing of spurious correlations",
      "relationship_sentence": "Provided the general theory of shortcut learning that underpins the paper\u2019s claim that models adopt a syntax\u2192domain shortcut that can override semantic understanding."
    },
    {
      "title": "Learning the Difference That Makes a Difference: Counterfactual Data Augmentation for Robustness",
      "authors": "Divyansh Kaushik, Eduard Hovy, Zachary C. Lipton",
      "year": 2020,
      "role": "Methodological precedent for causal probes",
      "relationship_sentence": "Demonstrated using counterfactual/synthetic interventions to break spurious correlations, directly informing this paper\u2019s synthetic training setup to induce and measure syntax\u2013domain effects."
    },
    {
      "title": "SCAN: Measuring compositional generalization by interpreting novel combinations of known primitives",
      "authors": "Brenden M. Lake, Marco Baroni",
      "year": 2018,
      "role": "Synthetic evaluation for controlled generalization tests",
      "relationship_sentence": "Inspired the use of controlled synthetic datasets and splits to isolate surface-form vs semantic generalization, a core methodological choice in this paper\u2019s evaluation design."
    },
    {
      "title": "Scaling Instruction-Finetuned Language Models",
      "authors": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, et al.",
      "year": 2022,
      "role": "Instruction-tuning data source and template influence",
      "relationship_sentence": "Introduced the FLAN instruction-tuning mixture (including FLANv2), whose templated instructions the paper audits to reveal syntax\u2013domain correlations in real training/eval data."
    },
    {
      "title": "Self-Instruct: Aligning Language Models with Self Generated Instructions",
      "authors": "Yizhong Wang, Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, et al.",
      "year": 2023,
      "role": "Construction of templated instruction corpora",
      "relationship_sentence": "Showed large-scale instruction generation via templating, supporting the paper\u2019s hypothesis that prevalent syntactic templates in instruction data can seed syntax\u2013domain shortcuts."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central claim\u2014that language models learn spurious syntax\u2013domain shortcuts that can override instruction semantics\u2014sits at the intersection of three influential threads. First, work on model shortcuts and dataset artifacts (Gururangan et al., 2018; McCoy et al., 2019; Geirhos et al., 2020) established that neural models often exploit superficial cues. HANS, in particular, provided controlled tests for syntactic heuristics, and the broader shortcut-learning framing clarifies why a syntax\u2192domain mapping can dominate semantic reasoning. Second, methodological advances in controlled evaluation (Lake & Baroni, 2018) and causal data interventions (Kaushik et al., 2020) demonstrated how synthetic or counterfactual setups can diagnose \u2013 and manipulate \u2013 specific correlations. The present paper builds on this by constructing a synthetic training corpus that intentionally correlates part-of-speech templates with domains, then quantifies the performance degradation on entity-knowledge tasks across OLMo-2 sizes, thereby causally attributing the failure mode to syntax\u2013domain coupling. Third, recent instruction-tuning pipelines (Chung et al., 2022; Wang et al., 2023) popularized large, templated instruction corpora (e.g., FLAN/FLANv2, Self-Instruct). These sources plausibly embed stable surface-form templates tied to task types or domains. Auditing such datasets, the paper introduces an evaluation framework that detects and measures syntax\u2013domain spurious correlations in trained models, showing the phenomenon on a subset of FLANv2. Together, these prior works directly shape the paper\u2019s conceptual framing (shortcuts), experimental methodology (synthetic/controlled probes), and empirical target (templated instruction datasets), enabling a precise diagnosis of syntax-driven domain misgeneralization in LMs.",
  "analysis_timestamp": "2026-01-07T00:05:12.515151"
}
{
  "prior_works": [
    {
      "title": "Visual Dialog",
      "authors": "Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 M. F. Moura, Devi Parikh, Dhruv Batra",
      "year": 2017,
      "role": "Introduced the multi-round, image-grounded questioner\u2013answerer framework and training strategies for cooperative visual dialog.",
      "relationship_sentence": "IDeal adopts the multi-turn questioner\u2013answerer paradigm from Visual Dialog to iteratively reduce uncertainty, extending it from 2D images to 3D scenes for retrieval refinement."
    },
    {
      "title": "GuessWhat?! Visual object discovery through multi-modal dialog",
      "authors": "Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, Aaron Courville",
      "year": 2017,
      "role": "Established goal-directed questioning to disambiguate a visual target via cooperative dialog.",
      "relationship_sentence": "IDeal\u2019s questioner is inspired by GuessWhat?! to ask discriminative, scene-grounded questions that tease apart candidate 3D scenes."
    },
    {
      "title": "Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information",
      "authors": "Sudha Rao, Hal Daum\u00e9 III",
      "year": 2018,
      "role": "Formalized clarifying question selection with an information-theoretic objective (EVPI) for maximal downstream gain.",
      "relationship_sentence": "IDeal\u2019s retrieval-refinement loop is guided by the principle of choosing questions that maximally reduce ambiguity, echoing EVPI-driven selection."
    },
    {
      "title": "Asking Clarifying Questions in Open-Domain Information-Seeking Conversations",
      "authors": "Alborz Aliannejadi, Hamed Zamani, Fabio Crestani, W. Bruce Croft",
      "year": 2019,
      "role": "Demonstrated that proactive clarification substantially improves retrieval in conversational search and proposed evaluation protocols.",
      "relationship_sentence": "Provides empirical grounding for integrating proactive clarification into IDeal\u2019s retrieval pipeline and for measuring interaction gains."
    },
    {
      "title": "WhittleSearch: Interactive Image Search with Relative Attribute Feedback",
      "authors": "Aditya Kovashka, Devi Parikh, Kristen Grauman",
      "year": 2012,
      "role": "Pioneered iterative, user-in-the-loop feedback to progressively narrow search via targeted comparisons.",
      "relationship_sentence": "IDeal generalizes WhittleSearch\u2019s iterative narrowing to language\u20133D alignment, trading a single-shot query for targeted interaction to converge on the desired scene."
    },
    {
      "title": "Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings",
      "authors": "Kevin Chen, Christopher B. Choy, Manolis Savva, Angel X. Chang, Silvio Savarese",
      "year": 2018,
      "role": "Established joint text\u20133D embeddings enabling text-driven 3D retrieval/generation on shapes.",
      "relationship_sentence": "IDeal builds on the joint embedding paradigm, extending it from object-level shapes to complex scenes and augmenting it with interactive refinement."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al.",
      "year": 2021,
      "role": "Introduced large-scale contrastive language\u2013image pretraining that underpins open-vocabulary alignment and transfers to 3D via 2D supervision.",
      "relationship_sentence": "IDeal leverages CLIP-style text\u2013visual alignment as a backbone, facilitating language supervision transfer to 3D scene encoders used for retrieval."
    }
  ],
  "synthesis_narrative": "IDeal\u2019s core contribution\u2014an interactive refinement loop that improves alignment between textual queries and 3D scenes via a questioner\u2013answerer\u2014sits at the intersection of dialog-based vision systems, clarifying question selection, interactive retrieval, and text\u20133D representation learning. Visual Dialog and GuessWhat?! provide the foundational blueprint for multi-turn, goal-oriented Q/A grounded in visual content, showing how a questioner can strategically probe to reduce uncertainty. This goal-driven dialog perspective is complemented by clarifying-question work in IR: Rao and Daum\u00e9\u2019s EVPI formulation gives a principled objective for selecting questions that maximize downstream utility, while Aliannejadi et al. empirically demonstrate that proactive clarification boosts retrieval effectiveness and provide evaluation practices for conversational search. On the retrieval side, WhittleSearch establishes that iterative, targeted user feedback can systematically prune the hypothesis space, a principle IDeal inherits and operationalizes through language-driven questions rather than attribute comparisons. Finally, Text2Shape and CLIP ground IDeal\u2019s cross-modal modeling: Text2Shape establishes joint text\u20133D embeddings for retrieval at the object level, and CLIP supplies robust language\u2013visual alignment that can be transferred to 3D scene encoders. Together, these works directly enable IDeal\u2019s design: a question-generating agent that, guided by information gain, engages in multi-round interaction to sharpen text\u20133D scene alignment and deliver superior retrieval in realistic, imperfect-query settings.",
  "analysis_timestamp": "2026-01-07T00:05:12.519964"
}
{
  "prior_works": [
    {
      "title": "Can decentralized algorithms outperform centralized algorithms? A case study for decentralized parallel stochastic gradient descent",
      "authors": "Xiangru Lian et al.",
      "year": 2017,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "DSpodFL generalizes the D-PSGD formulation by replacing its fixed, synchronous update-and-mix rule with per-iteration indicator random variables for both local gradient steps and neighbor exchanges, recovering D-PSGD as a special case."
    },
    {
      "title": "Asynchronous Decentralized Parallel Stochastic Gradient Descent",
      "authors": "Xiangru Lian et al.",
      "year": 2018,
      "arxiv_id": "1710.06952",
      "role": "Gap Identification",
      "relationship_sentence": "AD-PSGD highlights heterogeneity and uncoordinated client activity but assumes a specific asynchronous protocol; DSpodFL addresses this gap by providing a unified stochastic-indicator model and convergence guarantees that cover arbitrary, time-varying sporadic computation and communication."
    },
    {
      "title": "Stochastic Gradient Push for Distributed Deep Learning",
      "authors": "Mohamed B. Assran et al.",
      "year": 2019,
      "arxiv_id": "1811.10792",
      "role": "Extension",
      "relationship_sentence": "DSpodFL subsumes SGP by modeling directed/time-varying link activations via random edge indicators, thereby recovering push-sum style decentralized training as a specific instantiation of its unified framework."
    },
    {
      "title": "Randomized Gossip Algorithms",
      "authors": "Stephen Boyd et al.",
      "year": 2006,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "DSpodFL\u2019s representation of pairwise model exchanges as Bernoulli indicator activations directly adopts the randomized gossip abstraction originating from Boyd et al.\u2019s framework."
    },
    {
      "title": "Gossip Learning as a Decentralized Alternative to Federated Learning",
      "authors": "Istv\u00e1n Heged\u0171s et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "GossipFL operationalizes peer-to-peer model averaging in FL, and DSpodFL formalizes and analyzes this decentralized FL regime under heterogeneous, sporadic participation, showing it as a special case of the unified framework."
    },
    {
      "title": "Local SGD Converges Fast and Communicates Little",
      "authors": "Sebastian U. Stich",
      "year": 2019,
      "arxiv_id": "1805.09767",
      "role": "Inspiration",
      "relationship_sentence": "The idea of performing multiple local steps between aggregations in Local SGD motivates DSpodFL\u2019s stochastic modeling of whether a client executes a local gradient step at each iteration, generalizing periodic local computation to sporadic participation."
    }
  ],
  "synthesis_narrative": "Decentralized parallel SGD established a canonical formulation for serverless training over networks using mixing matrices, typically assuming a fixed, synchronous cadence of local gradient updates and neighbor averaging. Randomized gossip introduced the notion that pairwise exchanges occur randomly, with edge activations modeled probabilistically, enabling analysis over time-varying connectivity. Building on gossip, push-sum based stochastic gradient methods extended decentralized training to directed and lossy networks, where mixing weights evolve with the communication pattern. Asynchrony-focused decentralized SGD further exposed the reality of heterogeneous client speeds and uncoordinated progress but analyzed convergence under specific asynchronous protocols. In federated learning, gossip-style peer-to-peer model averaging demonstrated that decentralized learning can replace the central server, though typically with heuristic participation and fixed or scenario-specific exchange patterns. Meanwhile, local SGD showed that allowing multiple local steps between aggregations can drastically reduce communication, but under periodic and centrally coordinated schedules. Together, these works revealed that both computation and communication in decentralized FL are inherently irregular, yet prior analyses treated only fixed or protocol-specific schedules. The natural next step was to unify these strands by explicitly modeling per-iteration sporadicity in both local updates and edge activations via indicator random variables, thereby capturing heterogeneous and time-varying participation. By doing so, one can recover gossip-FL, D-PSGD, push-sum methods, and periodic local-update schemes as special cases, while deriving convergence guarantees that hold across this broader, realistic spectrum of decentralized federated learning behaviors.",
  "target_paper": {
    "title": "Decentralized Sporadic Federated Learning: A Unified Algorithmic Framework with Convergence Guarantees",
    "authors": "Shahryar Zehtabi, Dong-Jun Han, Rohit Parasnis, Seyyedali Hosseinalipour, Christopher Brinton",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Decentralized Federated Learning, Sporadicity, Unified Algorithmic Framework, Convergence Analysis",
    "abstract": "Decentralized federated learning (DFL) captures FL settings where both (i) model updates and (ii) model aggregations are exclusively carried out by the clients without a central server. Existing DFL works have mostly focused on settings where clients conduct a fixed number of local updates between local model exchanges, overlooking heterogeneity and dynamics in communication and computation capabilities. In this work, we propose Decentralized Sporadic Federated Learning ($\\texttt{DSpodFL}$), a DFL methodology built on a generalized notion of *sporadicity* in both local gradient and aggregation processes. $\\texttt{DSpodFL}$ subsumes many existing decentralized optimization methods under a unified algorithmic framework by modeling the per-iteration (i) occurrence of gradient descent at each client and (ii) exchange of models between client pairs as arbitrary indicator random variables, thus capturing *heterogeneous and time-varying* computation/communication scenarios. We analytically ch",
    "openreview_id": "cznqgb4DNv",
    "forum_id": "cznqgb4DNv"
  },
  "analysis_timestamp": "2026-01-06T18:46:21.749792"
}
{
  "prior_works": [
    {
      "title": "Evaluating Large Language Models Trained on Code",
      "authors": "Mark Chen et al.",
      "year": 2021,
      "arxiv_id": "2107.03374",
      "role": "Gap Identification",
      "relationship_sentence": "HumanEval established execution-based evaluation for text-to-code but confines tasks to short, self-contained functions, directly motivating BigCodeBench\u2019s shift to multi-library, multi-call code under complex instructions."
    },
    {
      "title": "Program Synthesis with Large Language Models",
      "authors": "Jacob Austin et al.",
      "year": 2021,
      "arxiv_id": "2108.07732",
      "role": "Gap Identification",
      "relationship_sentence": "MBPP\u2019s simple prompts and single-function Python solutions highlighted the lack of instruction complexity and real-world library usage that BigCodeBench explicitly targets."
    },
    {
      "title": "APPS: A Benchmark for Code Generation",
      "authors": "Dan Hendrycks et al.",
      "year": 2021,
      "arxiv_id": "2105.09938",
      "role": "Gap Identification",
      "relationship_sentence": "APPS scales difficulty but largely measures algorithmic problem solving rather than composing heterogeneous library calls, a key gap BigCodeBench fills."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "arxiv_id": "2210.03629",
      "role": "Inspiration",
      "relationship_sentence": "ReAct\u2019s coupling of deliberate reasoning with tool actions directly inspires BigCodeBench\u2019s emphasis on compositional multi-tool use, now instantiated through executable Python function calls."
    },
    {
      "title": "PAL: Program-Aided Language Models",
      "authors": "Luyu Gao et al.",
      "year": 2023,
      "arxiv_id": "2211.10435",
      "role": "Foundation",
      "relationship_sentence": "PAL showed that generating and executing Python programs improves task solving, providing the foundational idea that practical tasks can be assessed via code execution which BigCodeBench generalizes to diverse tool libraries."
    },
    {
      "title": "Gorilla: Large Language Models Are Connected with Massive APIs",
      "authors": "Shishir G. Patil et al.",
      "year": 2023,
      "arxiv_id": "2305.15334",
      "role": "Related Problem",
      "relationship_sentence": "Gorilla framed API-call generation and grounding to function signatures, which BigCodeBench extends by evaluating composition across multiple real-world Python libraries under complex instructions."
    },
    {
      "title": "AgentBench: Evaluating LLMs as Agents",
      "authors": "Xiao Liu et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "AgentBench\u2019s multi-tool agent evaluation highlighted the need to measure instruction following and tool orchestration, informing BigCodeBench\u2019s code-centric, execution-based assessment of multi-tool composition."
    }
  ],
  "synthesis_narrative": "HumanEval introduced the now-standard paradigm of execution-based evaluation for text-to-code generation but centered on short, self-contained function implementations, while MBPP similarly focused on simple prompts and single-function Python solutions. APPS broadened difficulty and scale yet primarily measured algorithmic problem solving rather than realistic composition over heterogeneous libraries. ReAct demonstrated that interleaving explicit reasoning with tool actions enables multi-step, tool-using behavior, crystallizing the importance of tool orchestration. PAL showed that generating and executing Python programs can materially improve task performance, grounding the idea that code itself can be the medium of problem solving. Gorilla reframed evaluation around correct API invocation and grounding to actual function signatures, pushing attention to real-world interfaces rather than abstract algorithms. AgentBench expanded this lens to agentic settings with multiple tools, emphasizing instruction following and complex task decomposition over single-step calls.\n\nTaken together, these works revealed a clear opportunity: existing code benchmarks lack diverse, compositional tool use under complex, natural instructions, even as tool-oriented reasoning and execution prove crucial. The natural next step is a benchmark that uses executable Python as the action language, requires composing multiple real-world libraries, and evaluates strict instruction adherence through unit-testable outcomes. Building on execution-based evaluation (HumanEval/MBPP/APPS), tool-augmented reasoning (ReAct/PAL), and API-grounded correctness (Gorilla) in multi-tool contexts (AgentBench), BigCodeBench synthesizes these strands to assess how well LLMs can follow complex instructions and orchestrate diverse function calls to solve practical tasks.",
  "target_paper": {
    "title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions",
    "authors": "Terry Yue Zhuo, Vu Minh Chien, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen GONG, James Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, Binyuan Hui, Niklas Muennighoff, David Lo, Daniel Fried, Xiaoning Du, Harm de Vries, Leandro Von Werra",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Code Generation, Tool Use, Instruction Following, Benchmark",
    "abstract": "Task automation has been greatly empowered by the recent advances in Large Language Models (LLMs) via Python code, where the tasks range from software engineering development to general-purpose reasoning. While current benchmarks have shown that LLMs can solve tasks using programs like human developers, the majority of their evaluations are limited to short and self-contained algorithmic tasks or standalone function calls. Solving challenging and practical tasks requires the capability of utilizing **diverse function calls as tools** to efficiently implement functionalities like data analysis and web development. In addition, using multiple tools to solve a task needs compositional reasoning by accurately understanding **complex instructions**. Fulfilling both of these characteristics can pose a great challenge for LLMs. To assess how well LLMs can solve challenging and practical tasks via programs, we introduce BigCodeBench, a benchmark that challenges LLMs to invoke multiple function",
    "openreview_id": "YrycTjllL0",
    "forum_id": "YrycTjllL0"
  },
  "analysis_timestamp": "2026-01-06T15:05:59.970404"
}
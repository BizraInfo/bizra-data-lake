{
  "prior_works": [
    {
      "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm",
      "authors": "David Silver, Thomas Hubert, Julian Schrittwieser, et al.",
      "year": 2017,
      "role": "Methodological foundation (AlphaZero framework)",
      "relationship_sentence": "Provides the self-play + MCTS + neural network training pipeline whose state distributions and optimization dynamics this paper analyzes for Zipfian structure and scaling behavior."
    },
    {
      "title": "Scaling Laws for Neural Language Models",
      "authors": "Jared Kaplan, Sam McCandlish, Tom Henighan, et al.",
      "year": 2020,
      "role": "Empirical scaling law baseline",
      "relationship_sentence": "Establishes power-law loss scaling with model/data/compute that this work tests in a new RL domain and relates to Zipf exponents."
    },
    {
      "title": "Training Compute-Optimal Large Language Models",
      "authors": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, et al.",
      "year": 2022,
      "role": "Refined scaling law characterization",
      "relationship_sentence": "Provides a precise data\u2013parameter scaling framework whose exponents this paper seeks to connect to Zipf\u2019s law in AlphaZero-generated data."
    },
    {
      "title": "Explaining Neural Scaling Laws",
      "authors": "Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, et al.",
      "year": 2021,
      "role": "Theoretical basis (task/quanta spectrum)",
      "relationship_sentence": "Proposes that power-law scaling can emerge from learning a spectrum of tasks/features ordered by frequency/difficulty, underpinning this paper\u2019s \u2018Zipf-distributed quanta learned in descending frequency\u2019 hypothesis."
    },
    {
      "title": "Zipf\u2019s Law in the Popularity Distribution of Chess Openings",
      "authors": "Bernd Blasius, Ralf T\u00f6njes",
      "year": 2009,
      "role": "Empirical link between game trees and Zipf",
      "relationship_sentence": "Shows that the opening-tree structure of chess yields Zipfian popularity distributions, directly motivating the paper\u2019s claim that AlphaZero state frequencies inherit Zipf from tree-like environments."
    },
    {
      "title": "Power-law distributions in empirical data",
      "authors": "Aaron Clauset, Cosma Rohilla Shalizi, M. E. J. Newman",
      "year": 2009,
      "role": "Methodology for fitting/testing power laws",
      "relationship_sentence": "Provides rigorous tools for estimating and validating power-law exponents used to correlate neural scaling exponents with Zipf exponents in AlphaZero data."
    },
    {
      "title": "Inverse Scaling: When Bigger Isn\u2019t Better",
      "authors": "Samuel R. Bowman, Ethan Perez, Ethan Dyer, Eric J. Michaud, Samuel McKenzie, et al. (Inverse Scaling Prize)",
      "year": 2022,
      "role": "Phenomenology of inverse scaling",
      "relationship_sentence": "Documents inverse scaling behaviors in language tasks, supplying conceptual and empirical context for the paper\u2019s observation of inverse scaling regimes in AlphaZero."
    }
  ],
  "synthesis_narrative": "This paper\u2019s core contribution\u2014linking neural scaling in AlphaZero to Zipf\u2019s law through a task/quanta learning model\u2014rests on three pillars drawn from prior work. First, empirical scaling laws from language modeling (Kaplan et al., refined by Hoffmann et al.) establish that loss tends to follow power laws in model size, data, and compute. The present work transposes that lens to reinforcement learning with self-play, asking whether analogous exponents appear and how they originate.\nSecond, theoretical accounts of why scaling laws arise (Bahri et al.) posit that models learn a spectrum of features or tasks in a frequency/difficulty-ordered manner, which can induce power-law learning curves. The authors adapt this idea to a Zipfian \u201cquanta\u201d view of AlphaZero\u2019s state space, testing whether agents reduce loss first on frequent states\u2014thus operationalizing the theory in an RL setting.\nThird, evidence that game environments exhibit Zipf structure (Blasius & T\u00f6njes) motivates the claim that AlphaZero\u2019s training and inference distributions inherit Zipf due to the branching game tree, providing the environmental mechanism feeding the quanta model. Methodologically, robust power-law fitting and validation practices (Clauset\u2013Shalizi\u2013Newman) support reliable estimation and comparison of Zipf and scaling exponents. Finally, recent observations of inverse scaling (Inverse Scaling Prize report) inform the paper\u2019s analysis of when and why larger AlphaZero models may underperform, connecting deviations from monotonic improvement to properties of the Zipfian task spectrum. Together, these works directly enable the paper\u2019s cross-domain synthesis and its empirical tests relating Zipf exponents to neural scaling behavior in board-game RL.",
  "analysis_timestamp": "2026-01-07T00:21:32.264625"
}
{
  "prior_works": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "role": "Foundational vision\u2013language alignment enabling text as semantic descriptors",
      "relationship_sentence": "CRL\u2019s use of descriptive words as a semantic basis is operationalized by CLIP\u2019s shared image\u2013text embedding space, allowing text-derived vectors to define and retrieve task-relevant visual features without explicit labels."
    },
    {
      "title": "Attribute-Based Classification for Zero-Shot Learning",
      "authors": "Christoph H. Lampert, Hannes Nickisch, Stefan Harmeling",
      "year": 2014,
      "role": "Attributes as an explicit semantic basis for representation and transfer",
      "relationship_sentence": "CRL generalizes the attribute-space idea by automatically constructing an attribute-like basis from user-specified criteria via LLM-generated descriptors, avoiding manual attribute engineering while retaining targeted semantics."
    },
    {
      "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
      "authors": "Been Kim et al.",
      "year": 2018,
      "role": "Concept vectors as linear directions in latent space",
      "relationship_sentence": "CRL\u2019s claim that semantics of a space are determined by its basis builds on TCAV\u2019s demonstration that concept directions in representation space capture interpretable semantics that can guide and probe models."
    },
    {
      "title": "Learning to Prompt for Vision-Language Models (CoOp)",
      "authors": "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu",
      "year": 2022,
      "role": "Text prompt adaptation to customize CLIP for downstream tasks",
      "relationship_sentence": "CoOp shows text prompts can tailor CLIP to new tasks with minimal labeling; CRL advances this by constructing a full semantic basis (not just class prompts) from generated descriptions to align representations with arbitrary user criteria."
    },
    {
      "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Images",
      "authors": "Or Patashnik, Zongze Wu, Eli Shechtman, Dani Lischinski, Daniel Cohen-Or",
      "year": 2021,
      "role": "Text embeddings as controllable directions governing semantics",
      "relationship_sentence": "StyleCLIP\u2019s finding that text embeddings define steerable semantic directions motivates CRL\u2019s basis view, where multiple text-derived directions span a customized subspace capturing user-prioritized factors."
    },
    {
      "title": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework",
      "authors": "Irina Higgins et al.",
      "year": 2017,
      "role": "Disentanglement as semantics aligned with axes/basis of latent space",
      "relationship_sentence": "CRL\u2019s premise that a representation\u2019s semantics are set by its basis echoes beta-VAE\u2019s evidence that meaningful factors can align with latent axes, guiding CRL to explicitly construct a task-aligned basis rather than discover it implicitly."
    },
    {
      "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
      "authors": "Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, Sameer Singh",
      "year": 2020,
      "role": "Automatic generation of effective textual prompts",
      "relationship_sentence": "CRL leverages the insight that prompts can be automatically produced; it uses an LLM to generate descriptive texts that seed the semantic basis, reducing annotation costs while improving task alignment."
    }
  ],
  "synthesis_narrative": "CRL\u2019s core contribution\u2014tailoring visual representations to arbitrary user-specified criteria by constructing a semantic basis from descriptive text\u2014sits at the intersection of vision\u2013language alignment, concept-based subspace reasoning, and automatic prompt generation. CLIP provides the enabling substrate: a shared image\u2013text space where text descriptors can serve as actionable semantic vectors, allowing CRL to project or build features aligned with user intent. The attribute-based zero-shot learning line (Lampert et al.) established the power of an explicit semantic basis to control which factors a model attends to; CRL extends this from manually curated attributes to automatically composed, criterion-specific descriptors. Concept-geometry works like TCAV formalized that semantics correspond to linear directions in representation space, directly motivating CRL\u2019s \u201csemantics-as-basis\u201d principle and its construction of customized subspaces. Prompt-learning for CLIP (CoOp) demonstrated that tuning textual prompts can adapt universal embeddings to downstream tasks with low supervision; CRL generalizes from per-class prompts to a basis of multiple concept directions spanning a task-aligned feature subspace. Complementary evidence from StyleCLIP and beta-VAE supports the geometric intuition: text embeddings can steer latent semantics and meaningful factors can align with basis directions, respectively. Finally, AutoPrompt shows that prompts themselves can be automatically synthesized, informing CRL\u2019s use of an LLM to generate descriptive words that approximate the desired basis, thereby reducing annotation/computation while preserving flexible, user-driven control of representation semantics.",
  "analysis_timestamp": "2026-01-07T00:21:32.237271"
}
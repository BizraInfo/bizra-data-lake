{
  "prior_works": [
    {
      "title": "TextWorld: A Learning Environment for Text-Based Games",
      "authors": "Marc-Alexandre C\u00f4t\u00e9 et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "TextWorld\u2019s procedural generation of quests via explicit dependency graphs directly underpins OmniBench\u2019s graph-based subtask composition, enabling controllable task complexity without heavy manual labeling."
    },
    {
      "title": "BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning",
      "authors": "Maxime Chevalier-Boisvert et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "BabyAI\u2019s compositional task design and curriculum with tunable difficulty inspired OmniBench\u2019s controllable-complexity synthesis and subtask-level capability evaluation."
    },
    {
      "title": "VirtualHome: Simulating Household Activities via Programs",
      "authors": "Xavier Puig et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "OmniBench extends VirtualHome\u2019s idea of representing tasks as programs/action-graphs beyond a single domain, using graph-structured subtasks to compose diverse virtual-agent tasks and define graph-based metrics."
    },
    {
      "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
      "authors": "Mohit Shridhar et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "ALFRED\u2019s reliance on substantial human annotation and limited fine-grained capability scoring exposed the need for OmniBench\u2019s automated synthesis and multidimensional, subtask-level evaluation."
    },
    {
      "title": "AgentBench: Evaluating LLMs as Agents",
      "authors": "Liu et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "As a primary prior benchmark for LLM agents, AgentBench motivates OmniBench\u2019s improvements by lacking controllable task complexity and explicit capability decomposition that OmniBench provides via graph-based generation and OmniEval."
    },
    {
      "title": "WebArena: A Realistic Open Web Environment for Building Autonomous Agents",
      "authors": "Zhou et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "WebArena\u2019s realistic but manually curated, variably difficult web tasks highlight the scalability and controllability gap that OmniBench addresses with its self-generating, graph-structured tasks across many scenarios."
    },
    {
      "title": "Procgen Benchmark: Procedurally-Generated Game-Like Tasks for Robust Reinforcement Learning",
      "authors": "Karl Cobbe et al.",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "Procgen\u2019s demonstration that procedural generation yields robust generalization directly motivates OmniBench\u2019s automated pipeline for synthesizing large, diverse task sets with controllable difficulty."
    }
  ],
  "synthesis_narrative": "OmniBench\u2019s core innovation\u2014a self-generating, graph-based benchmark with controllable complexity and multidimensional evaluation\u2014arises from a clear intellectual lineage in procedural, compositional task design and agent evaluation. TextWorld established the foundational idea of encoding tasks as dependency graphs, showing how quests can be procedurally composed and difficulty controlled without heavy annotation; OmniBench generalizes this graph-centric principle to multimodal virtual agents. BabyAI\u2019s compositional curricula and tunable difficulty directly inspired OmniBench\u2019s subtask-level capability design and controllable complexity knobs. VirtualHome contributed the notion that everyday activities can be represented as executable programs/action graphs; OmniBench extends this program-graph representation across many scenarios and layers graph-based metrics on top.\n\nAt the same time, several benchmarks exposed critical gaps that OmniBench explicitly addresses. ALFRED demonstrated the cost of extensive human annotation and the need for fine-grained capability scoring, which OmniBench meets via automated synthesis and OmniEval\u2019s subtask and graph metrics. AgentBench served as a baseline multi-environment evaluation for LLM agents but lacked controllable complexity and systematic capability decomposition\u2014precisely the axes OmniBench formalizes. WebArena offered realistic web interactions but required manual curation and suffered from uneven difficulty, motivating OmniBench\u2019s scalable, controllable generation across 20 scenarios. Finally, Procgen\u2019s success with procedural generation for robust generalization inspired OmniBench\u2019s automated pipeline, enabling 36k graph-structured tasks and improved cross-environment generalization.",
  "analysis_timestamp": "2026-01-06T23:07:19.632243"
}
{
  "prior_works": [
    {
      "title": "An analog of the minimax theorem for vector payoffs",
      "authors": "David Blackwell",
      "year": 1956,
      "role": "Foundational theory (approachability)",
      "relationship_sentence": "Established Blackwell approachability, the game-theoretic backbone behind calibration guarantees that this paper leverages when reducing calibration to an online optimization/approachability problem over convex sets."
    },
    {
      "title": "Asymptotic Calibration",
      "authors": "Dean P. Foster, Rakesh V. Vohra",
      "year": 1998,
      "role": "Foundational calibration result",
      "relationship_sentence": "Provided the first general existence results and constructions for calibrated forecasters via approachability, supplying the conceptual bridge that this work extends to high-dimensional, normed settings."
    },
    {
      "title": "Blackwell Approachability and No-Regret Learning are Equivalent",
      "authors": "Jacob Abernethy, Peter L. Bartlett, Elad Hazan, Alexander Rakhlin",
      "year": 2011,
      "role": "Equivalence linking approachability and online learning",
      "relationship_sentence": "Formally connected approachability (hence calibration) with no-regret online learning, directly enabling the paper\u2019s core reduction from high-dimensional calibration to external regret for online linear optimization."
    },
    {
      "title": "Approachability, Regret and Calibration",
      "authors": "Vianney Perchet",
      "year": 2011,
      "role": "Conceptual unification of calibration and regret",
      "relationship_sentence": "Showed fine-grained relationships among approachability, various regret notions, and calibration, informing the paper\u2019s use of regret guarantees (under dual norms) to certify calibration rates."
    },
    {
      "title": "From External to Internal Regret",
      "authors": "Avrim Blum, Yishay Mansour",
      "year": 2007,
      "role": "Algorithmic conversion to swap/internal regret",
      "relationship_sentence": "Gave reductions from external to internal/swap regret, which this paper implicitly exploits to route external-regret guarantees for OLO into calibration procedures that depend on swap-type guarantees."
    },
    {
      "title": "A decision-theoretic generalization of on-line learning and an application to boosting (Hedge)",
      "authors": "Yoav Freund, Robert E. Schapire",
      "year": 1997,
      "role": "Key external-regret algorithm on the simplex",
      "relationship_sentence": "Provides O(\u221a(T log d)) regret for experts on the simplex, which the paper uses to instantiate its general theorem and recover the d^{O(1/\u03b5^2)} calibration sample complexity in the simplex/\u21131 case."
    },
    {
      "title": "Online Learning and Online Convex Optimization",
      "authors": "Shai Shalev-Shwartz",
      "year": 2012,
      "role": "General OLO framework and norm-based regret bounds",
      "relationship_sentence": "Summarizes mirror-descent/FTRL analyses yielding O(\u221a(\u03c1T)) regret with geometry determined by norms and regularizers, directly matching the paper\u2019s assumption on OLO regret over convex sets with dual-norm-bounded losses."
    },
    {
      "title": "High-dimensional calibration over the simplex (rate d^{O(1/\u03b5^2)})",
      "authors": "Peng",
      "year": 2025,
      "role": "Recent benchmark result recovered",
      "relationship_sentence": "Established the d^{O(1/\u03b5^2)} round complexity for simplex calibration that this paper recovers as a corollary of its general reduction from OLO regret to calibration."
    }
  ],
  "synthesis_narrative": "The core innovation of High-Dimensional Calibration from Swap Regret is a general reduction from online calibration over an arbitrary convex set with respect to a norm to external-regret guarantees for online linear optimization against dual-norm-bounded losses, yielding an exponential-in-\u03c1/\u03b5\u00b2 sample complexity. This builds squarely on Blackwell\u2019s approachability, which underlies calibration algorithms and feasibility guarantees, and on Foster\u2013Vohra\u2019s seminal existence results that framed calibration as an approachability problem. The decisive conceptual link enabling this paper\u2019s reduction is the established equivalence between approachability and no-regret learning, as articulated by Abernethy, Bartlett, Hazan, and Rakhlin, and further elaborated by Perchet\u2019s unification of approachability, regret, and calibration. These works justify translating norm-sensitive OLO regret rates into calibration guarantees.\nAlgorithmically, the paper leverages reductions from external to swap/internal regret (Blum\u2013Mansour), allowing standard OLO tools to drive calibration procedures that hinge on swap-type guarantees. The norm-dependent O(\u221a(\u03c1T)) regret premise aligns with mirror-descent/FTRL analyses summarized by Shalev-Shwartz, where geometry enters through regularizers and dual norms, exactly matching the paper\u2019s setting. Instantiating the framework on the simplex with \u21131/\u2113\u221e geometry, the classical Hedge/experts algorithm (Freund\u2013Schapire) supplies the O(\u221a(T log d)) regret bound, leading directly to the d^{O(1/\u03b5\u00b2)} calibration rate and thereby recovering Peng (2025). Collectively, these prior works provide the theoretical equivalence, algorithmic reductions, and norm-sensitive regret bounds that the paper synthesizes into a clean, dimension-aware calibration guarantee.",
  "analysis_timestamp": "2026-01-07T00:02:04.954038"
}
{
  "prior_works": [
    {
      "title": "Learning Multiple Visual Domains with Residual Adapters",
      "authors": "Sylvestre-Alvise Rebuffi et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Introduced residual adapter modules and explored their placement in ResNets, providing the core idea of layer-inserted adapters that this work generalizes by automatically searching their arrangement for few-shot adaptation."
    },
    {
      "title": "AdaptFormer: Adapting Vision Transformers for Efficient Transfer Learning",
      "authors": "Shoufa Chen et al.",
      "year": 2022,
      "arxiv_id": "2205.13535",
      "role": "Baseline",
      "relationship_sentence": "Established adapter-based transfer for ViTs with fixed, hand-designed placement, serving as the primary manual baseline whose adapter insertion strategy this work replaces with a learned, task-driven search."
    },
    {
      "title": "AdapterDrop: On the Efficiency of Adapters in Transformers",
      "authors": "Philipp R\u00fcckl\u00e9 et al.",
      "year": 2021,
      "arxiv_id": "2104.09864",
      "role": "Gap Identification",
      "relationship_sentence": "Showed that not all layers require adapters and proposed heuristic dropping, directly motivating this paper\u2019s principled NAS to learn which layers need adapters versus being frozen or fine-tuned."
    },
    {
      "title": "SNAS: Stochastic Neural Architecture Search",
      "authors": "Sirui Xie et al.",
      "year": 2019,
      "arxiv_id": "1812.09926",
      "role": "Foundation",
      "relationship_sentence": "Provided the stochastic, differentiable NAS framework for optimizing discrete architectural choices, which this work adapts to select layer-wise adapter placement and freeze/fine-tune decisions under few-shot constraints."
    },
    {
      "title": "Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need",
      "authors": "Yonglong Tian et al.",
      "year": 2020,
      "arxiv_id": "2003.11539",
      "role": "Foundation",
      "relationship_sentence": "Demonstrated that strong pretraining with minimal adaptation is highly competitive, crystallizing the problem of designing effective fine-tuning strategies that this paper automates via search."
    },
    {
      "title": "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples",
      "authors": "Eleni Triantafillou et al.",
      "year": 2020,
      "arxiv_id": "1903.03096",
      "role": "Foundation",
      "relationship_sentence": "Defined a realistic, multi-domain few-shot evaluation protocol that this work explicitly targets and reports state-of-the-art results on, grounding the problem formulation and metrics."
    }
  ],
  "synthesis_narrative": "Residual adapters were introduced to enable lightweight, layer-inserted adaptation in deep networks, with early studies examining where in ResNets such modules should be placed to best handle domain shifts. Building on this, AdaptFormer established adapters as a practical, parameter-efficient mechanism for Vision Transformers, using fixed, hand-crafted choices for where to insert the modules along depth. Complementing these, AdapterDrop showed that inserting adapters everywhere is unnecessary and that dropping some layers can retain performance\u2014albeit via heuristics rather than learning the best configuration. Independently, stochastic neural architecture search provided a way to optimize discrete architectural decisions through differentiable, sampling-based relaxations, enabling efficient exploration of large combinatorial design spaces. Meanwhile, transfer-based few-shot works revealed that strong pretraining plus minimal adaptation is highly competitive, focusing attention on which parts of a pretrained model to update versus freeze for rapid generalization. Meta-Dataset then codified a challenging, multi-domain few-shot evaluation regime that stresses the robustness of adaptation strategies.\nThese strands collectively exposed a clear opportunity: adapters are effective, placement matters, not all layers need updating, and discrete design choices can be optimized via stochastic NAS. The present work synthesizes these insights by treating the few-shot adaptation recipe itself as an architecture to search\u2014jointly deciding where to place adapters and which layers to freeze or fine-tune\u2014yielding task-driven configurations that outperform hand-engineered strategies under Meta-Dataset-style evaluation.",
  "target_paper": {
    "title": "Neural Fine-Tuning Search for Few-Shot Learning",
    "authors": "Panagiotis Eustratiadis, \u0141ukasz Dudziak, Da Li, Timothy Hospedales",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "stochastic, neural, architecture, search, few, shot, learning, adapters",
    "abstract": "In few-shot recognition, a classifier that has been trained on one set of classes is required to rapidly adapt and generalize to a disjoint, novel set of classes. To that end, recent studies have shown the efficacy of fine-tuning with carefully-crafted adaptation architectures. However this raises the question of: How can one design the optimal adaptation strategy? In this paper, we study this question through the lens of neural architecture search (NAS). Given a pre-trained neural network, our algorithm discovers the optimal arrangement of adapters, which layers to keep frozen, and which to fine-tune. We demonstrate the generality of our NAS method by applying it to both residual networks and vision transformers and report state-of-the-art performance on Meta-Dataset and Meta-Album.",
    "openreview_id": "T7YV5UZKBc",
    "forum_id": "T7YV5UZKBc"
  },
  "analysis_timestamp": "2026-01-06T18:15:56.146937"
}
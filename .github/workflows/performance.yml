# BIZRA Data Lake - Performance Benchmark CI Pipeline
# Wires latency/throughput benchmarks into CI with regression gates
#
# Standing on Giants: Continuous Benchmarking (2018) | pytest-benchmark patterns
# Constitutional Constraint: P95 < 500ms, Throughput >= 10 req/s, Memory < 4GB
#
# Architecture:
# +-----------------------------------------------------------------------------+
# |  SETUP  ->  BENCHMARKS  ->  COMPARISON  ->  GATE  ->  REPORT  ->  ARTIFACT  |
# +-----------------------------------------------------------------------------+

name: Performance

on:
  push:
    branches: [main, master]
    paths:
      - 'core/**'
      - 'tools/**'
      - 'scripts/**'
      - 'pyproject.toml'
  pull_request:
    branches: [main, master]
    paths:
      - 'core/**'
      - 'tools/**'
      - 'scripts/**'
      - 'pyproject.toml'
  workflow_dispatch:
    inputs:
      p95_latency_threshold_ms:
        description: 'P95 latency threshold (ms)'
        required: false
        default: '500'
        type: string
      throughput_threshold_rps:
        description: 'Throughput threshold (req/s)'
        required: false
        default: '10'
        type: string
      memory_threshold_gb:
        description: 'Memory threshold (GB)'
        required: false
        default: '4'
        type: string
      regression_tolerance_pct:
        description: 'Regression tolerance (%)'
        required: false
        default: '10'
        type: string
      run_full_suite:
        description: 'Run full benchmark suite (slower)'
        required: false
        default: 'false'
        type: boolean

env:
  PYTHON_VERSION: '3.12'
  # Default thresholds (overridden by workflow_dispatch inputs)
  P95_LATENCY_THRESHOLD_MS: ${{ github.event.inputs.p95_latency_threshold_ms || '500' }}
  THROUGHPUT_THRESHOLD_RPS: ${{ github.event.inputs.throughput_threshold_rps || '10' }}
  MEMORY_THRESHOLD_GB: ${{ github.event.inputs.memory_threshold_gb || '4' }}
  REGRESSION_TOLERANCE_PCT: ${{ github.event.inputs.regression_tolerance_pct || '10' }}

concurrency:
  group: perf-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ===========================================================================
  # Job 1: Inference Latency Benchmark
  # ===========================================================================
  inference-latency:
    name: Inference Latency (P50/P95/P99)
    runs-on: ubuntu-latest
    outputs:
      p50_ms: ${{ steps.benchmark.outputs.p50_ms }}
      p95_ms: ${{ steps.benchmark.outputs.p95_ms }}
      p99_ms: ${{ steps.benchmark.outputs.p99_ms }}
      mean_ms: ${{ steps.benchmark.outputs.mean_ms }}
      gate_passed: ${{ steps.benchmark.outputs.gate_passed }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -e ".[dev]"
          pip install psutil pytest-benchmark

      - name: Download baseline (if exists)
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: perf-baseline-inference
          path: .perf-baseline

      - name: Run Inference Latency Benchmark
        id: benchmark
        run: |
          python scripts/ci_perf_benchmark.py \
            --benchmark inference-latency \
            --iterations 50 \
            --warmup 5 \
            --p95-threshold ${{ env.P95_LATENCY_THRESHOLD_MS }} \
            --regression-tolerance ${{ env.REGRESSION_TOLERANCE_PCT }} \
            --baseline-path .perf-baseline/inference-latency.json \
            --output-path results/inference-latency.json \
            --json
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: perf-results-inference-latency
          path: results/inference-latency.json
          retention-days: 30

  # ===========================================================================
  # Job 2: Apex/Throughput Benchmark
  # ===========================================================================
  apex-throughput:
    name: Apex Throughput (req/s)
    runs-on: ubuntu-latest
    outputs:
      qps: ${{ steps.benchmark.outputs.qps }}
      batch_efficiency: ${{ steps.benchmark.outputs.batch_efficiency }}
      gate_passed: ${{ steps.benchmark.outputs.gate_passed }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -e ".[dev]"
          pip install psutil

      - name: Download baseline (if exists)
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: perf-baseline-throughput
          path: .perf-baseline

      - name: Run Throughput Benchmark
        id: benchmark
        run: |
          python scripts/ci_perf_benchmark.py \
            --benchmark apex-throughput \
            --iterations 100 \
            --warmup 10 \
            --throughput-threshold ${{ env.THROUGHPUT_THRESHOLD_RPS }} \
            --regression-tolerance ${{ env.REGRESSION_TOLERANCE_PCT }} \
            --baseline-path .perf-baseline/apex-throughput.json \
            --output-path results/apex-throughput.json \
            --json
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: perf-results-apex-throughput
          path: results/apex-throughput.json
          retention-days: 30

  # ===========================================================================
  # Job 3: Memory Usage Benchmark
  # ===========================================================================
  memory-usage:
    name: Memory Usage (Peak MB)
    runs-on: ubuntu-latest
    outputs:
      peak_memory_mb: ${{ steps.benchmark.outputs.peak_memory_mb }}
      avg_memory_mb: ${{ steps.benchmark.outputs.avg_memory_mb }}
      gate_passed: ${{ steps.benchmark.outputs.gate_passed }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -e ".[dev]"
          pip install psutil memory-profiler

      - name: Download baseline (if exists)
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: perf-baseline-memory
          path: .perf-baseline

      - name: Run Memory Usage Benchmark
        id: benchmark
        run: |
          python scripts/ci_perf_benchmark.py \
            --benchmark memory-usage \
            --iterations 20 \
            --warmup 3 \
            --memory-threshold ${{ env.MEMORY_THRESHOLD_GB }} \
            --regression-tolerance ${{ env.REGRESSION_TOLERANCE_PCT }} \
            --baseline-path .perf-baseline/memory-usage.json \
            --output-path results/memory-usage.json \
            --json
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: perf-results-memory-usage
          path: results/memory-usage.json
          retention-days: 30

  # ===========================================================================
  # Job 4: Startup Time Benchmark
  # ===========================================================================
  startup-time:
    name: Cold Start Latency
    runs-on: ubuntu-latest
    outputs:
      cold_start_ms: ${{ steps.benchmark.outputs.cold_start_ms }}
      warm_start_ms: ${{ steps.benchmark.outputs.warm_start_ms }}
      gate_passed: ${{ steps.benchmark.outputs.gate_passed }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -e ".[dev]"
          pip install psutil

      - name: Download baseline (if exists)
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          name: perf-baseline-startup
          path: .perf-baseline

      - name: Run Startup Time Benchmark
        id: benchmark
        run: |
          python scripts/ci_perf_benchmark.py \
            --benchmark startup-time \
            --iterations 10 \
            --warmup 2 \
            --startup-threshold 5000 \
            --regression-tolerance ${{ env.REGRESSION_TOLERANCE_PCT }} \
            --baseline-path .perf-baseline/startup-time.json \
            --output-path results/startup-time.json \
            --json
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: perf-results-startup-time
          path: results/startup-time.json
          retention-days: 30

  # ===========================================================================
  # Job 5: Performance Gate (Aggregates all results)
  # ===========================================================================
  performance-gate:
    name: Performance Gate
    runs-on: ubuntu-latest
    needs: [inference-latency, apex-throughput, memory-usage, startup-time]
    if: always()
    outputs:
      overall_passed: ${{ steps.gate.outputs.overall_passed }}
      summary: ${{ steps.gate.outputs.summary }}
    steps:
      - uses: actions/checkout@v4

      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: perf-results-*
          path: results
          merge-multiple: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: pip install psutil

      - name: Evaluate Performance Gate
        id: gate
        run: |
          python << 'EOF'
          import json
          import os
          import sys
          from pathlib import Path

          results_dir = Path("results")
          gates_passed = []
          gates_failed = []
          summary_lines = []

          thresholds = {
              "p95_latency_ms": float(os.environ.get("P95_LATENCY_THRESHOLD_MS", "500")),
              "throughput_rps": float(os.environ.get("THROUGHPUT_THRESHOLD_RPS", "10")),
              "memory_gb": float(os.environ.get("MEMORY_THRESHOLD_GB", "4")),
          }

          # Check inference latency
          inf_file = results_dir / "inference-latency.json"
          if inf_file.exists():
              data = json.loads(inf_file.read_text())
              p95 = data.get("results", {}).get("p95_ms", 0)
              passed = p95 <= thresholds["p95_latency_ms"]
              status = "PASS" if passed else "FAIL"
              summary_lines.append(f"Inference P95: {p95:.2f}ms (threshold: {thresholds['p95_latency_ms']}ms) - {status}")
              (gates_passed if passed else gates_failed).append("inference-latency")

          # Check throughput
          tput_file = results_dir / "apex-throughput.json"
          if tput_file.exists():
              data = json.loads(tput_file.read_text())
              qps = data.get("results", {}).get("qps", 0)
              passed = qps >= thresholds["throughput_rps"]
              status = "PASS" if passed else "FAIL"
              summary_lines.append(f"Throughput: {qps:.2f} req/s (threshold: {thresholds['throughput_rps']} req/s) - {status}")
              (gates_passed if passed else gates_failed).append("apex-throughput")

          # Check memory
          mem_file = results_dir / "memory-usage.json"
          if mem_file.exists():
              data = json.loads(mem_file.read_text())
              peak_mb = data.get("results", {}).get("peak_memory_mb", 0)
              peak_gb = peak_mb / 1024
              passed = peak_gb <= thresholds["memory_gb"]
              status = "PASS" if passed else "FAIL"
              summary_lines.append(f"Peak Memory: {peak_gb:.2f}GB (threshold: {thresholds['memory_gb']}GB) - {status}")
              (gates_passed if passed else gates_failed).append("memory-usage")

          # Check startup
          startup_file = results_dir / "startup-time.json"
          if startup_file.exists():
              data = json.loads(startup_file.read_text())
              cold_ms = data.get("results", {}).get("cold_start_ms", 0)
              passed = cold_ms <= 5000  # 5 second startup threshold
              status = "PASS" if passed else "FAIL"
              summary_lines.append(f"Cold Start: {cold_ms:.2f}ms (threshold: 5000ms) - {status}")
              (gates_passed if passed else gates_failed).append("startup-time")

          overall_passed = len(gates_failed) == 0
          summary = "\n".join(summary_lines)

          print("=" * 70)
          print("PERFORMANCE GATE EVALUATION")
          print("=" * 70)
          for line in summary_lines:
              print(f"  {line}")
          print("=" * 70)
          print(f"Gates Passed: {len(gates_passed)}")
          print(f"Gates Failed: {len(gates_failed)}")
          print(f"Overall: {'PASS' if overall_passed else 'FAIL'}")
          print("=" * 70)

          # Write outputs
          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"overall_passed={'true' if overall_passed else 'false'}\n")
              # Escape newlines for multiline output
              escaped_summary = summary.replace('\n', '%0A')
              f.write(f"summary={escaped_summary}\n")

          sys.exit(0 if overall_passed else 1)
          EOF
        env:
          P95_LATENCY_THRESHOLD_MS: ${{ env.P95_LATENCY_THRESHOLD_MS }}
          THROUGHPUT_THRESHOLD_RPS: ${{ env.THROUGHPUT_THRESHOLD_RPS }}
          MEMORY_THRESHOLD_GB: ${{ env.MEMORY_THRESHOLD_GB }}

      - name: Performance Gate Summary
        run: |
          echo "## Performance Gate Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Thresholds" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Threshold |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-----------|" >> $GITHUB_STEP_SUMMARY
          echo "| P95 Latency | ${{ env.P95_LATENCY_THRESHOLD_MS }}ms |" >> $GITHUB_STEP_SUMMARY
          echo "| Throughput | ${{ env.THROUGHPUT_THRESHOLD_RPS }} req/s |" >> $GITHUB_STEP_SUMMARY
          echo "| Memory | ${{ env.MEMORY_THRESHOLD_GB }}GB |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Results" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Inference Latency | ${{ needs.inference-latency.outputs.gate_passed == 'true' && 'PASS' || 'FAIL' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Apex Throughput | ${{ needs.apex-throughput.outputs.gate_passed == 'true' && 'PASS' || 'FAIL' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Memory Usage | ${{ needs.memory-usage.outputs.gate_passed == 'true' && 'PASS' || 'FAIL' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Startup Time | ${{ needs.startup-time.outputs.gate_passed == 'true' && 'PASS' || 'FAIL' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Detailed Metrics" >> $GITHUB_STEP_SUMMARY
          echo "- **Inference P95**: ${{ needs.inference-latency.outputs.p95_ms }}ms" >> $GITHUB_STEP_SUMMARY
          echo "- **Inference P99**: ${{ needs.inference-latency.outputs.p99_ms }}ms" >> $GITHUB_STEP_SUMMARY
          echo "- **Throughput**: ${{ needs.apex-throughput.outputs.qps }} req/s" >> $GITHUB_STEP_SUMMARY
          echo "- **Peak Memory**: ${{ needs.memory-usage.outputs.peak_memory_mb }}MB" >> $GITHUB_STEP_SUMMARY
          echo "- **Cold Start**: ${{ needs.startup-time.outputs.cold_start_ms }}ms" >> $GITHUB_STEP_SUMMARY

  # ===========================================================================
  # Job 6: Generate Performance Report
  # ===========================================================================
  generate-report:
    name: Generate Report
    runs-on: ubuntu-latest
    needs: [performance-gate]
    if: always()
    steps:
      - uses: actions/checkout@v4

      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: perf-results-*
          path: results
          merge-multiple: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Generate Markdown Report
        run: |
          python << 'EOF'
          import json
          import os
          from datetime import datetime, timezone
          from pathlib import Path

          results_dir = Path("results")
          report_lines = [
              "# BIZRA Performance Benchmark Report",
              "",
              f"**Generated**: {datetime.now(timezone.utc).isoformat()}",
              f"**Commit**: ${{ github.sha }}",
              f"**Branch**: ${{ github.ref_name }}",
              "",
              "## Summary",
              "",
          ]

          # Load all results
          benchmarks = {}
          for json_file in results_dir.glob("*.json"):
              try:
                  data = json.loads(json_file.read_text())
                  benchmarks[json_file.stem] = data
              except Exception as e:
                  print(f"Warning: Could not load {json_file}: {e}")

          # Inference Latency Section
          if "inference-latency" in benchmarks:
              data = benchmarks["inference-latency"]
              results = data.get("results", {})
              report_lines.extend([
                  "### Inference Latency",
                  "",
                  "| Metric | Value |",
                  "|--------|-------|",
                  f"| Mean | {results.get('mean_ms', 'N/A'):.2f}ms |" if results.get('mean_ms') else "| Mean | N/A |",
                  f"| Median | {results.get('median_ms', 'N/A'):.2f}ms |" if results.get('median_ms') else "| Median | N/A |",
                  f"| P95 | {results.get('p95_ms', 'N/A'):.2f}ms |" if results.get('p95_ms') else "| P95 | N/A |",
                  f"| P99 | {results.get('p99_ms', 'N/A'):.2f}ms |" if results.get('p99_ms') else "| P99 | N/A |",
                  f"| Min | {results.get('min_ms', 'N/A'):.2f}ms |" if results.get('min_ms') else "| Min | N/A |",
                  f"| Max | {results.get('max_ms', 'N/A'):.2f}ms |" if results.get('max_ms') else "| Max | N/A |",
                  "",
              ])

          # Throughput Section
          if "apex-throughput" in benchmarks:
              data = benchmarks["apex-throughput"]
              results = data.get("results", {})
              report_lines.extend([
                  "### Throughput",
                  "",
                  "| Metric | Value |",
                  "|--------|-------|",
                  f"| QPS | {results.get('qps', 'N/A'):.2f} req/s |" if results.get('qps') else "| QPS | N/A |",
                  f"| Batch Efficiency | {results.get('batch_efficiency', 'N/A'):.1f}% |" if results.get('batch_efficiency') else "| Batch Efficiency | N/A |",
                  "",
              ])

          # Memory Section
          if "memory-usage" in benchmarks:
              data = benchmarks["memory-usage"]
              results = data.get("results", {})
              report_lines.extend([
                  "### Memory Usage",
                  "",
                  "| Metric | Value |",
                  "|--------|-------|",
                  f"| Peak | {results.get('peak_memory_mb', 'N/A'):.2f}MB |" if results.get('peak_memory_mb') else "| Peak | N/A |",
                  f"| Average | {results.get('avg_memory_mb', 'N/A'):.2f}MB |" if results.get('avg_memory_mb') else "| Average | N/A |",
                  "",
              ])

          # Startup Section
          if "startup-time" in benchmarks:
              data = benchmarks["startup-time"]
              results = data.get("results", {})
              report_lines.extend([
                  "### Startup Time",
                  "",
                  "| Metric | Value |",
                  "|--------|-------|",
                  f"| Cold Start | {results.get('cold_start_ms', 'N/A'):.2f}ms |" if results.get('cold_start_ms') else "| Cold Start | N/A |",
                  f"| Warm Start | {results.get('warm_start_ms', 'N/A'):.2f}ms |" if results.get('warm_start_ms') else "| Warm Start | N/A |",
                  "",
              ])

          # Regression Analysis Section
          report_lines.extend([
              "## Regression Analysis",
              "",
          ])

          for name, data in benchmarks.items():
              regression = data.get("regression", {})
              if regression:
                  status = "Regressed" if regression.get("regressed") else "OK"
                  change = regression.get("change_percent", 0)
                  report_lines.append(f"- **{name}**: {status} ({change:+.1f}%)")

          report_lines.extend([
              "",
              "---",
              "",
              "*Standing on Giants: Continuous Benchmarking (2018) | pytest-benchmark patterns*",
          ])

          # Write report
          report_path = Path("results/PERFORMANCE_REPORT.md")
          report_path.write_text("\n".join(report_lines))
          print(f"Report written to {report_path}")

          # Also output to step summary
          summary_path = Path(os.environ.get("GITHUB_STEP_SUMMARY", "/dev/null"))
          with open(summary_path, "a") as f:
              f.write("\n".join(report_lines))
          EOF
        env:
          GITHUB_STEP_SUMMARY: ${{ github.step_summary }}

      - name: Upload Performance Report
        uses: actions/upload-artifact@v4
        with:
          name: performance-report
          path: results/PERFORMANCE_REPORT.md
          retention-days: 90

      - name: Upload Combined Results
        uses: actions/upload-artifact@v4
        with:
          name: performance-results-combined
          path: results/
          retention-days: 90

  # ===========================================================================
  # Job 7: Update Baseline (only on main branch merge)
  # ===========================================================================
  update-baseline:
    name: Update Baseline
    runs-on: ubuntu-latest
    needs: [performance-gate]
    if: github.ref == 'refs/heads/main' && needs.performance-gate.outputs.overall_passed == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: perf-results-*
          path: results
          merge-multiple: true

      - name: Upload as new baseline
        uses: actions/upload-artifact@v4
        with:
          name: perf-baseline-inference
          path: results/inference-latency.json
          retention-days: 365
          overwrite: true

      - name: Upload throughput baseline
        uses: actions/upload-artifact@v4
        with:
          name: perf-baseline-throughput
          path: results/apex-throughput.json
          retention-days: 365
          overwrite: true

      - name: Upload memory baseline
        uses: actions/upload-artifact@v4
        with:
          name: perf-baseline-memory
          path: results/memory-usage.json
          retention-days: 365
          overwrite: true

      - name: Upload startup baseline
        uses: actions/upload-artifact@v4
        with:
          name: perf-baseline-startup
          path: results/startup-time.json
          retention-days: 365
          overwrite: true

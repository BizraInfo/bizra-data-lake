{
  "prior_works": [
    {
      "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
      "authors": "Can Xu et al.",
      "year": 2023,
      "arxiv_id": "2304.12244",
      "role": "Extension",
      "relationship_sentence": "WizardMath directly extends WizardLM\u2019s Evol-Instruct paradigm by evolving math-specific instructions and reusing the evolution signals as feedback to drive its RLEIF training."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "arxiv_id": "2203.02155",
      "role": "Foundation",
      "relationship_sentence": "WizardMath adopts the RLHF recipe (SFT \u2192 reward modeling \u2192 PPO) established by InstructGPT, but replaces human preferences with Evol-Instruct-derived feedback tailored for mathematics."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "arxiv_id": "2212.08073",
      "role": "Inspiration",
      "relationship_sentence": "The shift from human labels to AI feedback in Constitutional AI directly motivates WizardMath\u2019s RL from Evol-Instruct Feedback, where automatically generated judgments supervise policy optimization."
    },
    {
      "title": "Self-Consistent Reasoners: STaR\u2014Bootstrapping Reasoning with Reasoning",
      "authors": "Ethan Zelikman et al.",
      "year": 2022,
      "arxiv_id": "2203.14465",
      "role": "Inspiration",
      "relationship_sentence": "STaR\u2019s core insight that supervising intermediate rationales improves reasoning underpins WizardMath\u2019s use of process supervision and step-level signals within its RL pipeline."
    },
    {
      "title": "Training Verifiers to Solve Math Word Problems (GSM8K)",
      "authors": "Karl Cobbe et al.",
      "year": 2021,
      "arxiv_id": "2110.14168",
      "role": "Foundation",
      "relationship_sentence": "GSM8K defines the grade-school math reasoning task and answer-checking protocol that WizardMath uses to evolve instructions and to generate outcome/process feedback for RLEIF."
    },
    {
      "title": "Measuring Mathematical Problem Solving With the MATH Dataset",
      "authors": "Dan Hendrycks et al.",
      "year": 2021,
      "arxiv_id": "2103.03874",
      "role": "Foundation",
      "relationship_sentence": "MATH provides competition-level problems and verifiable solutions that WizardMath leverages both to create challenging evolved instructions and to supervise process- and outcome-level rewards."
    }
  ],
  "synthesis_narrative": "Evol-Instruct introduced by WizardLM demonstrated that instructions can be automatically evolved to become more compositional and challenging, yielding higher-quality supervision signals for instruction-following models. InstructGPT established the practical pipeline for reinforcement learning from human feedback\u2014supervised fine-tuning followed by reward modeling and PPO\u2014that operationalized preference-based alignment at scale. Constitutional AI generalized RLHF by replacing costly human labels with AI feedback, showing that preference signals derived from model-based judges can effectively guide policy improvement. STaR revealed that supervising intermediate reasoning steps and bootstrapping from rationales substantially improves a model\u2019s ability to perform multi-step reasoning. GSM8K defined a clean, verifiable benchmark for grade-school math word problems with robust answer checking, while the MATH dataset supplied a harder, competition-level setting with formal solutions suitable for verifying both final answers and intermediate reasoning.\nCollectively, these works suggested a pathway to scalable math specialization: use automated instruction evolution to generate harder math tasks and accompanying feedback; exploit verifiable datasets to check both outcomes and steps; and drive learning via an RLHF-style loop but with AI-generated, process-aware feedback rather than human labels. WizardMath synthesizes these pieces by turning instruction evolution into a source of structured preference/process signals and embedding them in an RL framework (RLEIF), thereby unifying automatic data evolution, AI feedback, and process supervision to markedly strengthen mathematical reasoning.",
  "target_paper": {
    "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct",
    "authors": "Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-Guang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Yansong Tang, Dongmei Zhang",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Mathematical Reasoning, Evol-Instruct, Reinforcement Learning",
    "abstract": "Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical reasoning abilities of LLMs, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses all other open-source LLMs by a substantial margin. Furthermore, WizardMath 70B even outperforms ChatGPT-3.5, Claude Instant, Gemini Pro and Mistral Medium. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exce",
    "openreview_id": "mMPMHWOdOy",
    "forum_id": "mMPMHWOdOy"
  },
  "analysis_timestamp": "2026-01-06T19:50:09.348470"
}
{
  "prior_works": [
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "arxiv_id": "2210.03629",
      "role": "Inspiration",
      "relationship_sentence": "WebAgent operationalizes ReAct\u2019s interleaving of reasoning and acting by first producing explicit canonical sub-instructions that structure its planner before execution."
    },
    {
      "title": "PAL: Program-Aided Language Models",
      "authors": "Luyu Gao et al.",
      "year": 2023,
      "arxiv_id": "2211.10435",
      "role": "Extension",
      "relationship_sentence": "WebAgent extends PAL\u2019s executable-Python paradigm by synthesizing grounded browser-control programs that call a DOM/interaction API to perform web actions."
    },
    {
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances",
      "authors": "Michael Ahn et al.",
      "year": 2022,
      "arxiv_id": "2204.01691",
      "role": "Inspiration",
      "relationship_sentence": "WebAgent adopts SayCan\u2019s separation of high-level language plans from grounded low-level skills, mapping sub-instructions to callable browser primitives with feasibility checks."
    },
    {
      "title": "LongT5: Efficient Text-to-Text Transformer for Long Sequences",
      "authors": "Mandy Guo et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "WebAgent\u2019s HTML-T5 directly builds on LongT5\u2019s local+global attention and long-span denoising, retraining the recipe on HTML to summarize long DOM pages into task-relevant snippets."
    },
    {
      "title": "WebGPT: Browser-assisted Question-Answering with Human Feedback",
      "authors": "Reiichiro Nakano et al.",
      "year": 2021,
      "arxiv_id": "2112.09332",
      "role": "Gap Identification",
      "relationship_sentence": "WebAgent addresses WebGPT\u2019s limitations\u2014textual browsing actions and context bloat\u2014by replacing free-form actions with code-based execution and adding long-HTML summarization for grounding."
    },
    {
      "title": "Mind2Web: Towards a Generalist Agent for the Web",
      "authors": "Shuyan Zhou et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "WebAgent targets the Mind2Web problem formulation of cross-website instruction following and evaluates/trains in that setup, motivating its planning and long-context HTML modules."
    }
  ],
  "synthesis_narrative": "ReAct demonstrated that large language models can be more effective tool users when their internal deliberation is externalized as explicit thought\u2013action traces, showing the value of decomposing a task into intermediate steps before each action. PAL showed that having the model generate executable Python to call tools yields reliable, verifiable problem solving, establishing code generation as a robust control interface. SayCan introduced a separation between high-level language plans and grounded low-level skills, with an affordance check that constrains execution to feasible actions. LongT5 provided an architectural and training recipe\u2014local and global attention with long-span denoising\u2014that scales text-to-text models to very long inputs, enabling faithful summarization over long sequences. WebGPT pioneered browser-augmented LMs for web tasks but relied on free-form textual actions and suffered from long-context accumulation on open websites. Mind2Web formalized generalist web instruction following across diverse real sites, with multi-step trajectories and DOM-grounded actions that stress both planning and long-HTML understanding. Together, these works revealed a gap: real-web automation needs explicit multi-step planning, long-context HTML comprehension, and grounded, verifiable execution. WebAgent synthesizes these threads by turning ReAct-style stepwise reasoning into canonical sub-instructions (\u00e0 la SayCan\u2019s plan/skill split), using a LongT5-inspired HTML-T5 to condense long DOMs into task-relevant snippets, and extending PAL\u2019s executable-code control to synthesize Python programs that call browser/DOM APIs\u2014directly addressing the shortcomings observed in WebGPT and meeting the Mind2Web-style generalist setting.",
  "target_paper": {
    "title": "A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis",
    "authors": "Izzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "Web Navigation, Web Automation, Large Language Models, Language Model Agents, Tool Use, Program Synthesis",
    "abstract": "Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation.\nHowever, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.\nWe introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions.\nWebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those.\nWe design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.\nWe empirically demonstrate that our modular recipe improves the success on real websites by ov",
    "openreview_id": "9JQtrumvg8",
    "forum_id": "9JQtrumvg8"
  },
  "analysis_timestamp": "2026-01-06T22:40:43.713783"
}
{
  "prior_works": [
    {
      "title": "Learning to Play Minecraft with Video PreTraining",
      "authors": "Bowen Baker et al.",
      "year": 2022,
      "arxiv_id": "2206.11795",
      "role": "Baseline",
      "relationship_sentence": "This work established the dominant Minecraft imitation-learning baseline by training policies from labeled human play, which the current paper directly challenges by removing the need for action/text annotations and outperforming it while conditioning on reference videos as instructions."
    },
    {
      "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge",
      "authors": "Linxi (Jim) Fan et al.",
      "year": 2022,
      "arxiv_id": "2206.08853",
      "role": "Gap Identification",
      "relationship_sentence": "MineDojo (via MineCLIP) framed open-ended Minecraft goals using video\u2013text pairs, highlighting the reliance on expensive text-gameplay alignment that this paper explicitly avoids by using videos themselves as instructions and learning an instruction encoder without text."
    },
    {
      "title": "Learning Latent Plans from Play",
      "authors": "Corey Lynch et al.",
      "year": 2019,
      "arxiv_id": "1903.01973",
      "role": "Inspiration",
      "relationship_sentence": "This paper showed that unlabeled play can supervise goal-conditioned control by extracting a latent plan from a demo snippet, directly inspiring the idea of conditioning a policy on a compact representation of a reference video segment."
    },
    {
      "title": "VIMA: General Robot Manipulation with Multimodal Prompts",
      "authors": "Yunfan Jiang et al.",
      "year": 2023,
      "arxiv_id": "2210.03079",
      "role": "Foundation",
      "relationship_sentence": "VIMA introduced the formulation of following short visual demonstrations as task instructions via a learned prompt encoder, providing the core modality\u2014\u2018video as instruction\u2019\u2014that this paper adopts and adapts to open-world gameplay."
    },
    {
      "title": "Visual Reinforcement Learning with Imagined Goals (RIG)",
      "authors": "Ashvin Nair et al.",
      "year": 2018,
      "arxiv_id": "1807.04742",
      "role": "Extension",
      "relationship_sentence": "RIG\u2019s key idea of learning a latent goal space from visual observations is extended here to sequence-level video embeddings, so the video instruction encoder induces a structured goal space that conditions the policy."
    },
    {
      "title": "Generative Adversarial Imitation from Observation",
      "authors": "Faraz Torabi et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "By formalizing imitation-from-observation without action labels through aligning expert and learner state distributions, this work underpins the paper\u2019s learning-from-video paradigm where trajectories are matched to instruction videos without action supervision."
    }
  ],
  "synthesis_narrative": "Video-pretrained Minecraft agents demonstrated that large-scale human gameplay can train competent policies, but relied on labeled actions and heavy annotation pipelines (Baker et al.). MineDojo broadened the setting to open-ended goals and introduced MineCLIP to align text with video, crystallizing an instruction-following formulation in Minecraft yet depending on curated text\u2013video pairs (Fan et al.). In robotics, Learning Latent Plans from Play showed that unlabeled play trajectories contain rich supervision: a short demonstration snippet can be encoded into a compact latent plan that conditions a goal-conditioned policy over long horizons (Lynch et al.). VIMA advanced this promptable-control view by using short visual demonstrations as multimodal instructions and learning an instruction encoder that policies can follow (Jiang et al.). Complementing these, RIG established that learning a latent goal space from raw observations enables goal-conditioned control without explicit symbolic goals (Nair et al.), while Generative Adversarial Imitation from Observation formalized learning policies from videos alone via state-distribution alignment rather than action labels (Torabi et al.). Together, these works suggest a path: use videos directly as instruction specifications, learn an encoder that maps instruction clips into a structured goal space, and condition a policy on this representation to follow open-ended tasks in an open world. The present paper synthesizes these insights by replacing text with reference gameplay videos as goals, learning a video-instruction encoder that induces a compositional goal space, and training a causal-transformer policy to align its rollouts to those instruction embeddings, thereby surpassing labeled-data-heavy Minecraft baselines in open-ended skill execution.",
  "target_paper": {
    "title": "GROOT: Learning to Follow Instructions by Watching Gameplay Videos",
    "authors": "Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, Yitao Liang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Agent, Goal-conditioned Policy, Imitation Learning, Open World, Minecraft",
    "abstract": "We study the problem of building a controller that can follow open-ended instructions in open-world environments. We propose to follow reference videos as instructions, which offer expressive goal specifications while eliminating the need for expensive text-gameplay annotations. A new learning framework is derived to allow learning such instruction-following controllers from gameplay videos while producing a video instruction encoder that induces a structured goal space. We implement our agent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers. We evaluate GROOT against open-world counterparts and human players on a proposed Minecraft SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap as well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis of the induced goal space further demonstrates some interesting emergent properties, including the goal composition and ",
    "openreview_id": "uleDLeiaT3",
    "forum_id": "uleDLeiaT3"
  },
  "analysis_timestamp": "2026-01-06T20:08:28.574366"
}
{
  "prior_works": [
    {
      "title": "A Kernel Two-Sample Test",
      "authors": "Arthur Gretton et al.",
      "year": 2012,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work defines Maximum Mean Discrepancy (MMD) as an RKHS-based distance between empirical distributions, which the paper directly uses to measure similarity between the node-representation distributions of two graphs and to construct a positive-definite graph kernel."
    },
    {
      "title": "Kernel Mean Embedding of Distributions: A Review and Beyond",
      "authors": "Krikamol Muandet et al.",
      "year": 2017,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "The kernel mean embedding framework underpins the paper\u2019s formulation of graphs as sets of node embeddings and justifies computing a valid kernel via inner products of empirical mean embeddings (and distances via MMD) between graphs."
    },
    {
      "title": "Propagation Kernels: Efficient Graph Kernels from Propagated Information",
      "authors": "Marian Neumann et al.",
      "year": 2016,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "This method compares graphs by iteratively propagating node information and matching distributions across iterations, which the paper generalizes by replacing histogram-based comparisons with MMD over continuous, message-passing node representations."
    },
    {
      "title": "Message Passing Graph Kernels",
      "authors": "Giannis Nikolentzos et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "As a primary competitor that embeds message passing directly into a kernel, this work motivates the paper\u2019s key step of computing a distributional set-kernel over propagated node embeddings\u2014here realized via MMD to yield a PSD, learnable metric."
    },
    {
      "title": "Weisfeiler-Lehman Graph Kernels",
      "authors": "Nino Shervashidze et al.",
      "year": 2011,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "The WL framework of iteratively refining node labels to build graph kernels provides the propagation paradigm that the paper adopts with continuous message-passing features and compares with via an MMD-based distributional matching."
    },
    {
      "title": "Optimal Assignment Kernels for Attributed Graphs",
      "authors": "Nils M. Kriege et al.",
      "year": 2016,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "This work\u2019s assignment-based graph similarities expose issues of indefiniteness and computational cost, directly motivating the paper\u2019s use of MMD to compare node-embedding distributions in a PSD and scalable manner."
    },
    {
      "title": "Generative Moment Matching Networks",
      "authors": "Yujia Li et al.",
      "year": 2015,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "By learning deep feature maps under which MMD is computed, this work inspires the paper\u2019s deep MMD-based graph kernels that adaptively learn both the kernel and the graph feature extractor in unsupervised and supervised settings."
    }
  ],
  "synthesis_narrative": "Maximum Mean Discrepancy (MMD) introduced an RKHS-based discrepancy for comparing empirical distributions with strong statistical guarantees, and the kernel mean embedding program established that distributions can be represented as mean elements, yielding valid kernels and distances directly from samples. In graph similarity, Weisfeiler\u2013Lehman kernels operationalized iterative refinement of node representations to derive powerful graph features, while propagation kernels extended this paradigm by diffusing attributes and comparing the evolving node-label distributions across iterations. Message Passing Graph Kernels further integrated message passing into the kernel computation, contrasting graphs via similarities between propagated node representations. In parallel, optimal assignment kernels compared node sets via matchings but raised practical issues of indefiniteness and computational cost. Orthogonally, Generative Moment Matching Networks demonstrated that MMD can be paired with learned deep feature maps to adapt distribution comparisons by optimizing the embedding under which MMD is measured.\nThese strands collectively suggested a natural opportunity: treat each graph as a distribution over its message-passing node embeddings and compare graphs via an RKHS distance on these distributions. MMD offers a principled, PSD, and sample-efficient way to perform this comparison, while deep parameterizations enable learning both the kernel and the feature map, with supervision further shaping discriminative metrics. The resulting synthesis resolves limitations of histogram or assignment matching, aligns with propagation-based kernels, and delivers adaptable, theoretically grounded graph metrics.",
  "target_paper": {
    "title": "MMD Graph Kernel: Effective Metric Learning for Graphs via Maximum Mean Discrepancy",
    "authors": "Yan Sun, Jicong Fan",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "graph kernel, graph metric learning, maximum mean discrepancy",
    "abstract": "This paper focuses on graph metric learning. First, we present a class of maximum mean discrepancy (MMD) based graph kernels, called MMD-GK. These kernels are computed by applying MMD to the node representations of two graphs with message-passing propagation. \nSecondly, we provide a class of deep MMD-GKs that are able to learn graph kernels and implicit graph features adaptively in an unsupervised manner. Thirdly, we propose a class of supervised deep MMD-GKs that are able to utilize label information of graphs and hence yield more discriminative metrics. Besides the algorithms, we provide theoretical analysis for the proposed methods. The proposed methods are evaluated in comparison to many baselines such as graph kernels and graph neural networks in the tasks of graph clustering and graph classification. The numerical results demonstrate the effectiveness and superiority of our methods.",
    "openreview_id": "GZ6AcZwA8r",
    "forum_id": "GZ6AcZwA8r"
  },
  "analysis_timestamp": "2026-01-06T15:38:43.859338"
}
{
  "prior_works": [
    {
      "title": "Machine Teaching: Designing Training Sets for Machine Learning",
      "authors": "Xiaojin Zhu et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "GraNT adopts the machine-teaching paradigm of optimizing example sets to steer a learner, directly grounding its goal of selecting graph\u2013property pairs to accelerate learning in this foundational formulation."
    },
    {
      "title": "Greedy Function Approximation: A Gradient Boosting Machine",
      "authors": "Jerome H. Friedman",
      "year": 2001,
      "role": "Foundation",
      "relationship_sentence": "GraNT\u2019s core step\u2014recasting parameter updates as functional gradient descent\u2014builds on Friedman\u2019s function-space viewpoint, enabling teaching to be formulated on the evolution of the target function rather than parameters."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "GraNT leverages the NTK insight that gradient descent on parameters induces (approximately) kernelized functional dynamics, which justifies analyzing and teaching the learner in function space."
    },
    {
      "title": "Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels",
      "authors": "Simon S. Du et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "By extending NTK to graph neural networks, GNTK provides the concrete bridge GraNT needs to translate GCN training dynamics into functional gradients over graphs for example selection."
    },
    {
      "title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "authors": "Thomas N. Kipf et al.",
      "year": 2017,
      "role": "Baseline",
      "relationship_sentence": "GCN is the primary graph property learner whose training dynamics GraNT analyzes and whose convergence GraNT accelerates via nonparametric teaching."
    },
    {
      "title": "Neural Message Passing for Quantum Chemistry",
      "authors": "Justin Gilmer et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "This work formalized graph-to-property learning for molecules, supplying the problem setting and tasks that GraNT targets with its teaching-by-example paradigm."
    },
    {
      "title": "Simplifying Graph Convolutional Networks",
      "authors": "Felix Wu et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "By showing GCNs act as low-pass Laplacian smoothers, this work exposed how graph structure shapes learning; GraNT explicitly analyzes this structure\u2013gradient interplay and addresses the resulting convergence inefficiencies via principled example selection."
    }
  ],
  "synthesis_narrative": "GraNT\u2019s core innovation\u2014teaching graph property learners by selecting examples while analyzing training in function space\u2014emerges from the confluence of machine teaching and functional training dynamics in neural networks. The machine teaching paradigm (Zhu) provides the conceptual backbone: the teacher optimizes a training set to steer the learner toward a target more quickly. To make this actionable for deep graph learners, GraNT adopts the functional gradient view from Friedman, treating learning as gradient descent over functions rather than parameters. Jacot\u2019s Neural Tangent Kernel then supplies the theoretical conduit that links parameter-space updates to function-space dynamics, enabling formal analysis of convergence under example selection. Critically for graphs, Du\u2019s Graph Neural Tangent Kernel extends this NTK lens to GNNs, letting GraNT express GCN evolution as functional gradients over graph-structured inputs. On the application side, Gilmer\u2019s message passing formulation defines the graph-to-property prediction problem\u2014especially molecular properties\u2014that GraNT aims to accelerate. Finally, Wu\u2019s simplification of GCNs clarifies how graph Laplacian smoothing and graph structure bias training, highlighting concrete inefficiencies that GraNT addresses by selecting graph\u2013property pairs that optimally shape the functional descent. Together, these works directly enable GraNT\u2019s theory and algorithm for nonparametric teaching of graph property learners and its acceleration of GCN training.",
  "analysis_timestamp": "2026-01-06T23:07:19.603258"
}
{
  "prior_works": [
    {
      "title": "On the Inductive Bias of Neural Tangent Kernels",
      "authors": "St\u00e9phane Bietti et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "This work quantified the benefits of convolutional inductive biases through kernel-based uniform-convergence bounds but lacked separating lower bounds and optimization-aware analysis, a gap this paper explicitly addresses."
    },
    {
      "title": "On the Power and Limitations of Convolutional Neural Networks",
      "authors": "Gal Yehudai et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By using simplified synthetic setups to probe CNN advantages, this paper highlighted that common tasks do not faithfully encode both locality and translation invariance, directly motivating the more realistic DSD task introduced here."
    },
    {
      "title": "Group Equivariant Convolutional Networks",
      "authors": "Taco Cohen et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized translation equivariance/invariance via weight sharing, providing the conceptual basis for modeling data as shifted local patches\u2014a structural assumption the DSD task explicitly encodes."
    },
    {
      "title": "SGD Learns Over-Parametrized Convolutional Networks (One-Filter Case) with Global Pooling",
      "authors": "Alon Brutzkus et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Proving that SGD can provably learn a single shared convolutional filter under a teacher\u2013student model inspired this paper\u2019s optimization-aware analysis showing how weight sharing reduces sample complexity on the DSD task."
    },
    {
      "title": "Convolutional Neural Tangent Kernels",
      "authors": "Sanjeev Arora et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By characterizing CNN inductive bias through the CNNTK and providing generalization upper bounds without matching lower bounds, this work underscored the need for the separation results established here."
    },
    {
      "title": "On the Expressive Power of Deep Convolutional Networks via Hierarchical Tensor Decompositions",
      "authors": "Nadav Cohen et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper connected locality and weight sharing to concrete representational advantages, grounding the idea that shared filters capture recurring local patterns across positions, as operationalized in the DSD construction."
    }
  ],
  "synthesis_narrative": "Kernel-based analyses established that convolutional inductive biases confer statistical advantages: Bietti and Mairal showed how locality and translation invariance manifest in neural tangent kernels, while Arora and collaborators\u2019 CNNTK characterized the function spaces induced by CNNs. These works offered uniform-convergence style upper bounds but stopped short of matching lower bounds or optimizer-aware learning guarantees. From a representational perspective, Cohen and colleagues formalized how weight sharing and local connectivity encode translation equivariance and compositional structure, theoretically grounding the role of shared filters in capturing repeated local patterns. Complementing these, Brutzkus and collaborators provided optimization results indicating SGD can learn a single shared filter with global pooling in a teacher\u2013student setting, evidencing an algorithmic pathway for exploiting convolutional structure. Meanwhile, Yehudai and Shamir used simplified synthetic tasks to probe CNN strengths and weaknesses, illuminating that commonly used benchmarks do not simultaneously capture realistic locality and translation invariance. Collectively, these works revealed a gap: we had either representational or kernel-based arguments without separating lower bounds and with oversimplified tasks, or optimization results on too narrow setups. The current paper synthesizes these strands by introducing the Dynamic Signal Distribution task, which encodes both locality and translation invariance, and by giving optimization-aware analyses with separating sample-complexity bounds for CNNs versus LCNs and FCNs, showing precisely how weight sharing and locality reduce data requirements.",
  "target_paper": {
    "title": "Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs",
    "authors": "Aakash Lahoti, Stefani Karp, Ezra Winston, Aarti Singh, Yuanzhi Li",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Deep Learning Theory, Sample Complexity, Convolutional Neural Networks",
    "abstract": "Vision tasks are characterized by the properties of locality and translation invariance. \n    The superior performance of convolutional neural networks (CNNs) on these tasks is widely attributed to the inductive bias of locality and weight sharing baked into their architecture.\n    Existing attempts to quantify the statistical benefits of these biases in CNNs over locally connected convolutional neural networks (LCNs) and fully connected neural networks (FCNs) fall into one of the following categories: either they disregard the optimizer and only provide uniform convergence upper bounds with no separating lower bounds, \n    or they consider simplistic tasks that do not truly mirror the locality and translation invariance as found in real-world vision tasks.\n    To address these deficiencies, we introduce the Dynamic Signal Distribution (DSD) classification task that models an image as consisting of $k$ patches, each of dimension $d$, and the label is determined by a $d$-sparse signal v",
    "openreview_id": "AfnsTnYphT",
    "forum_id": "AfnsTnYphT"
  },
  "analysis_timestamp": "2026-01-06T07:47:37.341374"
}
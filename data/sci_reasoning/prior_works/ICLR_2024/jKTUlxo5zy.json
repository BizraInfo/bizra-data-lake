{
  "prior_works": [
    {
      "title": "Why Should I Trust You? Explaining the Predictions of Any Classifier",
      "authors": "Marco Tulio Ribeiro et al.",
      "year": 2016,
      "arxiv_id": "1602.04938",
      "role": "Foundation",
      "relationship_sentence": "This work introduced superpixel-based interpretable components for image explanations and a submodular-pick principle for diverse, succinct explanations, which directly motivates framing region-level explanation as a subset selection problem over interpretable regions."
    },
    {
      "title": "Anchors: High-Precision Model-Agnostic Explanations",
      "authors": "Marco Tulio Ribeiro et al.",
      "year": 2018,
      "arxiv_id": "1801.05075",
      "role": "Inspiration",
      "relationship_sentence": "Anchors\u2019 emphasis on high-precision (confidence) constraints for local explanations informs the paper\u2019s confidence requirement that the selected regions alone sustain the model\u2019s decision."
    },
    {
      "title": "Learning to Explain: An Information-Theoretic Perspective on Model Interpretation",
      "authors": "Jianbo Chen et al.",
      "year": 2018,
      "arxiv_id": "1802.07814",
      "role": "Foundation",
      "relationship_sentence": "L2X formalized explanations as selecting a fixed-size subset that is sufficient for the prediction, which the paper adapts from feature vectors to spatial regions via a submodular objective and selection."
    },
    {
      "title": "Understanding Deep Networks via Meaningful Perturbations",
      "authors": "Ruth Fong et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "The idea of finding minimal-area evidence that preserves the class score directly inspires replacing continuous mask optimization with discrete selection of few accurate regions under a principled objective."
    },
    {
      "title": "Explanations from Deep Networks via Extremal Perturbations",
      "authors": "Ruth Fong et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Building on extremal perturbations\u2019 area-constrained evidence masks, the paper generalizes from a single contiguous mask to multiple discrete regions and embeds additional constraints via a submodular framework."
    },
    {
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
      "authors": "Ramprasaath R. Selvaraju et al.",
      "year": 2017,
      "arxiv_id": "1610.02391",
      "role": "Baseline",
      "relationship_sentence": "As a primary baseline, Grad-CAM\u2019s tendency to produce coarse, imprecise small regions and unrevealing maps on misclassified samples motivates the paper\u2019s focus on selecting fewer, more accurate regions."
    },
    {
      "title": "An analysis of approximations for maximizing submodular set functions",
      "authors": "George L. Nemhauser et al.",
      "year": 1978,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The paper\u2019s greedy selection algorithm and its near-optimality rely on Nemhauser et al.\u2019s 1\u22121/e guarantee for monotone submodular maximization under cardinality constraints."
    }
  ],
  "synthesis_narrative": "Superpixel-based image explanations established that human-interpretable components can be formed by grouping pixels into regions and selecting a succinct subset, and the submodular-pick idea showed that a principled subset objective can encourage diversity and coverage in what is shown. High-precision local rules demonstrated that explanations should satisfy an explicit confidence requirement, ensuring that the evidence alone is sufficient for the prediction. Information-theoretic instance-wise feature selection framed explanations as choosing a fixed-size subset that maximizes sufficiency, linking explanation quality to how much of the prediction can be retained from a few selected elements. Perturbation-based saliency advanced the notion of minimal-area evidence that preserves the class score, while extremal perturbations formalized area-constrained masks that isolate the most supportive region. At the same time, gradient-based localization became the de facto baseline but often produced coarse, blob-like maps that fail to pin down accurate small regions and tend to falter on misclassified samples. Classic results on submodular maximization provided an efficient, theoretically grounded way to optimize set-valued objectives under size constraints.\nThese strands collectively suggested selecting a small set of spatial regions that is sufficient and precise, guided by explicit confidence-like constraints, yet computed via a discrete procedure with guarantees. The paper synthesizes these insights by casting image attribution as submodular subset selection over interpretable regions, designing a submodular utility that targets accurate small-region evidence, and imposing constraints for confidence, effectiveness, consistency, and collaboration to work robustly even on mispredictions\u2014an evolution from continuous mask optimization and coarse heatmaps to principled, few-region selection with theoretical support.",
  "target_paper": {
    "title": "Less is More: Fewer Interpretable Region via Submodular Subset Selection",
    "authors": "Ruoyu Chen, Hua Zhang, Siyuan Liang, Jingzhi Li, Xiaochun Cao",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "Interpretable AI, Submodular subset selection, Explainable AI, Image Attribution",
    "abstract": "Image attribution algorithms aim to identify important regions that are highly relevant to model decisions. Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate small regions thus misleading the direction of correct attribution, and 2) the model cannot produce good attribution results for samples with wrong predictions. To address the above challenges, this paper re-models the above image attribution problem as a submodular subset selection problem, aiming to enhance model interpretability using fewer regions. To address the lack of attention to local regions, we construct a novel submodular function to discover more accurate small interpretation regions. To enhance the attribution effect for all samples, we also impose four different constraints on the selection of sub-regions, i.e., confidence, effectiveness, consistency, and collaboration scores, to ",
    "openreview_id": "jKTUlxo5zy",
    "forum_id": "jKTUlxo5zy"
  },
  "analysis_timestamp": "2026-01-06T18:39:31.080062"
}
{
  "prior_works": [
    {
      "title": "How Powerful are Graph Neural Networks?",
      "authors": "Keyulu Xu et al.",
      "year": 2019,
      "arxiv_id": "1810.00826",
      "role": "Foundation",
      "relationship_sentence": "By establishing MPNNs\u2019 expressiveness via equivalence to 1-WL and formalizing injective multiset aggregation, this work provides the baseline expressiveness lens that our framework generalizes into explicit first-order counting formulas for message-passing updates."
    },
    {
      "title": "Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks",
      "authors": "Christopher Morris et al.",
      "year": 2019,
      "arxiv_id": "1810.02244",
      "role": "Extension",
      "relationship_sentence": "Their k-GNNs aligned with k-WL directly motivate our derivation of the corresponding C^k logical formulas, making the higher-order patterns these architectures capture explicit within a unified framework."
    },
    {
      "title": "The Logical Expressiveness of Graph Neural Networks",
      "authors": "Pablo Barcel\u00f3 et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "This paper\u2019s partial logical characterization of MPNNs under specific aggregators highlights missing coverage of broader architectures, a limitation we address by providing a complete recipe to obtain equivalent logical formulas for arbitrary GNN designs."
    },
    {
      "title": "The Logic of Graph Neural Networks",
      "authors": "Martin Grohe",
      "year": 2021,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Grohe\u2019s finite-model-theoretic view (bisimulation, FPC/FOC invariance) of GNNs inspires our general construction that translates computation graphs of diverse GNNs into matching logical specifications."
    },
    {
      "title": "An optimal lower bound on the number of variables for graph identification",
      "authors": "Jin-Yi Cai et al.",
      "year": 1992,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The CFI connection between k-variable first-order logic with counting (C^k) and k-WL indistinguishability underpins our equivalences for higher-order GNNs by grounding them in established logic\u2013WL correspondences."
    },
    {
      "title": "Expressive Power of Invariant and Equivariant Graph Neural Networks",
      "authors": "Mohammad Azizian and Marc Lelarge",
      "year": 2021,
      "arxiv_id": "2006.15646",
      "role": "Extension",
      "relationship_sentence": "Their characterization of invariant/equivariant GNNs via homomorphism-count polynomials is subsumed by our framework, which expresses homomorphism-based expressivity through explicit logical formulas and analyzes it from a logical perspective."
    },
    {
      "title": "Provably Powerful Graph Networks",
      "authors": "Haggai Maron et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "The higher-order tensor architectures with provable k-WL power studied here are covered in our case studies by deriving their exact equivalent logical formulas, unifying them with message-passing and other GNN families."
    }
  ],
  "synthesis_narrative": "Work on GNN expressiveness first grounded message passing in the Weisfeiler\u2013Lehman (WL) paradigm: Xu et al. formalized how injective multiset aggregation lets MPNNs match 1\u2011WL\u2019s distinguishing power, setting a canonical expressiveness lens. Morris et al. then introduced k\u2011GNNs aligned with k\u2011WL, revealing how higher\u2011order neighborhoods enable finer separation and implicitly pointing to the k\u2011variable counting logic C^k as the right logical counterpart. Barcel\u00f3 et al. took a direct logic turn, mapping certain MPNNs to fragments of first\u2011order logic with counting under specific aggregators, but left broader architectures outside a uniform logical account. Grohe\u2019s finite\u2011model\u2011theoretic treatment clarified the role of bisimulation and (fixed\u2011point) counting logics in capturing what GNNs can define. Foundationally, Cai\u2013F\u00fcrer\u2013Immerman established the deep link between k\u2011WL indistinguishability and C^k, providing the bridge between combinatorial tests and logical definability. Parallelly, Azizian and Lelarge characterized invariant/equivariant GNNs via homomorphism\u2011count polynomials, while Maron et al. showed higher\u2011order tensor networks achieving k\u2011WL\u2011level power. Together, these works exposed both the promise and the fragmentation of existing characterizations: WL-based metrics are powerful but opaque about patterns, and prior logic accounts were partial or architecture-specific. The natural next step is to synthesize these insights into a single, complete procedure that, given an arbitrary GNN architecture\u2014including higher\u2011order, invariant/equivariant, and homomorphism\u2011based variants\u2014yields the equivalent logical formula, thereby unifying disparate subareas and making captured graph patterns explicit.",
  "target_paper": {
    "title": "Towards a Complete Logical Framework for GNN Expressiveness",
    "authors": "Tuo Xu",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "graph neural networks, logic",
    "abstract": "Designing expressive Graph neural networks (GNNs) is an important topic in graph machine learning fields. Traditionally, the Weisfeiler-Lehman (WL) test has been the primary measure for evaluating GNN expressiveness. However, high-order WL tests can be obscure, making it challenging to discern the specific graph patterns captured by them. Given the connection between WL tests and first-order logic, some have explored the logical expressiveness of Message Passing Neural Networks. This paper aims to establish a comprehensive and systematic relationship between GNNs and logic. We propose a framework for identifying the equivalent logical formulas for arbitrary GNN architectures, which not only explains existing models, but also provides inspiration for future research. As case studies, we analyze multiple classes of prominent GNNs within this framework, unifying different subareas of the field. Additionally, we conduct a detailed examination of homomorphism expressivity from a logical per",
    "openreview_id": "pqOjj90Vwp",
    "forum_id": "pqOjj90Vwp"
  },
  "analysis_timestamp": "2026-01-06T15:08:12.718438"
}
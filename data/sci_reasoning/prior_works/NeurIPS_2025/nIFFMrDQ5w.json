{
  "prior_works": [
    {
      "title": "The Edge of Stability: Understanding the Dynamics of Gradient Descent in Deep Learning",
      "authors": "Jeremy Cohen et al.",
      "year": 2021,
      "role": "theoretical/empirical foundation for EoS",
      "relationship_sentence": "Established the edge-of-stability phenomenon (\u03b7\u00b7\u03bbmax \u2248 2) and showed how operating near this boundary biases gradient descent toward flatter regions, providing the exact framework that the present paper extends to variational learning."
    },
    {
      "title": "Weight Uncertainty in Neural Networks (Bayes by Backprop)",
      "authors": "Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra",
      "year": 2015,
      "role": "algorithmic foundation for variational learning in deep nets",
      "relationship_sentence": "Introduced stochastic variational training of neural networks with a parameterized posterior and Monte Carlo sampling, furnishing the knobs (posterior shape, number of samples) that this paper analyzes through the EoS lens."
    },
    {
      "title": "Importance Weighted Autoencoders",
      "authors": "Yuri Burda, Roger Grosse, Ruslan Salakhutdinov",
      "year": 2016,
      "role": "multi-sample variational objectives and sampling control",
      "relationship_sentence": "Showed how increasing the number of posterior samples tightens variational training and changes gradient estimators, directly informing this paper\u2019s result that sample count modulates EoS dynamics and flatness."
    },
    {
      "title": "Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks via PAC-Bayes",
      "authors": "Gintare Karolina Dziugaite, Daniel M. Roy",
      "year": 2017,
      "role": "theoretical link between variational posteriors and generalization",
      "relationship_sentence": "Demonstrated PAC-Bayes as a lens for understanding generalization in stochastic/variational training, motivating the present work\u2019s search for an implicit regularization mechanism (flatness) behind VL\u2019s success."
    },
    {
      "title": "A Practical Bayesian Framework for Backpropagation Networks",
      "authors": "David J. C. MacKay",
      "year": 1992,
      "role": "marginal likelihood/MDL perspective on flatness",
      "relationship_sentence": "Connected marginal likelihood and Occam\u2019s razor to model preference for broader (flatter) solutions, a classical principle that the current paper reinterprets via EoS-driven dynamics in variational learning."
    },
    {
      "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima",
      "authors": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang",
      "year": 2017,
      "role": "empirical foundation linking flatness and generalization",
      "relationship_sentence": "Identified the sharp vs. flat minima dichotomy and its generalization implications, grounding the new paper\u2019s focus on how VL\u2019s dynamics can find even flatter solutions."
    },
    {
      "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization",
      "authors": "Pierre Foret, Ariel Kleiner, Hossein Mobahi, Behnam Neyshabur",
      "year": 2021,
      "role": "algorithmic control of sharpness/flatness",
      "relationship_sentence": "Provided an explicit mechanism to control sharpness during training, offering a contrast to the present paper\u2019s implicit flatness control via EoS through variational posterior shape and sampling."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014showing that variational learning (VL) naturally operates at the Edge of Stability (EoS) and can find even flatter solutions than standard gradient descent\u2014rests on two pillars: the EoS theory for gradient methods and the stochastic structure of modern variational training. The EoS foundation is provided by Cohen et al. (2021), who identified the \u03b7\u00b7\u03bbmax \u2248 2 criticality and linked training at this boundary to flatness; the present work mirrors that methodology (quadratics-to-deep-nets) to extend EoS analysis to VL. On the variational side, Bayes by Backprop (Blundell et al., 2015) established stochastic variational training with parameterized posteriors, while IWAE (Burda et al., 2016) clarified how the number of posterior samples affects the training objective and gradient estimators\u2014precisely the levers this paper shows can steer EoS dynamics toward flatter minima. Classical Bayesian principles from MacKay (1992) connect marginal likelihood/MDL with a preference for broad basins, providing a conceptual bridge between flatness and VL\u2019s objectives. Empirically and motivationally, the flat\u2013sharp generalization link from Keskar et al. (2017) underpins why flatter solutions matter, and SAM (Foret et al., 2021) exemplifies explicit sharpness control; the current work complements SAM by revealing an implicit route to flatness via the interplay of posterior shape and sampling at the EoS. Together, these works directly inform the paper\u2019s theoretical extension of EoS to VL and its practical guidance for setting variational noise and sample counts to induce flatter, better-generalizing solutions.",
  "analysis_timestamp": "2026-01-07T00:21:32.355352"
}
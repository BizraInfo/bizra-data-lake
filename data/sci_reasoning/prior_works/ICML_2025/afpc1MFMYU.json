{
  "prior_works": [
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "NsDiff retains the DDPM denoising diffusion framework but replaces its additive, fixed-variance forward noise (ANM) with a location-scale formulation to enable non-stationary uncertainty."
    },
    {
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "authors": "Yang Song et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "NsDiff builds on the score-based/Reverse-SDE perspective for conditional generative modeling, generalizing it by making the diffusion coefficient data-dependent via a location-scale noise model and an uncertainty-aware schedule."
    },
    {
      "title": "Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting (TimeGrad)",
      "authors": "Rasul et al.",
      "year": 2021,
      "role": "Baseline",
      "relationship_sentence": "TimeGrad is the primary diffusion-based TS forecasting baseline that NsDiff directly improves by addressing its fixed global noise schedule and implicit stationarity of uncertainty."
    },
    {
      "title": "Improved Denoising Diffusion Probabilistic Models",
      "authors": "Alex Nichol et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "While this work shows the importance of beta/sigma schedules and learns denoising variance, it remains globally specified and input-agnostic; NsDiff addresses this gap with an uncertainty-aware, data-conditional noise schedule."
    },
    {
      "title": "Elucidating the Design Space of Diffusion Models",
      "authors": "Tero Karras et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "Their analysis of noise level parameterizations and schedules motivates NsDiff\u2019s advance: replacing handcrafted/global schedules with per-step, data-driven noise levels tied to estimated uncertainty."
    },
    {
      "title": "DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks",
      "authors": "David Salinas et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "DeepAR\u2019s heteroscedastic likelihood (learning time-varying mean and variance) directly inspires NsDiff\u2019s pre-trained conditional mean/variance estimator used to parameterize the endpoint distribution and guide the diffusion process."
    },
    {
      "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?",
      "authors": "Alex Kendall et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "The heteroscedastic (data-dependent) aleatoric uncertainty modeling in this work informs NsDiff\u2019s adoption of a location-scale noise model and the design of an uncertainty-aware noise schedule."
    }
  ],
  "synthesis_narrative": "NsDiff sits at the intersection of diffusion generative modeling and heteroscedastic time-series forecasting. The core denoising diffusion machinery comes from DDPM and its score-based SDE formulation, which establish the Gaussian forward process and reverse-time denoising dynamics. Diffusion was first adapted to forecasting by TimeGrad, which framed multivariate probabilistic TS forecasting as conditional diffusion; however, its fixed global variance schedule implicitly assumes stationary uncertainty. Concurrently, the diffusion literature recognized the importance of noise schedules and variance choices\u2014Improved DDPM and Elucidating the Design Space of Diffusion Models analyzed and optimized schedules\u2014but these remained input-agnostic and did not account for time-varying uncertainty in the data. In contrast, the forecasting community (e.g., DeepAR) and broader deep learning (Kendall & Gal) demonstrated the value of heteroscedastic modeling by learning conditional means and variances that vary over time or input. NsDiff fuses these strands: it preserves the diffusion/score-based framework while replacing the additive, fixed-variance assumption with a location-scale noise model driven by a pre-trained mean/variance estimator, and it introduces an uncertainty-aware noise schedule that adapts per step to the data\u2019s time-varying uncertainty. This explicit incorporation of heteroscedasticity directly addresses the gaps in prior diffusion forecasting methods and schedule design work.",
  "analysis_timestamp": "2026-01-06T23:07:19.588150"
}
{
  "prior_works": [
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei et al.",
      "year": 2022,
      "role": "Foundational technique for eliciting step-by-step verbal reasoning in LLMs.",
      "relationship_sentence": "VAP generalizes CoT from purely textual rationales to a dual-modality setting by inducing aligned chains of thought in both language and synthesized imagery."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang et al.",
      "year": 2022,
      "role": "Ensemble-style aggregation of multiple CoT samples to improve reliability and accuracy.",
      "relationship_sentence": "VAP\u2019s conclusive reasoning via self-alignment mirrors self-consistency by consolidating multiple intermediate cross-modal reasoning states into a stable final answer."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2022,
      "role": "Interleaves natural language reasoning with tool-use actions to solve tasks.",
      "relationship_sentence": "VAP operationalizes the ReAct paradigm by treating drawing and editing operations as external actions interleaved with textual and visual thoughts, guiding iterative diagram construction."
    },
    {
      "title": "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models",
      "authors": "Chenfei Wu et al.",
      "year": 2023,
      "role": "Demonstrated LLM orchestration of visual tools for generating and modifying images from text.",
      "relationship_sentence": "VAP builds on this tool-use blueprint to automatically synthesize and refine problem-specific diagrams from textual clues as a visual workspace for reasoning."
    },
    {
      "title": "LLaVA: Large Language and Vision Assistant",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "role": "Instruction-tuned vision-language model enabling image-grounded dialogue and reasoning.",
      "relationship_sentence": "VAP leverages capabilities typified by LLaVA\u2014interpreting images and integrating visual cues with text\u2014to realize cross-modal chains of thought over synthesized diagrams."
    },
    {
      "title": "Multimodal Chain-of-Thought Reasoning in Language Models (MM-CoT)",
      "authors": "Zhang et al.",
      "year": 2023,
      "role": "Extended CoT to include visual grounding and intermediate multimodal steps.",
      "relationship_sentence": "VAP advances MM-CoT by explicitly constructing a visual workspace via drawing tools and co-evolving visual and textual reasoning through iterative refinement."
    },
    {
      "title": "Self-Refine: Iterative Refinement with Self-Feedback",
      "authors": "Kartikeya Madaan et al.",
      "year": 2023,
      "role": "Introduced iterative self-critique and revision loops to improve outputs.",
      "relationship_sentence": "VAP\u2019s loop that repeatedly updates the synthesized image and the textual rationale instantiates self-refinement across modalities to tighten visual-textual alignment."
    }
  ],
  "synthesis_narrative": "Vision-Augmented Prompting (VAP) fuses established advances in LLM reasoning, multimodal grounding, and tool orchestration into a unified dual-modality scheme. At its core, VAP extends Chain-of-Thought (Wei et al., 2022) beyond language by maintaining coordinated textual and visual chains, while its final answer selection echoes Self-Consistency (Wang et al., 2022) via a self-alignment step that consolidates multiple intermediate hypotheses. The iterative loop of thinking and acting is rooted in ReAct (Yao et al., 2022): VAP treats drawing and editing operations as external actions, enabling the model to materialize spatial hypotheses, verify them, and course-correct.\n\nCrucially, prior work on LLM-operated visual toolchains\u2014Visual ChatGPT (Wu et al., 2023)\u2014provides the practical mechanism for synthesizing and updating diagrams from text. VAP adopts this paradigm to construct a persistent visual workspace that supports reasoning with spatial cues. To interpret and reason over these images, VAP leverages capabilities exemplified by instruction-tuned VLMs such as LLaVA (Liu et al., 2023), integrating visual evidence with language during the reasoning process. Conceptually, VAP builds on MM-CoT (Zhang et al., 2023), but goes further by explicitly generating the visual context rather than passively consuming given images, allowing visual and textual thoughts to co-evolve. Finally, the framework\u2019s iterative image-and-text updates mirror Self-Refine (Madaan et al., 2023), providing cross-modal self-feedback that tightens alignment and improves solution quality. Together, these strands directly inform VAP\u2019s key contribution: a tool-enabled, co-evolving visual-textual chain-of-thought for enhanced reasoning with visual and spatial clues.",
  "analysis_timestamp": "2026-01-06T23:33:36.280667"
}
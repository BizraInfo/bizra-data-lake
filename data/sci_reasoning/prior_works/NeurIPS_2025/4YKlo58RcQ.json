{
  "prior_works": [
    {
      "title": "Natural speech reveals the semantic maps that tile human cerebral cortex",
      "authors": "Alexander G. Huth, Wendy A. de Heer, Thomas L. Griffiths, Fr\u00e9d\u00e9ric E. Theunissen, Jack L. Gallant",
      "year": 2016,
      "role": "Paradigm/dataset foundation for naturalistic language neuroscience",
      "relationship_sentence": "Established the naturalistic story-listening paradigm and voxel-wise encoding methodology that the present paper extends to long audiobook stimuli with temporally resolved signals."
    },
    {
      "title": "Rapid transformation from auditory to linguistic representations of continuous speech",
      "authors": "Christian Brodbeck, Samu Taulu, Joshua B. H. Downer, L. Robert Slevc, Jonathan Z. Simon",
      "year": 2018,
      "role": "Empirical evidence for time-resolved hierarchical processing in the brain",
      "relationship_sentence": "Showed with MEG/TRF that early auditory responses precede lexical/linguistic responses, motivating the paper\u2019s core test that early LLM layers align with earlier brain signals and deeper layers with later signals."
    },
    {
      "title": "Incorporating Context into Language Encoding Models for fMRI",
      "authors": "Kshitij Jain, Alexander G. Huth",
      "year": 2018,
      "role": "Methodological precedent on context length in brain encoding",
      "relationship_sentence": "Demonstrated that incorporating longer linguistic context improves brain response prediction, directly informing this paper\u2019s claim that context length modulates brain\u2013LLM alignment emergence."
    },
    {
      "title": "Two distinct neural timescales for predictive speech processing",
      "authors": "Paul M. Donhauser, Sylvain Baillet",
      "year": 2020,
      "role": "Empirical/theoretical support for predictive processing at multiple timescales",
      "relationship_sentence": "Identified distinct temporal integration windows in MEG during speech comprehension, underpinning the paper\u2019s hypothesis that scaling and context steer models onto temporally ordered processing paths comparable to the brain."
    },
    {
      "title": "The neural architecture of language: Integrative modeling converges on predictive processing",
      "authors": "Martin Schrimpf, Idan Blank, Greta Tuckute, Carina Kauf, Eghbal A. Hosseini, Nancy Kanwisher, Joshua B. Tenenbaum, Evelina Fedorenko",
      "year": 2021,
      "role": "Benchmark showing ANN language models predict brain and behavior",
      "relationship_sentence": "Provided large-scale evidence that next-word prediction\u2013optimized models best explain neural data, which this paper extends by mapping alignment across time and across architectures and by probing scaling and context effects."
    },
    {
      "title": "Brains and algorithms partially converge in natural language processing",
      "authors": "Florent Caucheteux, Jean-Remi King",
      "year": 2022,
      "role": "Empirical link between LM layer representations and brain activity",
      "relationship_sentence": "Reported layer-wise correspondences between GPT-2 and cortical responses in natural language, directly inspiring the present paper\u2019s systematic depth-to-latency alignment across many LLMs and neural modalities."
    },
    {
      "title": "Linking artificial and human neural representations of language",
      "authors": "Jon Gauthier, Roger P. Levy",
      "year": 2019,
      "role": "Methodological framework for representational alignment (RSA/encoding)",
      "relationship_sentence": "Proposed techniques to align model layers with human neural responses, informing the paper\u2019s cross-model, cross-time alignment analyses and its emphasis on representational order."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014showing that LLMs and the human brain traverse a similar temporal sequence of computations, and that this ordered alignment emerges with scaling and sufficient context\u2014builds on three converging lines of prior work. First, naturalistic neuroscience established the experimental and analytical foundation: Huth et al. introduced story-listening and voxel-wise encoding, while Brodbeck et al. and Donhauser & Baillet used time-resolved MEG/TRF to reveal rapid transitions from acoustic to lexical/linguistic processing and distinct neural timescales supporting predictive comprehension. These studies motivate testing whether representational depth in models corresponds to latency in brain responses. Second, integrative modeling evidence from Schrimpf et al. showed that predictive (next-word) language models best explain neural and behavioral data, framing LLMs as plausible algorithmic accounts of language processing. Caucheteux & King then demonstrated layer-wise correspondences between GPT-2 and cortical activity during natural language, suggesting a representational hierarchy that the present work generalizes across architectures and, crucially, pins to temporal dynamics. Third, methodological frameworks for alignment (Gauthier & Levy) and explicit demonstrations of the value of longer linguistic context for predicting brain signals (Jain & Huth) shape the study\u2019s design, enabling cross-model layer-to-latency mapping and targeted manipulations of context window. Together, these works directly inform the paper\u2019s innovation: a unified, temporally resolved alignment analysis across 17 LLMs showing that scaling and context push diverse architectures onto a brain-like computational trajectory from early to late processing stages.",
  "analysis_timestamp": "2026-01-07T00:02:04.962998"
}
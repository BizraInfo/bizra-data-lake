{
  "prior_works": [
    {
      "title": "Distilling Free-Form Natural Laws from Experimental Data",
      "authors": "Michael Schmidt, Hod Lipson",
      "year": 2009,
      "role": "Seminal method for automated scientific equation discovery via symbolic regression (Eureqa)",
      "relationship_sentence": "Established the modern formulation of scientific equation discovery from data, defining the core task that LLM-SRBench aims to evaluate rigorously and motivating the need for principled benchmarks beyond hand-picked examples."
    },
    {
      "title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems (SINDy)",
      "authors": "Steven L. Brunton, Joshua L. Proctor, J. Nathan Kutz",
      "year": 2016,
      "role": "Method for parsimonious discovery of dynamical system equations from data",
      "relationship_sentence": "Showed that sparse model identification can recover governing equations across scientific domains, shaping the problem settings included in LLM-SRBench and underscoring the importance of evaluating discovery beyond memorized canonical forms."
    },
    {
      "title": "AI Feynman: A physics-inspired method for symbolic regression",
      "authors": "Silviu-Marian Udrescu, Max Tegmark",
      "year": 2020,
      "role": "Method and widely used physics equation dataset for symbolic regression",
      "relationship_sentence": "Provided the canonical Feynman benchmark of well-known physics formulas; LLM-SRBench directly addresses the memorization risk of such popular sets by transforming common models into less typical but equivalent representations to test genuine reasoning."
    },
    {
      "title": "SRBench: A Comprehensive Benchmark for Symbolic Regression",
      "authors": "William La Cava, Patryk Orzechowski, et al.",
      "year": 2021,
      "role": "Benchmarking framework and suite for evaluating symbolic regression methods",
      "relationship_sentence": "Pioneered systematic, multi-domain evaluation practices for SR; LLM-SRBench extends this benchmarking ethos to LLM-based equation discovery and adds anti-contamination/anti-memorization design principles."
    },
    {
      "title": "Nguyen Symbolic Regression Benchmark Functions",
      "authors": "Quang Uy Nguyen, Xuan Hoai Nguyen, Michael O\u2019Neill, R. Comiskey, Anthony Brabazon",
      "year": 2011,
      "role": "Classic set of SR benchmark functions widely used in GP literature",
      "relationship_sentence": "These widely memorized, formula-template benchmarks exemplify the pitfalls of surface-form familiarity; LLM-SRBench\u2019s LSR-Transform category explicitly counters such effects by generating less common but equivalent formulations."
    },
    {
      "title": "egg: Fast and extensible equality saturation",
      "authors": "Max Willsey, Yisu Remy Wang, et al.",
      "year": 2021,
      "role": "Framework for algebraic rewriting and equivalence-class reasoning via e-graphs",
      "relationship_sentence": "Informs the strategy of producing semantically equivalent but syntactically diverse expressions; this notion underlies LLM-SRBench\u2019s transformation-based tasks that stress reasoning over rote recall."
    },
    {
      "title": "Extracting Training Data from Large Language Models",
      "authors": "Nicholas Carlini, Florian Tramer, Eric Wallace, et al.",
      "year": 2021,
      "role": "Foundational study on memorization and data leakage in LLMs",
      "relationship_sentence": "Demonstrated that LLMs memorize and regurgitate training content; LLM-SRBench directly operationalizes anti-memorization evaluation by designing tasks that avoid common surface forms vulnerable to contamination."
    }
  ],
  "synthesis_narrative": "LLM-SRBench\u2019s core contribution\u2014a contamination-resistant benchmark for evaluating large language models on scientific equation discovery\u2014rests on two intellectual pillars: the equation discovery task formalized in symbolic regression, and the recognition that modern LLMs can memorize widely publicized formulas. Schmidt and Lipson\u2019s 2009 work established automated law discovery as a concrete machine learning problem, while SINDy (Brunton et al., 2016) broadened the landscape to dynamical systems, motivating multi-domain coverage. Subsequent community benchmarks\u2014most notably the Nguyen function set and the AI Feynman dataset\u2014provided accessible testbeds, but their fame and ubiquity foster surface-form familiarity, especially problematic for LLMs trained on public corpora. SRBench (La Cava et al., 2021) advanced the field by systematizing evaluation across methods and datasets; LLM-SRBench inherits this benchmarking rigor but specifically targets LLM-centric risks.\nAt the heart of LLM-SRBench is the idea that genuine discovery requires reasoning over equivalence classes of expressions rather than recalling popular canonical forms. This is conceptually aligned with equality-saturation and e-graph rewriting (Willsey et al., 2021), which formalize algebraic transformations to generate semantically identical yet syntactically diverse formulas. Finally, evidence that LLMs memorize training content (Carlini et al., 2021) provides the direct impetus for LLM-SRBench\u2019s transformation-based tasks and anti-contamination design. Together, these prior works shape a benchmark that stresses reasoning, robustness, and transfer beyond memorized equations, enabling fair comparison of LLM-based and traditional SR approaches on the true scientific discovery objective.",
  "analysis_timestamp": "2026-01-07T00:21:32.364598"
}
{
  "prior_works": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, et al.",
      "year": 2021,
      "role": "Foundational vision-language pretraining backbone and contrastive alignment objective",
      "relationship_sentence": "Paxion targets frozen video-language models that are CLIP-based or CLIP-aligned, and extends their contrastive alignment with a dynamics-aware discriminative objective while injecting new action knowledge without full finetuning."
    },
    {
      "title": "Frozen in Time: A Joint Video and Language Learning Framework for Video Retrieval",
      "authors": "Max Bain, Arsha Nagrani, G\u00fcl Varol, Andrew Zisserman",
      "year": 2021,
      "role": "Representative video-language foundation model trained with video\u2013text contrastive learning",
      "relationship_sentence": "As a canonical VidLM, Frozen-in-Time exemplifies the class of models Paxion patches\u2014revealing their action-knowledge gaps on ActionBench and serving as a frozen host into which the Knowledge Patcher/Fuser integrates."
    },
    {
      "title": "Parameter-Efficient Transfer Learning for NLP (Adapters)",
      "authors": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Sylvain Gelly, Aidan N. Gomez, \u0141ukasz Kaiser",
      "year": 2019,
      "role": "Technique for inserting small trainable modules into frozen transformers to add capabilities without catastrophic forgetting",
      "relationship_sentence": "Paxion\u2019s Knowledge Patcher is conceptually an adapter specialized for action dynamics, enabling parameter-efficient augmentation of frozen VidLMs while preserving their prior competencies."
    },
    {
      "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning",
      "authors": "Jonas Pfeiffer, Andreas R\u00fcckl\u00e9, Clifton Poth, Aishwarya Kamath, Ivan Vuli\u0107, Sebastian Ruder, Kyunghyun Cho, Iryna Gurevych",
      "year": 2021,
      "role": "Method to integrate multiple adapters into a base model without overwriting existing knowledge",
      "relationship_sentence": "Paxion\u2019s Knowledge Fuser echoes AdapterFusion\u2019s non-destructive integration by composing the learned action patch with the frozen VidLM to retain original abilities while adding action knowledge."
    },
    {
      "title": "Shuffle and Learn: Unsupervised Learning using Temporal Order Verification",
      "authors": "Ishan Misra, C. Lawrence Zitnick, Martial Hebert",
      "year": 2016,
      "role": "Pretext task establishing discriminative learning from temporal order and sequence reversal",
      "relationship_sentence": "Paxion\u2019s Discriminative Video Dynamics Modeling (DVDM) objective and the ActionBench Video Reversal probe build directly on the insight that temporal order/reversal discrimination teaches robust motion dynamics."
    },
    {
      "title": "Unsupervised Representation Learning by Sorting Sequences (OPN)",
      "authors": "Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, Ming-Hsuan Yang",
      "year": 2017,
      "role": "Temporal ordering objective for learning video dynamics from sequence permutations",
      "relationship_sentence": "OPN\u2019s sequence-order discrimination informs Paxion\u2019s design of a discriminative dynamics objective that explicitly encodes temporal directionality into video\u2013text alignment."
    },
    {
      "title": "The \"Something-Something\" Video Database for Learning and Evaluating Common Sense",
      "authors": "Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, et al.",
      "year": 2017,
      "role": "Dataset emphasizing fine-grained, often antonymic, action pairs requiring temporal reasoning over object identity",
      "relationship_sentence": "The prevalence of opposing action pairs in Something-Something motivates Paxion\u2019s Action Antonym probe and highlights the need to move beyond object shortcuts toward true action understanding."
    }
  ],
  "synthesis_narrative": "Paxion\u2019s core contribution\u2014patching explicit action knowledge into frozen video-language models with a discriminative dynamics objective\u2014sits at the intersection of three lines of work. First, vision-language pretraining established by CLIP and extended to video in Frozen-in-Time provides the frozen backbones and contrastive alignment paradigm that Paxion augments. These VidLMs excel at object-centric cues but underperform on temporal action understanding, motivating the need for targeted intervention. Second, parameter-efficient model augmentation via Adapters and their non-destructive composition in AdapterFusion directly inspire Paxion\u2019s two-component architecture: a Knowledge Patcher (an adapter-like module specialized for action dynamics) and a Knowledge Fuser that integrates this patch without eroding existing capabilities. This design explicitly operationalizes non-destructive knowledge injection for multimodal, temporal skills.\nThird, classic self-supervised video representation learning from temporal discrimination\u2014Shuffle and Learn and OPN\u2014demonstrates that predicting order and reversals is a powerful supervisory signal for motion and temporal direction. Paxion\u2019s Discriminative Video Dynamics Modeling (DVDM) inherits this insight, but couples it with video\u2013text alignment to encode action semantics, not just motion patterns. Complementing this, the Something-Something dataset\u2019s antonymic, fine-grained actions motivate ActionBench\u2019s Action Antonym and Video Reversal probes, which diagnose object-shortcut reliance. Together, these prior works directly scaffold Paxion\u2019s diagnostic benchmark, its adapter-style patch-and-fuse mechanism, and its dynamics-aware objective to endow VidLMs with robust action knowledge.",
  "analysis_timestamp": "2026-01-06T23:42:49.071745"
}
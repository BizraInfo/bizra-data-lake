{
  "prior_works": [
    {
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer",
      "year": 2022,
      "role": "Foundational generative model (Stable/Latent Diffusion) and training recipe",
      "relationship_sentence": "Mind-Video co-trains with an augmented Stable Diffusion backbone and generates videos in its latent space, directly building on the latent-diffusion formulation and guidance strategies introduced here."
    },
    {
      "title": "High-resolution image reconstruction with latent diffusion models from human brain activity",
      "authors": "Yu Takagi, Shinji Nishimoto",
      "year": 2023,
      "role": "Direct precursor for diffusion-based brain-to-image reconstruction",
      "relationship_sentence": "This work demonstrated that fMRI signals can be mapped into Stable Diffusion\u2019s latent/text spaces for high-fidelity static image reconstruction, a core idea Mind-Video extends to continuous video via temporal modeling and co-training."
    },
    {
      "title": "Reconstructing visual experiences from brain activity evoked by natural movies",
      "authors": "Shinji Nishimoto, An Vu, Jack Gallant and colleagues",
      "year": 2011,
      "role": "Seminal video decoding from fMRI",
      "relationship_sentence": "Mind-Video tackles the same goal of recovering continuous visual experiences but replaces encoding-based movie retrieval with modern deep generative modeling to produce high-quality videos at arbitrary frame rates."
    },
    {
      "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset (I3D)",
      "authors": "Joao Carreira, Andrew Zisserman",
      "year": 2017,
      "role": "Architectural inspiration for temporal inflation",
      "relationship_sentence": "Mind-Video\u2019s \u2018network temporal inflation\u2019 directly follows the I3D idea of inflating 2D convolutional (or U-Net) filters into 3D to capture spatiotemporal structure for video generation."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "authors": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, Ross Girshick",
      "year": 2022,
      "role": "Pretext task for masked representation learning",
      "relationship_sentence": "Mind-Video\u2019s masked brain modeling adapts the MAE paradigm to fMRI sequences, learning strong spatiotemporal cortical representations via reconstructing masked inputs."
    },
    {
      "title": "TimeSformer: Is Space-Time Attention All You Need for Video Understanding?",
      "authors": "Gedas Bertasius, Heng Wang, Lorenzo Torresani",
      "year": 2021,
      "role": "Spatiotemporal attention design for video",
      "relationship_sentence": "Mind-Video\u2019s multimodal contrastive learning with spatiotemporal attention draws on TimeSformer-style factorized attention to model space-time dependencies when aligning brain and visual features."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, et al.",
      "year": 2021,
      "role": "Multimodal contrastive learning framework",
      "relationship_sentence": "Mind-Video employs CLIP-like contrastive objectives and semantic embeddings to align fMRI-derived representations with visual semantics, improving semantic consistency in reconstructions."
    }
  ],
  "synthesis_narrative": "Mind-Video\u2019s core innovation\u2014high-quality video reconstruction from continuous fMRI\u2014emerges from fusing advances in brain decoding, video representation learning, and modern diffusion-based generation. At the generative core, Latent Diffusion (Stable Diffusion) supplies a scalable latent-space generator and guidance mechanisms. The brain-to-diffusion linkage was concretely validated by Takagi and Nishimoto, who showed that fMRI can drive Stable Diffusion to reconstruct static images; Mind-Video generalizes this mapping to the spatiotemporal domain and co-trains an augmented diffusion backbone rather than relying solely on regressors into a frozen model.\n\nAchieving temporal coherence required importing video inductive biases: following I3D, Mind-Video inflates 2D U-Net components into 3D to model time, enabling true video generation in the diffusion latent. Complementing this, TimeSformer\u2019s space-time attention informs the design of spatiotemporal attention used during multimodal contrastive learning, which aligns evolving cortical activity with evolving visual representations. CLIP\u2019s contrastive learning paradigm provides the blueprint for aligning heterogeneous modalities (here, fMRI and video/semantic embeddings), boosting semantic faithfulness of reconstructions.\n\nTo pretrain robust brain representations from limited labeled data, Mind-Video adapts Masked Autoencoders to \u201cmasked brain modeling,\u201d learning powerful spatiotemporal fMRI encodings by reconstructing masked inputs. Finally, the work stands on the shoulders of early continuous-movie decoding by Nishimoto et al., but replaces database-driven retrieval/encoding with a generative, temporally aware diffusion framework. Together, these strands\u2014diffusion generation, temporal inflation, spatiotemporal attention, contrastive alignment, and masked pretraining\u2014coalesce into a system capable of reconstructing semantically consistent, temporally coherent videos from brain activity.",
  "analysis_timestamp": "2026-01-07T00:02:04.822508"
}
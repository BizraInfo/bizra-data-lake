{
  "prior_works": [
    {
      "title": "A 128\u00d7128 120 dB 15 \u00b5s latency asynchronous temporal contrast vision sensor",
      "authors": "Patrick Lichtsteiner, Christoph Posch, Tobi Delbr\u00fcck",
      "year": 2008,
      "role": "Foundational event camera sensing",
      "relationship_sentence": "This work established the core principle of event-driven, microsecond-latency vision, directly motivating our use of an asynchronous processing branch to exploit the temporal sparsity and immediacy of event streams for millisecond anomaly detection."
    },
    {
      "title": "A 240\u00d7180 130 dB 3 \u00b5s Latency Global Shutter Spatiotemporal Vision Sensor (DAVIS)",
      "authors": "Christian Brandli, Raphael Berner, Minhao Yang, Shih-Chii Liu, Tobi Delbr\u00fcck",
      "year": 2014,
      "role": "Joint event+frame sensing enabling multimodal fusion",
      "relationship_sentence": "By combining standard intensity frames with events in a single sensor, DAVIS concretely motivated our multimodal design that fuses event-based temporal cues with RGB spatial features for robust, real-time driving anomaly detection."
    },
    {
      "title": "HATS: Histograms of Averaged Time Surfaces for Robust Event-Based Object Classification",
      "authors": "Amos Sironi, Manuele Brambilla, Nicolas Bourdis, Xavier Lagorce, Ryad Benosman",
      "year": 2018,
      "role": "Asynchronous event-native spatiotemporal descriptors",
      "relationship_sentence": "HATS demonstrated that local, time-surface neighborhoods yield strong event-based features; we build on this idea by structuring events into spatiotemporal neighborhoods for our asynchronous GNN to achieve low-latency, discriminative updates."
    },
    {
      "title": "Inductive Representation Learning on Temporal Graphs (TGAT)",
      "authors": "Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan",
      "year": 2020,
      "role": "Time-aware message passing with continuous-time encodings",
      "relationship_sentence": "TGAT\u2019s continuous-time attention and time encoding directly informed our event-wise, asynchronous message passing in the event-stream GNN, enabling inference at the precise arrival times of events without batching."
    },
    {
      "title": "Temporal Graph Networks (TGN) for Deep Learning on Dynamic Graphs",
      "authors": "Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, Michael M. Bronstein",
      "year": 2020,
      "role": "Streaming, memory-based GNN updates for event sequences",
      "relationship_sentence": "We adapt TGN\u2019s notion of memory/state updates on temporal events to maintain lightweight per-node states for the event graph, which is key to our millisecond-level response and computational efficiency."
    },
    {
      "title": "Events-to-Video: Bringing Modern Computer Vision to Event Cameras",
      "authors": "Henri Rebecq, Daniel Gehrig, Davide Scaramuzza",
      "year": 2019,
      "role": "Bridging event and frame modalities",
      "relationship_sentence": "This work evidenced the complementarity of events and frames, supporting our strategy to fuse event-driven temporal dynamics with CNN-extracted RGB spatial detail rather than relying on a single modality."
    },
    {
      "title": "Street Scene: A New Benchmark and Evaluation for Video Anomaly Detection",
      "authors": "Bharathkumar Ramachandra, Michael J. Jones",
      "year": 2020,
      "role": "Driving-scene anomaly detection protocol and metrics",
      "relationship_sentence": "Street Scene shaped the problem formulation and evaluation for urban anomalies; we extend this paradigm to a multimodal, latency-aware setting with event-RGB fusion to meet real-time autonomy constraints."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central contribution\u2014a multimodal, latency-aware anomaly detector that marries an asynchronous event-stream GNN with an RGB CNN\u2014emerges from three converging lines of prior work. First, neuromorphic sensing established the need and means for microsecond-latency perception. The DVS (Lichtsteiner et al., 2008) introduced event-driven vision, while DAVIS (Brandli et al., 2014) co-located events and frames, directly enabling practical event\u2013RGB fusion. Event-native feature design (Sironi et al., 2018, HATS) then demonstrated that spatiotemporal neighborhoods around events yield robust, low-latency descriptors, a principle our method inherits by organizing events into local structures suitable for graph processing.\nSecond, the asynchronous processing mechanics of dynamic graphs (Xu et al., 2020, TGAT; Rossi et al., 2020, TGN) provided the algorithmic blueprint for continuous-time, event-wise message passing with lightweight memory updates. We adapt these ideas to event-camera data, enabling our GNN to update representations at event arrival times without expensive batching\u2014crucial for millisecond response.\nThird, multimodal synergy and application grounding in driving anomalies informed our fusion and evaluation choices. Events-to-Video (Rebecq et al., 2019) showed the complementarity of event temporal detail with frame-based spatial content, justifying our hybrid architecture. Street Scene (Ramachandra & Jones, 2020) clarified anomaly definitions and metrics in urban settings, which we extend to real-time constraints. Together, these works directly shaped our asynchronous graph design, event\u2013RGB fusion strategy, and latency-first evaluation, culminating in a system that advances both accuracy and response time for safety-critical driving scenarios.",
  "analysis_timestamp": "2026-01-07T00:04:09.146493"
}
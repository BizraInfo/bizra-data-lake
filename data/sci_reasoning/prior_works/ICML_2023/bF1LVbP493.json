{
  "prior_works": [
    {
      "title": "Donut: Document Understanding Transformer without OCR",
      "authors": "Geewook Kim et al.",
      "year": 2022,
      "role": "Baseline",
      "relationship_sentence": "Donut established the OCR-free, image-to-text paradigm for document understanding that Pix2Struct adopts and generalizes, with Pix2Struct replacing task-specific generation by large-scale pretraining to generate webpage HTML from screenshots."
    },
    {
      "title": "Image-to-Markup Generation with Coarse-to-Fine Attention",
      "authors": "Yuntian Deng et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "This work formulated image-to-sequence markup generation (e.g., LaTeX) with an encoder\u2013decoder and attention, a foundational setup that Pix2Struct extends to parsing full web screenshots into HTML as its pretraining signal."
    },
    {
      "title": "pix2code: Generating Code from a Graphical User Interface Screenshot",
      "authors": "Tony Beltramelli",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "pix2code directly inspired the idea of converting UI screenshots into structured code/markup, which Pix2Struct scales and repurposes as a pretraining objective by predicting simplified HTML from webpage screenshots."
    },
    {
      "title": "LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding",
      "authors": "Yiheng Xu et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "LayoutLMv2\u2019s strong results depend on OCR tokens and modality-specific engineering, a limitation Pix2Struct explicitly targets by learning directly from pixels via screenshot-to-HTML pretraining to subsume OCR signals."
    },
    {
      "title": "TextVQA: Towards VQA Models That Can Read",
      "authors": "Anurag Singh et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "TextVQA crystallized the need to read text within images for visual question answering using OCR-centric pipelines, motivating Pix2Struct\u2019s OCR-free formulation that learns reading and reasoning through screenshot parsing."
    },
    {
      "title": "PubLayNet: Largest Dataset Ever for Document Layout Analysis",
      "authors": "Xu Zhong et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "PubLayNet showed how to mine large-scale supervision by leveraging existing digital document structure, an approach Pix2Struct echoes by exploiting web page HTML as paired supervision for pretraining from screenshots."
    }
  ],
  "synthesis_narrative": "Pix2Struct\u2019s core leap is to pretrain a general visual\u2013language model by parsing masked webpage screenshots into simplified HTML, thereby learning OCR, layout, and semantics from a single image-to-text objective. This builds squarely on the image-to-markup lineage: Im2Markup established encoder\u2013decoder generation of structured markup from images, and pix2code demonstrated screenshot-to-code translation for GUIs; Pix2Struct scales these ideas to the web and reframes them as a universal pretraining signal. In parallel, Donut crystallized the promise of OCR-free document understanding via end-to-end image-to-text generation; Pix2Struct takes this paradigm and replaces task-specific pretraining with a broad, naturally supervised objective\u2014predicting HTML from screenshots\u2014that better transfers across visually situated language tasks. The paper is also motivated by the limitations of OCR-dependent pipelines like LayoutLMv2 and TextVQA-era systems, which hinge on external OCR and bespoke modality fusion; Pix2Struct directly addresses these gaps by learning to \u201cread\u201d and structure content purely from pixels. Finally, PubLayNet\u2019s success at harvesting supervision from existing digital artifacts informs Pix2Struct\u2019s choice to mine supervision from the web\u2019s DOM/HTML, aligning visual elements with a clean structural target. Together, these works directly shaped Pix2Struct\u2019s decision to unify vision and language understanding through large-scale screenshot-to-HTML pretraining.",
  "analysis_timestamp": "2026-01-06T23:09:26.512751"
}
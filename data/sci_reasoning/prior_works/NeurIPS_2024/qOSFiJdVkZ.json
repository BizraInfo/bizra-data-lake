{
  "prior_works": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Theoretical foundation (NTK and lazy regime)",
      "relationship_sentence": "Introduced the NTK and the lazy-training limit, enabling the present paper\u2019s reinterpretation of a parametrized network as a set of fixed tangent features that can be treated as an ensemble of experts."
    },
    {
      "title": "Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent",
      "authors": "Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jeffrey Pennington, Jascha Sohl-Dickstein",
      "year": 2019,
      "role": "Training dynamics equivalence",
      "relationship_sentence": "Established that gradient descent training follows the linearized (NTK) dynamics, which this work leverages to show that Bayesian expert posterior updates align with a scaled, projected form of SGD on the original weights."
    },
    {
      "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
      "authors": "Martin A. Mandt, Matthew D. Hoffman, David M. Blei",
      "year": 2017,
      "role": "Bridge between SGD and Bayesian updates",
      "relationship_sentence": "Provides the conceptual and mathematical link between SGD and Bayesian posterior inference that underpins the paper\u2019s equivalence between expert posterior updates and SGD steps."
    },
    {
      "title": "Variational Continual Learning",
      "authors": "Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, Richard E. Turner",
      "year": 2018,
      "role": "Bayesian continual learning principle",
      "relationship_sentence": "Showed that applying Bayes\u2019 rule sequentially mitigates forgetting in neural networks, motivating the paper\u2019s Bayesian-ensemble approach over fixed experts for continual learning."
    },
    {
      "title": "Online Structured Laplace Approximations for Overcoming Catastrophic Forgetting",
      "authors": "Hippolyt Ritter, Aleksandar Botev, David Barber",
      "year": 2018,
      "role": "Online Bayesian posterior updates for CL",
      "relationship_sentence": "Demonstrated online posterior updates with curvature information for continual learning, closely related to the paper\u2019s scaled/projection view of SGD arising from expert posteriors."
    },
    {
      "title": "Overcoming Catastrophic Forgetting in Neural Networks",
      "authors": "James Kirkpatrick et al.",
      "year": 2017,
      "role": "Foundational continual learning baseline (Fisher-weighted updates)",
      "relationship_sentence": "Introduced Fisher-weighted constraints to preserve past knowledge, a precursor to interpreting parameter updates as curvature-scaled projections that the paper derives from Bayesian expert updates."
    },
    {
      "title": "Random Features for Large-Scale Kernel Machines",
      "authors": "Ali Rahimi, Benjamin Recht",
      "year": 2008,
      "role": "Fixed-feature ensemble perspective",
      "relationship_sentence": "Pioneered approximating kernels with fixed features linearly combined for prediction, conceptually informing the paper\u2019s view of a neural network as a weighted ensemble of fixed NTK \u2018experts.\u2019"
    }
  ],
  "synthesis_narrative": "The paper\u2019s core idea\u2014interpreting a neural network as an ensemble of neural tangent experts and deriving Bayesian updates that coincide with scaled, projected SGD\u2014rests on three converging lines of prior work. First, NTK theory (Jacot et al., 2018) and linearized training dynamics (Lee et al., 2019) formalize the lazy regime in which a network\u2019s tangent features are fixed, making it natural to treat parameters as weights over a basis of fixed functions. This fixed-feature perspective echoes the random features program (Rahimi & Recht, 2008), where prediction arises from linearly weighting immutable basis functions, anticipating the paper\u2019s \u201cexpert\u201d viewpoint.\nSecond, continual learning through Bayesian updating (Nguyen et al., 2018; Ritter et al., 2018) established Bayes\u2019 rule as a principled antidote to forgetting, with practical approximations via variational or Laplace methods. These works motivate the paper\u2019s strategy: if a single network can be recast as a Bayesian ensemble of fixed experts, then continual learning reduces to sequential posterior updates over those experts. EWC (Kirkpatrick et al., 2017) further highlighted the importance of curvature-weighted constraints, foreshadowing the curvature-scaled projections that emerge in the authors\u2019 analysis.\nThird, the equivalence between SGD and Bayesian inference (Mandt et al., 2017) provides the crucial bridge to show that expert posterior updates map to a particular scaled/projected SGD on network weights. Together, these threads directly enable the paper\u2019s main contribution: a theoretically grounded ensemble interpretation of neural networks that yields practical, forgetting-resistant continual learning algorithms.",
  "analysis_timestamp": "2026-01-06T23:33:35.522251"
}
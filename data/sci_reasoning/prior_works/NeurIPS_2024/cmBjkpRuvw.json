{
  "prior_works": [
    {
      "title": "Deep Reinforcement Learning from Human Preferences",
      "authors": "Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei",
      "year": 2017,
      "role": "Established the RLHF pipeline that learns a reward model via maximum-likelihood on pairwise human comparisons, typically instantiated with Bradley\u2013Terry\u2013Luce.",
      "relationship_sentence": "The paper\u2019s critique and axiomatization target precisely this RLHF reward-learning step, motivating axiomatic replacements for the BTL-style estimators used by Christiano et al."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "role": "Scaled RLHF to large language models using preference-based reward modeling with logistic (BTL/Plackett\u2013Luce) likelihoods.",
      "relationship_sentence": "By showing that standard preference MLE methods used in InstructGPT fail basic axioms, the paper justifies developing new aggregation rules with guarantees for high-stakes LLM alignment."
    },
    {
      "title": "Rank Analysis of Incomplete Block Designs I: The Method of Paired Comparisons",
      "authors": "Ralph A. Bradley, Milton E. Terry",
      "year": 1952,
      "role": "Introduced the Bradley\u2013Terry model for pairwise comparisons, the canonical logistic random-utility foundation for modern preference learning.",
      "relationship_sentence": "The authors demonstrate that BTL-based reward inference violates core social choice axioms and thus cannot serve as an axiomatic basis for RLHF aggregation."
    },
    {
      "title": "Individual Choice Behavior: A Theoretical Analysis",
      "authors": "R. Duncan Luce",
      "year": 1959,
      "role": "Formulated Luce\u2019s choice axiom and underpinned the Plackett\u2013Luce family of discrete-choice/random-utility models.",
      "relationship_sentence": "The paper\u2019s negative axiomatic results extend beyond BTL to Luce/Plackett\u2013Luce generalizations, motivating alternative aggregation rules."
    },
    {
      "title": "A law of comparative judgment",
      "authors": "L. L. Thurstone",
      "year": 1927,
      "role": "Early random-utility (probit) foundation for paired comparison data and maximum-likelihood estimation of latent utilities.",
      "relationship_sentence": "By evaluating broad RUM-based MLE (including Thurstone\u2013Mosteller models), the authors show that popular probabilistic preference estimators broadly fail desirable axioms."
    },
    {
      "title": "Social Choice and Individual Values",
      "authors": "Kenneth J. Arrow",
      "year": 1951,
      "role": "Founded axiomatic social choice, introducing the paradigm of evaluating aggregation rules via basic normative properties.",
      "relationship_sentence": "The paper adopts Arrow\u2019s axiomatic perspective to judge RLHF reward aggregation and to articulate the standards their new rules satisfy."
    },
    {
      "title": "The Characterization of Implementable Choice Rules",
      "authors": "Kevin W. S. Roberts",
      "year": 1979,
      "role": "Showed that, under quasilinear structure, implementable social choice functions are affine (weighted-sum) maximizers, a seminal \u2018linearity restricts rules\u2019 result.",
      "relationship_sentence": "Inspiration for the paper\u2019s \u2018linear social choice\u2019 paradigm: recognizing linear structure in the RLHF setting leads to strong characterization and design of axiomatic aggregation rules."
    }
  ],
  "synthesis_narrative": "The paper reframes reward learning in RLHF as a problem of preference aggregation and subjects it to a social-choice axiomatic lens. The immediate technical target is the dominant practice inaugurated by Christiano et al. (2017) and scaled by Ouyang et al. (2022), which fits reward models via maximum likelihood on pairwise comparisons under Bradley\u2013Terry\u2013Luce (BTL/Plackett\u2013Luce) families. These models\u2014rooted in Bradley and Terry\u2019s (1952) paired-comparison logistic formulation, Luce\u2019s (1959) choice axiom, and Thurstone\u2019s (1927) probit alternative\u2014provide the statistical backbone of modern preference-based reward inference. By evaluating these random-utility MLE estimators against classic social choice desiderata in Arrow\u2019s (1951) tradition, the authors show that both BTL and broader RUM generalizations violate basic axioms, revealing a mismatch between statistical convenience and normative soundness for alignment.\nRecognizing a special linear structure in the RLHF reward-learning problem, the authors then pivot to designing aggregation rules with provable axiomatic guarantees. This move is conceptually aligned with Roberts\u2019 (1979) characterization in mechanism design, where linear (quasilinear) environments sharply constrain admissible social choice rules to affine maximizers. Analogously, the paper\u2019s \u2018linear social choice\u2019 framework leverages linearity to narrow the feasible space of aggregation procedures and to deliver new, principled rules for learning reward functions. Together, these works directly inform the critique of existing RUM-based approaches and the construction of axiomatic, alignment-grounded alternatives.",
  "analysis_timestamp": "2026-01-07T00:02:04.758829"
}
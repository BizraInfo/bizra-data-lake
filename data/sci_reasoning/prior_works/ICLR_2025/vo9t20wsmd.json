{
  "prior_works": [
    {
      "title": "Fast Inference from Transformers via Speculative Decoding",
      "authors": "Yael Leviathan et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Introduces the quality-neutral speculative decoding mechanism\u2014drafting with a small model and verifying with a large model in parallel\u2014which this paper repurposes to implement cascade deferral via speculative execution."
    },
    {
      "title": "Accelerating Large Language Model Decoding with Speculative Sampling",
      "authors": "Chen et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "Generalizes speculative decoding to sampling-based generation and highlights that speedups preserve the large model\u2019s distribution, a property this paper leverages while embedding a deferral rule into speculative execution."
    },
    {
      "title": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance",
      "authors": "Jiang et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "Establishes LLM cascades that route to stronger models only for hard inputs and demonstrates superior cost\u2013quality trade-offs, providing the primary cascade baseline that this paper accelerates using speculative execution."
    },
    {
      "title": "Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer to a Human",
      "authors": "Jesse Madras et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Formalizes the learning-to-defer decision rule\u2014invoking a stronger expert when uncertain\u2014which directly underpins the optimal deferral formulation derived for cascading between small and large LMs."
    },
    {
      "title": "SelectiveNet: A Deep Neural Network with an Integrated Reject Option",
      "authors": "Yoav Geifman et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Provides a practical risk\u2013coverage framework and plug-in confidence-based gating for selective prediction, which motivates this paper\u2019s plug-in approximation to the optimal deferral rule."
    },
    {
      "title": "Confident Adaptive Language Modeling",
      "authors": "Tal Schuster et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "Demonstrates token-level adaptive inference for LMs via confidence thresholds and calibration, informing the design of lightweight, confidence-driven deferral rules in the proposed speculative cascade."
    },
    {
      "title": "On Optimum Recognition Error and Reject Tradeoff",
      "authors": "C. K. Chow",
      "year": 1970,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Derives the optimal reject (deferral) rule based on posteriors and reject cost, which this paper adapts to characterize the optimal deferral criterion for speculative cascades."
    }
  ],
  "synthesis_narrative": "Speculative decoding established a quality-neutral path to faster generation by having a lightweight draft model propose tokens that a larger model verifies in parallel, ensuring the final distribution matches the large model. Subsequent speculative sampling broadened this to stochastic decoding while reinforcing that gains come from parallel verification rather than changing the target distribution. In parallel, cascaded decision-making matured through formal learning-to-defer frameworks that trigger a stronger expert when uncertainty is high, and selective prediction methods that operationalize risk\u2013coverage trade-offs using confidence-based, plug-in gates. SelectiveNet showed how to implement practical reject options with calibrated confidence, while classical results on reject-option optimality precisely characterize when deferral minimizes expected cost given error penalties. For language models specifically, confident adaptive inference demonstrated that calibrated confidence thresholds can guide early exits and token-level adaptivity, and FrugalGPT exhibited that model cascades\u2014invoking larger models only for hard inputs\u2014can beat single-model baselines on cost\u2013quality trade-offs. Together, these works reveal a gap: classical cascades provide strong cost\u2013quality gains but lack the quality-neutral speedups of speculative execution, while speculative methods ensure neutrality yet do not exploit deferral-based cost\u2013quality improvements. The current paper synthesizes these threads by implementing the cascade deferral rule through speculative execution, deriving the optimal deferral criterion in the reject-option sense, and using a plug-in approximation to realize it in practice\u2014thereby unifying cascade routing benefits with the parallel, quality-preserving advantages of speculative decoding.",
  "target_paper": {
    "title": "Faster Cascades via Speculative Decoding",
    "authors": "Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Seungyeon Kim, Neha Gupta, Aditya Krishna Menon, Sanjiv Kumar",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Cascades, Speculative Decoding, Speculative execution, LLM, Inference, Adaptive Inference",
    "abstract": "Cascades and speculative decoding are two common approaches to improving language models' inference efficiency.  Both approaches interleave two models, but via fundamentally distinct mechanisms: deferral rule that invokes the larger model only for \u201chard\u201d inputs, while  speculative decoding uses speculative execution to primarily invoke the larger model in parallel scoring mode. These mechanisms offer different benefits: empirically, cascades offer compelling cost-quality trade-offs, often even outperforming the large model; speculative cascades offer impressive speed-ups, while guaranteeing quality-neutrality. In this paper, we leverage the best of both these approaches by designing new speculative cascading techniques that implement their deferral rule through speculative execution. We characterize the optimal deferral rule for our speculative cascades, and employ a plug-in approximation to the optimal rule.  Experiments with Gemma and T5 models on a range of language benchmarks show ",
    "openreview_id": "vo9t20wsmd",
    "forum_id": "vo9t20wsmd"
  },
  "analysis_timestamp": "2026-01-06T19:56:13.933896"
}
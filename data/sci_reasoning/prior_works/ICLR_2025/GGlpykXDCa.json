{
  "prior_works": [
    {
      "title": "Compositional Semantic Parsing on Semi-Structured Tables",
      "authors": "Pasupat et al.",
      "year": 2015,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "This single-table QA benchmark (WikiTableQuestions) defined table-based question answering but lacked cross-table reasoning, a limitation MMQA explicitly addresses by moving to multi-table, multi-hop settings."
    },
    {
      "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning (WikiSQL)",
      "authors": "Zhong et al.",
      "year": 2017,
      "arxiv_id": "1709.00103",
      "role": "Gap Identification",
      "relationship_sentence": "WikiSQL popularized text-to-SQL evaluation in a single-table regime, whose inability to test join reasoning directly motivated MMQA\u2019s multi-table and multi-hop evaluation design."
    },
    {
      "title": "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task",
      "authors": "Yu et al.",
      "year": 2018,
      "arxiv_id": "1809.08887",
      "role": "Foundation",
      "relationship_sentence": "Spider introduced cross-database multi-table text-to-SQL with explicit primary/foreign key relations, providing the core multi-table join problem formulation that MMQA generalizes into a broader multi-hop QA and diagnostic evaluation setting (including PK/FK selection)."
    },
    {
      "title": "HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data",
      "authors": "Chen et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "HybridQA demonstrated multi-hop reasoning that integrates tabular evidence with additional sources, inspiring MMQA\u2019s emphasis on multi-step inference chains, now grounded purely in inter-related tables."
    },
    {
      "title": "OTT-QA: Open Table-and-Text Question Answering",
      "authors": "Qu et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "OTT-QA\u2019s requirement to retrieve relevant tables for answering questions informed MMQA\u2019s inclusion of a dedicated multi-table retrieval component and evaluation of retrieval accuracy over relational tables."
    },
    {
      "title": "Dr.Spider: A Diagnostic Evaluation Benchmark for Text-to-SQL Parsers",
      "authors": "Li et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Dr.Spider\u2019s diagnostic breakdown (e.g., table/column selection and join path) directly influenced MMQA\u2019s comprehensive evaluation framework, which extends diagnostics to multi-table retrieval and explicit primary/foreign key selection tasks for LLMs."
    },
    {
      "title": "BIRD: Big Bench for Large-Scale Text-to-SQL",
      "authors": "Li et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "BIRD established large-scale, LLM-focused text-to-SQL evaluation over multi-table databases, serving as a primary baseline that MMQA contrasts with by expanding beyond SQL generation to multi-hop QA and schema-relation diagnostics."
    }
  ],
  "synthesis_narrative": "Early table QA efforts showed how natural language questions could be answered from structured tables, with WikiTableQuestions capturing compositional reasoning but constraining evidence to a single table. WikiSQL brought scale to text-to-SQL but remained single-table, precluding join reasoning. Spider changed the landscape by formalizing cross-database text-to-SQL where primary and foreign keys define join paths, making relational reasoning central and explicit. HybridQA demonstrated that questions often demand multi-step inference chains, linking table-derived facts with other evidence sources to answer complex questions. OTT-QA highlighted the necessity of retrieving relevant tables as a first-class component of table-centric QA, showing performance hinges on accurate table identification before reasoning. Dr.Spider then shifted evaluation toward skill diagnostics\u2014such as schema linking and join selection\u2014revealing that nuanced sub-competencies determine end-task success on relational queries. Finally, BIRD scaled text-to-SQL evaluation for the LLM era across large, multi-table databases, surfacing where LLMs succeed and fail on schema-rich problems. Together these works revealed a gap: there was no benchmark that simultaneously targets multi-table retrieval, multi-hop reasoning across relational tables, and explicit primary/foreign key competence for LLMs. Building on Spider\u2019s relational schema framing and Dr.Spider\u2019s diagnostic philosophy, while incorporating OTT-QA\u2019s retrieval emphasis and the LLM evaluation lessons from BIRD, the next natural step is a unified dataset and framework that evaluates multi-table retrieval, text-to-SQL, end-to-end QA, and key-selection skills\u2014precisely what MMQA provides.",
  "target_paper": {
    "title": "MMQA: Evaluating LLMs with Multi-Table Multi-Hop Complex Questions",
    "authors": "Jian Wu, Linyi Yang, Dongyuan Li, Yuliang Ji, Manabu Okumura, Yue Zhang",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "LLM evaluation, multi-table question answering; multi-hop question answering",
    "abstract": "While large language models (LLMs) have made strides in understanding tabular data, current tabular evaluation benchmarks, such as WikiTableQuestions and WikiSQL, are focus on single-table scenarios, which cannot necessarily reflect the complexity of real-world applications. To bridge this gap, we present a \\textbf{M}ulti-table and \nMulti-hop Question Answering (MMQA) dataset to assess LLMs' understanding and reasoning capabilities in handling multi-table tasks. The MMQA dataset demands that models perform multiple inferences by drawing evidence from various tables, which are designed to be connected with each other and require models to identify and utilize relationships such as foreign and primary keys. Then, we introduce a comprehensive evaluation framework that tailors to assess LLMs' capabilities in several aspects including Multi-Table Retrieval, Text-to-SQL Generation, Multi-Table QA, Primary Key Selection, and Foreign Key Selection. \nFinally, we propose a novel multi-table retr",
    "openreview_id": "GGlpykXDCa",
    "forum_id": "GGlpykXDCa"
  },
  "analysis_timestamp": "2026-01-06T15:50:43.665297"
}
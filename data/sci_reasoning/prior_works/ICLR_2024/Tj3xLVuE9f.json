{
  "prior_works": [
    {
      "title": "Shortcut Learning in Deep Neural Networks",
      "authors": "Robert Geirhos et al.",
      "year": 2020,
      "arxiv_id": "2004.07780",
      "role": "Foundation",
      "relationship_sentence": "It articulated the notion that models prefer 'shortcuts'\u2014features that are easier to extract than the intended signal\u2014directly motivating this paper\u2019s formalization of feature 'availability' and its quantitative study of shortcut use."
    },
    {
      "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
      "authors": "Robert Geirhos et al.",
      "year": 2019,
      "arxiv_id": "1811.12231",
      "role": "Inspiration",
      "relationship_sentence": "Its texture-over-shape finding suggested that locality and global integration govern ease of extraction, which this work systematically manipulates to test how availability competes with predictivity."
    },
    {
      "title": "Invariant Risk Minimization",
      "authors": "Martin Arjovsky et al.",
      "year": 2019,
      "arxiv_id": "1907.02893",
      "role": "Extension",
      "relationship_sentence": "The Colored MNIST setup with a core feature (digit shape) and a spurious feature (color) provided the two-feature generative template that this paper generalizes by decoupling predictivity from ease-of-extraction and introducing continuous availability knobs."
    },
    {
      "title": "Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Invariance",
      "authors": "Shiori Sagawa et al.",
      "year": 2020,
      "arxiv_id": "1911.08731",
      "role": "Gap Identification",
      "relationship_sentence": "By framing spurious correlation primarily in terms of group-wise predictivity (e.g., Waterbirds) and optimizing worst-group risk, it left unaddressed how availability shapes feature choice\u2014a limitation this work targets with a new bias metric and controlled datasets."
    },
    {
      "title": "Approximating CNNs with Bag-of-Local-Features models works surprisingly well on ImageNet",
      "authors": "Wieland Brendel et al.",
      "year": 2019,
      "arxiv_id": "1904.00760",
      "role": "Inspiration",
      "relationship_sentence": "Showing that local texture patches suffice for high ImageNet accuracy implied an architectural bias toward local cues, which informed the hypothesis and experiments that treat locality as a key determinant of feature availability."
    },
    {
      "title": "On the Spectral Bias of Neural Networks: Towards Understanding the Frequency Principle",
      "authors": "Nasim Rahaman et al.",
      "year": 2019,
      "arxiv_id": "1806.08734",
      "role": "Inspiration",
      "relationship_sentence": "Its result that networks learn low-frequency components first motivated the design of latent features with controlled frequency content to probe how frequency-based availability affects shortcut selection."
    },
    {
      "title": "Unmasking Clever Hans predictors and assessing what machines really learn",
      "authors": "Sebastian Lapuschkin et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "By uncovering cases where models rely on spurious context (e.g., backgrounds, watermarks) rather than the intended object, it provided concrete shortcut phenomena that this work reproduces in a controlled generative form to measure over-reliance."
    }
  ],
  "synthesis_narrative": "Shortcut learning was crystallized as a central paradigm by work showing that deep networks preferentially exploit features that are easier to extract than the intended signal, framing such cues as \u201cshortcuts\u201d rather than mere noise. Empirical evidence that ImageNet CNNs favor texture over shape pinpointed locality and global integration as determinants of ease, while bag-of-local-features results demonstrated that local patches can carry enough discriminative signal to drive high accuracy, revealing an architectural bias toward local, texture-like cues. Complementary theory established that networks tend to learn low-frequency or simpler components first, suggesting a frequency-based notion of feature ease. In parallel, the IRM framework introduced a clean two-feature synthetic template (Colored MNIST) to study spurious correlations via environments that modulate feature-label predictivity, and group-robust training/evaluation highlighted the worst-group failure modes when spurious correlations dominate, often illustrated by background\u2013foreground confounds. Explanatory analyses of \u201cClever Hans\u201d failures further documented real-world reliance on contextual artifacts like backgrounds and watermarks.\nTogether these works exposed a gap: existing benchmarks and methods largely operationalize spuriousness through predictivity (correlation strength) or environment variation, but lack a principled handle on the ease with which features are extracted. The present study synthesizes these insights by introducing a minimal generative setup with two latent features whose predictivity and availability (via locality, frequency, and linear decodability) can be independently tuned, and by defining a shortcut-bias metric to quantify over-reliance, thereby unifying empirical, architectural, and theoretical threads into a controlled analysis of when and why shortcuts prevail.",
  "target_paper": {
    "title": "On the Foundations of Shortcut Learning",
    "authors": "Katherine Hermann, Hossein Mobahi, Thomas FEL, Michael Curtis Mozer",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "shortcut learning, spurious correlations, architectural inductive bias",
    "abstract": "Deep-learning models can extract a rich assortment of features from data. Which features a model uses depends not only on *predictivity*---how reliably a feature indicates training-set labels---but also on *availability*---how easily the feature can be extracted from inputs. The literature on shortcut learning has noted examples in which models privilege one feature over another, for example texture over shape and image backgrounds over foreground objects. Here, we test hypotheses about which input properties are more available to a model, and systematically study how predictivity and availability interact to shape models' feature use. We construct a minimal, explicit generative framework for synthesizing classification datasets with two latent features that vary in predictivity and in factors we hypothesize to relate to availability, and we quantify a model's shortcut bias---its over-reliance on the shortcut (more available, less predictive) feature at the expense of the core (less av",
    "openreview_id": "Tj3xLVuE9f",
    "forum_id": "Tj3xLVuE9f"
  },
  "analysis_timestamp": "2026-01-06T10:20:04.522736"
}
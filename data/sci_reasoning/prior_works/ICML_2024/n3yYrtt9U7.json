{
  "prior_works": [
    {
      "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
      "authors": "Maziar Raissi et al.",
      "year": 2019,
      "role": "Baseline",
      "relationship_sentence": "P^2INNs directly builds on the PINN formulation\u2014minimizing PDE residual, boundary/initial losses with a neural surrogate\u2014but removes the need to retrain a separate PINN per parameter by conditioning the model on a compact parameter representation."
    },
    {
      "title": "DGM: A deep learning algorithm for solving partial differential equations",
      "authors": "Justin Sirignano et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "DGM established the core idea of training neural networks via physics residuals over collocation points, a foundational principle that underlies PINNs and is preserved in P^2INNs\u2019 physics-driven training across parameter samples."
    },
    {
      "title": "Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators",
      "authors": "Lu Lu et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "DeepONet formalized learning solution operators for parameterized PDEs, providing the operator-learning problem setup that P^2INNs also targets but within a PINN-style, physics-supervised setting."
    },
    {
      "title": "Physics-informed DeepONets: Learning operators that respect underlying physical laws",
      "authors": "Sifan Wang et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "Physics-informed DeepONet showed that operator learning across parameter spaces can be driven primarily by PDE residuals; P^2INNs adopts this physics-supervision idea while replacing the operator architecture with a PINN conditioned on a learned parameter embedding."
    },
    {
      "title": "Fourier Neural Operator for Parametric Partial Differential Equations",
      "authors": "Zongyi Li et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "FNO popularized fast parametric PDE surrogates but requires large paired datasets; P^2INNs explicitly addresses this limitation by achieving parameter generalization without supervised solution data via physics-informed training."
    },
    {
      "title": "PINO: Physics-Informed Neural Operator for learning PDEs",
      "authors": "Zongyi Li et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "PINO integrates physics residuals into operator learning across conditions; P^2INNs pursues the same goal of parameter generalization under physics guidance but does so within the PINN paradigm using an explicit latent encoding of PDE parameters."
    }
  ],
  "synthesis_narrative": "The core innovation of P^2INNs\u2014turning a PINN into a reusable surrogate that generalizes across a PDE\u2019s parameter space via an explicit parameter embedding\u2014arises from two converging threads of prior work. First, DGM and, more concretely, PINNs established the physics-residual training paradigm for neural PDE solvers; this enabled data-free supervision but left a practical gap: each new parameter instance typically required a fresh, time-consuming training run. Second, the operator-learning line (DeepONet) crystallized the problem formulation of learning mappings from parameters/forcing to solution fields, offering a path to amortize inference across parameter spaces. Physics-informed DeepONet demonstrated that such operator learning can be driven primarily by PDE physics rather than paired simulation data, directly inspiring P^2INNs to retain physics-only supervision while changing the representation: instead of a separate operator network, P^2INNs conditions a single PINN on a learned latent code of PDE parameters. In contrast, FNO made parametric surrogates highly efficient but data-hungry; P^2INNs explicitly targets this limitation by avoiding reliance on high-fidelity training pairs. Finally, PINO showed that physics-informed constraints can regularize operator learning across conditions, reinforcing the feasibility of physics-driven generalization that P^2INNs realizes within the PINN framework. Together, these works directly motivate and shape P^2INNs\u2019 design: physics-informed training, operator-style parameter generalization, and removal of repeated per-parameter retraining.",
  "analysis_timestamp": "2026-01-06T23:09:26.435411"
}
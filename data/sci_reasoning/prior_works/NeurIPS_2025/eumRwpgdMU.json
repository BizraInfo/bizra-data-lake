{
  "prior_works": [
    {
      "title": "Deep Reinforcement Learning in Large Discrete Action Spaces",
      "authors": "Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, et al.",
      "year": 2015,
      "role": "Large discrete action space modeling via action embeddings",
      "relationship_sentence": "ARIA\u2019s projection of free-form language actions into a low-dimensional intention space echoes Dulac-Arnold\u2019s embedding-based approach to generalize across similar discrete actions and make learning tractable in enormous action spaces."
    },
    {
      "title": "The Option-Critic Architecture",
      "authors": "Pierre-Luc Bacon, Jean Harb, Doina Precup",
      "year": 2017,
      "role": "Temporal abstraction and latent intention policies",
      "relationship_sentence": "ARIA\u2019s intention space plays a role analogous to options as latent, semantically coherent action abstractions, enabling more stable learning by aggregating behaviors that share a common intent."
    },
    {
      "title": "Hindsight Experience Replay",
      "authors": "Marcin Andrychowicz, Filip Wolski, Alex Ray, et al.",
      "year": 2017,
      "role": "Sparse-reward densification via relabeling",
      "relationship_sentence": "ARIA\u2019s reward aggregation over intention clusters conceptually parallels HER\u2019s relabeling to densify sparse rewards, reducing variance by crediting semantically similar achieved outcomes."
    },
    {
      "title": "Self-Critical Sequence Training for Image Captioning",
      "authors": "Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, Vaibhava Goel",
      "year": 2017,
      "role": "Variance reduction for sequence-level RL in language",
      "relationship_sentence": "ARIA targets the same sequence-level reward variance challenges addressed by SCST, but advances beyond better baselines by structurally aggregating rewards in an intention space to stabilize policy gradients."
    },
    {
      "title": "Deal or No Deal? End-to-End Learning for Negotiation Dialogues",
      "authors": "Mike Lewis, Denis Yarats, Yann N. Dauphin, Devi Parikh, Dhruv Batra",
      "year": 2017,
      "role": "Motivating open-ended language action environment (negotiation)",
      "relationship_sentence": "This work exposed the combinatorial nature and reward sparsity of negotiation with free-form language, directly motivating ARIA\u2019s need to cluster semantically similar utterances and share rewards."
    },
    {
      "title": "GuessWhat?! Visual object discovery through multi-modal dialogue",
      "authors": "Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, Aaron Courville",
      "year": 2017,
      "role": "Motivating open-ended question-asking game with sparse, delayed rewards",
      "relationship_sentence": "ARIA addresses the sparse, episodic rewards and large language action space exemplified by GuessWhat?! by aggregating rewards across intention-aligned questions to improve sample efficiency."
    },
    {
      "title": "TextWorld: A Learning Environment for Text-based Games",
      "authors": "Marc-Alexandre C\u00f4t\u00e9, et al.",
      "year": 2018,
      "role": "Benchmarking RL with combinatorial natural language actions",
      "relationship_sentence": "TextWorld formalizes the challenges of learning with free-form text actions; ARIA\u2019s intention-driven reward sharing provides a principled mechanism to generalize across semantically similar commands in such settings."
    }
  ],
  "synthesis_narrative": "ARIA\u2019s core contribution\u2014projecting free-form language actions into a low-dimensional intention space and aggregating rewards within that space\u2014sits at the intersection of three influential threads. First, prior language interaction environments such as negotiation (Deal or No Deal) and question-asking games (GuessWhat?!) revealed that free-form utterances create combinatorial action spaces and sparse, delayed rewards, a pattern further systematized in text-based game platforms like TextWorld. These works directly motivated the need for methods that tame language action spaces while improving credit assignment.\nSecond, the RL literature on large discrete action spaces and temporal abstraction provided structural tools. Dulac-Arnold et al. introduced action embeddings to generalize across similar actions, a key precursor to ARIA\u2019s intention projection. Option-Critic demonstrated that learning in a latent option space stabilizes control by grouping semantically coherent behaviors\u2014an idea ARIA adapts by clustering language actions into intention-aligned groups.\nThird, techniques for handling sparse and high-variance sequence-level rewards informed ARIA\u2019s variance reduction objective. Hindsight Experience Replay showed how relabeling can densify sparse rewards; ARIA analogously aggregates rewards across intention clusters to increase signal density. In language generation, Self-Critical Sequence Training tackled high-variance policy gradients; ARIA extends beyond improved baselines by restructuring the reward surface itself through intention-aware aggregation. Together, these prior works converge into ARIA\u2019s central innovation: intention-driven reward sharing that reduces variance and enables efficient RL for open-ended language agents.",
  "analysis_timestamp": "2026-01-07T00:02:04.952923"
}
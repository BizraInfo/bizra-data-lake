{
  "prior_works": [
    {
      "title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "authors": "Richard S. Sutton, Doina Precup, Satinder Singh",
      "year": 1999,
      "role": "Temporal abstraction foundation",
      "relationship_sentence": "The paper adopts the core idea of options to shorten effective decision horizons via temporally extended actions/subgoals, directly aligning horizon reduction with improved scalability."
    },
    {
      "title": "Policy invariance under reward transformations: Theory and application to reward shaping",
      "authors": "Andrew Y. Ng, Daishi Harada, Stuart Russell",
      "year": 1999,
      "role": "Reward shaping theory",
      "relationship_sentence": "Potential-based reward shaping provides a principled tool the authors leverage to reduce credit-assignment horizons without altering optimal policies, a key mechanism behind their horizon reduction toolkit."
    },
    {
      "title": "RUDDER: Return Decomposition for Delayed Rewards",
      "authors": "Sebastian Arjona-Medina, Arber Zela, J\u00fcrgen Schmidhuber, et al.",
      "year": 2019,
      "role": "Credit assignment via reward redistribution",
      "relationship_sentence": "RUDDER\u2019s demonstration that redistributing delayed returns dramatically shortens credit assignment directly motivates the paper\u2019s premise that moving rewards earlier is an effective horizon reduction strategy for scalability."
    },
    {
      "title": "MBPO: Model-Based Reinforcement Learning via Short Model Rollouts",
      "authors": "Michael Janner, Justin Fu, Marvin Zhang, Sergey Levine",
      "year": 2019,
      "role": "Short-horizon model-based planning",
      "relationship_sentence": "MBPO shows that restricting planning to short model rollouts mitigates compounding error, a concrete horizon reduction tactic the paper generalizes to the offline RL scaling regime."
    },
    {
      "title": "Conservative Q-Learning for Offline Reinforcement Learning",
      "authors": "Aviral Kumar, Aurick Zhou, George Tucker, Sergey Levine",
      "year": 2020,
      "role": "Influential offline RL baseline highlighting horizon-induced conservatism",
      "relationship_sentence": "CQL\u2019s strong performance yet saturation on long-horizon tasks frames the paper\u2019s diagnosis that horizon length drives poor scaling in offline RL and motivates interventions that shorten the effective horizon."
    },
    {
      "title": "Implicit Q-Learning (IQL): Offline RL with Implicit Value Regularization",
      "authors": "Ilya Kostrikov, Ashvin Nair, Sergey Levine",
      "year": 2021,
      "role": "Scalable offline RL baseline",
      "relationship_sentence": "IQL\u2019s practical success and remaining limitations on long-horizon problems provide an empirical baseline the paper builds on to show that horizon reduction unlocks further scaling with more data and compute."
    },
    {
      "title": "Hindsight Experience Replay",
      "authors": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, et al.",
      "year": 2017,
      "role": "Goal relabeling to reduce effective horizons in sparse-reward tasks",
      "relationship_sentence": "HER\u2019s goal relabeling converts long-horizon sparse rewards into denser short-horizon learning signals, a precursor technique the paper subsumes under the broader lens of horizon reduction for offline RL."
    }
  ],
  "synthesis_narrative": "The paper argues that long horizons are the principal bottleneck to scaling offline reinforcement learning and shows that deliberately shortening the effective horizon is the key to unlocking performance with large datasets. Foundationally, Options (Sutton et al., 1999) established temporal abstraction as a means to compress decision-making over time, while potential-based reward shaping (Ng et al., 1999) provided a theory for modifying rewards without changing optimal policies\u2014both directly inform the paper\u2019s horizon reduction toolkit via subgoals and principled shaping. RUDDER (Arjona-Medina et al., 2019) demonstrated that decomposing returns and redistributing delayed rewards shortens credit-assignment paths, reinforcing the central claim that moving learning signals earlier enables scalability. On the planning side, MBPO (Janner et al., 2019) showed that short model rollouts mitigate compounding error, a concrete instantiation of horizon reduction that the authors extend to the offline regime. Empirically, leading offline RL methods\u2014CQL (Kumar et al., 2020) and IQL (Kostrikov et al., 2021)\u2014deliver strong results yet often saturate on long-horizon tasks, providing both the motivation and baselines against which the paper demonstrates that horizon reduction restores scaling with more data and compute. Finally, HER (Andrychowicz et al., 2017) illustrates how goal relabeling effectively shrinks horizons in sparse-reward settings, anticipating the paper\u2019s broader unification: diverse techniques that shorten effective horizons\u2014temporal abstraction, shaping, reward redistribution, and short-horizon planning\u2014are the decisive lever for scalable offline RL.",
  "analysis_timestamp": "2026-01-07T00:02:04.965367"
}
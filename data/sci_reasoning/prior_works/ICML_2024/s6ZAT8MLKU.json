{
  "prior_works": [
    {
      "title": "The extragradient method for finding saddle points and other problems",
      "authors": "G. M. Korpelevich",
      "year": 1976,
      "role": "Algorithmic precursor (extrapolation for saddle-point/VI)",
      "relationship_sentence": "Alex-GDA\u2019s core idea\u2014taking gradients at extrapolated points\u2014directly builds on Korpelevich\u2019s extragradient principle, and the paper\u2019s improved iteration bounds for alternating updates echo the advantages extragradient brings to monotone saddle-point problems."
    },
    {
      "title": "Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators",
      "authors": "A. Nemirovski",
      "year": 2004,
      "role": "Theoretical baseline and complexity benchmark (Mirror-Prox)",
      "relationship_sentence": "Nemirovski\u2019s Mirror-Prox established sharp complexity guarantees for Lipschitz VI/saddle-point problems using extrapolated gradients; the new analysis positions Alt-GDA/Alex-GDA against these benchmarks and leverages similar stability from extrapolation while retaining simple gradient steps."
    },
    {
      "title": "A modification of the Arrow\u2013Hurwicz method for search of saddle points",
      "authors": "L. D. Popov",
      "year": 1980,
      "role": "Algorithmic precursor (optimistic/extrapolated gradients, OGDA antecedent)",
      "relationship_sentence": "Popov\u2019s method (an antecedent of OGDA) shows how prediction/extrapolation stabilizes min\u2013max dynamics; Alex-GDA can be viewed as a structured alternating instantiation of this extrapolative idea within the GDA family."
    },
    {
      "title": "Training GANs with Optimism",
      "authors": "C. Daskalakis, A. Ilyas, V. Syrgkanis, H. Zeng",
      "year": 2018,
      "role": "Modern revival of optimism/extrapolation in games; empirical/theoretical motivation",
      "relationship_sentence": "This work revived optimistic (extrapolative) updates for min\u2013max, demonstrating faster and more stable behavior than vanilla GDA; it directly motivates the paper\u2019s alternating-extrapolation design and its claim that extrapolated, non-simultaneous updates can be provably superior."
    },
    {
      "title": "A Unified Analysis of Extra-Gradient and Optimistic Gradient Methods for Saddle Point Problems",
      "authors": "K. Mokhtari, A. E. Ozdaglar, S. Pattathil",
      "year": 2020,
      "role": "Unifying theory and rates for extrapolated methods in SC\u2013SC and monotone settings",
      "relationship_sentence": "Unified analyses of EG/OGDA provide rate baselines under strong convexity\u2013concavity and Lipschitzness; the present paper extends this line by isolating the scheduling effect, proving Alt-GDA\u2019s faster global rates and framing Alex-GDA as a unifying alternating extrapolation scheme."
    },
    {
      "title": "Near-Optimal Algorithms for Minimax Optimization",
      "authors": "T. Lin, C. Jin, M. I. Jordan",
      "year": 2020,
      "role": "Complexity frontiers for SC\u2013SC min\u2013max (upper/lower bounds and accelerated schemes)",
      "relationship_sentence": "By charting near-optimal complexity for SC\u2013SC minimax, this work supplies the comparison standard; the new paper\u2019s fine-grained bounds for Sim-GDA vs Alt-GDA are interpreted relative to these frontiers and highlight a provable iteration-complexity gap favoring alternating updates."
    },
    {
      "title": "Parallel and Distributed Computation: Numerical Methods",
      "authors": "D. P. Bertsekas, J. N. Tsitsiklis",
      "year": 1989,
      "role": "Conceptual foundation (Jacobi vs Gauss\u2013Seidel: simultaneous vs alternating updates)",
      "relationship_sentence": "Classical insights that Gauss\u2013Seidel (alternating) schemes often converge faster than Jacobi (simultaneous) iterations inform the paper\u2019s central thesis; the authors formalize an analogous phenomenon for GDA in minimax settings with sharp iteration-complexity separation."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014proving a strict iteration-complexity advantage of alternating gradient descent\u2013ascent (Alt-GDA) over simultaneous GDA (Sim-GDA) and introducing an alternating-extrapolation framework (Alex-GDA)\u2014rests on two intertwined lines of prior work. First, the extrapolation paradigm for variational inequalities and saddle-point problems, pioneered by Korpelevich\u2019s extragradient method and refined by Nemirovski\u2019s Mirror-Prox, established that evaluating gradients at appropriately extrapolated points yields superior stability and complexity guarantees. Popov\u2019s method, later popularized in machine learning as optimistic gradient methods, reinforced that predictive/extrapolative steps tame rotational dynamics typical of min\u2013max games. Modern analyses unifying EG and OGDA (e.g., Mokhtari\u2013Ozdaglar\u2013Pattathil) provided tight rates in strongly-convex\u2013strongly-concave and monotone regimes, forming the technical baseline for understanding how extrapolation interacts with problem structure.\nSecond, the scheduling perspective\u2014whether to update variables simultaneously (Jacobi) or alternately (Gauss\u2013Seidel)\u2014has long been known in numerical methods to affect convergence speed. Bertsekas\u2013Tsitsiklis codified this dichotomy, and empirical advances in adversarial learning (e.g., Daskalakis et al.\u2019s optimism for GANs) suggested that non-simultaneous, extrapolative updates improve behavior in games. Against the broader complexity landscape charted by Lin\u2013Jin\u2013Jordan for SC\u2013SC minimax optimization, the present paper isolates and quantifies the scheduling effect, deriving fine-grained global rates that strictly separate Alt-GDA from Sim-GDA. It then synthesizes the extrapolation and alternation principles into Alex-GDA, a unifying scheme that inherits the stability of extragradient-style methods while preserving the simplicity of GDA, thereby achieving provably smaller iteration complexity.",
  "analysis_timestamp": "2026-01-07T00:02:04.883015"
}
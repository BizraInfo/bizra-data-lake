{
  "prior_works": [
    {
      "title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation",
      "authors": "Ruiz et al.",
      "year": 2022,
      "role": "Personalization baseline (training-heavy) and problem motivation",
      "relationship_sentence": "DreamBooth formalized identity-preserving generation by fine-tuning, highlighting the need for consistent characters across images; 1Prompt1Story explicitly seeks the same goal without any training or model modification."
    },
    {
      "title": "An Image Is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion",
      "authors": "Gal et al.",
      "year": 2022,
      "role": "Personalization via learned token (training-based) baseline",
      "relationship_sentence": "Textual Inversion showed that learning a single token can bind identity, but requires optimization and per-subject data; 1Prompt1Story contrasts this by achieving identity consistency purely via prompt design with zero optimization."
    },
    {
      "title": "Prompt-to-Prompt Image Editing with Cross-Attention Control",
      "authors": "Hertz et al.",
      "year": 2022,
      "role": "Training-free cross-attention control inspiration",
      "relationship_sentence": "Prompt-to-Prompt demonstrated that semantic consistency across related prompts can be maintained by manipulating cross-attention at inference; 1Prompt1Story extends the training-free ethos to multi-scene storytelling by binding scenes through a single, concatenated prompt."
    },
    {
      "title": "Text2Video-Zero: Zero-Shot Text-to-Video Synthesis",
      "authors": "Khachatryan et al.",
      "year": 2023,
      "role": "Training-free temporal consistency precedent",
      "relationship_sentence": "Text2Video-Zero preserved identity across frames by reusing attention and guidance without training, directly informing 1Prompt1Story\u2019s aim to secure cross-image consistency in a sequence using only inference-time prompt/context mechanisms."
    },
    {
      "title": "IP-Adapter: Text-to-Image Diffusion Models as Image Prompt Adapters",
      "authors": "Ye et al.",
      "year": 2023,
      "role": "Reference-based identity control via add-on modules (contrastive approach)",
      "relationship_sentence": "IP-Adapter achieves identity control by adding an auxiliary adapter, whereas 1Prompt1Story targets broad applicability across diffusion backbones by avoiding any auxiliary modules and relying solely on prompt concatenation."
    },
    {
      "title": "Compositional Visual Generation with Composable Diffusion Models",
      "authors": "Liu et al.",
      "year": 2022,
      "role": "Compositionality of multiple textual constraints",
      "relationship_sentence": "Composable Diffusion showed that multiple text constraints can be combined in a single generation process; 1Prompt1Story leverages a similar principle by consolidating all scene descriptions into one long prompt to enforce global identity coherence."
    },
    {
      "title": "MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation",
      "authors": "Bar-Tal et al.",
      "year": 2023,
      "role": "Training-free fusion of multiple conditions to maintain global consistency",
      "relationship_sentence": "MultiDiffusion demonstrated that merging multiple constraints during inference yields coherent outputs; 1Prompt1Story analogously fuses multi-scene constraints via a single concatenated prompt to maintain character identity across images."
    }
  ],
  "synthesis_narrative": "The core contribution of One-Prompt-One-Story (1Prompt1Story) is a training-free strategy for identity-consistent storytelling by concatenating all scene descriptions into a single prompt, exploiting the model\u2019s inherent context handling. This explicitly departs from personalization methods like DreamBooth and Textual Inversion, which introduced the identity-preservation problem but require per-subject optimization or fine-tuning, limiting portability across models and domains. Instead, 1Prompt1Story aligns with the training-free lineage exemplified by Prompt-to-Prompt, which showed that semantic consistency can be maintained across related prompts by leveraging the diffusion model\u2019s cross-attention dynamics at inference. Similarly, Text2Video-Zero provided a blueprint for maintaining consistency across sequential outputs without training, demonstrating that careful control of attention and guidance yields temporally coherent frames\u2014an idea 1Prompt1Story reinterprets for multi-scene story images. While reference-based modules such as IP-Adapter enable identity control via auxiliary components, 1Prompt1Story intentionally avoids architectural changes to maximize generality. Finally, compositional generation works\u2014Composable Diffusion and MultiDiffusion\u2014established that multiple textual constraints can be fused within a single generation process, motivating 1Prompt1Story\u2019s unification of all scene prompts into one long context to bind character identity globally. Together, these works shaped a path toward a simple, broadly applicable, and training-free solution to consistent multi-scene T2I generation.",
  "analysis_timestamp": "2026-01-07T00:02:04.910667"
}
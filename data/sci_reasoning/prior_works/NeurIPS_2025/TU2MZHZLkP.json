{
  "prior_works": [
    {
      "title": "Improving predictive inference under covariate shift by weighting the log-likelihood function",
      "authors": "Hidetoshi Shimodaira",
      "year": 2000,
      "role": "Foundational covariate shift/importance weighting framework",
      "relationship_sentence": "Provides the core importance-weighting principle for correcting covariate shift, which the paper leverages in weighted kernel ridge regression while studying its computational realization via random projections."
    },
    {
      "title": "Correcting Sample Selection Bias by Unlabeled Data",
      "authors": "Jiayuan Huang, Arthur Gretton, Karsten M. Borgwardt, Bernhard Sch\u00f6lkopf, Alexander J. Smola",
      "year": 2007,
      "role": "RKHS-based covariate shift correction (Kernel Mean Matching)",
      "relationship_sentence": "Introduces RKHS machinery (KMM) to estimate importance weights under covariate shift; the present work operates in RKHS and analyzes maintaining target-risk performance when learning is constrained to random subspaces for efficiency."
    },
    {
      "title": "Learning Bounds for Importance Weighting",
      "authors": "Corinna Cortes, Yishay Mansour, Mehryar Mohri",
      "year": 2010,
      "role": "Generalization theory under covariate shift",
      "relationship_sentence": "Provides finite-sample generalization bounds for importance-weighted ERM, which underpin the paper\u2019s target-risk guarantees when combining importance weighting with randomized projections in kernel ridge regression."
    },
    {
      "title": "Optimal rates for the regularized least-squares algorithm",
      "authors": "Andrea Caponnetto, Ernesto De Vito",
      "year": 2007,
      "role": "Statistical baseline for KRR in RKHS",
      "relationship_sentence": "Establishes optimal rates for kernel ridge regression under source/capacity conditions; the new paper preserves these statistical guarantees (up to constants) while reducing computation via random subspace projections, even under covariate shift."
    },
    {
      "title": "Random Features for Large-Scale Kernel Machines",
      "authors": "Ali Rahimi, Benjamin Recht",
      "year": 2007,
      "role": "Random subspace/projection methodology for kernels",
      "relationship_sentence": "Introduces random Fourier features as a random RKHS subspace, the computational device the paper analyzes to obtain memory/time savings for KRR while controlling excess risk under covariate shift."
    },
    {
      "title": "Less is More: Nystr\u00f6m Computational Regularization",
      "authors": "Alessandro Rudi, Roberto Camoriano, Lorenzo Rosasco",
      "year": 2015,
      "role": "Computational\u2013statistical trade-offs for Nystr\u00f6m KRR",
      "relationship_sentence": "Shows how low-rank Nystr\u00f6m projections act as computational regularization achieving near-optimal rates; the current paper extends such trade-off analyses to the covariate shift setting with appropriately weighted objectives."
    },
    {
      "title": "Generalization Properties of Random Features",
      "authors": "Alessandro Rudi, Lorenzo Rosasco",
      "year": 2017,
      "role": "Sharp risk/rank bounds for random-feature ridge regression",
      "relationship_sentence": "Derives feature-number requirements to match full KRR accuracy; the paper adapts these results to importance-weighted losses, specifying projection sizes that retain target-domain performance under covariate shift."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014showing that randomized projections can deliver substantial computational and memory savings for kernel ridge regression under covariate shift without sacrificing target-domain accuracy\u2014rests on two intertwined lines of prior work. First, the covariate shift literature provides the statistical framework for correcting distribution mismatch via importance weighting. Shimodaira (2000) introduced the fundamental weighted-risk principle, while Huang et al. (2007) operationalized it in RKHS through kernel mean matching. Cortes, Mansour, and Mohri (2010) supplied learning bounds for importance-weighted ERM, giving the theoretical scaffolding to reason about generalization on the target distribution.\nSecond, advances in scalable kernel methods established how random subspaces approximate RKHS learners with controlled accuracy. Caponnetto and De Vito (2007) characterized optimal KRR rates under source/capacity conditions, defining the statistical benchmark to match. Rahimi and Recht (2007) introduced random features as computationally efficient random projections in RKHS, and subsequent analyses by Rudi, Camoriano, and Rosasco (2015) for Nystr\u00f6m and Rudi and Rosasco (2017) for random features quantified the computation\u2013statistical trade-offs, identifying projection dimensions that preserve KRR rates.\nBy merging these threads, the present paper extends random projection theory from the i.i.d. setting to importance-weighted objectives reflecting covariate shift. It determines how projection dimension, regularization, and weight dispersion interact so that the target-risk guarantees of full KRR are maintained, thereby delivering principled computational efficiency specifically tailored to covariate shift scenarios.",
  "analysis_timestamp": "2026-01-07T00:02:04.961803"
}
{
  "prior_works": [
    {
      "title": "Video Diffusion Models",
      "authors": "Jonathan Ho et al.",
      "year": 2022,
      "role": "Core generative architecture for predictive video modeling",
      "relationship_sentence": "VPP\u2019s core hypothesis\u2014that video diffusion models encode both present and future dynamics\u2014is directly grounded in VDMs\u2019 demonstrated capacity to predict temporally coherent, physics-aware futures, which VPP then taps as conditioning for action learning."
    },
    {
      "title": "Visual Foresight: Learning Visual Predictive Models of Physics for Control",
      "authors": "Frederik Ebert et al.",
      "year": 2018,
      "role": "Pioneering use of video prediction for robot control",
      "relationship_sentence": "Visual Foresight established that predicting future visual observations aids control; VPP modernizes this idea by using diffusion-based future representations rather than deterministic pixel predictions to guide action selection."
    },
    {
      "title": "Dreamer: Reinforcement Learning with World Models",
      "authors": "Danijar Hafner et al.",
      "year": 2020,
      "role": "Latent world models for policy learning from imagined futures",
      "relationship_sentence": "Dreamer showed policies improve when conditioned on predicted latent futures; VPP adopts this latent-foresight principle, using VDM-derived future embeddings as context for an implicit inverse dynamics policy."
    },
    {
      "title": "V-JEPA: Learning Video Representations by Predicting Embeddings",
      "authors": "Mido Assran et al.",
      "year": 2024,
      "role": "Predictive video representation learning that captures dynamics",
      "relationship_sentence": "V-JEPA evidences that prediction-in-latent-space yields dynamics-aware features transferable to control, directly motivating VPP\u2019s reliance on predictive video representations (here, from VDMs) for action learning."
    },
    {
      "title": "Behavior Cloning from Observation (BCO): Learning Control Policies without Action Labels",
      "authors": "Z. Torabi, G. Warnell, P. Stone",
      "year": 2018,
      "role": "Inverse dynamics as a policy-learning mechanism",
      "relationship_sentence": "BCO formalized learning an inverse dynamics model to infer actions from state transitions; VPP extends this by conditioning an implicit inverse dynamics mapper on VDM-predicted future embeddings."
    },
    {
      "title": "Video PreTraining (VPT): Learning to Act from Large-Scale Internet Videos",
      "authors": "Bowen Baker et al.",
      "year": 2022,
      "role": "Using internet human videos to pretrain control-relevant policies",
      "relationship_sentence": "VPT established the pipeline of leveraging large-scale internet human videos for policy learning; VPP adopts this paradigm by fine-tuning a video foundation model on internet manipulation videos plus robot data to improve future prediction for control."
    },
    {
      "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
      "authors": "Anthony Brohan et al.",
      "year": 2023,
      "role": "Foundation-model pretraining and robot fine-tuning for generalist control",
      "relationship_sentence": "RT-2 demonstrated that large-scale pretraining combined with targeted robot fine-tuning yields generalist policies; VPP analogously fine-tunes a pre-trained video foundation (diffusion) model on robot and internet manipulation data to obtain action-useful predictive representations."
    }
  ],
  "synthesis_narrative": "VPP\u2019s key contribution\u2014using predictive visual representations from video diffusion models (VDMs) to condition an implicit inverse dynamics policy\u2014sits at the intersection of foresight-based control, predictive representation learning, and foundation-model scaling. The lineage begins with Visual Foresight, which showed that predicting future observations provides a useful substrate for robotic control, and with Dreamer\u2019s latent world models, which established that policies benefit from conditioning on imagined futures in a compact representation space. VDMs further advanced future modeling by delivering temporally coherent, physics-aware video predictions; VPP leverages this capability not for pixel-level planning but to extract future-aware latent features that guide action selection.\n\nOn the representation side, V-JEPA crystallized the idea that prediction-in-embedding-space yields dynamics-sensitive features that transfer well to embodied tasks\u2014a principle VPP adopts while instantiating the predictor as a VDM. For action learning, BCO\u2019s use of inverse dynamics from state transitions provides the template that VPP adapts: rather than explicit transitions, VPP conditions its inverse dynamics implicitly on VDM-predicted future embeddings. Finally, scaling and data strategy draw from VPT and RT-2: both demonstrated that pretraining on broad internet data and fine-tuning on robot datasets yields generalist, transferable policies. VPP mirrors this recipe by fine-tuning a pre-trained video foundation model on robot and internet human manipulation video, improving future prediction fidelity and, in turn, downstream control performance.",
  "analysis_timestamp": "2026-01-07T00:21:33.195979"
}
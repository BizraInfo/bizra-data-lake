{
  "prior_works": [
    {
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": "Diederik P. Kingma et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "AdaMDOS/AdaMDOF inherit the core per-coordinate adaptive preconditioning idea from Adam, and the convergence analysis explicitly builds on the adaptive moment framework that Adam popularized."
    },
    {
      "title": "On the Convergence of Adam and Beyond",
      "authors": "Sashank J. Reddi et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "The AMSGrad correction identified in this work directly motivates the stable adaptive updates used in AdaMDOS/AdaMDOF and underpins their provable convergence with adaptive learning rates."
    },
    {
      "title": "Can decentralized algorithms outperform centralized algorithms? A case study for decentralized parallel stochastic gradient descent",
      "authors": "Xiangru Lian et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized decentralized SGD and its nonconvex analysis baseline, defining the decentralized optimization setting and consensus/mixing framework that AdaMDOS/AdaMDOF operate within and improve upon."
    },
    {
      "title": "Exact diffusion for distributed optimization and learning over networks",
      "authors": "Kun Yuan et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Exact diffusion established the exact-consensus/gradient-tracking paradigm over networks, a mechanism AdaMDOS/AdaMDOF leverage to couple adaptive updates with decentralized information mixing to attain faster rates."
    },
    {
      "title": "SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient",
      "authors": "Lam M. Nguyen et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "AdaMDOF\u2019s finite-sum design and analysis rely on the SARAH-style stochastic recursive gradient estimator as the variance-reduction backbone that enables near-optimal sample complexity."
    },
    {
      "title": "SPIDER: Near-Optimal Nonconvex Optimization via Stochastic Path-Integrated Differential Estimator",
      "authors": "Hongzhou Fang et al.",
      "year": 2018,
      "role": "Related Problem",
      "relationship_sentence": "SPIDER\u2019s variance-reduced estimator and its \u03b5^{-3} nonconvex complexity bound provide the near-optimal target and technical tools that AdaMDOF adapts to the decentralized, adaptive setting."
    },
    {
      "title": "STORM: A Variance-Reduced Stochastic Gradient Method with Adaptive Step Sizes",
      "authors": "Kunal Cutkosky et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "STORM showed how adaptive/recursive estimators achieve \u03b5^{-3} complexity in stochastic nonconvex problems, directly inspiring AdaMDOS\u2019s design and analysis to reach near-optimal sample complexity in decentralized settings."
    }
  ],
  "synthesis_narrative": "The core innovation of Faster Adaptive Decentralized Learning Algorithms is to combine adaptive per-coordinate preconditioning with decentralized consensus/gradient tracking and variance-reduced gradient estimators to achieve near-optimal sample complexity in both stochastic and finite-sum nonconvex regimes. The adaptive component traces directly to Adam, whose per-coordinate second-moment normalization is the architectural template for AdaMDOS/AdaMDOF. AMSGrad\u2019s convergence-fixing modification identifies the stability gap of Adam and directly informs the provably convergent adaptive updates used here. On the decentralized side, Lian et al. framed the decentralized SGD setting\u2014mixing matrices, consensus error, and nonconvex analysis\u2014establishing the baseline that the present work accelerates. Exact diffusion then supplied the exact-consensus/gradient-tracking mechanism that enables accurate network-wide gradient aggregation, a structural ingredient AdaMDOS/AdaMDOF exploit to marry adaptivity with decentralized information mixing. For finite-sum objectives, SARAH and SPIDER developed stochastic recursive gradient estimators and proved \u03b5^{-3}-type near-optimal bounds in centralized nonconvex optimization; AdaMDOF extends these estimators into the decentralized, adaptive regime to preserve optimal sample complexity. For the pure stochastic case, STORM demonstrated how adaptive, momentum-based recursive estimators attain \u03b5^{-3} complexity, directly motivating AdaMDOS\u2019s estimator design and the associated convergence proof. Together, these works form the direct intellectual lineage: adaptive preconditioning (Adam/AMSGrad), decentralized exact aggregation (Exact diffusion, Lian et al.), and near-optimal variance reduction (SARAH/SPIDER/STORM) are integrated to yield faster adaptive decentralized algorithms with rigorous near-optimal guarantees.",
  "analysis_timestamp": "2026-01-06T23:09:26.446888"
}
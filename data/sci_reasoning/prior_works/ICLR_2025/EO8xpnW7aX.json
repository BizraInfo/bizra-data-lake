{
  "prior_works": [
    {
      "title": "Discrete Denoising Diffusion Probabilistic Models",
      "authors": "Jacob Austin et al.",
      "year": 2021,
      "arxiv_id": "2107.03006",
      "role": "Extension",
      "relationship_sentence": "SymmetricDiffusers directly extends D3PM by instantiating the forward noising and learned reverse denoising transitions on the permutation group S_n, replacing generic categorical flips with group-structured kernels and a permutation-specific reverse parameterization."
    },
    {
      "title": "Multinomial Diffusion for Multivariate Categorical Data",
      "authors": "Emiel Hoogeboom et al.",
      "year": 2021,
      "arxiv_id": "unknown",
      "role": "Related Problem",
      "relationship_sentence": "The paper adapts the multinomial discrete-diffusion training objective and parameterization ideas to a combinatorial state space, motivating how to formulate and learn discrete reverse transitions that are then specialized to permutations."
    },
    {
      "title": "How Many Riffle Shuffles to Randomize a Deck?",
      "authors": "Dave Bayer and Persi Diaconis",
      "year": 1992,
      "arxiv_id": "unknown",
      "role": "Foundation",
      "relationship_sentence": "Their mixing-time analysis of the Gilbert\u2013Shannon\u2013Reeds riffle shuffle (\u2248(3/2) log2 n shuffles) provides the theoretical basis for choosing the diffusion length and denoising schedule used in the permutation forward process."
    },
    {
      "title": "Theory of Riffle Shuffles (GSR model)",
      "authors": "James Reeds",
      "year": 1981,
      "arxiv_id": "unknown",
      "role": "Foundation",
      "relationship_sentence": "The GSR riffle-shuffle model defines the specific forward transition kernel on S_n that SymmetricDiffusers adopts as the discrete diffusion\u2019s noising step."
    },
    {
      "title": "The Analysis of Permutations",
      "authors": "Robin L. Plackett",
      "year": 1975,
      "arxiv_id": "unknown",
      "role": "Extension",
      "relationship_sentence": "The classic Plackett\u2013Luce model underlies the paper\u2019s reverse-transition parameterization, which is explicitly generalized and proven more expressive than standard PL for modeling denoising distributions over permutations."
    },
    {
      "title": "Learning Latent Permutations with Gumbel-Sinkhorn Networks",
      "authors": "Germ\u00e1n Mena et al.",
      "year": 2018,
      "arxiv_id": "1802.08665",
      "role": "Gap Identification",
      "relationship_sentence": "Continuous relaxations to the Birkhoff polytope used here highlight the limitations of learning exact distributions on S_n, motivating a fully discrete, permutation-respecting diffusion approach with exact support on permutations."
    }
  ],
  "synthesis_narrative": "Discrete diffusion on categorical domains established that complex distributions can be learned via simple forward noising and learned reverse denoising transitions, with concrete training losses and parameterizations for non-Gaussian spaces; multinomial diffusion clarified how to design and optimize such discrete reverse kernels. Independently, the Gilbert\u2013Shannon\u2013Reeds riffle-shuffle model specified a natural random walk on the symmetric group, and the Bayer\u2013Diaconis analysis quantified its mixing behavior, showing that roughly (3/2) log2 n shuffles suffice to approach uniformity. In ranking and permutations, the Plackett\u2013Luce model provided a tractable, factorized distribution over orderings, revealing both its computational convenience and its expressiveness limits (e.g., IIA). Meanwhile, Gumbel\u2013Sinkhorn methods offered differentiable surrogates for permutations by relaxing to the Birkhoff polytope, but at the cost of exact support on S_n and potential bias from continuous approximations.\nBringing these strands together, the next step was to instantiate discrete diffusion directly on S_n by choosing a principled group-respecting forward kernel and leveraging mixing theory to set diffusion length and denoising schedules. The riffle shuffle offers exactly such a kernel with well-understood convergence, while PL suggests a natural starting point for modeling reverse transitions that can be generalized for greater expressiveness. Addressing the shortcomings of relaxation-based permutation methods, the synthesis yields a fully discrete, theoretically grounded diffusion framework on permutations with a generalized PL reverse model, aligning learning and sampling with the combinatorial structure of S_n.",
  "target_paper": {
    "title": "SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric Groups",
    "authors": "Yongxing Zhang, Donglin Yang, Renjie Liao",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Finite Symmetric Groups, Discrete Diffusion, Permutations, Riffle Shuffles, Plackett-Luce Distribution, Sorting, Jigsaw Puzzle",
    "abstract": "The group of permutations $S_n$, also known as the finite symmetric groups, are essential in fields such as combinatorics, physics, and chemistry. However, learning a probability distribution over $S_n$ poses significant challenges due to its intractable size and discrete nature. In this paper, we introduce *SymmetricDiffusers*, a novel discrete diffusion model that simplifies the task of learning a complicated distribution over $S_n$ by decomposing it into learning simpler transitions of the reverse diffusion using deep neural networks. We identify the riffle shuffle as an effective forward transition and provide empirical guidelines for selecting the diffusion length based on the theory of random walks on finite groups. Additionally, we propose a generalized Plackett-Luce (PL) distribution for the reverse transition, which is provably more expressive than the PL distribution. We further introduce a theoretically grounded \"denoising schedule\" to improve sampling and learning efficienc",
    "openreview_id": "EO8xpnW7aX",
    "forum_id": "EO8xpnW7aX"
  },
  "analysis_timestamp": "2026-01-06T18:06:17.219145"
}
{
  "prior_works": [
    {
      "title": "A Mathematical Theory of Evidence",
      "authors": "Glenn Shafer",
      "year": 1976,
      "role": "Foundation",
      "relationship_sentence": "TMCEK\u2019s multi-view fusion is grounded in Shafer\u2019s Dempster\u2013Shafer framework for belief masses and evidence combination, which the paper explicitly adopts and then augments with distribution-aware opinions and expert constraints."
    },
    {
      "title": "Subjective Logic: A Formalism for Reasoning Under Uncertainty",
      "authors": "Audun J\u00f8sang",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "TMCEK\u2019s \u201cdistribution-aware subjective opinion\u201d mechanism directly builds on subjective logic\u2019s notion of opinions (Dirichlet-based belief models), extending it to capture distributional properties in multi-view fusion rather than relying on point estimates."
    },
    {
      "title": "Evidential Deep Learning to Quantify Classification Uncertainty",
      "authors": "Murat Sensoy et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "TMCEK extends EDL\u2019s Dirichlet-parameterized evidential outputs by moving beyond first-order (mean) use of belief masses to a distribution-aware opinion that yields more reliable confidence and is adapted to Dempster\u2013Shafer multi-view fusion."
    },
    {
      "title": "Predictive Uncertainty Estimation via Prior Networks",
      "authors": "Andrey Malinin et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "The idea of explicitly modeling a distribution over categorical probabilities (Dirichlet priors) in Prior Networks directly motivates TMCEK\u2019s shift from first-order confidence scores to a distribution-aware opinion used in trusted multi-view fusion."
    },
    {
      "title": "Deng entropy: A new uncertainty measure of Dempster\u2013Shafer theory",
      "authors": "Yong Deng",
      "year": 2016,
      "role": "Gap Identification",
      "relationship_sentence": "TMCEK targets the limitation of belief-entropy measures like Deng entropy that summarize uncertainty from mass functions via first-order statistics, replacing them with a theoretically stronger distribution-aware uncertainty measure."
    },
    {
      "title": "Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations",
      "authors": "Andrew Ross et al.",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "TMCEK\u2019s expert-knowledge constraints for feature-level interpretability are directly inspired by training with explanation/gradient constraints, adapting this idea to inject domain expert priors into the multi-view evidential learning objective."
    },
    {
      "title": "Combining Labeled and Unlabeled Data with Co-Training",
      "authors": "Avrim Blum et al.",
      "year": 1998,
      "role": "Foundation",
      "relationship_sentence": "TMCEK inherits the core multi-view problem formulation from co-training, leveraging the assumption of complementary views while replacing agreement-based learning with DS-theoretic trusted fusion under expert constraints."
    }
  ],
  "synthesis_narrative": "TMCEK stands at the intersection of multi-view learning, evidential reasoning, and knowledge-constrained training. Its fusion backbone is inherited from Dempster\u2013Shafer theory (Shafer), providing the belief-mass calculus and combination rules that undergird trusted multi-view aggregation. The multi-view problem setting itself follows the co-training paradigm (Blum & Mitchell), where distinct views contribute complementary evidence. On the uncertainty side, TMCEK explicitly embraces subjective logic\u2019s formalization of opinions (J\u00f8sang), where Dirichlet distributions encode evidence and base rates. Building on evidential deep learning (Sensoy et al.), which popularized Dirichlet-parameterized outputs for classification, and the Prior Networks perspective (Malinin & Gales) on modeling distributions over categorical probabilities, TMCEK departs from first-order reliance on expected probabilities by introducing a distribution-aware opinion that preserves higher-order uncertainty information during fusion. This directly addresses shortcomings of prevalent DS uncertainty summaries such as Deng entropy, whose mass-only, first-order characterization can obscure intrinsic evidence variability. Finally, TMCEK\u2019s feature-level interpretability arises from integrating expert knowledge constraints inspired by \u201cRight for the Right Reasons\u201d (Ross et al.), operationalizing domain priors as training-time constraints that shape evidential assignments at the feature level. Together, these strands produce a trusted multi-view classifier that both explains its decisions through expert-guided feature attributions and yields theoretically stronger, distribution-aware uncertainty estimates for safety-critical decision-making.",
  "analysis_timestamp": "2026-01-06T23:07:19.635369"
}
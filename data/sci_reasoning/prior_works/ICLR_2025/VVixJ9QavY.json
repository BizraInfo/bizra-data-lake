{
  "prior_works": [
    {
      "title": "Learning the Difference that Makes a Difference: Counterfactually-Augmented Data Improves Robustness",
      "authors": "Kaushik et al.",
      "year": 2020,
      "arxiv_id": "1909.12434",
      "role": "Foundation",
      "relationship_sentence": "This work established using paired factual\u2013counterfactual examples as supervision to reduce spurious cues, directly motivating the paper\u2019s use of counterfactual feedback and its balanced factual/counterfactual evaluation metrics."
    },
    {
      "title": "WIQA: A Dataset for 'What If' Reasoning Over Procedural Text",
      "authors": "Tandon et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "WIQA formalized counterfactual 'what-if' question answering, providing the problem setting and evaluation paradigm that this paper targets when eliciting causal reasoning in LMs."
    },
    {
      "title": "STaR: Bootstrapping Reasoning With Reasoning",
      "authors": "Zelikman et al.",
      "year": 2022,
      "arxiv_id": "2203.14465",
      "role": "Baseline",
      "relationship_sentence": "STaR showed that fine-tuning on rationales selected by outcome correctness elicits reasoning, which this paper extends by selecting and training with counterfactual-consistency feedback rather than correctness alone."
    },
    {
      "title": "Let\u2019s Verify Step by Step",
      "authors": "Wang et al.",
      "year": 2023,
      "arxiv_id": "2305.20050",
      "role": "Inspiration",
      "relationship_sentence": "By introducing process-level verification feedback for chain-of-thought, this paper inspired the use of counterfactual violations as a process supervision signal to shape reasoning steps."
    },
    {
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "authors": "Rafailov et al.",
      "year": 2023,
      "arxiv_id": "2305.18290",
      "role": "Extension",
      "relationship_sentence": "DPO provided a simple preference-learning objective that this work instantiates with factual-versus-counterfactual preference pairs to fine-tune reasoning without reinforcement learning."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT (ROME) and the CounterFact Benchmark",
      "authors": "Meng et al.",
      "year": 2022,
      "arxiv_id": "2202.05262",
      "role": "Inspiration",
      "relationship_sentence": "CounterFact\u2019s dual evaluation\u2014adopting edited counterfactuals while retracting originals\u2014inspired the paper\u2019s metrics that jointly assess factual and counterfactual accuracy to capture causal consistency."
    },
    {
      "title": "ProofWriter: Generating Proofs for Rule-Based Reasoning",
      "authors": "Tafjord et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "ProofWriter provides a deductive reasoning testbed with stepwise proofs, grounding the paper\u2019s generalization claims and informing process-oriented fine-tuning targets."
    }
  ],
  "synthesis_narrative": "Counterfactual supervision in NLP was crystallized by Kaushik et al., who paired factual and counterfactual examples to force models to rely on causally relevant features rather than spurious correlations; their setup highlighted the need to perform well on both original and counterfactual variants. WIQA then framed a concrete \u201cwhat-if\u201d question answering task over procedural text, turning counterfactual reasoning into a standardized evaluation problem. Complementing these, Meng et al. introduced CounterFact and emphasized dual objectives\u2014adopt the counterfactual while retracting the original\u2014suggesting evaluation should jointly score counterfactual adoption and factual consistency. On the training side, Zelikman et al.\u2019s STaR showed that selecting rationales linked to correct outcomes and fine-tuning on them can elicit reasoning, while Wang et al. demonstrated process-level verification feedback as a way to shape chain-of-thought steps. Finally, Rafailov et al. provided DPO, a lightweight preference-optimization objective that operationalizes pairwise feedback without reinforcement learning. Together, these threads exposed an opportunity: leverage paired factual\u2013counterfactual signals not only as data but as feedback to shape reasoning processes, and assess success with metrics that balance both sides. Building on STaR and verifier-style process supervision but replacing correctness-based signals with counterfactual consistency, and operationalizing the signal via a DPO-style objective, the paper fine-tunes models to prefer reasoning that remains valid under counterfactual interventions. The CounterFact-inspired dual metrics formalize the target behavior, while WIQA and proof-style datasets like ProofWriter ground evaluation across counterfactual, inductive, and deductive regimes.",
  "target_paper": {
    "title": "Reasoning Elicitation in Language Models via Counterfactual Feedback",
    "authors": "Alihan H\u00fcy\u00fck, Xinnuo Xu, Jacqueline R. M. A. Maasch, Aditya V. Nori, Javier Gonzalez",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "language models, reasoning, fine-tuning, counterfactuals",
    "abstract": "Despite the increasing effectiveness of language models, their reasoning capabilities remain underdeveloped. In particular, causal reasoning through counterfactual question answering is lacking. This work aims to bridge this gap. We first derive novel metrics that balance accuracy in factual and counterfactual questions, capturing a more complete view of the reasoning abilities of language models than traditional factual-only based metrics. Second, we propose several fine-tuning approaches that aim to elicit better reasoning mechanisms, in the sense of the proposed metrics. Finally, we evaluate the performance of the fine-tuned language models in a variety of realistic scenarios. In particular, we investigate to what extent our fine-tuning approaches systemically achieve better generalization with respect to the base models in several problems that require, among others, inductive and deductive reasoning capabilities.",
    "openreview_id": "VVixJ9QavY",
    "forum_id": "VVixJ9QavY"
  },
  "analysis_timestamp": "2026-01-06T11:13:40.262347"
}
{
  "prior_works": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "authors": "Thomas Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, George Drettakis",
      "year": 2023,
      "role": "Foundational representation and training procedure for 3DGS; establishes the target model whose redundancy the paper seeks to reduce.",
      "relationship_sentence": "The proposed global reduction operates directly on the 3DGS primitives and then fine-tunes color/opacity in the SH appearance head introduced by Kerbl et al., making 3DGS the essential substrate and motivation for compaction."
    },
    {
      "title": "Kullback\u2013Leibler Approach to Gaussian Mixture Reduction",
      "authors": "Andrew R. Runnalls",
      "year": 2007,
      "role": "Classical global Gaussian mixture reduction (GMR) framework with fidelity guarantees via divergence control.",
      "relationship_sentence": "The paper generalizes Runnalls\u2019 GMR idea from KL-based pairwise merges to a global, transport-based objective, providing a principled fidelity-guaranteed route to shrink large Gaussian sets in 3DGS."
    },
    {
      "title": "Wasserstein Geometry of Gaussian Measures",
      "authors": "Asuka Takatsu",
      "year": 2011,
      "role": "Provides closed-form 2-Wasserstein/Bures metrics between Gaussian distributions and the associated Riemannian structure.",
      "relationship_sentence": "Closed-form W2 costs between Gaussians underpin the composite transport divergence used to measure geometric fidelity when reallocating and merging 3DGS primitives."
    },
    {
      "title": "Learning Generative Models with Sinkhorn Divergences",
      "authors": "Aude Genevay, Gabriel Peyr\u00e9, Marco Cuturi",
      "year": 2018,
      "role": "Introduces stabilized, unbiased OT-like divergences enabling scalable, differentiable optimization.",
      "relationship_sentence": "The method leverages entropic-regularized OT/divergence machinery to make global mixture reduction differentiable and stable, facilitating end-to-end optimization and appearance fine-tuning."
    },
    {
      "title": "Tree-Sliced Wasserstein Distances",
      "authors": "Trung Le, Makoto Yamada, Hiroshi Kashima",
      "year": 2019,
      "role": "Shows how hierarchical/tree partitions approximate OT efficiently while preserving global transport structure.",
      "relationship_sentence": "The use of a KD-tree partition to structure global transport mirrors the tree-based OT approximation idea, enabling scalable optimization of transport objectives over millions of Gaussians."
    },
    {
      "title": "Kernel Herding: The Power of Deterministic Sampling",
      "authors": "Yutian Chen, Max Welling",
      "year": 2010,
      "role": "Demonstrates deterministic selection/migration of weighted atoms to approximate a target distribution under a discrepancy measure.",
      "relationship_sentence": "The paper\u2019s \u2018Gaussian herding\u2019 perspective echoes kernel herding\u2019s deterministic sample relocation, but replaces MMD with OT-based divergence to drive where Gaussians are merged and moved across spatial \u201cpens.\u201d"
    },
    {
      "title": "Fast and Robust Earth Mover\u2019s Distances",
      "authors": "Shai Pele, Michael Werman",
      "year": 2009,
      "role": "Pioneers practical, scalable approximations for OT/EMD in high-dimensional vision tasks.",
      "relationship_sentence": "The emphasis on scalable transport computation informs the paper\u2019s composite transport over spatial partitions, making global fidelity objectives tractable for large 3DGS sets."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014casting 3D Gaussian Splatting (3DGS) compaction as a global Gaussian mixture reduction problem optimized via optimal transport\u2014sits at the intersection of three lines of work. First, Kerbl et al. introduced 3DGS as a high-fidelity, real-time radiance field representation whose millions of Gaussians motivate compaction; their parameterization (positions, covariances, opacity, SH colors) defines the variables the new method reduces and then fine-tunes. Second, classical Gaussian mixture reduction (GMR), epitomized by Runnalls\u2019 KL-driven framework, provides the blueprint for principled, fidelity-aware shrinking of mixtures. The present paper extends this paradigm by replacing KL with transport-based divergences that better capture geometric displacement and mass rearrangement among spatial Gaussians. This is enabled mathematically by the closed-form 2-Wasserstein/Bures metrics between Gaussians (Takatsu), and computationally by modern OT machinery such as Sinkhorn divergences (Genevay\u2013Peyr\u00e9\u2013Cuturi) that deliver stable, differentiable objectives for large-scale optimization. To make global transport tractable over massive 3DGS sets, the method borrows from hierarchical OT approximations\u2014tree-sliced Wasserstein ideas (Le\u2013Yamada\u2013Kashima) and fast EMD principles (Pele\u2013Werman)\u2014operationalized here as a KD-tree partition guiding composite transport. Finally, the \u2018herding\u2019 intuition traces to kernel herding (Chen\u2013Welling): deterministically relocating a small set of weighted atoms to approximate a target distribution. The paper adapts this notion from MMD to OT, using \u201cGaussian herding across pens\u201d to globally reallocate and merge primitives, then decouple geometry from appearance via lightweight color/opacity fine-tuning.",
  "analysis_timestamp": "2026-01-07T00:05:12.517694"
}
{
  "prior_works": [
    {
      "title": "On a Measure of the Information Provided by an Experiment",
      "authors": "D. V. Lindley",
      "year": 1956,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work defines the Expected Information Gain as the KL contrast between posterior and prior that the paper directly maximizes and differentiates."
    },
    {
      "title": "Simulation-based optimal Bayesian experimental design for nonlinear systems",
      "authors": "Xun Huan et al.",
      "year": 2013,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "It derives Monte Carlo estimators for EIG and its gradients but incurs prohibitive nested sampling costs in complex, high-dimensional settings, motivating the paper\u2019s tractable pooled-posterior gradient and diffusion-based sampling scheme."
    },
    {
      "title": "Bayesian Experimental Design for Implicit Models via Mutual Information Neural Estimation",
      "authors": "Annika Kleinegesse et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By optimizing variational lower bounds on mutual information for BOED in implicit models, this work introduces bias/looseness that the paper explicitly avoids by deriving a non\u2013lower-bound EIG gradient via a pooled posterior."
    },
    {
      "title": "A General Framework for Amortized Bayesian Experimental Design",
      "authors": "Adam D. Foster et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "It establishes a bilevel, amortized optimization framework for BOED using variational MI bounds, which the paper adopts in spirit\u2014keeping the joint sampling\u2013optimization loop\u2014while replacing bound-based objectives with a diffusion-driven exact-gradient approach."
    },
    {
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "authors": "Yang Song et al.",
      "year": 2021,
      "arxiv_id": "2011.13456",
      "role": "Foundation",
      "relationship_sentence": "This provides the score-based diffusion SDE framework and samplers that the paper leverages to compute and evolve the dynamics of its pooled posterior distribution."
    },
    {
      "title": "Diffusion Posterior Sampling",
      "authors": "Hyungjin Chung et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "By showing how to combine a diffusion prior with likelihood gradients for posterior sampling in inverse problems, this work directly informs the paper\u2019s use of conditional diffusion to realize pooled-posterior dynamics for design-dependent EIG optimization."
    }
  ],
  "synthesis_narrative": "Lindley\u2019s formulation of Expected Information Gain frames experimental design as maximizing the KL contrast between posterior and prior, fixing the objective as a contrastive quantity. Huan and Marzouk develop Monte Carlo estimators for this EIG and its gradient in nonlinear Bayesian inverse problems, but the nested expectations and repeated posterior updates make gradients costly and fragile in high dimensions. Kleinegesse and Gutmann introduce variational lower-bound surrogates for mutual information in implicit models, enabling neural BOED but at the price of bias from loose bounds. Foster and colleagues generalize amortized Bayesian experimental design into a bilevel paradigm, coupling design optimization with learned inference via variational MI objectives and providing a practical joint optimization loop. In parallel, Song and coauthors establish score-based diffusion via reverse SDEs, enabling efficient sampling and differentiable dynamics of complex distributions. Chung and collaborators then show how to incorporate likelihood gradients into diffusion priors to sample true posteriors for inverse problems without explicit normalization.\nTogether, these works expose a gap: exact EIG gradients are too expensive, while variational bounds compromise fidelity, yet diffusion samplers offer efficient, differentiable dynamics for posterior updates. The paper synthesizes these insights by defining a pooled posterior that captures the EIG contrast, deriving a tractable exact-gradient expression, and employing conditional diffusion to evolve this pooled distribution within a Foster-style bilevel loop\u2014thereby sidestepping lower bounds while retaining computational efficiency.",
  "target_paper": {
    "title": "Bayesian Experimental Design Via Contrastive Diffusions",
    "authors": "Jacopo Iollo, Christophe Heinkel\u00e9, Pierre Alliez, Florence Forbes",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Bayesian Optimal Experimental Design, Conditional Diffusion Models, score based sampling, Bayesian Inverse Problems, Experimental Design, Sampling as Optimization",
    "abstract": "Bayesian Optimal Experimental Design (BOED) is a powerful tool to reduce the cost of running a sequence of experiments.\nWhen based on the Expected Information Gain (EIG), design optimization corresponds to the maximization of some intractable expected  *contrast* between prior and posterior distributions.\nScaling this maximization to high dimensional and complex settings has been an issue due to BOED inherent computational complexity.\nIn this work, we introduce an *pooled posterior* distribution with cost-effective sampling properties and provide a tractable access to the EIG contrast maximization via a new EIG gradient expression. Diffusion-based samplers are used to compute the dynamics of the pooled posterior and ideas from bi-level optimization are leveraged to derive an efficient joint sampling-optimization loop, without resorting to lower bound approximations of the EIG. The resulting efficiency gain allows to extend BOED to the well-tested generative capabilities of diffusion mo",
    "openreview_id": "h8yg0hT96f",
    "forum_id": "h8yg0hT96f"
  },
  "analysis_timestamp": "2026-01-06T14:29:43.823832"
}
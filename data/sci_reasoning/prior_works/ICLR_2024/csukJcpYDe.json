{
  "prior_works": [
    {
      "title": "Neuro-Dynamic Programming",
      "authors": "Dimitri P. Bertsekas and John N. Tsitsiklis",
      "year": 1996,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "TTPI instantiates generalized policy iteration from neuro-dynamic programming but replaces generic function approximators with tensor-train representations to conduct policy evaluation and improvement over hybrid action spaces."
    },
    {
      "title": "Reinforcement Learning with Parameterized Actions",
      "authors": "S. N. Masson, P. Ranchod, and G. Konidaris",
      "year": 2016,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "TTPI adopts the parameterized-action MDP formulation from this work and targets its coupled discrete\u2013continuous Bellman updates using TT-based value and advantage approximations."
    },
    {
      "title": "Deep Reinforcement Learning in Parameterized Action Space",
      "authors": "Timothy P. Hausknecht and Peter Stone",
      "year": 2016,
      "arxiv_id": "1511.04143",
      "role": "Baseline",
      "relationship_sentence": "This deep RL architecture for PAMDPs serves as a primary baseline that TTPI improves upon by substituting neural Q/actor heads with structured low-rank TT value/advantage representations to better scale with hybrid action complexity."
    },
    {
      "title": "Tensor-Train Decomposition",
      "authors": "Ivan V. Oseledets",
      "year": 2011,
      "arxiv_id": "0909.5671",
      "role": "Foundation",
      "relationship_sentence": "TTPI relies on the TT factorization introduced here to compactly represent the state-value and advantage functions and to enable efficient Bellman backups in high-dimensional hybrid domains."
    },
    {
      "title": "Quasioptimal TT-cross approximation for multidimensional arrays",
      "authors": "Dmitry V. Savostyanov",
      "year": 2011,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "TTPI leverages TT-cross/interpolation ideas from this work to fit TT-structured value and advantage functions from sampled rollouts during policy evaluation and improvement."
    },
    {
      "title": "Function Train: A Continuous Analogue of the Tensor-Train Decomposition",
      "authors": "Alex A. Gorodetsky and Youssef M. Marzouk",
      "year": 2015,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By demonstrating that low-rank separated representations can support efficient approximation and updates of high-dimensional functions, this work directly inspired TTPI\u2019s use of TT-structured value/advantage approximation within policy iteration."
    }
  ],
  "synthesis_narrative": "Generalized policy iteration from neuro-dynamic programming established the template of alternating policy evaluation and improvement with approximate value functions, providing the procedural backbone for many ADP methods. The parameterized-action MDP formulation formalized how discrete action choices couple to continuous parameters, defining the hybrid action setting and Bellman structure that modern methods must address. Deep reinforcement learning in parameterized action spaces operationalized this formulation with neural architectures, revealing practical challenges when representing coupled discrete\u2013continuous decisions, including instability and poor scaling as dimensionality grows. Tensor-Train decomposition introduced a scalable, low-rank factorization for high-dimensional functions, making it possible to store and manipulate value-like objects with costs that grow only linearly in dimension under low rank. TT-cross approximation supplied a sampling-based mechanism to learn TT representations from pointwise evaluations, enabling black-box function fitting without full grids. Function Trains, as a continuous analogue of TT, showed that separated low-rank formats can support efficient approximation and updates of complex high-dimensional functions, foreshadowing their utility for value and advantage approximation in control.\nTogether, these works reveal a gap: policy iteration for hybrid action problems needs a representation that captures discrete\u2013continuous coupling while scaling gracefully in dimension and supporting Bellman-style updates. By marrying the PAMDP formulation and GPI loop with TT/TT-cross low-rank approximations inspired by FT/TT theory, the current work naturally emerges\u2014performing value and advantage approximation and policy improvement in a structured format that directly addresses the instability and scalability limitations of prior neural hybrid-action baselines.",
  "target_paper": {
    "title": "Generalized Policy Iteration using Tensor Approximation for Hybrid Control",
    "authors": "Suhan Shetty, Teng Xue, Sylvain Calinon",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Optimal Control, Hybrid Actions, Robotics, Approximate Dynamic Programming, Tensor Approximation",
    "abstract": "Control of dynamic systems involving hybrid actions is a challenging task in robotics.  To address this, we present a novel algorithm called Generalized Policy Iteration using Tensor Train (TTPI) that belongs to the class of Approximate Dynamic Programming (ADP). We use a low-rank tensor approximation technique called Tensor Train (TT) to approximate the state-value and advantage function which enables us to efficiently handle hybrid systems. We demonstrate the superiority of our approach over previous baselines for some benchmark problems with hybrid action spaces. Additionally, the robustness and generalization of the policy for hybrid systems are showcased through a real-world robotics experiment involving a non-prehensile manipulation task which is considered to be a highly challenging control problem.",
    "openreview_id": "csukJcpYDe",
    "forum_id": "csukJcpYDe"
  },
  "analysis_timestamp": "2026-01-06T06:00:52.007302"
}
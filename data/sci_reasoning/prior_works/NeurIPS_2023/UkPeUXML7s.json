{
  "prior_works": [
    {
      "title": "On the heavy-tailed behavior of the stochastic gradient noise in deep neural networks",
      "authors": "Umut \u015eim\u015fekli, Levent Sagun, others",
      "year": 2019,
      "role": "Established that mini-batch gradient noise in deep learning is often heavy-tailed and can be modeled with \u03b1-stable laws, using generalized CLT arguments.",
      "relationship_sentence": "Provides the core heavy-tail mechanism (\u03b1-stable gradient noise under i.i.d. sampling) that this paper extends from online/infinite-data settings to the finite-data, multi-pass (offline) regime."
    },
    {
      "title": "Fluctuation-Dissipation Relations for Stochastic Gradient Descent",
      "authors": "Sho Yaida",
      "year": 2018,
      "role": "Empirically and theoretically demonstrated non-Gaussian, heavy-tailed characteristics of SGD noise and linked them to stationary distributions under constant step sizes.",
      "relationship_sentence": "Supplies empirical/theoretical evidence of heavy-tailed stationary behavior in SGD, motivating the need to explain how such tails arise in realistic finite-dataset training considered by this work."
    },
    {
      "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
      "authors": "Mattias T. J. Mandt, Matthew D. Hoffman, David M. Blei",
      "year": 2017,
      "role": "Modeled constant\u2013step size SGD as an Ornstein\u2013Uhlenbeck process with Gaussian noise, yielding a stationary Gaussian approximation.",
      "relationship_sentence": "Serves as the light-tailed (Gaussian) baseline theory of SGD\u2019s stationary distribution that the present paper contrasts with by proving approximate power-law tails for offline SGD."
    },
    {
      "title": "Without-Replacement Sampling for Stochastic Gradient Methods",
      "authors": "Ohad Shamir",
      "year": 2016,
      "role": "Analyzed SGD without replacement (random reshuffling), clarifying differences between online i.i.d. sampling and finite-data multi-pass regimes.",
      "relationship_sentence": "Provides the algorithmic and sampling framework for offline/multi-pass SGD that this paper uses to move beyond the infinite-data online assumptions in prior heavy-tail analyses."
    },
    {
      "title": "Why Random Reshuffling Beats Stochastic Gradient Descent",
      "authors": "Mert G\u00fcrb\u00fczbalaban, Asuman E. Ozdaglar, Pablo A. Parrilo",
      "year": 2019,
      "role": "Developed Markovian and Poisson-equation tools to analyze multi-pass SGD with random reshuffling in finite datasets.",
      "relationship_sentence": "Offers technical machinery for treating offline SGD as a Markov process over a finite dataset, enabling this paper\u2019s tail analysis in the multi-pass setting."
    },
    {
      "title": "On the rate of convergence in Wasserstein distance of the empirical measure",
      "authors": "Nicolas Fournier, Arnaud Guillin",
      "year": 2015,
      "role": "Provided sharp nonasymptotic rates for the convergence of empirical distributions to the population distribution in Wasserstein metrics.",
      "relationship_sentence": "Directly underpins the paper\u2019s key bound: the approximation error in the stationary heavy tails is controlled by the rate at which the empirical data distribution converges to the true distribution."
    },
    {
      "title": "Markov Chains and Stochastic Stability",
      "authors": "Sean P. Meyn, Richard L. Tweedie",
      "year": 1993,
      "role": "Foundational framework for invariant measures, drift conditions, and tail behavior in Markov chains.",
      "relationship_sentence": "Provides the stability and drift-condition toolkit the paper leverages to establish existence and tail properties of the stationary distribution for offline SGD."
    }
  ],
  "synthesis_narrative": "The paper\u2019s main advance is to explain why and how heavy-tailed stationary behavior emerges in practical, finite-data training where SGD repeatedly cycles over a dataset. Prior heavy-tail theories largely rested on online, i.i.d. sampling and infinite-data assumptions. Two streams of work directly feed into this result. First, heavy-tail modeling of SGD noise: Simsekli and collaborators formalized that mini-batch gradient noise obeys \u03b1-stable laws via generalized CLT, and Yaida provided empirical/theoretical evidence that constant\u2013step size SGD exhibits non-Gaussian, heavy-tailed stationary behavior. These studies motivate a tail-focused theory but assume i.i.d. sampling, leaving the finite-dataset, multi-pass mechanism unclear. Second, offline SGD analysis: Shamir and G\u00fcrb\u00fczbalaban\u2013Ozdaglar\u2013Parrilo established the distinct Markovian nature of without-replacement/random-reshuffling dynamics and provided tools (e.g., Poisson equation approaches) to study multi-pass SGD. Building on the Markov chain stability toolkit of Meyn and Tweedie, the present paper proves that the stationary distribution of offline SGD inherits power-law tails only approximately. Crucially, the deviation from an ideal power law is quantified using empirical-measure convergence rates in Wasserstein distance (Fournier\u2013Guillin), directly tying the tail approximation error to the finiteness of data. Together, these works enable a precise bridge: replacing the online infinite-data assumption with finite-sample, multi-pass dynamics while preserving (approximately) the heavy-tailed stationary behavior\u2014and rigorously controlling the approximation via statistical rates of empirical distribution convergence.",
  "analysis_timestamp": "2026-01-06T23:42:49.127412"
}
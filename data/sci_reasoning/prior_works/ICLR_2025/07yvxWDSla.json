{
  "prior_works": [
    {
      "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks",
      "authors": "Suchin Gururangan et al.",
      "year": 2020,
      "arxiv_id": "2004.10964",
      "role": "Baseline",
      "relationship_sentence": "Establishes domain- and task-adaptive continued pretraining (DAPT/TAPT), which this work keeps as the training setup but replaces the scarce in-domain corpus with a large EntiGraph-synthesized corpus to overcome DAPT\u2019s small-data limitations."
    },
    {
      "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
      "authors": "Yizhong Wang et al.",
      "year": 2023,
      "arxiv_id": "2212.10560",
      "role": "Inspiration",
      "relationship_sentence": "Introduces a bootstrapping recipe that expands a tiny seed into a diverse synthetic dataset using LLMs, directly inspiring the paper\u2019s strategy of using a small domain corpus to prompt generation of broad, varied synthetic pretraining data."
    },
    {
      "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
      "authors": "Ronen Eldan et al.",
      "year": 2023,
      "arxiv_id": "2305.07759",
      "role": "Inspiration",
      "relationship_sentence": "Shows that carefully curated, didactic synthetic text can efficiently teach language models, motivating the design of entity-connected synthetic narratives that are easier for models to learn factual knowledge from."
    },
    {
      "title": "CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning",
      "authors": "Bill Yuchen Lin et al.",
      "year": 2020,
      "arxiv_id": "1911.03705",
      "role": "Foundation",
      "relationship_sentence": "Formalizes concept-set-to-text generation\u2014producing fluent text that connects specified concepts\u2014which the paper operationalizes by generating sentences and passages that explicitly link extracted domain entities."
    },
    {
      "title": "KnowBERT: Knowledge Enhanced Contextual Word Representations",
      "authors": "Matthew E. Peters et al.",
      "year": 2019,
      "arxiv_id": "1909.04164",
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrates the benefit of entity-centric knowledge integration but relies on external knowledge bases, a limitation addressed here by extracting salient entities from the small domain corpus and synthesizing connections without an external KB."
    },
    {
      "title": "PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them",
      "authors": "Patrick Lewis et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "Shows that large-scale LM-generated QA pairs can pretrain models to improve factual recall, informing the decision to use LM-synthesized in-domain facts/questions as fuel for continued pretraining."
    }
  ],
  "synthesis_narrative": "Domain- and task-adaptive continued pretraining (DAPT/TAPT) established that additional pretraining on in-domain text improves downstream performance, but its effectiveness hinges on having enough domain data to avoid overfitting and under-coverage. Self-Instruct demonstrated a practical recipe to bootstrap from a small seed into a broad, diverse synthetic dataset by prompting a strong LLM, highlighting how synthetic expansion can overcome limited real data while maintaining coverage and diversity. TinyStories showed that didactic, well-structured synthetic text can be far more learnable than unstructured web text, suggesting that carefully designed synthetic corpora can teach facts efficiently. CommonGen introduced the constrained concept-set-to-text paradigm\u2014explicitly composing sentences that link specified concepts\u2014clarifying how to elicit generations that connect multiple units of knowledge. KnowBERT evidenced the value of entity-centric representations and relational grounding for injecting knowledge into language models, while exposing a dependency on external knowledge bases that is often impractical for niche domains. PAQ established that large-scale LM-generated QA can effectively pretrain models for factual recall, validating synthetic knowledge generation as a route to better question answering.\n\nTaken together, these works point to an opportunity: marry DAPT\u2019s objective with Self-Instruct-style bootstrapping and the CommonGen constraint to synthesize didactic, entity-connected in-domain corpora that obviate reliance on external KBs. By extracting salient entities and generating diverse texts that explicitly link them, one can create a large, learnable synthetic corpus that fuels continued pretraining, thereby improving factual acquisition and both QA and instruction-following in data-scarce domains.",
  "target_paper": {
    "title": "Synthetic continued pretraining",
    "authors": "Zitong Yang, Neil Band, Shuangping Li, Emmanuel Candes, Tatsunori Hashimoto",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "large language model, synthetic data, continued pretraining",
    "abstract": "Pretraining on large-scale, unstructured internet text enables language models to acquire a significant amount of world knowledge.\nHowever, this knowledge acquisition is data-inefficient---to learn a fact, models must be trained on hundreds to thousands of diverse representations of it.\nThis poses a challenge when adapting a pretrained model to a small corpus of domain-specific documents, where each fact may appear rarely or only once.\nWe propose to bridge this gap with synthetic continued pretraining: using the small domain-specific corpus to synthesize a large corpus more amenable to learning, and then performing continued pretraining on the synthesized corpus.\nWe instantiate this proposal with EntiGraph, a synthetic data augmentation algorithm that extracts salient entities from the source corpus and then generates diverse text by drawing connections between those entities.\nSynthetic continued pretraining with EntiGraph enables a language model to answer questions and follow generic",
    "openreview_id": "07yvxWDSla",
    "forum_id": "07yvxWDSla"
  },
  "analysis_timestamp": "2026-01-06T18:55:32.200471"
}
{
  "prior_works": [
    {
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
      "authors": "Colin Raffel et al.",
      "year": 2020,
      "arxiv_id": "1910.10683",
      "role": "Foundation",
      "relationship_sentence": "By casting all tasks\u2014including scalar predictions like STS-B\u2014into text-to-text form trained with cross-entropy, this work established the autoregressive regression formulation that RAFT critiques and makes decision-aware."
    },
    {
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": "Jacob Devlin et al.",
      "year": 2019,
      "arxiv_id": "1810.04805",
      "role": "Baseline",
      "relationship_sentence": "BERT\u2019s widely adopted practice of adding a separate regression head trained with squared error for scalar outputs is the contrasting baseline RAFT unifies with autoregressive approaches under a Bayes decision view."
    },
    {
      "title": "Pix2Seq: A Language Modeling Framework for Object Detection",
      "authors": "Ting Chen et al.",
      "year": 2021,
      "arxiv_id": "2109.10852",
      "role": "Foundation",
      "relationship_sentence": "Pix2Seq demonstrated that continuous targets can be discretized and learned autoregressively via token-level cross-entropy, directly motivating a principled, loss-aware treatment of autoregressive regression."
    },
    {
      "title": "Minimum Risk Training for Neural Machine Translation",
      "authors": "Shiqi Shen et al.",
      "year": 2016,
      "arxiv_id": "1512.02433",
      "role": "Inspiration",
      "relationship_sentence": "This work introduced optimizing expected task loss for sequence models, directly inspiring RAFT\u2019s use of Bayes-risk principles for regression-aware fine-tuning."
    },
    {
      "title": "High Quality Neural Machine Translation by Minimum Bayes Risk Decoding",
      "authors": "Markus Freitag et al.",
      "year": 2022,
      "arxiv_id": "2211.01928",
      "role": "Inspiration",
      "relationship_sentence": "By showing that choosing outputs to minimize expected evaluation loss outperforms likelihood-based decoding, it underpins RAFT\u2019s decision-theoretic alignment of training/inference for regression."
    },
    {
      "title": "Strictly Proper Scoring Rules, Prediction, and Estimation",
      "authors": "Tilmann Gneiting et al.",
      "year": 2007,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper formalizes that under squared error the Bayes-optimal action is the conditional mean, the key decision-theoretic insight RAFT operationalizes for autoregressive numeric prediction."
    },
    {
      "title": "Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation",
      "authors": "Bryan Eikema et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By showing that MAP decoding misaligns with evaluation risk, it highlights the need for risk-aware decision rules that RAFT adapts to regression with LLMs."
    }
  ],
  "synthesis_narrative": "A line of work established two dominant ways to use large language models for scalar prediction. One casts numeric targets as strings and trains decoder models with cross-entropy, as in the text-to-text paradigm where even regression-style tasks (e.g., STS-B) are generated token by token. Another keeps the model as a feature extractor and attaches a separate regression head trained with squared error, a practice popularized in BERT fine-tuning for similarity scoring and other continuous outputs. Extending the former, Pix2Seq showed that even inherently continuous targets like bounding boxes can be discretized and learned autoregressively with token-level likelihood, confirming the viability of sequence modeling for regression-like outputs. In parallel, decision-theoretic advances in sequence modeling\u2014minimum risk training and minimum Bayes risk decoding\u2014demonstrated that optimizing or decoding with respect to expected task loss can outperform likelihood-based objectives, while analyses of MAP\u2019s inadequacy emphasized that mode-seeking decisions often misalign with evaluation risk. The statistical foundation for these ideas is the proper scoring rules literature, which specifies Bayes-optimal actions under given losses (e.g., conditional means for squared error).\nTogether, these strands revealed a gap: autoregressive CE training and regression-head MSE training succeed in practice but ignore the Bayes-optimal decision for regression losses. The natural next step is to make fine-tuning and inference explicitly regression-aware: learn a predictive distribution with a decoder LLM and choose actions per the Bayes rule for the target loss, thereby unifying the two baselines and aligning training with the evaluation objective.",
  "target_paper": {
    "title": "Better autoregressive regression with LLMs via regression-aware fine-tuning",
    "authors": "Michal Lukasik, Zhao Meng, Harikrishna Narasimhan, Yin-Wen Chang, Aditya Krishna Menon, Felix Yu, Sanjiv Kumar",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "regression, LLMs",
    "abstract": "Decoder-based large language models (LLMs) have proven highly versatile, with remarkable successes even on problems ostensibly removed from traditional language generation.  One such example is solving regression problems, where the targets are real numbers rather than textual tokens.  A common approach to use LLMs on such problems is to perform fine-tuning based on the cross-entropy loss, and use autoregressive sampling at inference time. Another approach relies on fine-tuning a separate predictive head with a suitable loss such as squared error. While each approach has had success, there has been limited study on principled ways of using decoder LLMs for regression. In this work, we compare different prior works under a unified view, and introduce regression-aware fine-tuning(RAFT), a novel approach based on the Bayes-optimal decision rule. We demonstrate how RAFT improves over established baselines on several benchmarks and model families.",
    "openreview_id": "xGs7Ch3Vyo",
    "forum_id": "xGs7Ch3Vyo"
  },
  "analysis_timestamp": "2026-01-06T08:28:12.773876"
}
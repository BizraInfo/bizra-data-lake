{
  "prior_works": [
    {
      "title": "Multi-Agent Reinforcement Learning in Sequential Social Dilemmas",
      "authors": "Joel Z. Leibo et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized mixed-motive (sequential social dilemma) settings and provided canonical benchmarks, directly defining the problem regime in which conflict between individual and collective objectives arises and that our method targets."
    },
    {
      "title": "Prosocial Learning Agents Solve Generalized Stag Hunts Better",
      "authors": "Alexander Peysakhovich et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Introduces reward restructuring via prosocial (other-regarding) preferences by linearly blending others\u2019 rewards, a primary cooperation baseline our method replaces with gradient-level balancing to avoid altering task-specific rewards."
    },
    {
      "title": "Inequity Aversion Improves Cooperation in Intertemporal Social Dilemmas",
      "authors": "Edward Hughes et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Proposes intrinsic inequity-aversion terms to promote fairness and cooperation, whose limitation\u2014modifying the reward signal rather than preserving fairness over original task-specific rewards\u2014is explicitly addressed by our conflict-aware gradient adjustment."
    },
    {
      "title": "Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning",
      "authors": "Natasha Jaques et al.",
      "year": 2019,
      "role": "Baseline",
      "relationship_sentence": "Uses intrinsic social-influence rewards to encourage coordination, representing the intrinsic-motivation class of cooperation methods our approach supersedes by directly balancing individual vs. collective policy gradients without auxiliary rewards."
    },
    {
      "title": "Learning with Opponent-Learning Awareness",
      "authors": "Jakob N. Foerster et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrates that modifying gradient updates to shape social outcomes in mixed-motive games can induce cooperation; we adopt the idea of gradient-level intervention but redirect it to reconcile conflicts between self and collective gradients for fairness."
    },
    {
      "title": "Multi-Task Learning as Multi-Objective Optimization",
      "authors": "Ozan Sener et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "Provides the MGDA framework for aggregating conflicting gradients across objectives; our method extends this multi-objective gradient perspective to MARL by balancing gradients from individual and collective objectives when they conflict."
    },
    {
      "title": "Gradient Surgery for Multi-Task Learning",
      "authors": "Tianhe Yu et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "Introduces PCGrad to project away conflicting gradient components; we adapt the conflict-aware projection idea to policy gradients, dynamically adjusting between self and social objectives to preserve both cooperation and fairness."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014an adaptive, conflict-aware gradient adjustment that balances policy gradients from individual and collective objectives while preserving fairness\u2014arises at the intersection of mixed-motive MARL and multi-objective gradient methods. Leibo et al. established sequential social dilemmas as the canonical mixed-motive setting where individual and collective incentives can conflict, defining the exact regime targeted here. Prior cooperation strategies primarily relied on reward restructuring: prosocial value mixing (Peysakhovich & Lerer) and intrinsic motivations such as inequity aversion (Hughes et al.) and social influence (Jaques et al.). While effective at fostering cooperation, these methods modify the reward landscape, leading to the key gap our paper tackles: they do not guarantee fairness with respect to the original, task-specific rewards. Methodologically, the paper draws inspiration from gradient-based outcome shaping in MARL (LOLA), which showed that carefully designed gradient updates can steer social behavior. To resolve objective conflicts without altering task rewards, the work extends multi-objective gradient aggregation ideas from MTL: MGDA (Sener & Koltun) supplies the Pareto-stationarity lens for combining objectives, and PCGrad (Yu et al.) provides a concrete conflict-aware projection mechanism. By adapting these gradient-conflict tools to policy gradients over individual vs. collective objectives, the proposed method directly addresses the identified gap, improving cooperation while explicitly preserving fairness across agents\u2019 original rewards.",
  "analysis_timestamp": "2026-01-06T23:08:23.941119"
}
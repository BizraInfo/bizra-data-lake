{
  "prior_works": [
    {
      "title": "How Powerful Are Graph Neural Networks?",
      "authors": "Keyulu Xu et al.",
      "year": 2019,
      "role": "Baseline",
      "relationship_sentence": "Established the WL hierarchy as the de facto yardstick for GNN expressivity and tied MPNNs to 1-WL, which this paper directly replaces with a finer, polynomial-degree\u2013based hierarchy."
    },
    {
      "title": "Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks",
      "authors": "Christopher Morris et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Introduced k-WL-inspired higher-order GNNs but retained WL\u2019s coarse granularity; the limitations of WL-based hierarchies highlighted here directly motivate the paper\u2019s alternative expressivity scale via equivariant polynomials."
    },
    {
      "title": "Invariant and Equivariant Graph Networks",
      "authors": "Haggai Maron et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Characterized permutation-invariant/equivariant linear maps on graphs via tensor constructions; the present work generalizes from linear maps to a complete basis of equivariant graph polynomials of arbitrary degree using related tensor contraction primitives."
    },
    {
      "title": "Provably Powerful Graph Networks",
      "authors": "Haggai Maron et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Showed that higher-order tensor contractions yield powerful (substructure-counting) GNNs; this directly inspires the paper\u2019s multigraph-indexed basis where evaluating an equivariant polynomial is realized as a tensor contraction."
    },
    {
      "title": "Universal Invariant and Equivariant Graph Neural Networks",
      "authors": "Boris Keriven et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Provided universality results for invariant/equivariant GNNs via tensorization and polynomial approximation; the current paper sharpens this by constructing an explicit, comprehensive basis of equivariant graph polynomials and organizing expressivity by polynomial degree."
    },
    {
      "title": "Large Networks and Graph Limits",
      "authors": "L\u00e1szl\u00f3 Lov\u00e1sz",
      "year": 2012,
      "role": "Foundation",
      "relationship_sentence": "Developed the homomorphism-count framework and graph algebra viewpoint where graph functionals are generated by counts of (multi)graph patterns; this foundational idea directly underpins the paper\u2019s multigraph-indexed basis for equivariant polynomials."
    },
    {
      "title": "Can Graph Neural Networks Count Substructures?",
      "authors": "Zhengdao Chen et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "Proved standard MPNNs fail to count certain substructures (e.g., cycles), a concrete shortcoming that the paper\u2019s polynomial-degree hierarchy and tensor-contraction basis address by precisely characterizing which multigraph counts are computable."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014replacing the WL-based lens on GNN expressivity with a hierarchy built on equivariant polynomials and providing a full, multigraph-indexed basis\u2014emerges from two converging lines of work. First, Xu et al. and Morris et al. cemented the WL hierarchy as the standard framework linking GNNs to graph isomorphism tests, but also exposed its coarseness and practical limitations for modern architectures. These gaps motivate a more constructive and fine-grained measure of expressivity. Second, the tensor-based theory of permutation invariance/equivariance for graphs (Maron et al.; Keriven & Peyr\u00e9) established that equivariant computations can be systematically built from tensor symmetries and contractions, and that such architectures possess universality and counting power. Building directly on this, the present work generalizes beyond linear layers to characterize all equivariant graph polynomials, showing each basis element corresponds to a multigraph and is computable via a specific tensor contraction. This construction is tightly connected to Lov\u00e1sz\u2019s homomorphism-count and graph algebra perspective, where graph functionals arise from counting pattern embeddings\u2014now extended to equivariant mappings with explicit bases. Finally, empirical and theoretical limitations in substructure counting identified by Chen et al. are addressed by organizing GNN capability by polynomial degree, yielding a principled, actionable expressivity hierarchy and computational blueprint that improves upon WL\u2019s guidance.",
  "analysis_timestamp": "2026-01-06T23:09:26.550113"
}
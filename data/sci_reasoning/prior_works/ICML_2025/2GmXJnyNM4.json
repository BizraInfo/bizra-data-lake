{
  "prior_works": [
    {
      "title": "Implicit Regularization in Matrix Factorization",
      "authors": "Suriya Gunasekar, Jason D. Lee, Daniel Soudry, Nathan Srebro",
      "year": 2018,
      "role": "Matrix implicit-bias blueprint",
      "relationship_sentence": "Provided the core analytical template showing that small-initialization gradient methods on overparameterized matrix factorizations converge to low-norm/low-rank solutions, a blueprint the paper extends from matrices to tubal tensor factorizations."
    },
    {
      "title": "Gradient Descent Aligns the Layers of Deep Linear Networks",
      "authors": "Ziwei Ji, Matus Telgarsky",
      "year": 2019,
      "role": "Dynamics and balancing in factorized models",
      "relationship_sentence": "Established layer-balancing and dynamical invariants for gradient descent in factorized linear models, techniques adapted to control and analyze the discrete-time GD trajectory in the paper\u2019s tensor-factorized setting."
    },
    {
      "title": "Convolutional Rectifier Networks as Generalized Tensor Decompositions",
      "authors": "Nadav Cohen, Or Sharir, Amnon Shashua",
      "year": 2016,
      "role": "Neural nets\u2013tensor link",
      "relationship_sentence": "Argued that convolutional architectures correspond to tensor decompositions, motivating the paper\u2019s focus on tensor factorizations (specifically t-product/tubal rank) as a principled surrogate for understanding implicit regularization in neural models."
    },
    {
      "title": "Third-order tensors as operators on matrices, Part I: Basic linear algebra via the t-product",
      "authors": "M. E. Kilmer, K. Braman, N. Hao, R. C. Hoover",
      "year": 2013,
      "role": "t-product foundation",
      "relationship_sentence": "Introduced the t-product algebra underlying tubal rank and t-SVD, providing the mathematical framework the paper uses to define the overparameterized tensor factorization and analyze its gradient dynamics."
    },
    {
      "title": "A Third-Order Generalization of the Matrix SVD Based on the t-Product",
      "authors": "M. E. Kilmer, C. D. Martin",
      "year": 2011,
      "role": "t-SVD/tubal rank definition",
      "relationship_sentence": "Formalized t-SVD and tubal rank, which are central to the paper\u2019s objective of proving that gradient descent implicitly biases solutions toward low tubal rank."
    },
    {
      "title": "Novel methods for multilinear data completion using tensor-SVD",
      "authors": "Zhihui Zhang, Gregory Ely, Shuchin Aeron, Ning Hao, M. E. Kilmer",
      "year": 2014,
      "role": "Low-tubal-rank modeling and relevance",
      "relationship_sentence": "Demonstrated the practical value of low-tubal-rank models (via t-SVD/TNN) for visual data, justifying the paper\u2019s choice of the tubal model as a target for implicit regularization results."
    },
    {
      "title": "On Lazy Training in Differentiable Programming",
      "authors": "L\u00e9na\u00efc Chizat, Francis Bach",
      "year": 2019,
      "role": "Lazy regime baseline",
      "relationship_sentence": "Characterized the lazy (NTK-like) regime, which prior tensor results largely adhered to; the paper explicitly goes beyond this regime by establishing implicit regularization for discrete gradient descent outside lazy training."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014establishing implicit regularization of discrete-time gradient descent for overparameterized tubal tensor factorizations beyond the lazy regime\u2014builds on two pillars: implicit bias analyses for factorized linear models and the t-product/tubal-rank tensor calculus. On the implicit-bias side, Gunasekar et al. (2018) provided the matrix-case blueprint showing that small-initialization gradient methods preferentially select low-norm/low-rank solutions in factorized parameterizations. Ji and Telgarsky (2019) contributed dynamical tools\u2014layer balancing and invariants\u2014that make discrete gradient descent analyzable in non-lazy, overparameterized settings. These ideas inform the paper\u2019s strategy to control GD trajectories and derive a low-rank bias, now in a tensorized setting.\nOn the modeling side, Kilmer and Martin (2011) and Kilmer et al. (2013) introduced the t-product and t-SVD, defining tubal rank and the algebra required to reason about tensor factorizations analogously to matrices. Zhang et al. (2014) demonstrated the efficacy of low-tubal-rank modeling for image data, motivating the specific choice of tubal structure as both practically relevant and mathematically tractable for implicit-bias analysis. Cohen, Sharir, and Shashua (2016) linked neural networks\u2014especially convolutional architectures\u2014to tensor decompositions, positioning tensor factorization as a proxy to study inductive biases of broader classes of networks. Finally, Chizat and Bach (2019) delineated the lazy training regime, which prior tensor results often assumed; the present work advances beyond this by proving an implicit low-tubal-rank bias for actual gradient descent, closing the gap between gradient-flow/lazy analyses and practical discrete-time optimization.",
  "analysis_timestamp": "2026-01-07T00:29:42.072881"
}
{
  "prior_works": [
    {
      "title": "Online Convex Programming and Generalized Infinitesimal Gradient Ascent",
      "authors": "Martin Zinkevich",
      "year": 2003,
      "role": "Foundational OCO",
      "relationship_sentence": "This paper established the OCO framework and first-order methods (OGD) with O(\u221aT) regret, which the NeurIPS 2024 paper refines and extends to the constrained, adversarial setting while preserving first-order simplicity."
    },
    {
      "title": "Logarithmic Regret Algorithms for Online Convex Optimization",
      "authors": "Elad Hazan, Amit Agarwal, Satyen Kale",
      "year": 2007,
      "role": "Strongly-convex OCO",
      "relationship_sentence": "The NeurIPS 2024 work leverages the classical O(log T) regret guarantees for strongly convex losses to obtain logarithmic regret while simultaneously controlling adversarial cumulative constraint violation."
    },
    {
      "title": "Online Learning with Primary and Secondary Losses",
      "authors": "Shie Mannor, Ofer Shamir, Ambuj Tewari, Nathan Srebro (often cited as Mannor, Shamir, Sridharan/Sridharan variants in early approachability-based constrained OL)",
      "year": 2009,
      "role": "Adversarial constraints via approachability/Lagrangian",
      "relationship_sentence": "This line of work introduced handling adversarial long-term constraints through Blackwell/dual formulations, a conceptual precursor to the NeurIPS 2024 paper\u2019s adversarial-constraints perspective and regret\u2013violation guarantees."
    },
    {
      "title": "Stochastic Network Optimization with Application to Communication and Queueing Systems",
      "authors": "Michael J. Neely",
      "year": 2010,
      "role": "Virtual-queue/primal\u2013dual methodology",
      "relationship_sentence": "Virtual-queue and drift-plus-penalty techniques from this monograph underpin many primal\u2013dual updates used to control time-averaged constraint violations, which the NeurIPS 2024 paper adapts to adversarial COCO with optimal O(\u221aT)\u2013O(\u221aT) rates."
    },
    {
      "title": "Online Convex Optimization with Long-Term Constraints",
      "authors": "M. Mahdavi, R. Jin, T. Yang",
      "year": 2012,
      "role": "First formal COCO trade-offs",
      "relationship_sentence": "This seminal COCO work formalized regret versus cumulative constraint violation trade-offs and proposed primal\u2013dual updates, directly motivating the NeurIPS 2024 paper\u2019s quest to close the gap to simultaneous optimal O(\u221aT) regret and ~O(\u221aT) CCV."
    },
    {
      "title": "A Low-Complexity Primal\u2013Dual Method with O(\u221aT) Regret and O(\u221aT) Constraint Violation for OCO with Long-Term Constraints",
      "authors": "Hao Yu, Michael J. Neely",
      "year": 2017,
      "role": "Near-optimal bounds under assumptions",
      "relationship_sentence": "Providing O(\u221aT)\u2013O(\u221aT) guarantees under conditions such as Slater or stochastic constraints, this work\u2019s primal\u2013dual/virtual-queue design is a direct algorithmic precursor that the NeurIPS 2024 paper strengthens to fully adversarial constraints without restrictive assumptions."
    },
    {
      "title": "Blackwell Approachability and No-Regret Learning are Equivalent",
      "authors": "Jacob Abernethy, Peter L. Bartlett, Elad Hazan, Alexander Rakhlin",
      "year": 2011,
      "role": "Approachability-duality foundation",
      "relationship_sentence": "This result formally links approachability and no-regret learning, providing the theoretical backbone for treating constraints as adversarial vector payoffs, a perspective operationalized and sharpened in the NeurIPS 2024 paper."
    }
  ],
  "synthesis_narrative": "The NeurIPS 2024 paper resolves a central open problem in constrained online convex optimization (COCO): achieving simultaneous O(\u221aT) regret and ~O(\u221aT) cumulative constraint violation (CCV) against adaptive adversaries with a simple first-order method. Its lineage begins with Zinkevich\u2019s foundational OCO framework and OGD, which defined first-order updates with O(\u221aT) regret. For strongly convex losses, Hazan\u2013Agarwal\u2013Kale established O(log T) regret; the new paper inherits and integrates this rate while still maintaining tight CCV control. The COCO agenda was crystallized by Mahdavi\u2013Jin\u2013Yang, who introduced long-term constraint violation as a performance criterion and initiated primal\u2013dual algorithms that, however, incurred suboptimal CCV rates. Two theoretical pillars then shaped subsequent progress: approachability/dual viewpoints for adversarial constraints (e.g., Mannor\u2013Shamir\u2013Sridharan-style primary/secondary losses and Abernethy\u2013Bartlett\u2013Hazan\u2013Rakhlin\u2019s equivalence) and virtual-queue/drift-plus-penalty methods from Neely\u2019s stochastic network optimization. Building concretely on these, Yu\u2013Neely demonstrated that primal\u2013dual updates can achieve O(\u221aT) regret and O(\u221aT) CCV under assumptions such as Slater or stochastic constraints. The NeurIPS 2024 work synthesizes these strands: it designs a streamlined first-order primal\u2013dual policy, dispenses with restrictive feasibility/stochastic assumptions, and establishes optimal O(\u221aT)\u2013O(\u221aT) guarantees against adaptive adversaries; moreover, it couples strong convexity step-size schedules to attain O(log T) regret without degrading the CCV rate. In doing so, it closes the gap left by prior COCO algorithms and provides a clean, assumption-light optimal solution.",
  "analysis_timestamp": "2026-01-06T23:39:42.968495"
}
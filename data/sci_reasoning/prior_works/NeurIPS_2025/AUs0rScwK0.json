{
  "prior_works": [
    {
      "title": "R\u00e9nyi Differential Privacy",
      "authors": "Ilya Mironov",
      "year": 2017,
      "role": "Privacy accounting foundation",
      "relationship_sentence": "SGM\u2019s joint privacy analysis explicitly relies on RDP to compose the randomness from sketching and the Gaussian mechanism into a single, tight privacy guarantee."
    },
    {
      "title": "Subsampled R\u00e9nyi Differential Privacy and Analytical Moments Accountant",
      "authors": "Yu-Xiang Wang, Borja Balle, Shiva P. Kasiviswanathan",
      "year": 2019,
      "role": "Tight accounting under sampling/composition",
      "relationship_sentence": "SGM uses these RDP tools to track per-round privacy in federated participation settings, enabling sharper bounds than na\u00efve composition of sketching and Gaussian noise."
    },
    {
      "title": "The Johnson\u2013Lindenstrauss Transform Itself Preserves Differential Privacy",
      "authors": "Jeremiah Blocki, Avrim Blum, Anupam Datta, Or Sheffet",
      "year": 2012,
      "role": "Privacy of sketching/random projections",
      "relationship_sentence": "This work formalizes that random projections provide DP with guarantees controlled by the sketch dimension, a key pillar SGM leverages to quantify the sketch-induced privacy component."
    },
    {
      "title": "Differentially Private Federated Learning: A Client Level Perspective",
      "authors": "Robin C. Geyer, Tassilo Klein, Moin Nabi",
      "year": 2017,
      "role": "Client-level DP in federated learning via clipping + Gaussian noise",
      "relationship_sentence": "SGM generalizes the standard client-level Gaussian mechanism introduced here by integrating sketching into the mechanism and analyzing the combined privacy."
    },
    {
      "title": "Deep Learning with Differential Privacy",
      "authors": "Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang",
      "year": 2016,
      "role": "Gaussian mechanism and tight accounting (moments accountant)",
      "relationship_sentence": "SGM builds on the clipping-plus-Gaussian-noise paradigm and tight accounting philosophy popularized here, replacing moments accountant with RDP for joint analysis."
    },
    {
      "title": "Federated Learning: Strategies for Communication-Efficient On-Device Machine Learning",
      "authors": "Jakub Kone\u010dn\u00fd, H. Brendan McMahan, Daniel Ramage, Peter Richt\u00e1rik",
      "year": 2016,
      "role": "Communication-efficient FL via structured/compressed updates",
      "relationship_sentence": "By motivating communication constraints and sketch-like structured updates in FL, this work sets the stage for SGM\u2019s use of sketching as a first-class primitive."
    },
    {
      "title": "FETCHSGD: Communication-Efficient Federated Learning with Sketching",
      "authors": "Daniel Rothchild et al.",
      "year": 2020,
      "role": "Practical sketching (e.g., CountSketch) for FL updates",
      "relationship_sentence": "Provides concrete sketching mechanisms for client updates that SGM can endow with joint privacy guarantees when combined with the Gaussian mechanism."
    }
  ],
  "synthesis_narrative": "Sketched Gaussian Mechanism (SGM) unifies two lines of work\u2014communication-efficient sketching and client-level differential privacy\u2014under a single, tighter privacy analysis. On the communication side, early federated learning work by Kone\u010dn\u00fd et al. framed the need for compressed/structured updates, and FETCHSGD established sketches (e.g., CountSketch) as practical client-side compressors. In parallel, the DP community showed that random projections themselves can act as privacy mechanisms, with Blocki et al. proving that Johnson\u2013Lindenstrauss-type sketching affords privacy controlled by the sketch dimension. On the privacy side specific to FL, Geyer et al. formalized client-level DP via clipping and a Gaussian aggregator, instantiating the dominant baseline mechanism. Abadi et al. further cemented the clipping-plus-Gaussian template and tight accounting ethos, while Mironov\u2019s R\u00e9nyi Differential Privacy and the subsampled RDP/Analytical Moments Accountant of Wang\u2013Balle\u2013Kasiviswanathan provided the modern, sharp tools for composing iterative mechanisms under sampling.\n\nSGM\u2019s key contribution is to replace the prevailing \u201cisolate-then-add\u201d accounting\u2014treat sketching\u2019s privacy and the Gaussian mechanism independently\u2014with a joint RDP analysis that captures how sketching\u2019s linear randomness reshapes sensitivity and interacts with Gaussian noise. By leveraging RDP composition (and its subsampled variants), SGM quantifies the combined effect of sketch dimension and noise scale across rounds, yielding more flexible and strictly sharper guarantees than na\u00efve composition. In essence, SGM operationalizes the theoretical DP of sketching in the federated setting and integrates it with the standard Gaussian mechanism using state-of-the-art RDP tools, thereby aligning communication efficiency with rigorous, improved client-level privacy.",
  "analysis_timestamp": "2026-01-06T23:42:48.126262"
}
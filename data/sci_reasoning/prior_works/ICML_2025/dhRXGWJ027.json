{
  "prior_works": [
    {
      "title": "Mathematical discoveries from program search with language models",
      "authors": "Bernardino Romera-Paredes et al.",
      "year": 2024,
      "role": "Extension",
      "relationship_sentence": "This paper directly adapts FunSearch\u2019s LLM-in-the-loop evolutionary program synthesis, replacing its mathematical objective with a behavioral fit objective and a cognitive-model DSL to discover symbolic cognitive algorithms."
    },
    {
      "title": "A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement",
      "authors": "Robert A. Rescorla et al.",
      "year": 1972,
      "role": "Foundation",
      "relationship_sentence": "The work\u2019s discovered programs target and generalize the foundational delta-rule framework introduced here, which defines the core problem formulation for associative reward learning used across species."
    },
    {
      "title": "An approximately Bayesian delta-rule model explains the dynamics of belief updating in a changing environment",
      "authors": "Mohamed R. Nassar et al.",
      "year": 2010,
      "role": "Baseline",
      "relationship_sentence": "This volatile-learning account is a primary state-of-the-art baseline for reversal/bandit tasks that the new discovered programs are evaluated against and outperform, addressing its parametric constraints."
    },
    {
      "title": "Uncertainty in perception and the Hierarchical Gaussian Filter",
      "authors": "Christoph D. Mathys et al.",
      "year": 2014,
      "role": "Baseline",
      "relationship_sentence": "HGF provides a dominant Bayesian volatility model for learning under uncertainty; the paper positions its discovered symbolic programs as outperforming or subsuming such hand-specified Bayesian updates on behavior."
    },
    {
      "title": "A simple model for learning in volatile environments",
      "authors": "Payam Piray et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "Piray and Daw\u2019s hazard-rate\u2013based model is a modern baseline for reversal learning; the present work directly compares to and surpasses this model, revealing algorithmic variants discovered by program search."
    },
    {
      "title": "Human-level concept learning through probabilistic program induction",
      "authors": "Brenden M. Lake et al.",
      "year": 2015,
      "role": "Inspiration",
      "relationship_sentence": "This paper\u2019s view of cognitive hypotheses as programs inspires the present work\u2019s framing of cognitive models as executable symbolic programs amenable to automated discovery."
    },
    {
      "title": "AI Feynman: A physics-inspired method for symbolic regression",
      "authors": "Silviu-Marian Udrescu et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "Symbolic regression for interpretable discovery motivates the pursuit of symbolic cognitive models; the present work extends beyond equation fitting by discovering algorithmic, stateful programs via LLM-guided search."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key innovation\u2014automatically discovering interpretable, symbolic cognitive models that explain human and animal reward-learning behavior\u2014emerges at the intersection of three intellectual lines. First, foundational associative learning work (Rescorla\u2013Wagner) and its state-of-the-art successors for volatile environments (Nassar et al., Mathys et al.\u2019s HGF, and Piray & Daw) define the core problem formulation and provide the primary hand-designed baselines the new approach seeks to surpass. These models formalized how learning rates adapt to uncertainty and change points, but they rely on specific parametric choices and limited functional forms, motivating a search beyond human-crafted variants. Second, the cognitive-science perspective that hypotheses can be represented as programs (Lake et al.) directly informs the decision to search in a space of executable symbolic algorithms, ensuring discovered solutions remain interpretable as cognitive theories. Third, FunSearch (Romera-Paredes et al.) supplies the concrete mechanism\u2014an LLM-driven evolutionary program search\u2014that the authors extend to this domain by introducing a cognitive-model DSL and a behavioral-likelihood objective. Compared to symbolic regression approaches like AI Feynman, which uncover static equations, the present method targets algorithmic, stateful computations central to learning and decision-making. Together, these prior works enable and motivate the paper\u2019s contribution: using LLM-guided program synthesis to discover novel, interpretable cognitive algorithms that outperform leading Bayesian and delta-rule baselines across species.",
  "analysis_timestamp": "2026-01-06T23:07:19.591732"
}
{
  "prior_works": [
    {
      "title": "Deep Ensembles: A Simple and Scalable Method",
      "authors": "Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell",
      "year": 2017,
      "role": "Empirical baseline and limitation",
      "relationship_sentence": "Established ensembles as a strong, practical approach to epistemic uncertainty, providing the benchmark that Epinet aims to surpass while highlighting the compute cost that motivates a cheaper architecture for joint predictions."
    },
    {
      "title": "Deep Exploration via Bootstrapped DQN",
      "authors": "Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy",
      "year": 2016,
      "role": "Architectural precursor (index-conditioned heads)",
      "relationship_sentence": "Introduced multi-head bootstrap networks with an implicit epistemic index to sample coherent value functions, directly inspiring ENN\u2019s index interface and Epinet\u2019s lightweight head attached to a shared feature trunk."
    },
    {
      "title": "Randomized Prior Functions for Deep Reinforcement Learning",
      "authors": "Ian Osband, John Aslanides, Albin Cassirer",
      "year": 2018,
      "role": "Methodological antecedent (prior functions shaping uncertainty)",
      "relationship_sentence": "Showed how adding fixed randomized prior functions to neural heads yields meaningful uncertainty and coherent function samples, a mechanism Epinet generalizes to supervised settings for joint predictions with modest compute."
    },
    {
      "title": "Conditional Neural Processes",
      "authors": "Marta Garnelo, Dan Rosenbaum, Chris J. Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, et al.",
      "year": 2018,
      "role": "Conceptual antecedent (latent index for function draws)",
      "relationship_sentence": "Demonstrated using a global latent variable to induce coherent joint predictions across inputs, informing ENN\u2019s epistemic index design as a generic interface for sampling functions rather than only marginal predictions."
    },
    {
      "title": "Gaussian Processes for Machine Learning",
      "authors": "Carl E. Rasmussen, Christopher K. I. Williams",
      "year": 2006,
      "role": "Theoretical foundation (gold standard for joint predictions)",
      "relationship_sentence": "Provided the canonical framework for coherent joint predictive distributions over multiple inputs, the property ENNs seek to approximate at scale with neural architectures like the Epinet."
    },
    {
      "title": "Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling",
      "authors": "Carlos Riquelme, George Tucker, Jasper Snoek",
      "year": 2018,
      "role": "Efficient uncertainty head precedent",
      "relationship_sentence": "Popularized neural-linear approaches that attach a lightweight Bayesian head to a deep feature extractor, a design pattern Epinet extends to produce coherent joint predictions with minimal incremental computation."
    },
    {
      "title": "A Bayesian Framework for Reinforcement Learning",
      "authors": "James Strens",
      "year": 2000,
      "role": "Motivating theory (posterior sampling requires joint predictions)",
      "relationship_sentence": "Formalized posterior sampling for RL, emphasizing the need for coherent function samples across many inputs; ENNs operationalize this requirement as a general interface, and Epinet provides a scalable instantiation."
    }
  ],
  "synthesis_narrative": "Epistemic Neural Networks (ENN) formalize a general interface for models that produce coherent joint predictions across multiple inputs via an epistemic index, and Epinet instantiates this with a lightweight head that augments any base network. This contribution crystallizes and unifies strands from uncertainty estimation, function-sampling, and efficient architectural design. Deep Ensembles established a strong practical baseline for epistemic uncertainty but at prohibitive compute, directly motivating a cheaper alternative that still yields coherent joint predictions. Bootstrapped DQN introduced index-conditioned multi-head networks to sample full value functions, while Randomized Prior Functions showed that injecting fixed priors into heads shapes meaningful uncertainty\u2014both ideas feed directly into ENN\u2019s index abstraction and Epinet\u2019s architectural recipe. From the probabilistic side, Gaussian Processes provide the gold standard for exact joint predictive distributions, and Conditional Neural Processes demonstrated how a global latent variable can induce coherent function draws in neural models; ENN\u2019s epistemic index adopts this function-sampling perspective without constraining to a specific Bayesian formulation. Practically, the neural-linear paradigm from the Deep Bayesian Bandits Showdown exemplified attaching a lightweight Bayesian head to a deep feature extractor for efficient uncertainty, a pattern Epinet generalizes to deliver scalable joint predictions that outperform large ensembles. Finally, the posterior-sampling view from Strens underscores why joint predictions matter for decision making, reinforcing ENN\u2019s design objective and evaluation criteria.",
  "analysis_timestamp": "2026-01-06T23:42:48.044968"
}
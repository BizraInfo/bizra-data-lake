{
  "prior_works": [
    {
      "title": "Extracting Training Data from Large Language Models",
      "authors": "Nicholas Carlini et al.",
      "year": 2021,
      "role": "Baseline",
      "relationship_sentence": "This work operationalized the completion-based test for detecting training data membership\u2014using n-gram uniqueness/overlap heuristics to define ground-truth\u2014which the current paper directly adopts as a baseline and then shows can be systematically gamed."
    },
    {
      "title": "The Secret Sharer: Measuring Unintended Memorization in Neural Networks",
      "authors": "Nicholas Carlini et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "It introduced the core notion and measurement of verbatim memorization (via canaries and exposure), providing the conceptual foundation that a model\u2019s ability to produce verbatim text is used as an indicator of training-set membership, which this paper critically reexamines."
    },
    {
      "title": "Deduplicating Training Data Makes Language Models Better",
      "authors": "Katherine Lee et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "By showing that duplicate and near-duplicate content drives memorization and proposing n-gram/MinHash-style dedup heuristics, this paper set assumptions that the current work targets\u2014demonstrating that even after such removals, models can still verbatim complete non-member sequences under fixed n-gram definitions."
    },
    {
      "title": "On the Resemblance and Containment of Documents",
      "authors": "Andrei Z. Broder",
      "year": 1997,
      "role": "Foundation",
      "relationship_sentence": "This classic work introduced shingling and MinHash for near-duplicate detection\u2014the methodological basis behind n-gram/overlap-style membership and dedup heuristics that the current paper shows are brittle for defining ground-truth membership in completion tests."
    },
    {
      "title": "Membership Inference Attacks Against Machine Learning Models",
      "authors": "Reza Shokri et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "It formalized the membership inference problem that this paper instantiates for generative LMs; the present work specifically interrogates how ground-truth membership is defined in this setting and why prevailing n-gram criteria can mislead MI conclusions."
    },
    {
      "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data",
      "authors": "Guillaume Wenzek et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "CCNet popularized large-scale web-corpus cleaning and near-duplicate removal using shingling/MinHash; the present paper\u2019s findings directly question the sufficiency of such fixed-overlap heuristics for certifying non-membership in LLM training data."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014showing that completion tests can declare membership even when a target sequence is not present under standard n-gram definitions\u2014arises directly from the trajectory of work on memorization, data extraction, and deduplication in language models. Carlini et al. (2019) established the conceptual basis for unintended memorization and its measurement, linking verbatim reproduction to training-set exposure. Building on that, Carlini et al. (2021) formalized completion-based extraction protocols and popularized the practice of defining ground-truth membership through n-gram uniqueness/overlap filters so as to distinguish genuine memorization from generic continuation. In parallel, the web-scale data cleaning and deduplication ecosystem (Broder, 1997; Wenzek et al., 2020) and the empirical demonstration that duplicate/near-duplicate content inflates memorization (Lee et al., 2022) entrenched fixed shingle/n-gram\u2013style heuristics as the practical proxy for both removal and membership labeling. Situated within the broader membership inference framework (Shokri et al., 2017), the present paper identifies a critical gap: these overlap-based ground truths are not reliable. Through retraining after removing all sequences flagged by the very completion tests and by constructing adversarial datasets, the authors show that models can still verbatim complete targets that are non-members at any chosen n, encompassing exact duplicates, near duplicates, and even short overlaps. The result directly challenges the assumptions encoded in extraction baselines and dedup heuristics, demonstrating that no single n suffices for principled membership definition in completion-based audits.",
  "analysis_timestamp": "2026-01-06T23:07:19.620385"
}
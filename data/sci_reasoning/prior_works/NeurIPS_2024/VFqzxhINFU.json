{
  "prior_works": [
    {
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer",
      "year": 2022,
      "role": "foundation architecture",
      "relationship_sentence": "StoryDiffusion augments the self-attention blocks inside a pre-trained latent diffusion U-Net, directly building on the architectural and training paradigm introduced by Latent Diffusion Models."
    },
    {
      "title": "Prompt-to-Prompt: Image Editing with Cross-Attention Control",
      "authors": "Hertz et al.",
      "year": 2022,
      "role": "attention-manipulation precedent",
      "relationship_sentence": "By demonstrating that manipulating diffusion attention maps preserves structure and semantics across edits, Prompt-to-Prompt motivates StoryDiffusion\u2019s idea of explicitly steering internal attention\u2014in their case self-attention\u2014to enforce content consistency across separate generations."
    },
    {
      "title": "TokenFlow: Consistent Diffusion Features for Long-Range Video Editing",
      "authors": "Bar-Tal et al.",
      "year": 2023,
      "role": "self-attention as correspondence signal",
      "relationship_sentence": "TokenFlow shows diffusion attention/feature tokens encode reliable inter-frame correspondences, directly inspiring StoryDiffusion\u2019s use of self-attention as the vehicle to propagate identity- and detail-level consistency across images."
    },
    {
      "title": "MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation",
      "authors": "Bar-Tal et al.",
      "year": 2023,
      "role": "multi-image consistency via synchronized denoising",
      "relationship_sentence": "MultiDiffusion\u2019s strategy of coordinating multiple diffusion trajectories to achieve coherent panoramas informs StoryDiffusion\u2019s design goal of maintaining long-range coherence across sequences without retraining the base model."
    },
    {
      "title": "Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation",
      "authors": "Wu et al.",
      "year": 2023,
      "role": "image-to-video extension baseline",
      "relationship_sentence": "Tune-A-Video establishes how to extend T2I diffusion to video with temporal conditioning, a baseline that StoryDiffusion advances by replacing latent-only temporal control with a semantic-space motion predictor for greater subject stability."
    },
    {
      "title": "AnimateDiff: Animate Your Personalized Text-to-Video",
      "authors": "Guo et al.",
      "year": 2023,
      "role": "motion module comparator",
      "relationship_sentence": "AnimateDiff popularized adding motion modules to frozen T2I UNets in latent space, a line StoryDiffusion directly contrasts by predicting motion in semantic space to reduce flicker and identity drift."
    },
    {
      "title": "Video Diffusion Models",
      "authors": "Jonathan Ho, Ajay Jain, Pieter Abbeel",
      "year": 2022,
      "role": "temporal diffusion backbone",
      "relationship_sentence": "VDM introduces temporal extensions of diffusion (e.g., spatiotemporal attention) that frame StoryDiffusion\u2019s video setting, against which its semantic-space motion conditioning is positioned as a more stable alternative."
    }
  ],
  "synthesis_narrative": "StoryDiffusion\u2019s core contribution\u2014Consistent Self-Attention (CSA) for multi-image consistency and a Semantic Motion Predictor (SMP) for long-range video\u2014sits at the intersection of three lines of work: latent diffusion backbones, attention manipulation for structural preservation, and temporal extensions of diffusion. Latent Diffusion Models provide the exact architecture whose internal self-attention blocks StoryDiffusion modifies in a zero-shot manner. Prompt-to-Prompt establishes that diffusion attention maps encode controllable structure; TokenFlow further demonstrates that diffusion features/attention form reliable correspondences across frames, directly suggesting that self-attention can propagate identity and fine details between separately generated images. For maintaining coherence across many outputs, MultiDiffusion shows the value of coordinating multiple denoising trajectories, a goal StoryDiffusion achieves not by fusing paths but by enforcing consistency within the model\u2019s own self-attention. \nOn the video side, Video Diffusion Models and Tune-A-Video lay the groundwork for adapting T2I diffusion to temporal data via spatiotemporal conditioning and cross-frame attention. AnimateDiff popularizes motion modules operating in latent space but also highlights stability limits (e.g., identity drift and flicker) when motion is modeled only there. StoryDiffusion\u2019s SMP responds by estimating motion in a higher-level semantic space and using it to drive transitions, yielding smoother, subject-consistent long-range videos while retaining the zero-shot, pretrained T2I backbone enabled by CSA.",
  "analysis_timestamp": "2026-01-06T23:42:49.045219"
}
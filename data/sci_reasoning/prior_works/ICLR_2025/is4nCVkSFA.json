{
  "prior_works": [
    {
      "title": "Sliced inverse regression for dimension reduction",
      "authors": "K. C. Li",
      "year": 1991,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Introduced inverse-regression/label-conditioned projections to recover the single-index direction under Gaussian covariates, directly motivating the paper\u2019s label-transformation step that induces alignment with the unknown signal."
    },
    {
      "title": "The generalized Lasso with non-linear observations",
      "authors": "Yaniv Plan and Roman Vershynin",
      "year": 2016,
      "arxiv_id": "1502.01711",
      "role": "Inspiration",
      "relationship_sentence": "Showed that with Gaussian features, appropriately transformed moments of labels (e.g., E[y x], E[(y^2\u22121) x x^T]) align with the true direction, providing the concrete label-transform template the paper generalizes to arbitrary losses/activations and Hermite ranks."
    },
    {
      "title": "Score Function Features for Discriminative Learning: Matrix and Tensor Framework",
      "authors": "M. Janzamin et al.",
      "year": 2014,
      "arxiv_id": "1412.2863",
      "role": "Extension",
      "relationship_sentence": "Developed Stein\u2019s identity/Hermite-based feature constructions to learn single-index directions via moments and tensors, which the paper adapts into a unified gradient-driven feature-learning procedure that leverages the same Hermite coefficients."
    },
    {
      "title": "Phase Retrieval via Wirtinger Flow: Theory and Algorithms",
      "authors": "E. J. Cand\u00e8s et al.",
      "year": 2015,
      "arxiv_id": "1407.1065",
      "role": "Related Problem",
      "relationship_sentence": "Established for the quadratic single-index case that a label-transformed spectral step followed by gradient descent recovers the signal, foreshadowing the paper\u2019s gradient-based pipeline that extends such ideas to general generative exponents s*."
    },
    {
      "title": "Solving random quadratic systems of equations is nearly as easy as solving linear systems",
      "authors": "Yuxin Chen and Emmanuel J. Cand\u00e8s",
      "year": 2015,
      "arxiv_id": "1509.06407",
      "role": "Inspiration",
      "relationship_sentence": "Introduced truncation/smoothing in Wirtinger Flow to stabilize nonconvex landscapes and enable polynomial-time recovery, directly inspiring the paper\u2019s \u2018landscape smoothing\u2019 mechanism for general single-index learning."
    },
    {
      "title": "Generalization error of random features and kernel methods for single-index models",
      "authors": "Song Mei and Andrea Montanari",
      "year": 2019,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Showed kernel/random-feature methods fail to exploit low Hermite rank in single-index models to achieve optimal rates, highlighting the need for true feature learning that the paper\u2019s gradient-based neural algorithm provides."
    },
    {
      "title": "Statistical algorithms and a lower bound for planted clique",
      "authors": "Vitaly Feldman et al.",
      "year": 2013,
      "arxiv_id": "1207.4920",
      "role": "Foundation",
      "relationship_sentence": "Formalized the statistical query framework and correlation-based lower bound technique underpinning the \u2126(d^{s*/2} \u2228 d) computational barrier the paper targets, providing the computational benchmark it aims to match."
    }
  ],
  "synthesis_narrative": "Sliced inverse regression introduced the core inverse-regression idea for single-index models: by conditioning on the response and projecting features, one can recover the index direction under Gaussian covariates, planting the seed for label-driven feature alignment. Building on Gaussian structure, Plan and Vershynin showed that specific label transformations yield feature-label moments that directly align with the true direction (e.g., first- or second-order moments depending on the Hermite rank), providing a concrete template for designing objectives that expose the signal. Janzamin and collaborators systematized this through Stein\u2019s identity and Hermite expansions, constructing matrix/tensor estimators whose coefficients match the link\u2019s Hermite components, thereby operationalizing feature constructions tailored to the generative exponent. In the quadratic single-index instance, Wirtinger Flow demonstrated a spectral initializer plus gradient descent can succeed, while Chen\u2013Cand\u00e8s\u2019 truncation/smoothing stabilized the nonconvex landscape\u2014evidence that careful loss shaping turns hard landscapes into algorithmically tractable ones. Complementarily, analyses of random features and kernels in single-index models revealed these methods\u2019 inability to exploit low Hermite rank, and the statistical query framework provided the computational benchmark governing sample requirements.\nTaken together, these works suggested a path: use Hermite-informed label transforms to expose the aligned feature direction and smooth the nonconvex landscape so gradient methods can reliably track it. The paper synthesizes these insights into a unified, gradient-based two-layer network procedure that adapts to general losses/activations, leverages Hermite structure across generative exponents, and is designed expressly to meet the SQ-informed optimal computational\u2013statistical tradeoff by provably aligning learned features with the latent signal.",
  "target_paper": {
    "title": "Can Neural Networks Achieve Optimal Computational-statistical Tradeoff? An Analysis on Single-Index Model",
    "authors": "Siyu Chen, Beining Wu, Miao Lu, Zhuoran Yang, Tianhao Wang",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "single-index model, feature learning, gradient-based method, computational-statistical tradeoff",
    "abstract": "In this work, we tackle the following question: Can neural networks trained with gradient-based methods achieve the optimal statistical-computational tradeoff in learning Gaussian single-index models? \nPrior research has shown that any polynomial-time algorithm under the statistical query (SQ) framework requires $\\Omega(d^{s^\\star/2}\\lor d)$ samples, where $s^\\star$ is the generative exponent representing the intrinsic difficulty of learning the underlying model.\nHowever, it remains unknown whether neural networks can achieve this sample complexity. \nInspired by prior techniques such as label transformation and landscape smoothing for learning single-index models, we propose a unified gradient-based algorithm for training a two-layer neural network in polynomial time.\nOur method is adaptable to a variety of loss and activation functions, covering a broad class of existing approaches.\nWe show that our algorithm learns a feature representation that strongly aligns with the unknown signal",
    "openreview_id": "is4nCVkSFA",
    "forum_id": "is4nCVkSFA"
  },
  "analysis_timestamp": "2026-01-06T06:39:29.955864"
}
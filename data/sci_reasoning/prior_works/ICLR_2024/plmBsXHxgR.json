{
  "prior_works": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "arxiv_id": "2103.00020",
      "role": "Foundation",
      "relationship_sentence": "This work\u2019s CLIP-style cross-modal embedding space is the mechanism the paper targets\u2014adversarial images are optimized to land near toxic text embeddings using only the vision encoder, exploiting CLIP\u2019s image\u2013text alignment."
    },
    {
      "title": "LLaVA: Large Language and Vision Assistant",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "arxiv_id": "2304.08485",
      "role": "Baseline",
      "relationship_sentence": "LLaVA\u2019s architecture\u2014projecting vision features into an aligned LLM and conditioning generation on the image\u2014provides the primary VLM baseline whose alignment is broken by pairing generic prompts with adversarially targeted images."
    },
    {
      "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
      "authors": "Deyao Zhu et al.",
      "year": 2023,
      "arxiv_id": "2304.10592",
      "role": "Baseline",
      "relationship_sentence": "MiniGPT-4\u2019s frozen-LLM + visual-encoder connector design is directly exploited by the attack, which manipulates only the vision side to steer the downstream LLM despite safety alignment."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "authors": "Andy Zou et al.",
      "year": 2023,
      "arxiv_id": "2307.15043",
      "role": "Gap Identification",
      "relationship_sentence": "This paper showed text-only jailbreaks via optimized suffixes but relies on direct LLM access and fails to exploit visual context, motivating a cross-modal attack that requires no LLM access."
    },
    {
      "title": "Adversarial manipulation of deep representations",
      "authors": "Sara Sabour et al.",
      "year": 2016,
      "arxiv_id": "1511.05122",
      "role": "Inspiration",
      "relationship_sentence": "The idea of crafting inputs by matching internal feature vectors inspires the paper\u2019s embedding-space image optimization that targets specific (toxic) text-aligned embeddings without querying the LLM."
    },
    {
      "title": "More than you\u2019ve asked for: A Comprehensive Analysis of Indirect Prompt Injection Attacks on LLMs",
      "authors": "Konrad Greschake et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "This work established that benign user prompts can be overridden by malicious context, directly inspiring the paper\u2019s compositional strategy of combining benign text with a malicious (adversarial) image as the controlling context."
    }
  ],
  "synthesis_narrative": "Cross-modal alignment via contrastive learning showed that images and text can share a joint semantic space; specifically, CLIP established that vision encoder outputs and text embeddings can be closely aligned, enabling images to effectively serve as context for language generation. LLaVA demonstrated a practical pipeline where projected visual features condition an aligned large language model during multi-modal dialogue, making downstream generation highly sensitive to the image embedding. MiniGPT-4 similarly integrated a frozen LLM with a visual encoder through a lightweight connector, highlighting a common design pattern where the vision side can steer the LLM without changing its parameters. Separately, jailbreak work on aligned LLMs revealed that targeted optimization can bypass safety with crafted prompts, but these methods typically assume text-only access and gradients from the LLM. Earlier research on feature-space adversaries established that one can manipulate inputs to match desired internal representations, suggesting a way to target embeddings rather than outputs. Finally, the prompt-injection literature showed that benign user instructions can be subverted by malicious external context, a compositional insight about control via conditioning. Taken together, these strands expose an opportunity: if images and text share a controllable embedding space and VLMs condition strongly on that space, then a feature-space attack on the vision encoder can implant \u201ctoxic\u201d semantics as context. By composing such adversarial images with innocuous prompts, one can override safety alignment in the LLM, while requiring no access to the LLM itself\u2014an inevitable next step given multimodal conditioning and indirect prompt-injection dynamics.",
  "target_paper": {
    "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
    "authors": "Erfan Shayegani, Yue Dong, Nael Abu-Ghazaleh",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Adversarial attacks, Vision encoders, Jailbreak, Prompt Injection, Security, Embedding space attacks, Black box, LLM, Vision-Language Models, Multi-Modal Models, VLM, Alignment, Cross-Modality alignment",
    "abstract": "We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders su",
    "openreview_id": "plmBsXHxgR",
    "forum_id": "plmBsXHxgR"
  },
  "analysis_timestamp": "2026-01-06T09:40:55.829096"
}
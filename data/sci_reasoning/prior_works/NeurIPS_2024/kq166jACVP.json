{
  "prior_works": [
    {
      "title": "Learning to summarize with human feedback",
      "authors": [
        "Nisan Stiennon",
        "et al."
      ],
      "year": 2020,
      "role": "Foundational preference modeling for alignment",
      "relationship_sentence": "Established pairwise preference data and reward modeling from preferred vs. dispreferred outputs, a supervision signal Aligner repurposes to learn residual corrections instead of a reward."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "et al."
      ],
      "year": 2022,
      "role": "RLHF pipeline baseline and motivation for efficiency",
      "relationship_sentence": "InstructGPT popularized RLHF for alignment but with costly RL and model-specific finetuning, motivating Aligner\u2019s model-agnostic, lightweight alternative that learns corrections without updating the base policy."
    },
    {
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "authors": [
        "Rafael Rafailov",
        "et al."
      ],
      "year": 2023,
      "role": "Simplified preference learning without RL",
      "relationship_sentence": "DPO showed that pairwise preference optimization can bypass RL, and Aligner similarly leverages preferred\u2013dispreferred pairs but trains a small corrective residual module rather than changing the base model\u2019s likelihoods."
    },
    {
      "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation",
      "authors": [
        "Sumanth Dathathri",
        "et al."
      ],
      "year": 2020,
      "role": "Model-agnostic, plug-in control of LMs",
      "relationship_sentence": "PPLM demonstrated attaching an auxiliary controller to steer generation without retraining the base LM, a plug-and-play design Aligner extends to preference-aligned correction applicable even to black-box APIs."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": [
        "Yuntao Bai",
        "et al."
      ],
      "year": 2022,
      "role": "Critique-and-revise paradigm and AI-labeled preferences",
      "relationship_sentence": "CAI formalized using AI feedback to critique and revise model outputs and to generate synthetic preference labels, directly inspiring Aligner\u2019s learn-to-correct mechanism and its bootstrapping of synthetic preference data."
    },
    {
      "title": "Self-Refine: Iterative Refinement with Self-Feedback",
      "authors": [
        "Aman Madaan",
        "et al."
      ],
      "year": 2023,
      "role": "Editing-based improvement of LM outputs",
      "relationship_sentence": "Self-Refine showed that iteratively editing initial generations improves alignment and quality, aligning with Aligner\u2019s idea of learning a residual editor and using corrected outputs to iteratively improve upstream models."
    }
  ],
  "synthesis_narrative": "Aligner\u2019s key contribution\u2014learning a small, model-agnostic module that corrects upstream model outputs using residuals derived from preferred vs. dispreferred answers\u2014sits at the intersection of preference learning, plug-and-play control, and edit-based refinement. Early preference work in summarization (Stiennon et al., 2020) and the RLHF pipeline (Ouyang et al., 2022) established the effectiveness of human preference signals but highlighted practical costs tied to reward models and RL fine-tuning. Direct Preference Optimization (Rafailov et al., 2023) demonstrated that pairwise preference learning can sidestep RL, shaping Aligner\u2019s decision to keep pairwise supervision while avoiding base model updates. On the control side, PPLM (Dathathri et al., 2020) provided a model-agnostic, plug-in approach to steer generation, a philosophy Aligner generalizes from attribute control to alignment-by-correction\u2014even for black-box APIs. Concurrently, Constitutional AI (Bai et al., 2022) framed alignment as critique-and-revise with AI feedback and synthetic preference labels, informing Aligner\u2019s correctional framing and its ability to bootstrap new preference data from corrected outputs. Finally, Self-Refine (Madaan et al., 2023) showed the power of iterative editing, which Aligner operationalizes as learning residual corrections that can be repeatedly applied and used to iteratively improve upstream models. Together, these strands crystallize in Aligner\u2019s simple, efficient correction module trained once and deployed broadly for rapid alignment iteration.",
  "analysis_timestamp": "2026-01-06T23:39:42.950694"
}
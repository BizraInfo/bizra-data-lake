{
  "prior_works": [
    {
      "title": "Curiosity-driven Exploration by Self-supervised Prediction",
      "authors": "Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell",
      "year": 2017,
      "role": "Latent-space discrepancy bonus",
      "relationship_sentence": "ICM established measuring novelty via L2 prediction error between adjacent states in a learned latent space, the exact class of L1/L2 discrepancy bonuses EME critiques and rethinks."
    },
    {
      "title": "Rewarding Impact-Driven Exploration for Procedurally-Generated Environments (RIDE)",
      "authors": "Roberta Raileanu, Tim Rockt\u00e4schel",
      "year": 2020,
      "role": "Adjacency-based feature change with episodic scaling",
      "relationship_sentence": "RIDE uses the norm of successive feature differences scaled by episodic visit counts, a design EME targets by removing count-based scaling and proposing a more principled metric."
    },
    {
      "title": "Never Give Up: Learning Directed Exploration Strategies",
      "authors": "Adri\u00e0 Puigdom\u00e8nech Badia, Pablo Sprechmann, Alexander Vitvitskyi, et al.",
      "year": 2020,
      "role": "Episodic novelty/density mechanisms",
      "relationship_sentence": "NGU popularized episodic novelty estimators that improve hard-exploration but add memory/count-like machinery; EME argues for a scalable metric-based alternative that avoids episodic count scaling."
    },
    {
      "title": "Unifying Count-Based Exploration and Intrinsic Motivation",
      "authors": "Marc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, R\u00e9mi Munos",
      "year": 2016,
      "role": "Foundations of count/pseudocount bonuses",
      "relationship_sentence": "This work formalized pseudocount bonuses that inspired later episodic count scalers; EME positions its metric bonus as eliminating dependence on such count-based terms for scalability."
    },
    {
      "title": "Count-Based Exploration with Neural Density Models",
      "authors": "Georg Ostrovski, Marc G. Bellemare, Aaron van den Oord, R\u00e9mi Munos",
      "year": 2017,
      "role": "Neural density-based novelty estimation",
      "relationship_sentence": "By approximating state novelty via neural densities (a learned count surrogate), this paper exemplifies the count-based lineage that EME seeks to replace with an effective metric-based bonus."
    },
    {
      "title": "Bisimulation Metrics for Continuous Markov Decision Processes",
      "authors": "Norman Ferns, Prakash Panangaden, Doina Precup",
      "year": 2004,
      "role": "Theoretical foundation for dynamics-aware state metrics",
      "relationship_sentence": "Ferns et al. defined bisimulation metrics as principled distances reflecting transition and reward equivalence, informing EME\u2019s aim to use dynamics-aware metrics instead of naive L1/L2 norms."
    },
    {
      "title": "DeepMDP: Learning Continuous Latent Space Models for Representation Learning in Reinforcement Learning",
      "authors": "Carles Gelada et al.",
      "year": 2019,
      "role": "Practical bisimulation-inspired representation learning",
      "relationship_sentence": "DeepMDP operationalized bisimulation-style objectives for learned representations, highlighting both the promise and the approximation pitfalls that EME addresses for robust exploration bonuses."
    }
  ],
  "synthesis_narrative": "EME\u2019s core contribution is to rethink how novelty is measured for intrinsic rewards by focusing on an effective metric over adjacent states that is both dynamics-aware and scalable without episodic count scaling. This builds directly on two major lines of prior work. First, latent-space discrepancy methods such as ICM introduced measuring novelty via L1/L2 distances or prediction errors between successive states, and RIDE refined this by rewarding feature change while tempering it with episodic counts to avoid dithering. NGU and neural density approaches like Ostrovski et al. extended the count-based perspective with episodic and density-based novelty, proving effective on hard-exploration tasks but incurring significant memory/compute overhead and scaling issues. EME explicitly targets these limitations by removing reliance on episodic counts and proposing a more principled metric-based bonus.\nThe second line is bisimulation theory and practice. Ferns, Panangaden, and Precup established bisimulation metrics as the right notion of state similarity that respects transitions and rewards, suggesting a theoretically grounded metric for exploration. Practical deep variants (e.g., DeepMDP) attempted to learn such representations, but approximation gaps often reduced effectiveness in challenging domains. EME leverages the bisimulation intuition\u2014novelty should reflect dynamics and rewards\u2014while addressing the practical shortcomings of prior approximations. By unifying the strengths of discrepancy-based bonuses and bisimulation-aware representation learning, and by discarding episodic count scaling, EME delivers a scalable, effective exploration bonus tailored to hard-exploration settings.",
  "analysis_timestamp": "2026-01-06T23:33:35.564044"
}
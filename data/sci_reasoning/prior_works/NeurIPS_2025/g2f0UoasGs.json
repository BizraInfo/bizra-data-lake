{
  "prior_works": [
    {
      "title": "Feature-based Dynamic Pricing",
      "authors": "Maxime Cohen, Ilan Lobel, Renato Paes Leme",
      "year": 2016,
      "role": "Foundational contextual dynamic pricing",
      "relationship_sentence": "Established the contextual dynamic pricing paradigm and GLM/linear valuation formulations, providing the baseline structure and regret targets that CM-TDP extends to the transfer (multi-market) setting."
    },
    {
      "title": "Improved Algorithms for Linear Stochastic Bandits (OFUL)",
      "authors": "Yasin Abbasi-Yadkori, D\u00e1vid P\u00e1l, Csaba Szepesv\u00e1ri",
      "year": 2011,
      "role": "Confidence-set machinery for linear models",
      "relationship_sentence": "Supplies the confidence-bound and self-normalized concentration tools that underlie the linear part of CM-TDP\u2019s analysis and its logarithmic-in-T pricing regret."
    },
    {
      "title": "Online Decision-Making with High-Dimensional Covariates",
      "authors": "Hamsa Bastani, Mohammad Mahdian Bayati",
      "year": 2020,
      "role": "Sparsity in bandits/high-dimensional pricing",
      "relationship_sentence": "Introduces sparsity-aware estimation (Lasso-bandit) techniques showing how sparse structure sharpens regret, directly informing CM-TDP\u2019s use of s0-sparse preference-shift to get (d/K + s0) log T rates."
    },
    {
      "title": "A Dirty Model for Multi-Task Learning",
      "authors": "Ali Jalali, Pradeep Ravikumar, Sujay Sanghavi, Chao Ruan",
      "year": 2010,
      "role": "Modeling shared parameters with sparse task-specific deviations",
      "relationship_sentence": "Provides the shared-plus-sparse-difference decomposition that motivates CM-TDP\u2019s cross-market preference-shift assumption and estimation strategy across auxiliary and target markets."
    },
    {
      "title": "Finite-time Analysis of Kernelised Contextual Bandits",
      "authors": "Michal Valko, Nathaniel Korda, R\u00e9mi Munos, Csaba Szepesv\u00e1ri",
      "year": 2013,
      "role": "RKHS bandit algorithms and analysis",
      "relationship_sentence": "Develops RKHS-based UCB methods and concentration tools that CM-TDP adapts to nonparametric utility models to obtain kernelized pricing regret guarantees."
    },
    {
      "title": "Lower Bounds on Regret for Gaussian Process Bandit Optimization",
      "authors": "Jonathan Scarlett, Ilija Bogunovic, Volkan Cevher",
      "year": 2017,
      "role": "Information-theoretic lower bounds in RKHS bandits",
      "relationship_sentence": "Provides minimax lower-bound techniques in kernelized settings that CM-TDP leverages to match rates (up to logs) for its nonparametric transfer-pricing regret."
    },
    {
      "title": "Provable Meta-Learning of Linear Representations",
      "authors": "Praneeth Netrapalli Tripuraneni, Michael I. Jordan, Chi Jin",
      "year": 2020,
      "role": "Transfer/meta-learning across tasks for linear bandits",
      "relationship_sentence": "Shows how pooling data across related tasks yields improved sample complexity via shared representations, directly inspiring CM-TDP\u2019s K-dependent gains under cross-market similarity."
    }
  ],
  "synthesis_narrative": "CM-TDP\u2019s core advance\u2014minimax-optimal transfer for contextual dynamic pricing under cross-market preference shift\u2014sits at the intersection of contextual pricing, bandit transfer, sparsity, and RKHS nonparametrics. Feature-based Dynamic Pricing (Cohen\u2013Lobel\u2013Paes Leme) formalized contextual pricing and regret objectives that CM-TDP seeks to improve by borrowing strength across markets. On the algorithmic backbone, OFUL (Abbasi-Yadkori et al.) provides confidence-set, self-normalized analysis tools for linear models, which CM-TDP adapts to pricing with auxiliary data. The linear-rate refinement comes from sparsity: Bastani\u2013Bayati showed how Lasso-style structure sharpens regret in high dimensions; CM-TDP leverages a sparse difference between target and sources to decompose regret into a pooled d/K term plus an adaptation cost s0, yielding (d/K + s0) log T.\n\nTo model heterogeneity across markets, the Dirty Model (Jalali et al.) motivates a shared component with sparse task-specific deviations, mirroring CM-TDP\u2019s preference-shift assumption and estimators that transfer while guarding against misspecification. Beyond linearity, KernelUCB (Valko et al.) supplies RKHS bandit machinery for nonparametric utilities; CM-TDP extends this to a multi-source setting with explicit task-similarity (H) and effective-dimension/complexity (\u03b1, \u03b2), attaining rates that capture both transfer and exploration. Finally, information-theoretic lower bounds for GP/RKHS bandits (Scarlett et al.) underpin CM-TDP\u2019s minimax claims by certifying optimal dependence on T and kernel complexity. Complementing these, meta-learning of linear representations (Tripuraneni\u2013Jin\u2013Jordan) demonstrates how shared structure across tasks yields K-dependent gains, a principle CM-TDP operationalizes for pricing under structured preference shifts.",
  "analysis_timestamp": "2026-01-07T00:02:04.974841"
}
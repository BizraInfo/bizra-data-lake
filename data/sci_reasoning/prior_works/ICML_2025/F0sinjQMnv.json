{
  "prior_works": [
    {
      "title": "Causal Inference Using the Algorithmic Markov Condition",
      "authors": "Janzing and Sch\u00f6lkopf",
      "year": 2010,
      "role": "Foundation",
      "relationship_sentence": "This paper articulated the AMC principle that the correct causal factorization yields a shorter description length, which our method directly operationalizes using variational Bayesian neural-network code lengths."
    },
    {
      "title": "Causal Inference by Compression",
      "authors": "Budhathoki and Vreeken",
      "year": 2017,
      "role": "Baseline",
      "relationship_sentence": "They implemented the AMC/MDL idea for pairwise causal direction using simple compressive models, and our work addresses their limited model expressiveness by replacing simple function classes with variational Bayesian neural networks."
    },
    {
      "title": "Inference of Cause and Effect with Gaussian Process Models",
      "authors": "Sgouritsa et al.",
      "year": 2015,
      "role": "Baseline",
      "relationship_sentence": "This GP-based approach compares directions via marginal likelihood (an MDL/Occam code), and our method replaces the GP Occam code with a variational Bayesian neural-network codelength to improve expressiveness while avoiding GP-level computational cost."
    },
    {
      "title": "Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights",
      "authors": "Hinton and van Camp",
      "year": 1993,
      "role": "Inspiration",
      "relationship_sentence": "They established that variational Bayesian learning corresponds to minimizing codelength, directly inspiring our use of variational Bayesian neural networks as a codelength proxy for causal factorization."
    },
    {
      "title": "Weight Uncertainty in Neural Networks",
      "authors": "Blundell et al.",
      "year": 2015,
      "role": "Extension",
      "relationship_sentence": "Bayes-by-Backprop provides a scalable variational objective with a bits-back codelength interpretation; we adapt this machinery to compute directional code lengths for causal scoring."
    },
    {
      "title": "Nonlinear causal discovery with additive noise models",
      "authors": "Hoyer et al.",
      "year": 2009,
      "role": "Related Problem",
      "relationship_sentence": "This work formalized pairwise cause-effect inference via model comparison across directions, and our approach inherits the pairwise setting while replacing functional fit tests with MDL/VB-based codelength comparison."
    },
    {
      "title": "Distinguishing cause from effect using observational data: methods and benchmarks",
      "authors": "Mooij et al.",
      "year": 2016,
      "role": "Gap Identification",
      "relationship_sentence": "By systematizing the pairwise cause\u2013effect problem and highlighting trade-offs between model fit and computational complexity, this work motivates our aim to achieve higher fidelity than simple MDL models without the heavy cost of GP approaches."
    }
  ],
  "synthesis_narrative": "The core idea of this paper\u2014deciding causal direction by comparing codelengths of the two causal factorizations\u2014directly descends from the algorithmic Markov condition (AMC) of Janzing and Sch\u00f6lkopf, which posits that the true causal direction admits a shorter description. Early operationalizations of AMC via MDL/compression, most prominently Budhathoki and Vreeken\u2019s Causal Inference by Compression, demonstrated practical value but relied on relatively simple function classes, creating a fit-versus-simplicity trade-off. In parallel, Sgouritsa et al. instantiated the same principle with Gaussian process marginal likelihoods, capturing richer functions yet incurring substantial computational cost and scaling issues. The present work\u2019s key innovation is to replace these MDL proxies with the variational Bayesian codelength of neural networks, thereby preserving an Occam-penalized objective while substantially improving expressiveness and efficiency. This move is directly enabled by the coding interpretation of variational Bayesian learning: Hinton and van Camp showed that variational training equates to minimizing description length, and Blundell et al. provided a scalable Bayes-by-Backprop objective with a bits-back codelength view. The pairwise cause-effect problem setting and empirical expectations were shaped by Hoyer et al.\u2019s additive-noise model framework and Mooij et al.\u2019s benchmarking, which also exposed the limitations our method targets. In sum, the paper fuses AMC/MDL causal scoring with the variational coding view of Bayesian neural networks to overcome the expressiveness and computational gaps of prior simple-model and GP-based approaches.",
  "analysis_timestamp": "2026-01-06T23:07:19.587612"
}
{
  "prior_works": [
    {
      "title": "Large Language Models Still Can\u2019t Plan",
      "authors": "Karthik Valmeekam, Mudit Verma, Siddhant Bhambri, Subbarao Kambhampati",
      "year": 2023,
      "role": "Empirical foundation and motivation",
      "relationship_sentence": "Provided rigorous evidence and analyses that contemporary autoregressive LLMs fail on even simple model-based planning tasks, directly motivating the paper\u2019s position that LLMs cannot, by themselves, plan or self-verify."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "Interaction pattern for bi-directional loops",
      "relationship_sentence": "Demonstrated that interleaving LLM reasoning with environment feedback improves performance, inspiring the paper\u2019s tighter, bi-directional LLM\u2013verifier interaction rather than one-way pipelining."
    },
    {
      "title": "PAL: Program-Aided Language Models",
      "authors": "Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Alexander Rush, Yiming Yang",
      "year": 2022,
      "role": "External execution/verifier template",
      "relationship_sentence": "Showed that delegating computation to external program executors can ensure correctness beyond the LLM\u2019s internal reasoning, directly informing the LLM-modulo idea of pairing LLMs with model-based verifiers."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "LLM-guided search with external evaluation",
      "relationship_sentence": "Positioned LLMs as generators of candidate thoughts combined with explicit search/control, reinforcing the paper\u2019s claim that external algorithmic structure is needed for reliable reasoning/planning."
    },
    {
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan)",
      "authors": "Michael Ahn et al.",
      "year": 2022,
      "role": "Model-based scoring to filter LLM proposals",
      "relationship_sentence": "Used value functions to evaluate and select LLM-suggested actions, exemplifying how external model-based verifiers can constrain LLM outputs\u2014a core mechanism in the proposed LLM-Modulo framework."
    },
    {
      "title": "VAL: A Plan Validation Tool for PDDL",
      "authors": "Amanda Howey, Derek Long, Maria Fox",
      "year": 2004,
      "role": "Canonical model-based verifier in planning",
      "relationship_sentence": "Established automated, explicit model-based plan validation, providing the kind of external verifier the paper advocates integrating tightly with LLMs."
    },
    {
      "title": "The DPLL(T) Framework for Satisfiability Modulo Theories",
      "authors": "Roberto Nieuwenhuis, Albert Oliveras, Cesare Tinelli",
      "year": 2006,
      "role": "Conceptual blueprint for \u201cX-modulo-Y\u201d architectures",
      "relationship_sentence": "Supplied the architectural analogy\u2014separating a general search engine from theory-specific solvers\u2014that directly inspires the paper\u2019s LLM-Modulo framing of coupling LLMs with model-based verifiers."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014a principled LLM-Modulo framework\u2014rests on two pillars: (i) that autoregressive LLMs lack the algorithmic machinery for planning and self-verification, and (ii) that their strengths can be harnessed by coupling them with explicit, model-based verifiers in tight, bi-directional loops. Valmeekam et al. (2023) provide the immediate empirical impetus by documenting systematic planning failures, clarifying why scaling or prompt engineering alone is insufficient. A sequence of tool-use and interaction works\u2014PAL and ReAct\u2014demonstrate that reliability emerges when LLMs defer to external executors or incorporate feedback, foreshadowing the need for structured interfaces with verifiers rather than stand-alone generation. Tree of Thoughts further recasts LLMs as proposal/heuristic generators embedded in explicit search, reinforcing the argument that external control is essential for complex reasoning. In robotics, SayCan exemplifies model-based scoring of LLM proposals, concretely showing how value functions can filter and ground language suggestions. On the symbolic planning side, VAL anchors the notion of plan validation against explicit domain models, representing the very verifier class the paper seeks to integrate. Finally, the DPLL(T) paradigm supplies the architectural metaphor: a general-purpose engine coordinating with specialized theory solvers. The LLM-Modulo framework synthesizes these strands, elevating LLMs to universal approximate knowledge sources that assist in model acquisition and proposal generation, while delegating correctness and search control to formal, model-based verifiers in a tightly coupled loop.",
  "analysis_timestamp": "2026-01-07T00:02:04.895618"
}
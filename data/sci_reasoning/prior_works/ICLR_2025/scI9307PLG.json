{
  "prior_works": [
    {
      "title": "Neural Sheaf Diffusion: A Topological Perspective on Graphs",
      "authors": "C. Bodnar et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "BuNN\u2019s discrete update reduces to neural sheaf diffusion when the bundle structure is realized as a sheaf with a sheaf Laplacian, making SNN the primary discrete instantiation that BuNN generalizes to a continuous diffusion on flat vector bundles."
    },
    {
      "title": "Laplacians on Cellular Sheaves",
      "authors": "J. Hansen and R. Ghrist",
      "year": 2019,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work formalized the sheaf Laplacian and diffusion on cellular sheaves, providing the spectral operator that BuNN instantiates on flat vector bundles as the generator of its diffusion dynamics."
    },
    {
      "title": "Vector Diffusion Maps and the Connection Laplacian",
      "authors": "A. Singer and H.-T. Wu",
      "year": 2012,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "By introducing orthogonal parallel transport on graphs and the connection Laplacian, this paper supplied the exact geometric mechanism\u2014flat connections with edge-wise orthogonal maps\u2014that BuNN leverages to define bundle-valued message diffusion."
    },
    {
      "title": "GRAND: Graph Neural Diffusion",
      "authors": "B. Chamberlain et al.",
      "year": 2021,
      "arxiv_id": "2106.10934",
      "role": "Inspiration",
      "relationship_sentence": "GRAND established continuous-time diffusion as an effective alternative to discrete message passing, directly inspiring BuNN\u2019s use of a diffusion-type PDE and motivating its scalability and anti-squashing design."
    },
    {
      "title": "On the Bottleneck of Graph Neural Networks and its Practical Alleviation",
      "authors": "U. Alon and E. Yahav",
      "year": 2021,
      "arxiv_id": "2006.05205",
      "role": "Gap Identification",
      "relationship_sentence": "By pinpointing over-squashing as an inherent limitation of local message passing, this paper motivated BuNN\u2019s continuous diffusion over bundle connections as a mechanism to improve long-range information flow."
    },
    {
      "title": "Graph Neural Networks Exponentially Lose Expressive Power as Depth Increases",
      "authors": "T. Oono and T. Suzuki",
      "year": 2020,
      "arxiv_id": "1905.10947",
      "role": "Gap Identification",
      "relationship_sentence": "The formalization of over-smoothing in deep MPNNs in this work directly motivated BuNN\u2019s use of orthogonal transports and diffusion generators designed to preserve feature geometry and mitigate smoothing."
    },
    {
      "title": "Understanding oversquashing and bottlenecks on graphs",
      "authors": "J. Topping et al.",
      "year": 2022,
      "arxiv_id": "2111.14522",
      "role": "Related Problem",
      "relationship_sentence": "By relating oversquashing to geometric bottlenecks and proposing curvature-based remedies, this paper informed BuNN\u2019s design choice to use continuous diffusion on structured transports as an alternative route to alleviate squashing without rewiring."
    }
  ],
  "synthesis_narrative": "Neural sheaf diffusion established a learnable diffusion operator on sheaves, showing how edge-wise linear maps can align local feature spaces and be propagated via a sheaf Laplacian. The foundational mathematics for this came from the spectral theory of cellular sheaves, which defined the sheaf Laplacian and its diffusion semantics on graphs. Earlier still, vector diffusion maps introduced the connection Laplacian and orthogonal parallel transport on graphs, demonstrating how bundle-like structures with edge-wise orthogonal maps support coherent diffusion of vector-valued data. In parallel, GRAND showed that replacing discrete message passing with continuous-time diffusion improves stability and scalability, offering a dynamical-systems perspective for information propagation on graphs. Complementing these operator and dynamics advances, the limitations of message passing were crystallized by works exposing over-squashing due to graph bottlenecks and over-smoothing from depth, with further analysis linking squashing to geometric constraints and motivating structured remedies. Together, these strands exposed a gap: sheaf-based alignment affords expressive local transports, while continuous diffusion improves global propagation, yet they had not been unified. Building on sheaf/connection Laplacians for structured transport and adopting continuous diffusion dynamics as in GRAND, the present work formulates message diffusion on flat vector bundles, yielding a continuous generator with orthogonal transports that preserves feature geometry, recovers sheaf diffusion in discrete form, and is expressly targeted at mitigating smoothing and squashing while enabling universality under injective positional encodings.",
  "target_paper": {
    "title": "Bundle Neural Network for message diffusion on graphs",
    "authors": "Jacob Bamberger, Federico Barbero, Xiaowen Dong, Michael M. Bronstein",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "graph neural network, sheaf neural network, geometric deep learning, algebraic topology, vector bundles, expressivity",
    "abstract": "The dominant paradigm for learning on graphs is message passing. Despite being a strong inductive bias, the local message passing mechanism faces challenges such as over-smoothing, over-squashing, and limited expressivity. To address these issues, we introduce Bundle Neural Networks (BuNNs), a novel graph neural network architecture that operates via *message diffusion* on *flat vector bundles* \u2014 geometrically inspired structures that assign to each node a vector space and an orthogonal map. A BuNN layer evolves node features through a diffusion-type partial differential equation, where its discrete form acts as a special case of the recently introduced Sheaf Neural Network (SNN), effectively alleviating over-smoothing. The continuous nature of message diffusion enables BuNNs to operate at larger scales, reducing over-squashing. We establish the universality of BuNNs in approximating feature transformations on infinite families of graphs with injective positional encodings, marking the",
    "openreview_id": "scI9307PLG",
    "forum_id": "scI9307PLG"
  },
  "analysis_timestamp": "2026-01-06T17:23:45.229407"
}
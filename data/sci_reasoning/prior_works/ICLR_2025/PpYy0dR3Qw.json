{
  "prior_works": [
    {
      "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data (FedAvg)",
      "authors": "H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Aguera y Arcas",
      "year": 2017,
      "role": "Origin of local training / periodic averaging",
      "relationship_sentence": "LoCoDL\u2019s use of multiple local steps between communications builds directly on FedAvg\u2019s periodic averaging paradigm, which established local training as a primary lever for reducing communication."
    },
    {
      "title": "Local SGD Converges Fast and Communicates Little",
      "authors": "Sebastian U. Stich",
      "year": 2019,
      "role": "Theoretical foundation for local updates",
      "relationship_sentence": "LoCoDL\u2019s provable benefits from local training leverage the convergence analyses of Local SGD, which quantify how infrequent synchronization can still achieve strong rates."
    },
    {
      "title": "QSGD: Communication-Efficient Stochastic Gradient Descent via Gradient Quantization and Encoding",
      "authors": "Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, Milan Vojnovic",
      "year": 2017,
      "role": "Unbiased quantization and compressor framework",
      "relationship_sentence": "LoCoDL\u2019s compressor class and dimension-dependent variance bounds trace to QSGD\u2019s unbiased quantization framework, which formalized how quantizers induce variance that scales with model dimension."
    },
    {
      "title": "FedPAQ: A Communication-Efficient Federated Learning Method with Periodic Averaging and Quantization",
      "authors": "Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, Ramtin Pedarsani",
      "year": 2020,
      "role": "Early integration of local training with compression",
      "relationship_sentence": "LoCoDL advances the line opened by FedPAQ\u2014combining local steps with quantized communication\u2014by handling a broad class of unbiased compressors and delivering stronger, accelerated complexity guarantees."
    },
    {
      "title": "Federated Optimization in Heterogeneous Networks (FedProx)",
      "authors": "Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, Virginia Smith",
      "year": 2020,
      "role": "Modeling and analysis under client heterogeneity",
      "relationship_sentence": "LoCoDL\u2019s analysis in the heterogeneous regime is informed by FedProx\u2019s formal treatment of non-identical client objectives, ensuring robustness of local training under heterogeneity."
    },
    {
      "title": "MARINA: Faster Non-Convex Distributed Learning with Compression",
      "authors": "Eduard Gorbunov, Dmitry Kovalev, Konstantin Mishchenko, Peter Richt\u00e1rik",
      "year": 2021,
      "role": "State-of-the-art theory for compressed communication",
      "relationship_sentence": "LoCoDL\u2019s accelerated communication complexity with unbiased compressors builds on MARINA\u2019s refined analysis of compressed optimization, extending it to incorporate local training and achieve doubly-accelerated dependence on condition number and dimension."
    }
  ],
  "synthesis_narrative": "LoCoDL\u2019s core innovation\u2014provably communication-efficient distributed learning that simultaneously exploits local training and unbiased compression with accelerated complexity\u2014emerges from the convergence of two lines of work. On the local-update side, FedAvg established periodic averaging as a practical means to reduce communication, while Local SGD theory quantified when and how such infrequent synchronization remains efficient. On the compression side, QSGD formalized unbiased quantization and its dimension-dependent variance, seeding a compressor framework that captures both sparsification and quantization operators. MARINA subsequently sharpened the theory of compressed distributed optimization, showing how carefully designed compressed updates can achieve fast rates with rigorous dependence on compressor parameters.\nBridging these strands, FedPAQ provided an early demonstration that local updates and quantized communication can be combined, though with more limited theory. At the same time, realistic federated regimes require robustness to client heterogeneity, as modeled in FedProx; LoCoDL targets this challenging setting for strongly convex objectives. Building on these foundations, LoCoDL unifies local training with a broad class of unbiased compressors and delivers doubly-accelerated communication complexity\u2014improving dependence on both the condition number and model dimension. In doing so, it extends prior results that treated local updates or compression in isolation, and strengthens earlier combined approaches by providing general, provable guarantees under heterogeneity while achieving superior empirical performance.",
  "analysis_timestamp": "2026-01-06T23:42:48.098929"
}
{
  "prior_works": [
    {
      "title": "Optimization and Nonsmooth Analysis",
      "authors": "F. H. Clarke",
      "year": 1983,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This monograph defines the Clarke subdifferential and establishes chain rules for Lipschitz compositions, providing the generalized derivative notion the paper proves AD selects in ReLU/maxpool networks."
    },
    {
      "title": "Variational Analysis",
      "authors": "R. T. Rockafellar and R. J.-B. Wets",
      "year": 1998,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Its calculus for Clarke subdifferentials\u2014especially for sums and pointwise maxima\u2014underpins the analysis of minibatch aggregation and maxpool, including the strict-inclusion phenomena (\u2202C(f+g) \u228a \u2202Cf + \u2202Cg) that motivate counterexamples and sufficient conditions."
    },
    {
      "title": "Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation (2nd ed.)",
      "authors": "Andreas Griewank and Andrea Walther",
      "year": 2008,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "This book explicitly notes that AD on programs with branches or non-differentiable primitives can return arbitrary values, directly motivating a precise characterization of what AD actually computes in nonsmooth neural networks."
    },
    {
      "title": "On the number of linear regions of deep neural networks",
      "authors": "Guido F. Mont\u00fafar et al.",
      "year": 2014,
      "arxiv_id": "1402.1869",
      "role": "Foundation",
      "relationship_sentence": "By formalizing ReLU networks as piecewise-linear, locally Lipschitz maps with region-dependent affine representations, this work provides the structural premise used to link backprop\u2019s output to Clarke subgradients."
    },
    {
      "title": "Clarke subgradients of stratifiable functions",
      "authors": "Jerome Bolte, Aris Daniilidis, Adrian Lewis, and Masahiro Shiota",
      "year": 2007,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Their subdifferential calculus for stratifiable (e.g., semi-algebraic) functions supplies the chain-rule machinery that justifies propagating Clarke selections through layered ReLU/max compositions."
    },
    {
      "title": "Complexity of linear regions of deep neural networks",
      "authors": "Boris Hanin and David Rolnick",
      "year": 2019,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Their generic-position arguments (e.g., distinct biases preventing ties) directly motivate the paper\u2019s distinct-bias assumption ensuring unique active sets so AD matches a valid Clarke selection."
    },
    {
      "title": "A Spline Theory of Deep Learning",
      "authors": "Anthony Balestriero and Richard G. Baraniuk",
      "year": 2021,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "Casting ReLU/maxpool networks as max-affine spline operators provides explicit active-affine and argmax descriptions that inform the paper\u2019s treatment of maxpool layers and tie cases in the AD\u2013Clarke correspondence."
    }
  ],
  "synthesis_narrative": "Clarke introduced the generalized gradient for locally Lipschitz functions along with a chain rule for compositions, establishing the Clarke subdifferential calculus that enables principled reasoning about nonsmooth derivatives. Rockafellar and Wets systematized this calculus, detailing how Clarke subdifferentials behave under sums and pointwise maxima, including when inclusions are strict\u2014facts crucial for analyzing minibatch aggregation and max-type primitives. Griewank and Walther\u2019s treatment of algorithmic differentiation emphasized that programs with branches and nonsmooth primitives can cause AD to return arbitrary values, highlighting a gap in understanding AD\u2019s semantics beyond smooth settings. Mont\u00fafar and colleagues showed that ReLU networks are piecewise-linear and locally Lipschitz, i.e., stratified by affine regions, which is precisely the structural context where Clarke calculus applies. Bolte, Daniilidis, Lewis, and Shiota developed subdifferential rules for stratifiable functions, providing the formal machinery to propagate Clarke selections through layered compositions typical of neural networks. Hanin and Rolnick\u2019s generic-position results underscore that distinct biases preclude ties and yield stable active sets. Finally, Balestriero and Baraniuk\u2019s spline viewpoint renders ReLU/maxpool networks as max-affine splines, making explicit the role of argmax sets and tie handling. Taken together, these works expose a natural opportunity: characterize the precise Clarke selection that AD computes on stratified, piecewise-linear networks, and identify when sum- and max-operations (minibatching, maxpool) can break this identification. Synthesizing Clarke calculus with structural properties of ReLU networks and AD\u2019s branch-wise behavior leads to sharp conditions\u2014such as single-sample batches and distinct biases\u2014under which backprop equals a Clarke subderivative, along with counterexamples and sufficient conditions beyond this regime.",
  "target_paper": {
    "title": "What does automatic differentiation compute for neural networks?",
    "authors": "Sejun Park, Sanghyuk Chun, Wonyeol Lee",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "automatic differentiation, correctness, neural networks, clarke subdifferential",
    "abstract": "Forward- or reverse-mode automatic differentiation (AD) is a popular algorithm for computing the derivative of a function expressed by a program. AD always outputs the correct derivative if a program does not use any non-differentiable functions and control flows; however, it may return an arbitrary value otherwise. In this work, we investigate what AD computes for neural networks that may contain non-differentiable functions such as ReLU and maxpools. We first prove that AD always returns a generalized derivative called a Clarke subderivative for networks with pointwise activation functions, if the minibatch size is one and all non-differentiable neurons have distinct bias parameters. We show that the same conclusion does not hold otherwise, but does hold under some mild sufficient conditions. We also prove similar results for more general networks that can use maxpools and bias parameters shared across different neurons. We empirically check our sufficient conditions over popular net",
    "openreview_id": "8vKknbgXxf",
    "forum_id": "8vKknbgXxf"
  },
  "analysis_timestamp": "2026-01-06T19:32:56.095025"
}
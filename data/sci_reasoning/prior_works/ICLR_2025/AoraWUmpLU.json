{
  "prior_works": [
    {
      "title": "Neural Ordinary Differential Equations",
      "authors": "Ricky T. Q. Chen et al.",
      "year": 2018,
      "arxiv_id": "1806.07366",
      "role": "Foundation",
      "relationship_sentence": "This work introduced the Neural ODE formulation and the continuous-time adjoint method, directly motivating the need to ensure well-posed forward and backward ODEs during training that this paper secures via activation smoothness."
    },
    {
      "title": "ANODE: Unconditionally Accurate Memory-Efficient Gradients for Neural ODEs",
      "authors": "Amir Gholami et al.",
      "year": 2019,
      "arxiv_id": "1902.10298",
      "role": "Gap Identification",
      "relationship_sentence": "By showing that the continuous adjoint can yield inaccurate gradients when dynamics are non-invertible or non-smooth, this paper exposed a concrete failure mode that the present work addresses by imposing smooth activations to guarantee globally unique forward/backward ODEs."
    },
    {
      "title": "Dissecting Neural ODEs",
      "authors": "Stefano Massaroli et al.",
      "year": 2020,
      "arxiv_id": "2002.08071",
      "role": "Gap Identification",
      "relationship_sentence": "This analysis highlighted well-posedness, stiffness, and gradient pathologies in Neural ODEs, motivating the present activation-based conditions that ensure stable dynamics and reliable gradient computation."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot et al.",
      "year": 2018,
      "arxiv_id": "1806.07572",
      "role": "Foundation",
      "relationship_sentence": "The NTK framework underpins the paper\u2019s convergence analysis by linking gradient descent dynamics to a kernel whose spectral properties must be preserved during training."
    },
    {
      "title": "Gradient Descent Finds Global Minima of Over-parameterized Neural Networks",
      "authors": "Simon S. Du et al.",
      "year": 2019,
      "arxiv_id": "1811.03804",
      "role": "Inspiration",
      "relationship_sentence": "Their proof that global convergence follows when the NTK Gram matrix has a uniform spectral lower bound\u2014achieved under sufficiently nonlinear (non-polynomial) activations\u2014inspires this work\u2019s requirement on activation nonlinearity to maintain NTK spectrum for Neural ODEs."
    },
    {
      "title": "A Convergence Theory for Deep Learning via Over-Parameterization",
      "authors": "Zeyuan Allen-Zhu et al.",
      "year": 2019,
      "arxiv_id": "1811.03962",
      "role": "Extension",
      "relationship_sentence": "Their analysis showing that wide networks converge because the NTK stays close to initialization is directly extended here to the continuous-depth setting by enforcing activation-driven conditions that keep the NTK\u2019s spectrum stable during training."
    }
  ],
  "synthesis_narrative": "Neural Ordinary Differential Equations introduced continuous-depth parameterizations and the adjoint method for training, making the correctness of both forward and backward ODE solutions central to learning dynamics. Subsequent work demonstrated that, in practice, the continuous adjoint can produce inaccurate gradients when the flow is not smooth or reversible, and exposed broader well-posedness and gradient pathologies in Neural ODEs, highlighting the need for structural conditions that ensure stable dynamics. In parallel, the Neural Tangent Kernel (NTK) offered a precise view of gradient descent in overparameterized networks, reducing training to kernel regression whose success depends on maintaining a favorable kernel spectrum. Global convergence results then established that if the NTK Gram matrix has a uniform spectral lower bound\u2014often guaranteed by sufficiently nonlinear (non-polynomial) activations\u2014and if the kernel remains near its initialization during training, gradient descent reaches global minima efficiently.\nTogether, these insights suggested a path for continuous-depth models: ensure the ODE vector field is smooth enough to guarantee unique forward and adjoint trajectories, while enforcing activation nonlinearity to secure and preserve the NTK\u2019s spectral properties. The present work synthesizes these strands by tying smooth activations to global well-posedness of the forward/backward flows and tying sufficient nonlinearity to the NTK spectrum\u2019s stability in the overparameterized regime, thereby extending finite-depth NTK convergence theory to Neural ODEs and closing the training-accuracy gap identified by prior analyses.",
  "target_paper": {
    "title": "Global Convergence in Neural ODEs: Impact of Activation Functions",
    "authors": "Tianxiang Gao, Siyuan Sun, Hailiang Liu, Hongyang Gao",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Neural ODEs, Gradient Descent, Neural Tangent Kernel (NTK)",
    "abstract": "Neural Ordinary Differential Equations (ODEs) have been successful in various applications due to their continuous nature and parameter-sharing efficiency. However, these unique characteristics also introduce challenges in training, particularly with respect to gradient computation accuracy and convergence analysis. In this paper, we address these challenges by investigating the impact of activation functions. We demonstrate that the properties of activation functions\u2014specifically smoothness and nonlinearity\u2014are critical to the training dynamics. Smooth activation functions guarantee globally unique solutions for both forward and backward ODEs, while sufficient nonlinearity is essential for maintaining the spectral properties of the Neural Tangent Kernel (NTK) during training. Together, these properties enable us to establish the global convergence of Neural ODEs under gradient descent in overparameterized regimes. Our theoretical findings are validated by numerical experiments, which ",
    "openreview_id": "AoraWUmpLU",
    "forum_id": "AoraWUmpLU"
  },
  "analysis_timestamp": "2026-01-06T12:21:11.295657"
}
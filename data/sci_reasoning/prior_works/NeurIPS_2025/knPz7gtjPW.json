{
  "prior_works": [
    {
      "title": "Scaling Laws for Neural Language Models",
      "authors": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei",
      "year": 2020,
      "role": "Empirical anchor for neural scaling",
      "relationship_sentence": "Established the power-law relationship between loss and model size that this paper aims to mechanistically explain via superposition."
    },
    {
      "title": "Training Compute-Optimal Large Language Models",
      "authors": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, et al.",
      "year": 2022,
      "role": "Refined scaling laws and regimes",
      "relationship_sentence": "Provided precise empirical scaling regimes that motivate seeking a mechanistic account; the present work explains when 1/dimension scaling should emerge."
    },
    {
      "title": "Toy Models of Superposition",
      "authors": "Anthropic Interpretability Team (Elhage et al.)",
      "year": 2022,
      "role": "Foundational mechanism and methodology",
      "relationship_sentence": "Introduced representation superposition and used weight decay as a knob to induce it; this paper directly builds on that toy model and control to study scaling behavior."
    },
    {
      "title": "Sparse Autoencoders Find Interpretable Features in Language Models",
      "authors": "Christopher P. Bricken, Neel Nanda, et al.",
      "year": 2023,
      "role": "Empirical evidence of superposition in LLMs",
      "relationship_sentence": "Demonstrated widespread feature superposition in LLM representations, supporting the paper\u2019s assumption that real models operate in a strong superposition regime."
    },
    {
      "title": "Multimodal Neurons in Artificial Neural Networks",
      "authors": "Gabriel Goh, Nick Cammarata, Chelsea Voss, et al.",
      "year": 2021,
      "role": "Polysemanticity as evidence for superposition",
      "relationship_sentence": "Showed polysemantic neurons that encode multiple features, motivating the geometric overlap perspective the paper uses to derive scaling under strong superposition."
    },
    {
      "title": "Zipf\u2019s word frequency law in natural language: A critical review",
      "authors": "Steven T. Piantadosi",
      "year": 2014,
      "role": "Power-law feature frequency foundation",
      "relationship_sentence": "Justifies modeling feature frequencies with power laws, which the paper shows is necessary for power-law loss scaling in the weak-superposition regime."
    },
    {
      "title": "Storing an infinite number of patterns in a spin-glass model of neural networks",
      "authors": "Daniel J. Amit, Hanoch Gutfreund, H. Sompolinsky",
      "year": 1985,
      "role": "Geometric interference and capacity scaling",
      "relationship_sentence": "Provided classic analysis of interference and capacity scaling ~O(N) in distributed memories; the paper\u2019s 1/dimension loss scaling under strong superposition parallels this geometric crosstalk argument."
    }
  ],
  "synthesis_narrative": "This paper seeks a mechanistic origin for neural scaling laws by positing representation superposition as a key driver of loss. The empirical scaling literature\u2014especially Kaplan et al. and Hoffmann et al.\u2014establishes robust power-law relations between performance and model size, creating the explanatory target: why does loss predictably fall with larger dimension? The immediate conceptual and methodological foundation comes from Anthropic\u2019s Toy Models of Superposition, which formalized how models represent more features than dimensions and introduced weight decay as a controllable knob that induces or suppresses superposition. Building on this, the present work varies superposition strength to derive distinct scaling regimes.\n\nTwo strands of empirical interpretability ground the paper\u2019s assumptions about contemporary LLMs. Goh et al. document polysemantic neurons, implying multiplexed features and geometric overlap in finite-dimensional spaces. Bricken et al. use sparse autoencoders to show extensive superposition in LLM residual streams, supporting the claim that real models operate in a strong superposition regime where geometric interference governs error.\n\nThe paper\u2019s bifurcation between weak and strong superposition hinges on data statistics. Piantadosi\u2019s synthesis of Zipfian power laws in language supports the result that, under weak superposition, power-law loss scaling appears only with power-law feature frequencies. Finally, classic distributed memory theory (Amit\u2013Gutfreund\u2013Sompolinsky) provides the geometric intuition that crosstalk errors decrease inversely with dimension, mirroring the paper\u2019s main theoretical prediction that strong superposition generically yields loss scaling ~1/dimension across broad frequency distributions.",
  "analysis_timestamp": "2026-01-07T00:05:12.540753"
}
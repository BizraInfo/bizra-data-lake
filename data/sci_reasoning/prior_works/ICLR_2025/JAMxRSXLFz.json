{
  "prior_works": [
    {
      "title": "Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Utility",
      "authors": "Sudha Rao et al.",
      "year": 2018,
      "arxiv_id": "1805.04655",
      "role": "Extension",
      "relationship_sentence": "This work\u2019s utility-based objective for selecting clarifying questions is directly generalized here into a Bayesian expected information gain criterion over latent task specifications."
    },
    {
      "title": "Asking Clarifying Questions in Open-Domain Information-Seeking Conversations",
      "authors": "Mohammad Aliannejadi et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By demonstrating clarifying-question selection with task-specific retrieval heuristics, this paper exposed the lack of a principled, model-based information-seeking objective that the present work addresses via Bayesian experimental design."
    },
    {
      "title": "AmbigQA: Answering Ambiguous Open-domain Questions",
      "authors": "Sewon Min et al.",
      "year": 2020,
      "arxiv_id": "2004.10645",
      "role": "Foundation",
      "relationship_sentence": "AmbigQA formally characterized ambiguity in user queries and used human-like clarifications, providing the conceptual foundation for defining and operationalizing task ambiguity beyond QA."
    },
    {
      "title": "Bayesian Active Learning by Disagreement",
      "authors": "Neil Houlsby et al.",
      "year": 2011,
      "arxiv_id": "1112.5745",
      "role": "Extension",
      "relationship_sentence": "The mutual-information acquisition principle from BALD is adapted to select clarifying questions that maximize information about the latent task specification rather than model parameters."
    },
    {
      "title": "The Measurement of Information Provided by an Experiment",
      "authors": "Dennis V. Lindley",
      "year": 1956,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Lindley\u2019s expected information gain formulation underpins the paper\u2019s framing of clarifying question asking as optimal Bayesian experimental design."
    },
    {
      "title": "Active Preference-Based Learning of Reward Functions",
      "authors": "Dorsa Sadigh et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "This paper\u2019s treatment of user intent as a latent variable elicited via information-gain-maximizing queries inspires modeling task specification as a latent hypothesis inferred through targeted questions."
    },
    {
      "title": "Self-Ask: A Simple Approach to Multi-Hop Question Answering",
      "authors": "Ofir Press et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Self-Ask\u2019s prompting to generate follow-up questions serves as a baseline whose lack of principled, information-seeking selection is addressed by the proposed Bayesian design objective."
    }
  ],
  "synthesis_narrative": "Work on clarifying questions in NLP first operationalized the goal of reducing user intent uncertainty by selecting questions that most improve utility. Rao and Daum\u00e9 III introduced a neural expected-utility objective for ranking clarification questions, tying question choice directly to downstream task success. Aliannejadi and colleagues extended this to open-domain conversational search, showing that clarifying questions can measurably disambiguate queries, though their selection procedures relied on domain heuristics and retrieval signals. AmbigQA formalized ambiguity in open-domain QA and demonstrated that predicting human-like clarifications can surface multiple plausible interpretations, highlighting the need for explicit ambiguity modeling. From the active learning literature, BALD provided a practical mutual-information acquisition principle for choosing queries that maximally reduce uncertainty, while Lindley\u2019s classical result grounded this strategy as expected information gain within Bayesian experimental design. In interactive learning, Sadigh et al. modeled user intent (a reward function) as a latent variable and selected human queries to maximally reduce uncertainty over that latent, demonstrating principled human-in-the-loop elicitation.\nTogether, these works reveal both the importance of clarifying questions for resolving underspecification and the absence of a general, model-based selection principle for arbitrary tasks. The natural synthesis is to treat task specification as a latent hypothesis space and choose natural-language questions via expected information gain\u2014thereby generalizing utility-based ranking and preference elicitation to LLM agents. This unifies prior clarifying-question insights with Bayesian design, shifting from ad hoc prompting (e.g., Self-Ask) to explicit, information-seeking interrogation of ambiguity.",
  "target_paper": {
    "title": "Active Task Disambiguation with LLMs",
    "authors": "Kasia Kobalczyk, Nicol\u00e1s Astorga, Tennison Liu, Mihaela van der Schaar",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Task Ambiguity, Bayesian Experimental Design, Large Language Models, Active Learning",
    "abstract": "Despite the impressive performance of large language models (LLMs) across various benchmarks, their ability to address ambiguously specified problems\u2014frequent in real-world interactions\u2014remains underexplored. To address this gap, we introduce a formal definition of task ambiguity and frame the problem of task disambiguation through the lens of Bayesian Experimental Design. By posing clarifying questions, LLM agents can acquire additional task specifications, progressively narrowing the space of viable solutions and reducing the risk of generating unsatisfactory outputs. Yet, generating effective clarifying questions requires LLM agents to engage in a form of meta-cognitive reasoning, an ability LLMs may presently lack. Our proposed approach of active task disambiguation enables LLM agents to generate targeted questions maximizing the information gain. Effectively, this approach shifts the load from implicit to explicit reasoning about the space of viable solutions. Empirical results de",
    "openreview_id": "JAMxRSXLFz",
    "forum_id": "JAMxRSXLFz"
  },
  "analysis_timestamp": "2026-01-06T09:15:21.594753"
}
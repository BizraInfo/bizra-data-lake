{
  "prior_works": [
    {
      "title": "Online Learning with Feedback Graphs",
      "authors": "Noga Alon, Nicol\u00f2 Cesa-Bianchi, Ofer Dekel, Tomer Koren",
      "year": 2015,
      "role": "Foundational model and baseline bounds for feedback-graph learning",
      "relationship_sentence": "Introduced strong/weak observability, established \u03b1-dependent regret (including O(\u221a(\u03b1 T ln K)) for strongly observable graphs) and \u03a9(\u221a(\u03b1 T)) lower bounds that the NeurIPS 2023 paper sharpens to O(\u221a(\u03b1 T(1+ln(K/\u03b1)))) while matching known lower bounds."
    },
    {
      "title": "From Bandits to Experts: A Graphical Model of Feedback",
      "authors": "Shie Mannor, Ohad Shamir",
      "year": 2011,
      "role": "Early formulation of graph-structured feedback in online learning",
      "relationship_sentence": "Laid the graph-feedback paradigm bridging experts and bandits that underpins the problem setting refined by Eldowa et al., including the use of independence/covering structures to design exploration."
    },
    {
      "title": "Online Learning with Feedback Graphs: Beyond Bandits",
      "authors": "Alon Cohen, Elad Hazan, Tomer Koren",
      "year": 2016,
      "role": "Algorithmic refinements for graph feedback (e.g., Exp3.G variants, dominating-set exploration)",
      "relationship_sentence": "Provided algorithmic templates and analyses for exploration via small dominating sets in strongly observable graphs that the new work leverages and tightens using sharper combinatorial bounds and regularization."
    },
    {
      "title": "Minimax Policies for Adversarial and Stochastic Bandits",
      "authors": "Jean-Yves Audibert, S\u00e9bastien Bubeck",
      "year": 2009,
      "role": "Log-free optimal adversarial bandit methodology (INF/implicit normalization)",
      "relationship_sentence": "Established techniques achieving \u0398(\u221a(K T)) without a ln K factor, a key benchmark for the \u03b1=K case that the NeurIPS 2023 paper must and does recover within its interpolating bound."
    },
    {
      "title": "Tsallis-INF: An Optimal Algorithm for Stochastic and Adversarial Bandits",
      "authors": "Julian Zimmert, Yevgeny Seldin",
      "year": 2019,
      "role": "Regularization choice and analysis with q-Tsallis entropy (FTRL/MD) for bandits",
      "relationship_sentence": "Demonstrated the power of q-Tsallis regularization (notably q=1/2) to obtain optimal, log-free adversarial bandit bounds; Eldowa et al. generalize this idea by selecting q\u2208[1/2,1) as a function of \u03b1 to interpolate between experts and bandits under graph feedback."
    },
    {
      "title": "Prediction, Learning, and Games",
      "authors": "Nicol\u00f2 Cesa-Bianchi, G\u00e1bor Lugosi",
      "year": 2006,
      "role": "Experts regime theory and lower bounds",
      "relationship_sentence": "Provides the \u0398(\u221a(T ln K)) rate and necessity of the \u221a(ln K) factor in the full-information (\u03b1=1) case that the new bound must preserve at one endpoint of its interpolation."
    },
    {
      "title": "On the ratio of optimal integral and fractional covers",
      "authors": "L\u00e1szl\u00f3 Lov\u00e1sz",
      "year": 1975,
      "role": "Combinatorial tool: domination/set cover bounds in terms of independence number",
      "relationship_sentence": "The classical bound \u03b3(G) \u2264 \u03b1(G)(1+ln(n/\u03b1(G))) underlies the improved exploration complexity (ln(K/\u03b1) instead of ln K), which is crucial for the refined regret upper bound in strongly observable feedback graphs."
    }
  ],
  "synthesis_narrative": "The NeurIPS 2023 paper advances the feedback-graph paradigm inaugurated by Mannor and Shamir and crystallized by Alon, Cesa-Bianchi, Dekel, and Koren, who defined strong observability and proved \u03b1-dependent regret bounds, including O(\u221a(\u03b1T ln K)) upper and \u03a9(\u221a(\u03b1T)) lower bounds. Subsequent algorithmic developments by Cohen, Hazan, and Koren operationalized exploration through small dominating sets in strongly observable graphs, creating the template the new work tightens. A central requirement for the authors\u2019 contribution is to match the canonical endpoints: the experts rate \u0398(\u221a(T ln K)) (Cesa-Bianchi and Lugosi) when \u03b1=1, and the adversarial bandit rate \u0398(\u221a(KT)) without extra logarithms when \u03b1=K. Techniques achieving log-free adversarial bandit regret, pioneered by Audibert and Bubeck via implicit normalization and extended by Zimmert and Seldin using q-Tsallis entropy in FTRL/MD, directly inspire the core methodological choice here: Tsallis-regularized FTRL with a q tuned to the independence number \u03b1. Finally, the paper\u2019s sharper interpolation hinges on a classical combinatorial inequality due to Lov\u00e1sz relating domination and independence\u2014\u03b3(G) \u2264 \u03b1(G)(1+ln(n/\u03b1(G)))\u2014which reduces the exploration complexity from ln K to ln(K/\u03b1). By combining Tsallis-entropy regularization (generalizing q beyond 1/2) with this refined graph-theoretic bound, the authors derive an upper bound O(\u221a(\u03b1T(1+ln(K/\u03b1)))) that simultaneously matches the experts and bandits limits and tightens the dependence for intermediate \u03b1.",
  "analysis_timestamp": "2026-01-06T23:42:49.100525"
}
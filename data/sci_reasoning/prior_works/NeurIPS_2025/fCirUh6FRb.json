{
  "prior_works": [
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei et al.",
      "year": 2022,
      "role": "Foundational concept (textual reasoning intermediates)",
      "relationship_sentence": "FSDrive is motivated by limitations of textual CoT\u2014popularized by this work\u2014and replaces it with a visual, spatio-temporal CoT that preserves fine-grained geometric and temporal cues essential for driving."
    },
    {
      "title": "Multimodal Chain-of-Thought Reasoning in Language Models",
      "authors": "Zhou et al.",
      "year": 2023,
      "role": "Methodological antecedent (CoT for VLMs)",
      "relationship_sentence": "By extending CoT to vision-language settings, this work set the stage for intermediate reasoning in multimodal models; FSDrive advances this idea by making the intermediate chain explicitly visual and temporally grounded rather than textual."
    },
    {
      "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
      "authors": "Brohan et al.",
      "year": 2023,
      "role": "Architectural precedent (VLA paradigm)",
      "relationship_sentence": "RT-2 demonstrated that a single VLA can couple perception and action; FSDrive builds on this paradigm, equipping a VLA to both generate a visual future (world-modeling) and produce trajectories (action), using the same backbone."
    },
    {
      "title": "Dreamer: Learning Behaviors by Latent Imagination",
      "authors": "Danijar Hafner et al.",
      "year": 2020,
      "role": "Foundational world-model learning",
      "relationship_sentence": "FSDrive\u2019s first stage acts as a learned world model that imagines future scene frames, directly inspired by Dreamer\u2019s principle of planning via predicted futures."
    },
    {
      "title": "BEVFormer: Learning Bird\u2019s-Eye-View Representations from Multi-Camera Images via Spatiotemporal Transformers",
      "authors": "Zhu et al.",
      "year": 2022,
      "role": "Spatial\u2013temporal scene modeling for driving",
      "relationship_sentence": "FSDrive\u2019s unified visual CoT that overlays lanes and 3D boxes on a predicted future view echoes BEVFormer\u2019s insight that explicit spatial\u2013temporal structure is crucial for downstream planning."
    },
    {
      "title": "UniAD: A Unified Model for 3D Perception, Forecasting, and Planning",
      "authors": "Qiao et al.",
      "year": 2023,
      "role": "Unified perception\u2013prediction\u2013planning framework",
      "relationship_sentence": "FSDrive extends UniAD\u2019s unification idea by turning the intermediate into a generated, physically-consistent future frame that jointly encodes lanes and 3D agents as a planning-ready visual rationale."
    },
    {
      "title": "Diffuser: Diffusion Models for Planning",
      "authors": "Janner et al.",
      "year": 2022,
      "role": "Planning from target states/trajectories",
      "relationship_sentence": "FSDrive\u2019s second stage\u2014using the imagined visual CoT to condition trajectory generation\u2014aligns with Diffuser\u2019s notion of conditioning a planner on target futures, operationalized here as an inverse-dynamics step within a VLA."
    }
  ],
  "synthesis_narrative": "FSDrive\u2019s core contribution\u2014replacing textual chains-of-thought with a spatio-temporal visual CoT\u2014emerges from combining three lines of prior work. First, Chain-of-Thought prompting and its multimodal extensions established the power of explicit intermediate reasoning but largely relied on text. These approaches are ill-suited for driving because symbolic summaries obscure metric geometry and fine appearance cues. FSDrive directly addresses this by making the intermediate chain an image that encodes the scene\u2019s spatial layout and temporal evolution.\nSecond, world-model research (e.g., Dreamer) showed that predicting future observations enables stronger decision making. FSDrive adopts this principle to generate a physically plausible future frame, then augments it with lanes and 3D boxes to make the imagined future explicitly structure-aware. This design resonates with driving-centric spatio-temporal modeling such as BEVFormer and the unification ethos of UniAD, which demonstrated that tightly coupling perception, prediction, and planning with structured representations benefits control.\nThird, the Vision-Language-Action paradigm (RT-2) illustrated that a single foundation model can map high-dimensional perception to actions. FSDrive leverages a similar VLA backbone in a dual role: (1) as a world model that imagines a future visual CoT, and (2) as an inverse-dynamics planner that conditions on this visual CoT to output trajectories\u2014conceptually related to conditioning in diffusion-based planners (Diffuser). By unifying imagination and control inside one VLA and by making the intermediate explicitly visual and temporally grounded, FSDrive bridges the perception\u2013planning gap while preserving the model\u2019s language-grounded understanding.",
  "analysis_timestamp": "2026-01-07T00:21:32.347744"
}
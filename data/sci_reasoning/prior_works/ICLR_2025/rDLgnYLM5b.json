{
  "prior_works": [
    {
      "title": "SPICE: Semantic Propositional Image Caption Evaluation",
      "authors": "Peter Anderson et al.",
      "year": 2016,
      "arxiv_id": "1607.08822",
      "role": "Extension",
      "relationship_sentence": "ISG generalizes SPICE\u2019s scene-graph-based semantic comparison from single-image caption evaluation to interleaved text\u2013image sequences by introducing cross-modal nodes/edges and multi-level scoring."
    },
    {
      "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
      "authors": "Ranjay Krishna et al.",
      "year": 2017,
      "arxiv_id": "1602.07332",
      "role": "Foundation",
      "relationship_sentence": "ISG adopts the Visual Genome scene graph formalism (objects, attributes, relations) as its core representation and extends it to link textual blocks with image regions across a response."
    },
    {
      "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering",
      "authors": "Drew A. Hudson et al.",
      "year": 2019,
      "arxiv_id": "1902.09506",
      "role": "Inspiration",
      "relationship_sentence": "ISG borrows GQA\u2019s practice of deriving structured, compositional questions from scene graphs to produce interpretable question\u2013answer feedback tied to specific graph nodes and relations."
    },
    {
      "title": "TIFA: Text-to-Image Faithfulness Evaluation",
      "authors": "Yushi Hu et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "ISG addresses TIFA\u2019s unstructured, image-only QA evaluation by replacing it with a structured interleaved scene graph that enables holistic, structural, block-level, and image-specific consistency checks."
    },
    {
      "title": "T2I-CompBench: A Comprehensive Benchmark for Compositional Text-to-Image Generation",
      "authors": "Xuanlin Li et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "ISG overcomes T2I-CompBench\u2019s image-only, detector-based checks of objects/attributes/relations by introducing graph-based cross-modal alignment and block-level coherence for interleaved outputs."
    },
    {
      "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models",
      "authors": "Meta AI et al.",
      "year": 2024,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "ISG targets the interleaved text-and-image generation capability exemplified by Chameleon, providing the missing evaluation protocol for assessing holistic and structural consistency of mixed-modality generations."
    },
    {
      "title": "Emu2: Multimodal Pretraining for Interleaved Image\u2013Text Generation",
      "authors": "Chunyuan Li et al.",
      "year": 2024,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "ISG is designed to evaluate models like Emu2 that natively generate interleaved sequences, filling the measurement gap left by image-only or text-only metrics."
    }
  ],
  "synthesis_narrative": "Scene-graph ideas matured through two strands that are directly relevant here. SPICE showed that converting captions into a scene-graph representation of objects, attributes, and relations enables semantics-aware evaluation beyond n-gram overlap, establishing graphs as a metric substrate. Visual Genome standardized this formalism with dense object\u2013attribute\u2013relation annotations, making graphs a lingua franca for connecting visual content and language. Building on this, GQA demonstrated how to derive compositional questions from scene graphs, producing interpretable QA that probes specific nodes and relations rather than holistic heuristics. In parallel, automatic evaluation of text-to-image models advanced with TIFA, which uses question answering to assess image\u2013prompt faithfulness but remains image-only and unstructured; and with T2I-CompBench, which tests compositionality using detectors on objects, attributes, and relations but likewise focuses solely on images. Meanwhile, new generative models such as Chameleon and Emu2 began producing interleaved sequences of text and images, creating outputs that require alignment and coherence checks within and across modalities and blocks.\nTogether, these threads exposed a gap: existing metrics either model semantics via graphs but only for captions, or evaluate images via QA/detectors without cross-modal structure, while modern generators output interleaved multimodal content. ISG synthesizes SPICE\u2019s graph-based semantics, Visual Genome\u2019s object\u2013relation schema, and GQA\u2019s graph-derived QA with the problem setting introduced by Chameleon/Emu2, yielding an interleaved scene graph that supports holistic, structural, block-level, and image-specific evaluation with interpretable QA grounded in the graph.",
  "target_paper": {
    "title": "Interleaved Scene Graphs for Interleaved Text-and-Image Generation Assessment",
    "authors": "Dongping Chen, Ruoxi Chen, Shu Pu, Zhaoyi Liu, Yanru Wu, Caixi Chen, Benlin Liu, Yue Huang, Yao Wan, Pan Zhou, Ranjay Krishna",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Interleaved Text-and-Image Generation, Generative Models, Multimodal Large Language Model, Scene Graphs, Automatic Evaluation, Benchmark",
    "abstract": "Many real-world user queries (e.g. *\"How do to make egg fried rice?\"*) could benefit from systems capable of generating responses with both textual steps with accompanying images, similar to a cookbook.\nModels designed to generate interleaved text and images face challenges in ensuring consistency within and across these modalities.\nTo address these challenges, we present ISG, a comprehensive evaluation framework for interleaved text-and-image generation. ISG leverages a scene graph structure to capture relationships between text and image blocks, evaluating responses on four levels of granularity: holistic, structural, block-level, and image-specific. This multi-tiered evaluation allows for a nuanced assessment of consistency, coherence, and accuracy, and provides interpretable question-answer feedback.\nIn conjunction with ISG, we introduce a benchmark, ISG-Bench, encompassing 1,150 samples across 8 categories and 21 subcategories. This benchmark dataset includes complex language-visi",
    "openreview_id": "rDLgnYLM5b",
    "forum_id": "rDLgnYLM5b"
  },
  "analysis_timestamp": "2026-01-06T15:31:09.747495"
}
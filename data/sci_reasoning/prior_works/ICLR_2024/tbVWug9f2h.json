{
  "prior_works": [
    {
      "title": "A Grammar of Kalamang",
      "authors": "Eline Visser",
      "year": 2018,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This descriptive grammar provides the single human-readable source the benchmark uses as its sole supervision signal, concretely enabling the \u201ctranslate from one book\u201d task design."
    },
    {
      "title": "The FLORES-200 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation",
      "authors": "Naman Goyal et al.",
      "year": 2022,
      "arxiv_id": "2207.04670",
      "role": "Gap Identification",
      "relationship_sentence": "FLORES-200 established rigorous evaluation practices for low-resource MT but primarily targets web-present languages, directly motivating the benchmark\u2019s focus on a truly unseen language with virtually no internet footprint."
    },
    {
      "title": "No Language Left Behind: Scaling Human-Centered Machine Translation",
      "authors": "NLLB Team",
      "year": 2022,
      "arxiv_id": "2207.04672",
      "role": "Related Problem",
      "relationship_sentence": "NLLB demonstrated state-of-the-art massively multilingual MT under data-rich mining and supervised regimes, contextualizing the need for a setting where translation must be learned from non-parallel linguistic descriptions rather than web-mined text."
    },
    {
      "title": "Unsupervised Machine Translation Using Monolingual Corpora Only",
      "authors": "Guillaume Lample et al.",
      "year": 2018,
      "arxiv_id": "1711.00043",
      "role": "Gap Identification",
      "relationship_sentence": "This work showed how to train MT without parallel data but still assumes sizable monolingual corpora, a key limitation the benchmark addresses by replacing web corpora with a single descriptive grammar as supervision."
    },
    {
      "title": "The ODIN Project: On Extracting and Using Low-Resource Language Materials from the Web",
      "authors": "Fei Xia and William D. Lewis",
      "year": 2007,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "ODIN established that field-linguistic artifacts (e.g., interlinear glossed text) can be harvested and operationalized for NLP, inspiring the core idea of learning translation from human-written linguistic descriptions."
    },
    {
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP",
      "authors": "Patrick Lewis et al.",
      "year": 2020,
      "arxiv_id": "2005.11401",
      "role": "Baseline",
      "relationship_sentence": "The benchmark adapts RAG-style retrieval over the grammar book as a primary baseline, using retrieved sections to condition the translator on relevant linguistic facts at inference."
    },
    {
      "title": "Leveraging Passage Retrieval with Generative Models for Open-Domain Question Answering (FiD)",
      "authors": "Gautier Izacard and Edouard Grave",
      "year": 2021,
      "arxiv_id": "2007.01282",
      "role": "Extension",
      "relationship_sentence": "The FiD paradigm of fusing multiple retrieved passages is directly extended as a book-reading strategy to aggregate dispersed grammatical evidence for translation decisions."
    }
  ],
  "synthesis_narrative": "Field linguistics has long shown that human-authored linguistic resources can supervise NLP for low-resource languages: ODIN demonstrated how interlinear glossed text and other artifacts could be extracted and operationalized, establishing that descriptive materials themselves can be supervision. Parallelly, FLORES-200 introduced rigorous evaluation methodology for low-resource MT across many languages, while NLLB pushed multilingual translation at scale using mined corpora. Unsupervised MT advanced the idea of translation without bitext (e.g., via back-translation) but still presupposed sizable monolingual data, a requirement often impossible for endangered or minimally documented languages. On the systems side, retrieval-augmented generation (RAG) and Fusion-in-Decoder (FiD) showed that models can read and aggregate information from long, dispersed sources by retrieving relevant passages and conditioning generation on them. Finally, the Kalamang descriptive grammar by Visser provided a comprehensive, human-readable account of a language with almost no web presence, offering a unique, compact supervision source.\nTogether these works exposed a gap: while low-resource MT benchmarks and systems matured, they largely relied on web-minable corpora, leaving truly unseen languages underserved; yet retrieval methods and linguistic documentation hinted that a single, descriptive source could be leveraged. The current benchmark synthesizes these threads by formulating translation as learning from one grammar book: it operationalizes field-linguistic description as supervision, evaluates under FLORES-style rigor but on an unseen language, and employs RAG/FiD-style book-reading as principled baselines, creating a natural next step for studying genuine adaptation to new languages.",
  "target_paper": {
    "title": "A Benchmark for Learning to Translate a New Language from One Grammar Book",
    "authors": "Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, Luke Melas-Kyriazi",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "low-resource languages, indigenous languages, endangered languages, long context, field linguistics, unseen tasks, large language models, machine translation, benchmark",
    "abstract": "Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is explicitly motivated and bottlenecked by a scarcity of web data: low-resource languages. In this paper, we introduce MTOB (Machine Translation from One Book), a benchmark for learning to translate between English and Kalamang\u2014a language with less than 200 speakers and therefore virtually no presence on the web\u2014using several hundred pages of field linguistics reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data, more akin to L2 language learning than L1 language acquisition. We demonstrate that baselines using current LLMs are promising but fall short of h",
    "openreview_id": "tbVWug9f2h",
    "forum_id": "tbVWug9f2h"
  },
  "analysis_timestamp": "2026-01-06T07:54:27.555553"
}
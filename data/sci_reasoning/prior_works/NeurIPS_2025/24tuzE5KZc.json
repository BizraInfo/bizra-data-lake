{
  "prior_works": [
    {
      "title": "Pointer Networks",
      "authors": "Oriol Vinyals, Meire Fortunato, Navdeep Jaitly",
      "year": 2015,
      "role": "Foundational attention mechanism for combinatorial optimization",
      "relationship_sentence": "OPTFM builds on the idea introduced by Pointer Networks of using attention to construct permutations for combinatorial problems, extending it from task-specific sequence models to a general graph foundation model that can handle diverse CO tasks."
    },
    {
      "title": "Attention, Learn to Solve Routing Problems!",
      "authors": "Wouter Kool, Herke van Hoof, Max Welling",
      "year": 2019,
      "role": "Transformer-based CO solver",
      "relationship_sentence": "This work demonstrated the effectiveness of Transformers for routing problems, directly informing OPTFM\u2019s choice of attention-based architectures for CO and motivating a unified, scalable graph-transformer that generalizes across problem families."
    },
    {
      "title": "Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks",
      "authors": "Juho Lee et al.",
      "year": 2019,
      "role": "Scalable hybrid self-/cross-attention for sets",
      "relationship_sentence": "Set Transformer\u2019s induced set attention blocks (cross-attention to inducing points followed by latent self-attention) inspire OPTFM\u2019s hybrid self- and cross-attention design that achieves O(N) complexity while preserving permutation invariance and global semantics."
    },
    {
      "title": "Perceiver: General Perception with Iterative Attention",
      "authors": "Andrew Jaegle et al.",
      "year": 2021,
      "role": "Latent cross-attention for scalable multi-view modeling",
      "relationship_sentence": "Perceiver\u2019s latent bottleneck via cross-attention provides a blueprint for OPTFM\u2019s multi-view cross-attention that aggregates heterogeneous graph views into a consistent latent space, enabling scalability without losing semantic alignment."
    },
    {
      "title": "Heterogeneous Graph Transformer",
      "authors": "Ziniu Hu, Yuxiao Dong, Kuansan Wang, Yizhou Sun",
      "year": 2020,
      "role": "Type-aware attention on heterogeneous graphs",
      "relationship_sentence": "HGT\u2019s relation- and type-specific attention mechanisms influence OPTFM\u2019s design for handling heterogeneous graph structures and semantics, which are fused via cross-attention across views to maintain consistency at scale."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners (MAE)",
      "authors": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, Ross Girshick",
      "year": 2022,
      "role": "Masked reconstruction pretraining objective",
      "relationship_sentence": "OPTFM\u2019s node-level graph reconstruction objective draws directly from MAE\u2019s masked-reconstruction paradigm, adapting it to graphs to learn robust local representations that serve as the lower level of its dual-level pretraining."
    },
    {
      "title": "MVGRL: Multi-View Graph Representation Learning via Mutual Information Maximization",
      "authors": "Kaveh Hassani, Amir Hosein Khasahmadi",
      "year": 2020,
      "role": "Instance-level multi-view contrastive learning on graphs",
      "relationship_sentence": "OPTFM\u2019s instance-level contrastive pretraining is motivated by MVGRL\u2019s multi-view mutual information maximization, extending it to align graph instances across views and distributions as the higher-level objective in the dual-level framework."
    }
  ],
  "synthesis_narrative": "OPTFM\u2019s core innovation\u2014a scalable multi-view graph transformer paired with dual-level pretraining\u2014sits at the intersection of three lines of prior work. First, attention for combinatorial optimization established that end-to-end learned attention can construct solutions to NP-hard problems. Pointer Networks introduced attention-based decoding for permutations, and Kool et al. showed Transformers can effectively solve routing, directly motivating OPTFM\u2019s use of attention as a unifying backbone for diverse CO tasks.\nSecond, scalability and semantics in attention architectures informed the proposed hybrid self-/cross-attention with linear time. Set Transformer pioneered cross-attention to a small set of inducing tokens followed by latent self-attention, achieving O(N) complexity while maintaining permutation invariance. Perceiver extended this idea with a latent bottleneck for multi-modal inputs, illustrating how cross-attention can aggregate disparate views into a coherent latent representation. Heterogeneous Graph Transformer provided type-aware attention for hetero-graphs, guiding OPTFM\u2019s design to maintain semantic consistency across node/edge types while scaling to large, structurally diverse instances.\nThird, OPTFM\u2019s dual-level pretraining draws from self-supervised learning on graphs. MAE\u2019s masked reconstruction paradigm underpins OPTFM\u2019s node-level graph reconstruction objective for robust local features, while MVGRL\u2019s multi-view instance-level contrastive learning inspires the graph-instance objective that aligns representations across distributions. Together, these works crystallize into OPTFM\u2019s hierarchical pretraining and scalable multi-view transformer, enabling a general-purpose graph foundation model for combinatorial optimization.",
  "analysis_timestamp": "2026-01-07T00:21:32.266828"
}
{
  "prior_works": [
    {
      "title": "Diffusion Schr\u00f6dinger Bridge",
      "authors": [
        "Guillaume De Bortoli",
        "James Thornton",
        "J\u00e9r\u00e9mie Heng",
        "Arnaud Doucet"
      ],
      "year": 2021,
      "role": "Foundational SB-based diffusion sampler for generative modeling",
      "relationship_sentence": "This work established practical Schr\u00f6dinger-bridge diffusion samplers via IPF/bridge constructions; ASBS keeps the SB control formulation but replaces costly IPF/importance weighting with adjoint-based matching objectives for scalable training without target samples."
    },
    {
      "title": "Optimal Steering of a Linear Stochastic System to a Final Probability Distribution",
      "authors": [
        "Yongxin Chen",
        "Tryphon T. Georgiou",
        "Michele Pavon"
      ],
      "year": 2016,
      "role": "Stochastic optimal control formulation of Schr\u00f6dinger bridges",
      "relationship_sentence": "Provides the control-theoretic view of SB with kinetic-energy cost and forward\u2013backward (adjoint) equations; ASBS leverages this structure to derive its adjoint learning rule and kinetic-optimal transport interpretation."
    },
    {
      "title": "A Computational Fluid Mechanics Solution to the Monge\u2013Kantorovich Mass Transfer Problem",
      "authors": [
        "Jean-David Benamou",
        "Yann Brenier"
      ],
      "year": 2000,
      "role": "Dynamic optimal transport (kinetic energy) foundation",
      "relationship_sentence": "Supplies the Benamou\u2013Brenier dynamic OT formulation underpinning kinetic-optimal transport; ASBS draws on this to motivate efficient probability transport in its sampler and objective."
    },
    {
      "title": "Neural Ordinary Differential Equations",
      "authors": [
        "Ricky T. Q. Chen",
        "Yulia Rubanova",
        "Jesse Bettencourt",
        "David Duvenaud"
      ],
      "year": 2018,
      "role": "Adjoint sensitivity for scalable training of continuous-time models",
      "relationship_sentence": "Introduced memory-efficient adjoint differentiation for continuous dynamics; ASBS adapts adjoint methods to controlled diffusions/bridges to obtain scalable gradients for its SB control objectives."
    },
    {
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "authors": [
        "Yang Song",
        "Jascha Sohl-Dickstein",
        "Diederik P. Kingma",
        "Abhishek Kumar",
        "Stefano Ermon",
        "Ben Poole"
      ],
      "year": 2021,
      "role": "Diffusion/SDE framework for generative sampling",
      "relationship_sentence": "Established the SDE and probability-flow viewpoints for diffusion samplers; ASBS operates within this SDE framework but learns a control via SB from an energy function rather than from data samples or importance weighting."
    },
    {
      "title": "Stochastic Interpolants: A Unifying Framework for Flows and Diffusions",
      "authors": [
        "Michael S. Albergo",
        "Katherine L. Boffi",
        "Eric Vanden-Eijnden"
      ],
      "year": 2023,
      "role": "Matching-based training objectives for flows/diffusions",
      "relationship_sentence": "Introduced simple regression-style (flow-matching) objectives; ASBS designs analogous matching losses in the SB setting to avoid explicit target-sample estimation while training a diffusion control."
    },
    {
      "title": "Boltzmann Generators: Sampling Equilibrium States of Many-Body Systems with Deep Learning",
      "authors": [
        "Frank No\u00e9",
        "Simon Olsson",
        "Jonas K\u00f6hler",
        "Hao Wu"
      ],
      "year": 2019,
      "role": "Deep generative transport for Boltzmann sampling",
      "relationship_sentence": "Demonstrated the promise and challenges of learning transports for Boltzmann distributions; ASBS targets the same application domain with an SB/diffusion approach that improves scalability and reduces training complexity."
    }
  ],
  "synthesis_narrative": "ASBS sits at the intersection of Schr\u00f6dinger bridges, stochastic optimal control, and modern matching-based training for continuous-time generative models. The Diffusion Schr\u00f6dinger Bridge (De Bortoli et al., 2021) showed that SBs can yield powerful diffusion samplers, but practical training often relies on iterative proportional fitting and importance weighting, which hinder scalability for energy-defined targets. Chen, Georgiou, and Pavon (2016) provide the control-theoretic backbone of SBs as kinetic-energy optimal stochastic control with forward\u2013backward (adjoint) equations, while Benamou\u2013Brenier (2000) grounds the kinetic-optimal transport view that motivates efficient movement of probability mass.\n\nASBS reframes SB learning through an adjoint lens to avoid expensive estimation of target samples. This is enabled technically by the continuous-time adjoint sensitivity method popularized by Neural ODEs (Chen et al., 2018), which ASBS adapts to controlled diffusions to obtain scalable gradients for SB control parameters. From the generative modeling side, ASBS adopts the SDE/probability-flow perspective of score-based diffusion models (Song et al., 2021), but replaces data-driven score learning with control learned directly from the energy via SB constraints. Finally, inspired by the simplicity and scalability of stochastic interpolants/flow-matching objectives (Albergo et al., 2023), ASBS designs matching-based losses tailored to the SB setting, eliminating the need for importance-weighted target-sample surrogates. In the context of molecular and statistical mechanics applications highlighted by Boltzmann Generators (No\u00e9 et al., 2019), this synthesis yields a sampler that is both principled\u2014through kinetic-optimal transport\u2014and practical\u2014through adjoint-driven, scalable training.",
  "analysis_timestamp": "2026-01-06T23:42:48.134583"
}
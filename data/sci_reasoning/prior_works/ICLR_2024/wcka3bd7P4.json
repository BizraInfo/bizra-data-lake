{
  "prior_works": [
    {
      "title": "Neural Ordinary Differential Equations",
      "authors": "Ricky T. Q. Chen et al.",
      "year": 2018,
      "arxiv_id": "1806.07366",
      "role": "Foundation",
      "relationship_sentence": "FROND adopts the continuous-depth dynamical systems view of Neural ODEs but replaces the integer-order time derivative with a Caputo fractional derivative to model history-dependent dynamics on graphs."
    },
    {
      "title": "GRAND: Graph Neural Diffusion",
      "authors": "Benjamin P. Chamberlain et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "FROND directly generalizes GRAND\u2019s Laplacian-driven diffusion ODE x\u0307 = \u2212Lx by formulating a Caputo time-fractional diffusion d^\u03b1x/dt^\u03b1 = \u2212Lx, yielding non-Markovian feature evolution with provable oversmoothing mitigation."
    },
    {
      "title": "The Random Walk\u2019s Guide to Anomalous Diffusion: A Fractional Dynamics Approach",
      "authors": "Ralf Metzler et al.",
      "year": 2000,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "The established equivalence between time-fractional diffusion (with Caputo derivatives) and non-Markovian continuous-time random walks directly motivates FROND\u2019s non-Markovian random-walk interpretation of feature updates."
    },
    {
      "title": "Fractional Differential Equations",
      "authors": "Igor Podlubny",
      "year": 1999,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "FROND\u2019s modeling and analysis hinge on Caputo derivative definitions and properties from Podlubny, including treatment of initial conditions and memory kernels in fractional-order dynamical systems."
    },
    {
      "title": "Graph Neural Networks Exponentially Lose Expressive Power for Node Classification",
      "authors": "Kenta Oono et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "This work\u2019s contraction analysis of deep GNNs formalizes the oversmoothing problem that FROND explicitly addresses by weakening the contraction via time-fractional dynamics."
    },
    {
      "title": "Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning",
      "authors": "Qimai Li et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By identifying GCN propagation as Laplacian smoothing that homogenizes features, this paper motivates FROND\u2019s shift from integer-time diffusion to fractional-time diffusion to retain discriminative signals."
    }
  ],
  "synthesis_narrative": "Neural Ordinary Differential Equations introduced a continuous-depth viewpoint where hidden states evolve under time-parameterized dynamics, providing the formal machinery to treat layer-wise propagation as the solution of a differential equation. GRAND specialized this idea to graphs by casting node feature propagation as a graph-Laplacian-driven diffusion ODE, tightly linking representation learning to the heat equation on networks. Parallel to these developments, fractional calculus established by Podlubny codified Caputo derivatives and their memory kernels, enabling well-posed fractional-order dynamical systems with natural initial conditions. Metzler and Klafter connected time-fractional diffusion equations to non-Markovian continuous-time random walks with heavy-tailed waiting times, showing how history-dependent dynamics slow mixing and preserve heterogeneity relative to Markovian diffusion. In graph learning, Li, Han, and Wu characterized standard GCN propagation as Laplacian smoothing, explaining feature homogenization through diffusion. Oono and Suzuki further proved that deep GNNs contract exponentially, formalizing oversmoothing as an inherent limitation of repeated diffusion-like updates.\nBringing these strands together suggested a natural opportunity: retain the powerful continuous-depth and diffusion formalisms of GRAND/Neural ODEs while replacing the integer-order time derivative by a Caputo fractional derivative to inject explicit memory into graph propagation. This synthesis yields a non-Markovian, time-fractional diffusion on graphs that analytically weakens the contraction underpinning oversmoothing and admits a random-walk interpretation grounded in anomalous diffusion theory, while remaining a drop-in generalization of established continuous GNN baselines.",
  "target_paper": {
    "title": "Unleashing the Potential of Fractional Calculus in Graph Neural Networks with FROND",
    "authors": "Qiyu Kang, Kai Zhao, Qinxu Ding, Feng Ji, Xuhao Li, Wenfei Liang, Yang Song, Wee Peng Tay",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "graph neural network",
    "abstract": "We introduce the FRactional-Order graph Neural Dynamical network (FROND), a new continuous graph neural network (GNN) framework. Unlike traditional continuous GNNs that rely on integer-order differential equations, FROND employs the Caputo fractional derivative to leverage the non-local properties of fractional calculus. This approach enables the capture of long-term dependencies in feature updates, moving beyond the Markovian update mechanisms in conventional integer-order models and offering enhanced capabilities in graph representation learning. \nWe offer an interpretation of the node feature updating process in FROND from a non-Markovian random walk perspective when the feature updating is particularly governed by a diffusion process.\nWe demonstrate analytically that oversmoothing can be mitigated in this setting.\nExperimentally, we validate the FROND framework by comparing the fractional adaptations of various established integer-order continuous GNNs, demonstrating their consiste",
    "openreview_id": "wcka3bd7P4",
    "forum_id": "wcka3bd7P4"
  },
  "analysis_timestamp": "2026-01-06T14:23:46.127971"
}
{
  "prior_works": [
    {
      "title": "Deep Sets",
      "authors": "Zaheer et al.",
      "year": 2017,
      "arxiv_id": "1703.06114",
      "role": "Foundation",
      "relationship_sentence": "Introduced the sum-decomposition for permutation-invariant multiset functions that underpins the multiset encoders whose separation properties this paper quantitatively evaluates via H\u00f6lder-in-expectation."
    },
    {
      "title": "How Powerful are Graph Neural Networks?",
      "authors": "Xu et al.",
      "year": 2019,
      "arxiv_id": "1810.00826",
      "role": "Baseline",
      "relationship_sentence": "Established the 1-WL-equivalent separability criterion and the injective sum-aggregator (GIN), which this work revisits by replacing binary separation with a quantitative H\u00f6lder stability measure capturing separation quality."
    },
    {
      "title": "Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks",
      "authors": "Morris et al.",
      "year": 2019,
      "arxiv_id": "1810.02244",
      "role": "Foundation",
      "relationship_sentence": "Formalized the WL\u2013GNN expressivity connection and \u2018maximally separating\u2019 viewpoint that this paper argues is too coarse, motivating a refined, pairwise stability-based analysis."
    },
    {
      "title": "Invariant and Equivariant Graph Networks",
      "authors": "Maron et al.",
      "year": 2019,
      "arxiv_id": "1812.09902",
      "role": "Related Problem",
      "relationship_sentence": "Provided theoretically maximally expressive graph networks beyond 1-WL that serve as reference points for which this paper assesses the strength (not just existence) of separations under finite precision."
    },
    {
      "title": "Universal Invariant and Equivariant Graph Neural Networks",
      "authors": "Keriven and Peyr\u00e9",
      "year": 2019,
      "arxiv_id": "1905.04943",
      "role": "Inspiration",
      "relationship_sentence": "Showed universality of invariant/equivariant GNNs under continuity assumptions, highlighting the importance of regularity that this work directly quantifies through H\u00f6lder-type stability of parametric networks."
    },
    {
      "title": "Stability of Graph Neural Networks",
      "authors": "Gama et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Developed Lipschitz-based stability analyses for GNNs to input/graph perturbations, which this paper extends by adapting Lipschitz/H\u00f6lder notions to parametric function families and using them as a pairwise separation-quality metric."
    },
    {
      "title": "On the Limitations of Representing Functions on Sets",
      "authors": "Wagstaff et al.",
      "year": 2019,
      "arxiv_id": "1901.09006",
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrated that Deep Sets-style sum decompositions can require large widths and suffer finite-precision collisions, motivating the need to quantify how strongly multisets are separated rather than whether they are separable."
    }
  ],
  "synthesis_narrative": "Permutation-invariant learning on multisets was grounded by Deep Sets, which introduced the sum-decomposition f(X)=\u03c1(\u2211x\u2208X \u03c6(x)) and established the architectural template for multiset encoders studied in graph neural networks. Building on this, Xu et al. connected message passing GNNs to the 1-WL test and proposed the GIN as an injective multiset aggregator, formalizing a binary notion of separability. Morris et al. extended the WL\u2013GNN correspondence to higher orders, sharpening the expressivity hierarchy and the idea of \u2018maximally separating\u2019 architectures. Maron et al. developed invariant/equivariant tensor-based networks with provable power beyond 1-WL, offering canonical \u201cmaximal\u201d expressivity baselines. Keriven and Peyr\u00e9 proved universality of invariant/equivariant GNNs under continuity, emphasizing that regularity assumptions matter in function-space characterizations. In parallel, Gama et al. initiated Lipschitz-style stability analyses for GNNs under input and graph perturbations, providing a formal language for quantitative robustness rather than mere distinguishability. Complementing these, Wagstaff et al. showed that sum-decomposition models can need large widths and collide under finite precision, revealing that theoretical separability may be numerically weak. Together, these works expose a gap: expressivity results certify whether separation exists, but not how strong, stable, or practically meaningful it is under finite precision and width constraints. The present paper synthesizes the WL-based separability viewpoint with stability theory, adapting Lipschitz/H\u00f6lder notions to parametric networks and introducing a H\u00f6lder-in-expectation, pairwise framework that quantifies separation quality without requiring global separability, naturally addressing the empirical weaknesses highlighted by prior multiset and GNN studies.",
  "target_paper": {
    "title": "On the H\u00f6lder Stability of Multiset and Graph Neural Networks",
    "authors": "Yair Davidson, Nadav Dym",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "graph neural networks, message passing neural networks, multiset neural networks, neural network stability, expressive power, WL tests",
    "abstract": "Extensive research efforts have been put into characterizing and constructing maximally separating multiset and graph neural networks. \nHowever, recent empirical evidence suggests the notion of separation itself doesn't capture several interesting phenomena. On the one hand, the quality of this separation may be very weak, to the extent that the embeddings of  \"separable\" objects might even be considered identical when using fixed finite precision. On the other hand, architectures which aren't capable of separation in theory, somehow achieve separation when taking the network to be wide enough.\n\nIn this work, we address both of these issues, by proposing a novel pair-wise separation quality analysis framework which is based on an adaptation of Lipschitz and H\u00f6lder stability to parametric functions. The proposed framework, which we name H\u00f6lder in expectation, allows for separation quality analysis, without restricting the analysis to embeddings that can separate all the input space simu",
    "openreview_id": "P7KIGdgW8S",
    "forum_id": "P7KIGdgW8S"
  },
  "analysis_timestamp": "2026-01-06T17:41:11.908106"
}
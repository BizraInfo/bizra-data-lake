{
  "prior_works": [
    {
      "title": "World Models",
      "authors": "David Ha; J\u00fcrgen Schmidhuber",
      "year": 2018,
      "role": "world-model precursor",
      "relationship_sentence": "Introduced the idea of learning a generative world model and using imagination (dreaming) for policy learning, establishing the core paradigm that DMWM extends with a logic-guided second system."
    },
    {
      "title": "Learning Latent Dynamics for Planning from Pixels (PlaNet)",
      "authors": "Danijar Hafner et al.",
      "year": 2019,
      "role": "RSSM foundation",
      "relationship_sentence": "Proposed the Recurrent State-Space Model (RSSM) for latent dynamics and planning; DMWM\u2019s System 1 (RSSM-S1) is architecturally grounded in this formulation."
    },
    {
      "title": "Dreamer: Reinforcement Learning with Imagination",
      "authors": "Danijar Hafner; Timothy P. Lillicrap; Jimmy Ba; Mohammad Norouzi",
      "year": 2020,
      "role": "latent imagination and policy learning",
      "relationship_sentence": "Demonstrated sample-efficient policy learning via latent imagination rollouts using RSSM; DMWM builds on this but addresses error compounding by adding logic-driven guidance."
    },
    {
      "title": "Thinking, Fast and Slow",
      "authors": "Daniel Kahneman",
      "year": 2011,
      "role": "theoretical inspiration (dual-process theory)",
      "relationship_sentence": "Provides the System 1/System 2 cognitive framework that DMWM operationalizes by coupling intuitive RSSM dynamics with a deliberative logic-reasoning module."
    },
    {
      "title": "Neural Logic Machines",
      "authors": "Honghua Dong; Jiayuan Mao; Tian Lin; Chong Wang; Lihong Li; Denny Zhou",
      "year": 2019,
      "role": "hierarchical neural-symbolic reasoning",
      "relationship_sentence": "Introduces differentiable, multi-step logical reasoning over learned predicates; informs DMWM\u2019s LINN-S2 design for hierarchical deep logical reasoning to guide imagination."
    },
    {
      "title": "DeepProbLog: Neural Probabilistic Logic Programming",
      "authors": "Robin Manhaeve; Sebastijan Duman\u010di\u0107; Angelika Kimmig; Thomas Demeester; Luc De Raedt",
      "year": 2018,
      "role": "neurosymbolic integration",
      "relationship_sentence": "Shows how to integrate neural perception with probabilistic logic in a single differentiable framework, a key template for DMWM\u2019s logic-integrated neural component."
    },
    {
      "title": "Learning and Reasoning with Logic Tensor Networks",
      "authors": "Luciano Serafini; Artur d'Avila Garcez",
      "year": 2016,
      "role": "logic constraints as differentiable objectives",
      "relationship_sentence": "Provides mechanisms to encode first-order logical constraints as continuous losses, directly inspiring DMWM\u2019s inter-system feedback that enforces logical consistency during imagination."
    }
  ],
  "synthesis_narrative": "DMWM\u2019s core innovation is to marry an RSSM-based world model with a deliberative, logic-integrated reasoning module that constrains long-horizon imagination to be logically consistent. This builds directly on the trajectory of latent world modeling for control: Ha and Schmidhuber\u2019s World Models established the imagination paradigm, while PlaNet introduced the RSSM formulation that underpins reliable latent dynamics learning. Dreamer then showed how imagined rollouts in RSSM space can drive sample-efficient policy learning, but also highlighted the practical limits of long-horizon prediction due to error compounding and weak multi-step consistency.\nTo overcome this, DMWM operationalizes dual-process theory (Kahneman) by assigning intuitive, fast state propagation to System 1 (RSSM-S1) and deliberate, rule-driven guidance to System 2 (LINN-S2). The design of LINN-S2 is informed by neurosymbolic reasoning advances: Neural Logic Machines demonstrate how to perform hierarchical multi-hop inference over learned predicates, a capability DMWM leverages to vet and steer imagined trajectories. DeepProbLog provides a blueprint for integrating neural perception with probabilistic logical semantics in a differentiable fashion, enabling end-to-end training with logical supervision. Finally, Logic Tensor Networks motivate encoding first-order constraints as differentiable penalties, which DMWM adapts into an inter-system feedback mechanism that regularizes imagination toward rule-consistent futures. Together, these works directly scaffold DMWM\u2019s contribution: a dual-mind world model that achieves longer-term, logically coherent imagination for improved policy learning.",
  "analysis_timestamp": "2026-01-07T00:05:12.521758"
}
{
  "prior_works": [
    {
      "title": "Efficient and Accurate Estimation of Lipschitz Constant of Deep Neural Networks",
      "authors": "J. Fazlyab et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "Introduced the SDP/IQC-based global \u21132 Lipschitz certification that this paper exactly matches; Wang and Manchester give a smooth, complete parameterization of precisely the set of networks satisfying this SDP bound."
    },
    {
      "title": "Semidefinite Relaxations for Certifying Robustness to Adversarial Examples",
      "authors": "A. Raghunathan et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrated the strength but high computational cost of SDP-based robustness certification; the new work removes this bottleneck by replacing post-hoc SDPs with a direct, trainable parameterization of the SDP-feasible set."
    },
    {
      "title": "Lipschitz Regularity of Deep Neural Networks: Analysis and Efficient Training",
      "authors": "H. Virmaux et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Provided layer-wise spectral-norm-based Lipschitz control and training heuristics that are conservative; the proposed sandwich-layer parameterization supersedes these by achieving the tight SDP-certified global bound."
    },
    {
      "title": "Parseval Networks: Improving Robustness to Adversarial Examples",
      "authors": "M. Ciss\u00e9 et al.",
      "year": 2017,
      "role": "Baseline",
      "relationship_sentence": "Early direct construction of near-1-Lipschitz networks via orthonormal (Parseval) constraints; the present paper improves on this line by parameterizing exactly the SDP-tight class rather than enforcing approximate orthogonality."
    },
    {
      "title": "Sorting out Lipschitz Function Approximation",
      "authors": "C. Anil et al.",
      "year": 2019,
      "role": "Baseline",
      "relationship_sentence": "Proposed direct 1-Lipschitz architectures (e.g., GroupSort with spectral normalization) that guarantee bounds but can be restrictive/conservative; Wang and Manchester instead directly parameterize the full SDP-feasible set with completeness."
    },
    {
      "title": "Regularisation of Neural Networks by Enforcing Lipschitz Continuity",
      "authors": "O. Gouk et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Used projections/regularizers to enforce Lipschitz constraints, incurring computational overhead; the new work avoids such inner approximation/projection steps via a smooth surjective parameterization that enforces the constraint by construction."
    },
    {
      "title": "The Singular Values of Convolutional Layers",
      "authors": "M. Sedghi et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "Characterized linear operator norms of convolutions, informing Lipschitz control in CNNs; the present paper extends its direct SDP-tight parameterization from fully connected to convolutional layers leveraging this operator-view."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014an exact, smooth parameterization of deep networks that guarantees the tightest-known global \u21132 Lipschitz bounds\u2014emerges from the SDP/IQC line of work on robustness certification. Fazlyab et al. (2019) provided the key foundation by casting the global Lipschitz computation as a semidefinite program via incremental quadratic constraints, yielding tight certificates but requiring heavy optimization. Earlier, Raghunathan et al. (2018) established the promise of SDP relaxations for certification more broadly, while highlighting their computational burden in training loops. In contrast to layer-wise spectral-norm heuristics and orthogonality-based constructions, such as Virmaux & Scaman (2018), Parseval Networks (Ciss\u00e9 et al., 2017), and GroupSort-based 1-Lipschitz architectures (Anil et al., 2019), which provide conservative or restrictive guarantees and often rely on iterative projections or special activations, Wang and Manchester give a complete and direct parameterization of exactly the SDP-feasible set. This eliminates inner approximations, projections, and barrier terms, enabling standard gradient training with guarantees equivalent to the SDP certificate. Works like Gouk et al. (2018) underscore the practical limitations of projection-based Lipschitz enforcement that the new parameterization sidesteps. Finally, for convolutional networks, operator-theoretic insights into convolutional singular values (Sedghi et al., 2019) inform how to extend the SDP-tight parameterization from fully connected layers to CNNs. Together, these threads motivate and directly shape a parameter-sharing \u2018sandwich layer\u2019 that turns tight SDP certification into a tractable, complete, and trainable architecture.",
  "analysis_timestamp": "2026-01-06T23:09:26.545208"
}
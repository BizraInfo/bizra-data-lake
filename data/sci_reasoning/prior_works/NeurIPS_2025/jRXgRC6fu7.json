{
  "prior_works": [
    {
      "title": "Attributes as Operators: Factorizing unseen attribute-object compositions",
      "authors": [
        "Tushar Nagarajan",
        "Kristen Grauman"
      ],
      "year": 2018,
      "role": "Compositional state modeling",
      "relationship_sentence": "SAGE\u2019s decomposition of object states into shareable, language-described visual concepts mirrors the A-O idea of modeling attributes as operators on object embeddings, enabling generalization to unseen state\u2013object/action compositions."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": [
        "Alec Radford et al."
      ],
      "year": 2021,
      "role": "Vision\u2013language backbone for open-world concepts",
      "relationship_sentence": "SAGE relies on VLMs to ground and refine language-defined state concepts in visual data, directly building on CLIP\u2019s text-aligned embedding space for open-world generalization."
    },
    {
      "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
      "authors": [
        "Ranjay Krishna et al."
      ],
      "year": 2017,
      "role": "Graph-structured visual knowledge (objects\u2013attributes\u2013relations)",
      "relationship_sentence": "SAGE\u2019s State-Action Graph extends scene-graph style representations of objects, attributes, and relations popularized by Visual Genome to explicitly capture action-induced state transitions."
    },
    {
      "title": "Learning by Abstraction: The Neural State Machine",
      "authors": [
        "Drew A. Hudson",
        "Christopher D. Manning"
      ],
      "year": 2019,
      "role": "Structured latent graph for visual reasoning",
      "relationship_sentence": "SAGE generalizes the Neural State Machine\u2019s idea of a structured, concept-level graph by introducing dynamic state-action edges and multimodal updating for video-based state transitions."
    },
    {
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan)",
      "authors": [
        "Michael Ahn et al."
      ],
      "year": 2022,
      "role": "LLM priors grounded by perception/action",
      "relationship_sentence": "SAGE\u2019s pipeline of using an LLM to propose a structured action/state prior and then grounding/refining it with visual models parallels SayCan\u2019s LLM-for-structure plus perceptual grounding paradigm."
    },
    {
      "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
      "authors": [
        "Andy Zeng et al."
      ],
      "year": 2022,
      "role": "Multimodal LLM\u2013VLM collaboration",
      "relationship_sentence": "SAGE adopts the Socratic Models principle of letting LLMs and VLMs iteratively inform each other, using language to scaffold structure and vision to validate/refine it for state transitions."
    },
    {
      "title": "CLEVRER: Collision Events for Video Representation and Reasoning",
      "authors": [
        "Kexin Yi et al."
      ],
      "year": 2020,
      "role": "Temporal causal reasoning about states/events",
      "relationship_sentence": "SAGE\u2019s focus on modeling physical state changes over time echoes CLEVRER\u2019s event-centric, temporally grounded reasoning, but instantiates it in an open-world, language-labeled concept space."
    }
  ],
  "synthesis_narrative": "SAGE\u2019s key innovation\u2014unifying object state recognition and state transitions via a language-grounded, shareable concept graph built by LLMs and refined by VLMs\u2014sits at the confluence of compositional vision, scene-graph reasoning, and LLM-guided grounding. The compositional backbone comes from attribute-based recognition, particularly Attributes-as-Operators, which demonstrated that attributes (states) can act as transferable transformations over object embeddings to achieve generalization to unseen attribute\u2013object pairs. SAGE extends this idea to state\u2013action dynamics, treating states as reusable, language-defined concepts across objects and actions.\nScene-graph traditions from Visual Genome and structured reasoning with the Neural State Machine contribute the representational bias: entities, attributes, and relations instantiated as nodes/edges, now augmented in SAGE with explicit action edges and temporal transitions. For open-world grounding, CLIP provides the text-aligned embedding space that lets SAGE operationalize language-defined state concepts and transfer them to novel objects and actions. On the algorithmic side, SayCan and Socratic Models supply the blueprint for coupling LLM structural priors with perceptual validation\u2014SAGE uses an LLM to draft a State-Action Graph and then employs VLM-based multimodal checks to refine and ground it. Finally, CLEVRER\u2019s emphasis on temporally localized causal events informs SAGE\u2019s treatment of state changes as temporally anchored transitions in video. Together, these works directly enable SAGE\u2019s unified, generalizable framework for recognizing and reasoning about object states and their transformations.",
  "analysis_timestamp": "2026-01-07T00:05:12.546978"
}
{
  "prior_works": [
    {
      "title": "Llama Guard: Open and Transparent Safety Classifiers for LLMs",
      "authors": "Sriram et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "This work is the primary guardrail baseline that performs category-wise safety classification largely independently, and R^2-Guard directly improves on it by adding a knowledge-enhanced logical reasoning layer to capture inter-category dependencies."
    },
    {
      "title": "OpenAI Moderation Models",
      "authors": "OpenAI",
      "year": 2023,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "As a widely used moderation baseline that outputs per-category unsafety scores without explicit relational reasoning, it motivates R^2-Guard\u2019s design of using such scores as inputs to a reasoning module that jointly infers correlated safety risks."
    },
    {
      "title": "ThinkGuard: Language Models Can Defend Themselves by Thinking Step-by-Step",
      "authors": "Wang et al.",
      "year": 2024,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By showing that explicit chain-of-thought style reasoning improves safety judgments, this work inspires R^2-Guard\u2019s move from pure classification to reasoning\u2014extended here with structured, knowledge-grounded logical inference rather than free-form rationales."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "authors": "Zou et al.",
      "year": 2023,
      "arxiv_id": "2307.15043",
      "role": "Gap Identification",
      "relationship_sentence": "This paper\u2019s jailbreak attacks expose the brittleness of existing guardrails, directly motivating R^2-Guard\u2019s robustness goal via cross-category constraints and reasoning that resist prompt-based evasions."
    },
    {
      "title": "Classifier Chains for Multi-Label Classification",
      "authors": "Read et al.",
      "year": 2011,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work establishes that modeling label dependencies boosts multi-label performance, especially for rare labels, a key insight R^2-Guard adapts by propagating information across correlated safety categories through logic rather than chain orderings."
    },
    {
      "title": "Probabilistic Soft Logic",
      "authors": "Broecheler et al.",
      "year": 2010,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Providing a framework for fusing uncertain predictions with soft logical constraints, PSL underpins R^2-Guard\u2019s core idea of refining category probabilities via knowledge-enhanced logical reasoning over a safety ontology."
    }
  ],
  "synthesis_narrative": "Llama Guard introduced an open safety classifier for LLMs that treats safety categories largely independently, operationalizing policy prompts into multi-label decisions without explicit modeling of inter-category structure. OpenAI\u2019s Moderation Models similarly output per-category unsafety scores, emphasizing reliable category-wise detection rather than relational reasoning among classes. ThinkGuard demonstrated that adding explicit reasoning\u2014via step-by-step rationales\u2014improves safety judgments over pure classification, pointing to the value of making the decision process more deliberate. In parallel, adversarial work on universal and transferable jailbreaks revealed how prompt-based attacks can systematically evade standard guardrails, highlighting that surface-level signals are brittle when categories are judged in isolation. From the multi-label learning literature, classifier chains established that modeling label dependencies improves performance, particularly for long-tail classes, by leveraging inter-label correlations. Finally, Probabilistic Soft Logic showed how to integrate uncertain predictions with soft logical constraints, enabling structured inference over knowledge graphs or ontologies to correct and calibrate raw scores.\nTogether these strands created a clear opportunity: use the strong category-wise signals of existing guardrails as inputs, but upgrade the decision layer with explicit, knowledge-grounded logical reasoning that encodes inter-category relations and policy structure. By fusing multi-label dependency insights with PSL-style soft logic, and by adopting a reasoning mindset inspired by ThinkGuard, the approach naturally addresses long-tail categories and increases resistance to jailbreaks through cross-category constraints, while remaining flexible to new safety classes by editing the knowledge and rules rather than retraining monolithic classifiers.",
  "target_paper": {
    "title": "$R^2$-Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning",
    "authors": "Mintong Kang, Bo Li",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "LLM guardrail model, content moderation",
    "abstract": "As large language models (LLMs) become increasingly prevalent across various applications, it is critical to establish safety guardrails to moderate input/output content of LLMs and ensure compliance with safety policies. Existing guardrail models, such as OpenAI Mod and LlamaGuard, treat various safety categories (e.g., self-harm, self-harm/instructions) independently and fail to explicitly capture the intercorrelations among them. This has led to limitations such as ineffectiveness due to inadequate training on long-tail data from correlated safety categories, susceptibility to jailbreaking attacks, and inflexibility regarding new safety categories.\nTo address these limitations, we propose $R^2$-Guard, a robust reasoning enabled LLM guardrail via knowledge-enhanced logical reasoning. Specifically, $R^2$-Guard comprises two parts: data-driven guardrail models and reasoning components. The data-driven guardrail models provide unsafety probabilities of moderated content on different saf",
    "openreview_id": "CkgKSqZbuC",
    "forum_id": "CkgKSqZbuC"
  },
  "analysis_timestamp": "2026-01-06T13:19:37.411970"
}
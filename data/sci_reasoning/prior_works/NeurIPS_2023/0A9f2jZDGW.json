{
  "prior_works": [
    {
      "title": "Editing Models with Task Arithmetic",
      "authors": "Gabriel Ilharco, Suchin Gururangan, Mitchell Wortsman, Matthew Peters, Ali Farhadi, Ludwig Schmidt",
      "year": 2023,
      "role": "originating task arithmetic phenomenon",
      "relationship_sentence": "This paper established that adding and subtracting fine-tuned weight deltas composes and negates capabilities, providing the empirical phenomenon and benchmarks that the NeurIPS 2023 work analyzes mechanistically and improves via tangent-space fine-tuning."
    },
    {
      "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without extra training",
      "authors": "Mitchell Wortsman, Gabriel Ilharco, Samir Gadre, Rebecca Roelofs, Raphael Gontijo Lopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, Ludwig Schmidt",
      "year": 2022,
      "role": "weight-space composition precedent",
      "relationship_sentence": "By showing that simple averaging of fine-tuned checkpoints can improve performance, this work supplied core evidence that linear operations in weight space can preserve and combine functions, a premise leveraged and refined by tangent-space task arithmetic."
    },
    {
      "title": "Mode connectivity in loss landscapes of neural networks",
      "authors": "Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, Andrew Gordon Wilson",
      "year": 2018,
      "role": "loss-landscape foundation for linear combinations",
      "relationship_sentence": "Mode connectivity demonstrated low-loss linear paths between solutions, supporting the idea that linear interpolation/addition in parameter space can maintain function, which underpins both task arithmetic and the paper\u2019s disentanglement-based explanation."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "theoretical basis for tangent-space linearization",
      "relationship_sentence": "NTK formalizes linearizing a network around parameters and optimizing in the tangent space, directly enabling the paper\u2019s method of fine-tuning via a linearized model to amplify weight disentanglement and improve task arithmetic."
    },
    {
      "title": "Wide Neural Networks of Any Depth Evolve as Linear Models under Gradient Descent",
      "authors": "Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, Jeffrey Pennington",
      "year": 2019,
      "role": "validation of linearized training dynamics",
      "relationship_sentence": "This work shows that, in the wide-network regime, training dynamics are well-approximated by a linearized model, reinforcing the paper\u2019s claim that operating in the tangent space yields predictable, less entangled parameter updates."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever",
      "year": 2021,
      "role": "base vision-language pretraining",
      "relationship_sentence": "CLIP provides the pre-trained vision-language models on which task vectors are computed, and its pretraining-induced structure is central to the paper\u2019s observation that weight disentanglement emerges and can be amplified by tangent-space finetuning."
    },
    {
      "title": "ROME: Locating and Editing Factual Associations in GPT",
      "authors": "Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov",
      "year": 2022,
      "role": "model editing via local linearization",
      "relationship_sentence": "ROME uses Jacobian-based local linearization to compute targeted weight updates with limited side effects, inspiring the paper\u2019s function-space view of weight edits and motivating the use of tangent-space optimization to control interference."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014showing that task arithmetic improves when models are edited in their tangent space and that weight disentanglement underlies its effectiveness\u2014builds directly upon two intertwined threads: weight-space composition and linearized training. Ilharco et al.\u2019s task arithmetic crystallized the empirical fact that adding or subtracting task-specific deltas can compose or erase capabilities, while model soups and mode connectivity established that simple linear operations in parameter space can preserve and even enhance function. These empirical precedents suggest a latent linear structure in pre-trained networks that the NeurIPS 2023 paper seeks to explain through disentangled directions in weight space.\n\nThe theoretical machinery enabling this explanation comes from the Neural Tangent Kernel and subsequent work showing that wide networks evolve as linear models under gradient descent. By adopting a linearization around pre-trained weights, the paper effectively treats fine-tuning as optimization in a fixed tangent feature space, which both clarifies and strengthens disentanglement: task updates align with approximately orthogonal Jacobian features, reducing interference and boosting arithmetic performance. ROME complements this perspective by demonstrating that Jacobian-based, locally linear edits can target functions while limiting collateral changes, reinforcing the merits of operating in tangent space for controlled model editing. Finally, CLIP provides the practical substrate: large, pre-trained vision-language models where pretraining induces the emergent disentanglement the paper measures and exploits. Together, these works motivate and technically ground the shift from naive weight addition to principled tangent-space editing for robust task arithmetic.",
  "analysis_timestamp": "2026-01-07T00:02:04.819567"
}
{
  "prior_works": [
    {
      "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
      "authors": "Tolga Bolukbasi et al.",
      "year": 2016,
      "role": "Foundational representation debiasing",
      "relationship_sentence": "Introduced identifying a bias subspace and neutralizing it in embeddings, directly inspiring SFID\u2019s selective removal/pruning of bias-aligned feature directions while preserving task semantics."
    },
    {
      "title": "Removing Protected Attributes by Iterative Nullspace Projection (INLP)",
      "authors": "Shauli Ravfogel et al.",
      "year": 2020,
      "role": "Post-hoc, model-agnostic debiasing",
      "relationship_sentence": "Demonstrated that one can post-hoc remove attribute information from representations across tasks without retraining, a key precedent for SFID\u2019s unified, training-free debiasing across modalities."
    },
    {
      "title": "Amnesic Probing: Behavioral Explanation of Neural Representations",
      "authors": "Yanai Elazar et al.",
      "year": 2021,
      "role": "Guidance on selective attribute removal with utility preservation",
      "relationship_sentence": "Showed how targeted removal of attributes can trade off with utility, motivating SFID\u2019s selective feature imputation to maintain semantic integrity while suppressing biased evidence."
    },
    {
      "title": "Network Dissection: Quantifying Interpretability of Deep Visual Representations",
      "authors": "David Bau et al.",
      "year": 2017,
      "role": "Concept-level unit identification for pruning",
      "relationship_sentence": "Provided methods to map units to human-interpretable concepts, informing SFID\u2019s feature pruning strategy to excise bias-correlated channels/features rather than wholesale model changes."
    },
    {
      "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
      "authors": "Been Kim et al.",
      "year": 2018,
      "role": "Concept-direction estimation in latent spaces",
      "relationship_sentence": "Established estimating concept directions and their influence, underpinning SFID\u2019s identification of bias-aligned feature directions for pruning and imputation in shared VLM embeddings."
    },
    {
      "title": "RUBi: Reducing Unimodal Biases for Visual Question Answering",
      "authors": "R\u00e9mi Cadene et al.",
      "year": 2019,
      "role": "Confidence/gating-based bias suppression",
      "relationship_sentence": "Introduced confidence-based gating to suppress biased shortcuts, directly influencing SFID\u2019s low confidence imputation (LCI) that triggers corrective feature imputation when reliance on biased cues is detected."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "role": "Core VLM backbone and evaluation setting",
      "relationship_sentence": "Provided the widely used vision\u2013language representation and zero-shot paradigm that SFID operates on, enabling unified, post-hoc debiasing across classification, retrieval, captioning, and generation."
    }
  ],
  "synthesis_narrative": "SFID\u2019s unified, training-free debiasing strategy is rooted in two converging lines of work: representation-level concept/attribute removal and confidence-driven bias suppression at inference. From Bolukbasi et al., the paper inherits the core idea that bias is often encoded along identifiable subspaces that can be neutralized without end-to-end retraining. INLP extends this to a general, post-hoc procedure for stripping protected attributes from intermediate representations across tasks, a principle SFID leverages to remain modality- and task-agnostic. Amnesic Probing underscores the need to preserve downstream utility when excising attributes, motivating SFID\u2019s selective design and its use of imputation to retain semantics.\nInterpretability tools like Network Dissection and TCAV inform how to localize and quantify concept-aligned directions or units, directly shaping SFID\u2019s feature pruning to target bias-correlated channels rather than blunt model edits. Complementing this structural component, RUBi contributes the insight that confidence can signal shortcut reliance; SFID\u2019s low confidence imputation (LCI) operationalizes this by replacing uncertain, bias-prone evidence with neutral alternatives at test time. Finally, CLIP provides the shared vision\u2013language embedding space and zero-shot operating regime where post-hoc, feature-level interventions can generalize, enabling SFID to work across image\u2013text classification, retrieval, captioning, and even generation. Together, these works culminate in SFID\u2019s selective pruning plus confidence-triggered imputation, achieving broad, retraining-free debiasing while maintaining semantic fidelity.",
  "analysis_timestamp": "2026-01-06T23:39:42.955812"
}
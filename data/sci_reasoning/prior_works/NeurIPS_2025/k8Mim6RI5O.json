{
  "prior_works": [
    {
      "title": "Semi-Supervised Learning by Entropy Minimization",
      "authors": "Yves Grandvalet, Yoshua Bengio",
      "year": 2004,
      "role": "Theoretical foundation",
      "relationship_sentence": "EMPO\u2019s core idea\u2014driving a model to make confident predictions on unlabeled inputs by minimizing entropy\u2014directly instantiates Grandvalet & Bengio\u2019s entropy-minimization principle in the context of LLM reasoning."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Denny Zhou",
      "year": 2023,
      "role": "Unsupervised signal design",
      "relationship_sentence": "EMPO operationalizes an unsupervised training signal by minimizing semantic disagreement across multiple samples, building on Self-Consistency\u2019s insight that agreement among sampled chains correlates with correctness."
    },
    {
      "title": "Proximal Policy Optimization Algorithms",
      "authors": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov",
      "year": 2017,
      "role": "Algorithmic building block",
      "relationship_sentence": "EMPO is framed as a policy optimization method; its update mechanics and stability constraints are adapted from PPO to optimize a nonstandard (entropy-based) objective for LLM policies."
    },
    {
      "title": "Fine-Tuning Language Models from Human Preferences",
      "authors": "Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, Jan Leike, et al.",
      "year": 2019,
      "role": "KL-regularized RL for LMs",
      "relationship_sentence": "EMPO inherits the RLHF recipe of KL-regularized policy updates against a reference model, but replaces preference-derived rewards with an unsupervised semantic-entropy signal."
    },
    {
      "title": "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence",
      "authors": "Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Han Zhang",
      "year": 2020,
      "role": "SSL methodology (consistency + low-entropy)",
      "relationship_sentence": "EMPO mirrors FixMatch\u2019s consistency-regularization and confidence-driven (low-entropy) training on unlabeled data, adapting these SSL principles to open-ended reasoning via semantic agreement."
    },
    {
      "title": "Self-Taught Reasoner: Bootstrapping Reasoning with Reasoning",
      "authors": "Eric Zelikman, Yuhuai (Tony) Wu, Jesse Mu, Noah D. Goodman",
      "year": 2022,
      "role": "Self-improvement paradigm for reasoning",
      "relationship_sentence": "EMPO extends the self-improvement ethos of STaR\u2014using the model\u2019s own generations to improve reasoning\u2014by removing dependence on labeled answers and optimizing an unsupervised entropy signal instead."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, et al.",
      "year": 2022,
      "role": "Reducing external supervision via AI feedback",
      "relationship_sentence": "EMPO is conceptually aligned with replacing external supervision: like Constitutional AI reduces human labels via AI feedback, EMPO eliminates external reward models by using intrinsic semantic-entropy as the training signal."
    }
  ],
  "synthesis_narrative": "EMPO\u2019s key contribution\u2014fully unsupervised incentivization of LLM reasoning by minimizing semantic entropy\u2014rests on fusing classic entropy-minimization from semi-supervised learning with modern policy optimization for language models. The core theoretical impetus comes from Grandvalet and Bengio\u2019s entropy minimization, which prescribes pushing model predictions on unlabeled inputs toward confident, low-entropy distributions. EMPO translates this to open-ended reasoning by leveraging the Self-Consistency finding that agreement among multiple sampled chains correlates with correctness; disagreement becomes a measurable, label-free proxy for uncertainty that the algorithm seeks to reduce.\nAlgorithmically, EMPO adopts the KL-regularized policy optimization scaffolding established by PPO and RLHF (Ziegler et al.), ensuring stable updates to an LLM policy while constraining divergence from a reference model. Where RLHF relies on preference or reward models, EMPO replaces the reward with a semantic-entropy signal computed over generations, paralleling SSL methods like FixMatch that couple consistency regularization with confidence-driven training to exploit unlabeled data. Relative to prior self-improvement approaches for reasoning such as STaR\u2014which still leverage answer labels\u2014EMPO removes external supervision entirely, optimizing only on unlabeled questions. Finally, its philosophical alignment with Constitutional AI underscores a broader shift from human-intensive supervision toward intrinsic or AI-mediated signals; EMPO advances this trajectory by demonstrating that semantic agreement alone can drive sizable reasoning gains without labels or reward models.",
  "analysis_timestamp": "2026-01-07T00:21:32.343951"
}
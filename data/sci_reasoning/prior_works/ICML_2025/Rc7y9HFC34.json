{
  "prior_works": [
    {
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer",
      "year": 2022,
      "role": "Foundational conditioning mechanism for diffusion via cross-attention",
      "relationship_sentence": "ConceptAttention directly builds on the insight from latent diffusion that text conditioning is implemented through cross-attention maps, using them as the prevailing baseline it seeks to surpass with sharper, more faithful concept localization derived from attention outputs."
    },
    {
      "title": "DiT: Self-Attention Diffusion Models",
      "authors": "William Peebles, Saining Xie",
      "year": 2023,
      "role": "Architectural basis introducing diffusion transformers with rich attention layers",
      "relationship_sentence": "The method relies on the multi-head attention structure of DiTs; ConceptAttention repurposes DiT attention parameters and output spaces to compute contextualized concept embeddings without additional training."
    },
    {
      "title": "Prompt-to-Prompt Image Editing with Cross Attention Control",
      "authors": "Amir Hertz, Ron Mokady, Rinon Gal, Omer Tov, Daniel Cohen-Or",
      "year": 2022,
      "role": "Established cross-attention maps as spatial carriers of textual concepts in diffusion models",
      "relationship_sentence": "By showing that cross-attention maps can localize and control concepts during generation, this work motivates ConceptAttention\u2019s comparison point and highlights the limitations of raw cross-attention maps that ConceptAttention addresses."
    },
    {
      "title": "Transformer Interpretability Beyond Attention Visualization",
      "authors": "Hila Chefer, Shir Gur, Lior Wolf",
      "year": 2021,
      "role": "Demonstrated that attention weights alone are insufficient and proposed parameter-aware attribution",
      "relationship_sentence": "ConceptAttention\u2019s use of attention layer parameters and output projections echoes the idea that meaningful explanations emerge when incorporating learnable projections, not just raw attention weights."
    },
    {
      "title": "Emerging Properties in Self-Supervised Vision Transformers",
      "authors": "Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, Armand Joulin",
      "year": 2021,
      "role": "Showed ViT attention heads contain object/region information usable for segmentation",
      "relationship_sentence": "The observation that attention features in transformers naturally encode segmentable structure inspires ConceptAttention\u2019s pursuit of high-quality saliency masks from internal attention representations."
    },
    {
      "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
      "authors": "Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Vi\u00e9gas, Rory Sayres",
      "year": 2018,
      "role": "Introduced linear concept directions in representation spaces for interpretability",
      "relationship_sentence": "ConceptAttention\u2019s central move\u2014linear projections in the attention output space to obtain concept embeddings\u2014draws on the TCAV principle that linear directions can capture human-meaningful concepts in deep representations."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, et al.",
      "year": 2021,
      "role": "Established powerful text\u2013image alignment and popularized zero-shot vision tasks",
      "relationship_sentence": "By framing textual concepts as vectors aligned with visual features, CLIP informs ConceptAttention\u2019s text-driven concept localization objective and the evaluation on zero-shot segmentation benchmarks."
    }
  ],
  "synthesis_narrative": "ConceptAttention emerges at the intersection of three lines of work: conditioning in diffusion models, interpretability of transformer attention, and concept-based linear probing. Latent Diffusion Models established cross-attention as the mechanism binding text to spatial image synthesis, and follow-up editing methods like Prompt-to-Prompt validated that these cross-attention maps carry usable spatial signals\u2014but also exposed their coarseness and instability as explanations. In parallel, Diffusion Transformers (DiT) re-architected diffusion to operate entirely with transformer attention layers, yielding rich, uniform attention blocks whose parameters and outputs can be systematically analyzed.\nWork on transformer interpretability underscored that attention weights alone are incomplete as explanations. Chefer et al. introduced parameter-aware attribution, advocating that meaningful explanations should incorporate the model\u2019s learned projections rather than visualizing raw weights. Complementarily, the ViT literature (e.g., DINO) showed that attention representations naturally encode object-level structure that can be converted into segmentation masks, suggesting that attention outputs may be a better substrate than cross-attention weights for localization. Finally, TCAV demonstrated that linear directions in deep representation spaces can correspond to human-understandable concepts.\nSynthesizing these insights, ConceptAttention repurposes the learned projections within DiT attention layers and applies linear projections in the attention output space to form contextualized concept embeddings, yielding sharper, more faithful saliency maps than cross-attention maps. This design leverages DiT\u2019s homogeneous attention blocks, TCAV\u2019s linear concept directions, and established text\u2013image alignment (CLIP) to achieve state-of-the-art zero-shot segmentation, with a formulation that also transfers to video diffusion transformers.",
  "analysis_timestamp": "2026-01-07T00:21:32.365267"
}
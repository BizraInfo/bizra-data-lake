{
  "prior_works": [
    {
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": "Edward J. Hu et al.",
      "year": 2022,
      "arxiv_id": "2106.09685",
      "role": "Foundation",
      "relationship_sentence": "LoRA provides the core low-rank adapter parameterization (W + BA) that SD-LoRA directly builds upon and re-parameterizes by separating update magnitude and direction across tasks."
    },
    {
      "title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
      "authors": "Liu et al.",
      "year": 2024,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "DoRA introduced decomposing weights into magnitude and direction within a LoRA-style update, and SD-LoRA extends this idea to the continual learning regime by continually decoupling and updating the LoRA components\u2019 magnitude and direction to achieve stability-plasticity without rehearsal."
    },
    {
      "title": "Learning to Prompt for Continual Learning",
      "authors": "Wang et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "L2P established rehearsal-free class-incremental learning with prompt pools that grow with tasks, a primary baseline whose pool-expansion scalability limitation SD-LoRA explicitly avoids by a single decoupled LoRA path."
    },
    {
      "title": "DualPrompt: Complementary Prompting for Rehearsal-Free Continual Learning",
      "authors": "Zhou et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "DualPrompt\u2019s complementary prompt pools improved rehearsal-free CIL but still required expanding and managing multiple prompts per task, a scalability gap SD-LoRA addresses by eliminating pool expansion through decoupled LoRA updates."
    },
    {
      "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks",
      "authors": "Tim Salimans et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Weight Normalization\u2019s decoupling of weight magnitude and direction motivates SD-LoRA\u2019s explicit separation of LoRA update magnitude and direction to stabilize optimization across sequential tasks."
    },
    {
      "title": "Continual Learning of Context-Dependent Processing in Neural Networks",
      "authors": "Guangshe Zeng et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "The OWM approach\u2019s idea of protecting prior knowledge by constraining update directions inspires SD-LoRA\u2019s focus on controlling the directional component of LoRA updates to reduce interference across tasks."
    },
    {
      "title": "Mode Connectivity in Loss Landscapes of Neural Networks",
      "authors": "Timur Garipov et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "The demonstration of low-loss trajectories and overlapping basins between solutions informs SD-LoRA\u2019s theoretical and empirical claim that decoupled LoRA updates can follow low-loss paths that intersect across tasks."
    }
  ],
  "synthesis_narrative": "Low-rank adaptation introduced a compact update parameterization by injecting trainable rank-constrained matrices into pretrained weights, enabling effective fine-tuning with very few parameters. Weight-decomposed adaptations advanced this by explicitly factorizing weight updates into magnitude and direction, revealing that decoupling these components can yield more stable and efficient optimization. Classical reparameterization via weight normalization showed that separating scale from direction simplifies optimization geometry and improves convergence behavior. In continual learning, Learning to Prompt demonstrated that rehearsal-free class-incremental learning is achievable by selecting task-appropriate prompts from a growing pool, while DualPrompt further improved accuracy via complementary prompt sets but retained the intrinsic pool expansion and management overhead. Orthogonal Weight Modification highlighted that preserving past knowledge can be achieved by controlling update directions, reducing interference across tasks without storing data. Concurrently, mode connectivity revealed that seemingly different solutions can be connected by low-loss paths and even share overlapping low-loss regions, suggesting that updates guided along appropriate directions can remain within shared basins. Together, these works suggested an opportunity: combine the parameter efficiency of low-rank adapters with an explicit magnitude\u2013direction decomposition to control interference, while avoiding the scalability issues of prompt or adapter pools. By continually decoupling and steering the directional component of low-rank updates along low-loss trajectories and managing magnitude separately, it becomes possible to achieve rehearsal-free, scalable class-incremental learning that preserves prior tasks and maintains parameter efficiency.",
  "target_paper": {
    "title": "SD-LoRA: Scalable Decoupled Low-Rank Adaptation for Class Incremental Learning",
    "authors": "Yichen Wu, Hongming Piao, Long-Kai Huang, Renzhen Wang, Wanhua Li, Hanspeter Pfister, Deyu Meng, Kede Ma, Ying Wei",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Continual learning; Low-rank adaptation",
    "abstract": "Continual Learning (CL) with foundation models has recently emerged as a promising paradigm to exploit abundant knowledge acquired during pre-training for tackling sequential tasks. However, existing prompt-based and Low-Rank Adaptation-based (LoRA-based) methods often require expanding a prompt/LoRA pool or retaining samples of previous tasks, which poses significant scalability challenges as the number of tasks grows. \nTo address these limitations, we propose Scalable Decoupled LoRA (SD-LoRA) for class incremental learning, which continually separates the learning of the magnitude and direction of LoRA components without rehearsal. Our empirical and theoretical analysis reveals that SD-LoRA tends to follow a low-loss trajectory and converges to an overlapping low-loss region for all learned tasks, resulting in an excellent stability-plasticity trade-off. Building upon these insights, we introduce two variants of SD-LoRA with further improved parameter efficiency. All parameters of SD",
    "openreview_id": "5U1rlpX68A",
    "forum_id": "5U1rlpX68A"
  },
  "analysis_timestamp": "2026-01-06T16:29:51.118552"
}
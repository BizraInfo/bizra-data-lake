{
  "prior_works": [
    {
      "title": "Distilling the Knowledge in a Neural Network",
      "authors": "Geoffrey Hinton, Oriol Vinyals, Jeff Dean",
      "year": 2015,
      "role": "Teacher\u2013student distillation framework for transferring behavior from a high-capacity model to a smaller one.",
      "relationship_sentence": "TimeX trains an interpretable surrogate to mimic a pretrained time-series model\u2019s behavior, extending the distillation paradigm to explanation models rather than compact predictors."
    },
    {
      "title": "Relational Knowledge Distillation",
      "authors": "Wonpyo Park, Dongju Kim, Yan Lu, Minsu Cho",
      "year": 2019,
      "role": "Introduced preserving inter-instance relations (e.g., pairwise distances/angles) between teacher and student feature spaces.",
      "relationship_sentence": "TimeX\u2019s core \u201cmodel behavior consistency\u201d directly echoes RKD by enforcing that relations in the teacher\u2019s latent space are preserved in the explainer\u2019s latent space."
    },
    {
      "title": "Learning to Explain: An Information-Theoretic Perspective on Model Interpretation (L2X)",
      "authors": "Jianbo Chen, Le Song, Martin J. Wainwright, Michael I. Jordan",
      "year": 2018,
      "role": "Trains a separate explainer to select a discrete subset of inputs that maximally preserves a black-box model\u2019s output.",
      "relationship_sentence": "TimeX similarly learns an explainer driven by fidelity to a pretrained model, leveraging a dedicated network to produce discrete attribution maps for time series."
    },
    {
      "title": "INVASE: Instance-wise Variable Selection using Neural Networks",
      "authors": "Jinsung Yoon, James Jordon, Mihaela van der Schaar",
      "year": 2018,
      "role": "A selector network performs instance-wise discrete feature selection to mimic a black-box predictor.",
      "relationship_sentence": "TimeX adopts the core idea of training a separate selector/explainer network guided by the target model\u2019s behavior, but adapts it to temporal segments and latent-space consistency."
    },
    {
      "title": "Learning Time-Series Shapelets",
      "authors": "Nils Grabocka, Nicolas Schilling, Martin Wistuba, Lars Schmidt-Thieme",
      "year": 2014,
      "role": "Pioneered learning discriminative temporal motifs (shapelets) as interpretable patterns driving classification.",
      "relationship_sentence": "TimeX\u2019s discrete attribution maps and emphasis on matching salient temporal patterns are conceptually aligned with shapelet-based interpretability for time series."
    },
    {
      "title": "This Looks Like That: Deep Learning for Interpretable Image Recognition (ProtoPNet)",
      "authors": "Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, Jonathan Su",
      "year": 2019,
      "role": "Learns a prototype-based latent space that provides case-based, landmark-style explanations.",
      "relationship_sentence": "TimeX\u2019s learned latent space of explanations and use of landmarks for visualization parallel ProtoPNet\u2019s prototype-based interpretability, adapted to time-series explanations."
    },
    {
      "title": "Self-Explaining Neural Networks",
      "authors": "David Alvarez-Melis, Tommi S. Jaakkola",
      "year": 2018,
      "role": "Introduces models that jointly learn interpretable concepts and provide faithful, stable explanations via an explanation latent space.",
      "relationship_sentence": "TimeX builds on the idea of learning an explanation latent space but ties it to a black-box model through behavior consistency to ensure faithfulness on time-series tasks."
    }
  ],
  "synthesis_narrative": "TimeX\u2019s key contribution\u2014training an interpretable time-series explainer that faithfully mimics a pretrained model by preserving latent-space relations\u2014sits at the intersection of distillation, instance-wise feature selection, and prototype/concept-based interpretability. The teacher\u2013student perspective of Hinton et al. established the central idea that one model can be trained to emulate another\u2019s behavior, which TimeX repurposes for explanations rather than compression. Crucially, TimeX\u2019s model behavior consistency operationalizes faithfulness by aligning relational structure between samples across teacher and explainer latent spaces, a direct conceptual extension of Relational Knowledge Distillation that prioritizes preserving pairwise/triangular relationships rather than only matching logits.\nBuilding a dedicated explainer aligns with L2X and INVASE, which train selector networks to identify discrete, instance-specific inputs that best preserve a black-box model\u2019s output. TimeX adapts this paradigm to time-series by producing discrete temporal attributions and coupling selection with relational consistency to ensure global behavioral fidelity, even without ground-truth explanation labels. To render explanations interpretable as patterns, TimeX connects to the shapelet literature, which formalized interpretable temporal motifs as drivers of classification. Finally, TimeX\u2019s learned latent space of explanations and its use of visualization landmarks resonate with ProtoPNet\u2019s prototype-based explanations and the broader principles of Self-Explaining Neural Networks, which advocate learning interpretable concepts with guarantees of faithfulness and stability. Together, these strands directly motivate TimeX\u2019s design: an explainer that selects discrete temporal evidence, organizes it in an interpretable latent space, and is constrained to be faithful by preserving the teacher\u2019s relational geometry.",
  "analysis_timestamp": "2026-01-06T23:42:49.055329"
}
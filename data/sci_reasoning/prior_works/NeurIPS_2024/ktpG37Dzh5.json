{
  "prior_works": [
    {
      "title": "Structured Bayesian Pruning via Log-Normal Multiplicative Noise",
      "authors": "Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, Dmitry Vetrov",
      "year": 2017,
      "role": "Bayesian structured pruning with multiplicative noise",
      "relationship_sentence": "BMRS adopts the core idea of group-wise stochastic gates drawn from log-normal multiplicative noise to enable end-to-end Bayesian structured pruning and extends it with model-evidence\u2013based selection."
    },
    {
      "title": "Variational Dropout Sparsifies Deep Neural Networks",
      "authors": "Dmitry Molchanov, Arsenii Ashukha, Dmitry Vetrov",
      "year": 2017,
      "role": "Sparsity via variational dropout and sparsity-inducing priors",
      "relationship_sentence": "BMRS builds on the variational dropout framework and sparsity-inducing Bayesian priors that enable pruning by learning noise scales which signal removable weights/structures."
    },
    {
      "title": "Variational Dropout and the Local Reparameterization Trick",
      "authors": "Diederik P. Kingma, Tim Salimans, Max Welling",
      "year": 2015,
      "role": "Low-variance stochastic gradients for multiplicative noise",
      "relationship_sentence": "BMRS leverages local reparameterization to stably train multiplicative noise variables (gates) at the neuron/filter level, a key ingredient for scalable Bayesian structured pruning."
    },
    {
      "title": "Bayesian model reduction and empirical Bayes for group (DCM)",
      "authors": "Karl J. Friston, Will D. Penny, et al.",
      "year": 2016,
      "role": "Bayesian Model Reduction (BMR) for efficient evidence under prior changes",
      "relationship_sentence": "BMRS directly imports BMR to compute changes in model evidence when tightening priors on gates, enabling fast evaluation of pruned candidates without full retraining."
    },
    {
      "title": "Learning Sparse Neural Networks through L0 Regularization",
      "authors": "Christos Louizos, Max Welling, Diederik P. Kingma",
      "year": 2018,
      "role": "Stochastic gating for structured sparsity",
      "relationship_sentence": "BMRS\u2019s structured gating perspective aligns with L0-style stochastic gates, but replaces ad-hoc penalties with a principled Bayesian evidence-based pruning criterion."
    },
    {
      "title": "A Practical Bayesian Framework for Backpropagation Networks",
      "authors": "David J. C. MacKay",
      "year": 1992,
      "role": "Model evidence and Occam\u2019s razor for neural networks",
      "relationship_sentence": "BMRS\u2019s core objective\u2014ranking/pruning structures by marginal likelihood\u2014traces to MacKay\u2019s evidence framework that balances fit and complexity in Bayesian neural networks."
    },
    {
      "title": "Sparse Bayesian Learning and the Relevance Vector Machine",
      "authors": "Michael E. Tipping",
      "year": 2001,
      "role": "Automatic Relevance Determination (ARD) sparsity principle",
      "relationship_sentence": "BMRS generalizes ARD-style relevance at the group level, using Bayesian evidence to identify and prune irrelevant neurons/filters under sparsity-promoting priors."
    }
  ],
  "synthesis_narrative": "BMRS fuses two strands of prior work to deliver a principled, end-to-end Bayesian method for structured pruning. From the pruning side, Neklyudov et al. introduced Bayesian structured pruning via log-normal multiplicative noise, establishing the idea of learnable stochastic gates at the neuron/filter level. This builds upon variational dropout and sparsity-inducing priors (Molchanov et al.) and relies on the local reparameterization trick (Kingma et al.) to obtain low-variance gradients for multiplicative noise, making group-wise stochastic gating scalable and effective. Conceptually related stochastic gating ideas from L0 regularization (Louizos et al.) further shaped the modern view of end-to-end trainable, structured sparsity mechanisms.\nFrom the Bayesian model selection side, BMRS directly incorporates Bayesian Model Reduction (Friston et al.), which provides a mechanism to efficiently recompute model evidence under changes to priors without retraining. This enables BMRS to tighten priors on specific gates\u2014corresponding to potential neuron/filter removals\u2014and rapidly assess the marginal likelihood of pruned variants. The overall rationale follows MacKay\u2019s evidence framework and ARD principles (Tipping), where structures with weak posterior support under sparsity-promoting priors are deemed irrelevant and can be removed. By uniting multiplicative-noise structured gating with BMR-based evidence evaluation, BMRS yields practical, theoretically grounded pruning that can instantiate different behaviors via alternative priors (e.g., truncated log-normal), achieving reliable compression without repeated costly retraining.",
  "analysis_timestamp": "2026-01-06T23:33:36.290557"
}
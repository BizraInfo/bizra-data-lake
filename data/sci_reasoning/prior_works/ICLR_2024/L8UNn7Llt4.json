{
  "prior_works": [
    {
      "title": "DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections",
      "authors": "Ofir Nachum et al.",
      "year": 2019,
      "arxiv_id": "1906.04733",
      "role": "Foundation",
      "relationship_sentence": "ODICE starts from the DualDICE saddle-point objective and shows its value-function true-gradient decomposes into forward (current-state) and backward (next-state) terms, then alters the update by orthogonalizing the backward component to avoid canceling the forward effect."
    },
    {
      "title": "AlgaeDICE: Policy Gradient from Arbitrary Experience",
      "authors": "Ofir Nachum et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "AlgaeDICE\u2019s density-ratio-regularized objective induces the same next-state (backward) gradient component whose interference ODICE explicitly diagnoses and mitigates with an orthogonal-gradient update."
    },
    {
      "title": "Imitation Learning via Off-Policy Distribution Matching (ValueDICE)",
      "authors": "Ilija Kostrikov et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "ValueDICE extends DICE to imitation learning with state-action distribution matching, and ODICE directly upgrades this class by projecting the backward gradient to preserve the action-level (forward) constraint signal."
    },
    {
      "title": "Conservative Q-Learning for Offline Reinforcement Learning",
      "authors": "Aviral Kumar et al.",
      "year": 2020,
      "arxiv_id": "2006.04779",
      "role": "Baseline",
      "relationship_sentence": "CQL epitomizes successful action-level behavior constraints, motivating ODICE\u2019s identification of the forward gradient as effectively imposing such a constraint and its design to prevent degradation by the backward gradient."
    },
    {
      "title": "Offline Reinforcement Learning with Implicit Q-Learning",
      "authors": "Ilya Kostrikov et al.",
      "year": 2022,
      "arxiv_id": "2110.06169",
      "role": "Baseline",
      "relationship_sentence": "IQL\u2019s strong offline performance using only action-level constraints highlights the performance gap with DICE methods, directly motivating ODICE\u2019s reinterpretation of DICE\u2019s forward term as an action-level constraint to be preserved."
    },
    {
      "title": "Gradient Surgery for Multi-Task Learning",
      "authors": "Tianhe Yu et al.",
      "year": 2020,
      "arxiv_id": "2001.06782",
      "role": "Inspiration",
      "relationship_sentence": "ODICE borrows the core idea of projecting away conflicting gradient components from PCGrad to make the DICE backward gradient orthogonal to the forward gradient, avoiding destructive interference."
    }
  ],
  "synthesis_narrative": "DualDICE established the modern formulation of distribution correction estimation by learning a discounted stationary distribution ratio via a saddle-point objective whose value-function gradient contains contributions from both current-state and next-state terms. AlgaeDICE reframed policy optimization with a density-ratio regularizer derived from the same distribution-correction principle, implicitly inducing a next-state gradient that can interact with the current-state term during training. In imitation learning, ValueDICE applied state-action distribution matching in the DICE style, bringing the same gradient structure to IL objectives. Meanwhile, Conservative Q-Learning demonstrated that purely action-level regularization\u2014penalizing Q-values for out-of-distribution actions\u2014produces robust offline performance without density ratios. Implicit Q-Learning similarly showed that advantage-weighted regression with action-level constraints can achieve state-of-the-art results, reinforcing the effectiveness of action-level signals. Separately, PCGrad introduced a practical mechanism to handle conflicting gradients by projecting one task\u2019s gradient away from another\u2019s direction to prevent interference. Together, these works reveal a tension: DICE\u2019s theoretically appealing state-action distribution correction underperforms action-level baselines, and the shared structure suggests the DICE value gradients blend a beneficial action-level signal with a potentially conflicting next-state term. ODICE synthesizes these insights by explicitly decomposing DICE\u2019s true gradient into forward (action-level\u2013like) and backward components and then applying a PCGrad-style orthogonal projection to the backward update. This preserves the effective action-level constraint while retaining next-state information in a non-conflicting direction, closing the empirical gap between DICE and leading offline RL/IL methods.",
  "target_paper": {
    "title": "ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update",
    "authors": "Liyuan Mao, Haoran Xu, Weinan Zhang, Xianyuan Zhan",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "offline reinforcement learning, imitation learning, distribution correction estimation",
    "abstract": "In this study, we investigate the DIstribution Correction Estimation (DICE) methods, an important line of work in offline reinforcement learning (RL) and imitation learning (IL). DICE-based methods impose state-action-level behavior constraint, which is an ideal choice for offline learning. However, they typically perform much worse than current state-of-the-art (SOTA) methods that solely use action-level behavior constraint. After revisiting DICE-based methods, we find there exist two gradient terms when learning the value function using true-gradient update: forward gradient (taken on the current state) and backward gradient (taken on the next state). Using forward gradient bears a large similarity to many offline RL methods, and thus can be regarded as applying action-level constraint. However, directly adding the backward gradient may degenerate or cancel out its effect if these two gradients have conflicting directions. To resolve this issue, we propose a simple yet effective modi",
    "openreview_id": "L8UNn7Llt4",
    "forum_id": "L8UNn7Llt4"
  },
  "analysis_timestamp": "2026-01-06T12:05:58.979572"
}
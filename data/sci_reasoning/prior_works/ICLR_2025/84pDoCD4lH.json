{
  "prior_works": [
    {
      "title": "Space in Language and Cognition: Explorations in Cognitive Diversity",
      "authors": "Stephen C. Levinson",
      "year": 2003,
      "role": "Foundation",
      "relationship_sentence": "COMFORT operationalizes Levinson\u2019s typology of frames of reference (relative, intrinsic, absolute) and his cross-linguistic findings on FoR conventions as the core theoretical basis for its multilingual, ambiguity-focused evaluation."
    },
    {
      "title": "SemEval-2015 Task 8: SpaceEval",
      "authors": "Parisa Kordjamshidi et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "SpaceEval\u2019s formalization and evaluation of spatial semantics directly informed COMFORT\u2019s design of systematic, controlled tests for spatial language phenomena, which COMFORT extends to vision-language settings and FoR ambiguity."
    },
    {
      "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
      "authors": "Justin Johnson et al.",
      "year": 2017,
      "role": "Gap Identification",
      "relationship_sentence": "While CLEVR established controlled diagnostics for visual-spatial reasoning, it assumes a fixed camera-centric frame and lacks FoR ambiguity or multilingual variation\u2014gaps COMFORT explicitly targets."
    },
    {
      "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering",
      "authors": "Drew A. Hudson et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "GQA evaluates spatial reasoning in natural images but does not probe which frame of reference models adopt or their consistency under ambiguity, directly motivating COMFORT\u2019s FoR-focused probes."
    },
    {
      "title": "A Corpus for Reasoning about Natural Language with Real Images (NLVR2)",
      "authors": "Alane Suhr et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "NLVR2 assesses compositional visual reasoning in natural images yet lacks explicit tests for FoR choice and cross-lingual conventions, limitations COMFORT addresses with controlled FoR manipulations."
    },
    {
      "title": "Modeling Referring Expressions in Images",
      "authors": "Licheng Yu et al.",
      "year": 2016,
      "role": "Related Problem",
      "relationship_sentence": "RefCOCO family datasets foreground spatial terms in referring expressions, but they do not disambiguate or test multiple frames of reference; COMFORT builds on this spatial grounding focus to isolate FoR ambiguity."
    },
    {
      "title": "Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset",
      "authors": "Saurav Thapliyal et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "Crossmodal-3600 highlights English-dominance and uneven cross-lingual transfer in multimodal models; COMFORT extends this critique specifically to spatial semantics by testing language- and culture-specific FoR conventions."
    }
  ],
  "synthesis_narrative": "COMFORT\u2019s core innovation\u2014systematically evaluating how vision-language models resolve frame-of-reference (FoR) ambiguities across languages\u2014rests on two intertwined lineages. From cognitive linguistics, Levinson\u2019s taxonomy of relative, intrinsic, and absolute FoRs and his cross-cultural evidence that languages adopt different FoR preferences provide the theoretical backbone that COMFORT directly operationalizes. On the NLP spatial semantics side, SpaceEval established a rigorous tradition of specifying and evaluating spatial roles and relations; COMFORT inherits this disciplined evaluation framing but extends it from text-only to multimodal settings and makes FoR ambiguity the focal construct.\n\nThe benchmark design is also shaped by gaps in existing VLM evaluations. CLEVR pioneered controlled diagnostics for spatial reasoning, yet presumes a fixed viewpoint and omits FoR ambiguity and multilingual variation. Large-scale natural-image benchmarks like GQA and NLVR2 test spatial and compositional reasoning but do not reveal which FoR a model adopts, how consistent that choice is, or whether it flexibly adapts under ambiguity. Datasets for referring expression comprehension (e.g., RefCOCO) emphasize spatial words in grounding but rarely disentangle FoR as a latent variable. Finally, multilingual multimodal evaluations such as Crossmodal-3600 document English-centric transfer, motivating COMFORT\u2019s explicit tests of language- and culture-specific FoR conventions. Together, these strands culminate in COMFORT\u2019s controlled, multilingual probes that uncover VLMs\u2019 lack of robustness, limited FoR flexibility, and English-dominant behavior in spatial understanding.",
  "analysis_timestamp": "2026-01-06T23:08:23.933118"
}
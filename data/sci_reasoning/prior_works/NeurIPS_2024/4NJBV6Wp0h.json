{
  "prior_works": [
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Bai et al.",
      "year": 2022,
      "role": "AI-feedback alignment framework",
      "relationship_sentence": "By replacing human judges with an LLM-as-critic, this work crystallized the evaluator\u2013evaluatee coupling that the NeurIPS 2024 paper interrogates, motivating a study of self-preference when the same (or closely related) model family evaluates its own generations."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Ouyang et al.",
      "year": 2022,
      "role": "RLHF and preference modeling",
      "relationship_sentence": "Established the preference-based evaluation and reward-modeling pipeline that later systems increasingly substitute with LLM judges, setting the stage for analyzing how evaluator identity (being the same model) can bias scores toward one\u2019s own outputs."
    },
    {
      "title": "MT-Bench: Evaluating LLMs with Multi-Turn Open-Ended Questions",
      "authors": "Zheng et al.",
      "year": 2023,
      "role": "LLM-as-a-judge benchmarking protocol",
      "relationship_sentence": "Popularized pairwise judging with GPT-4 as an automatic evaluator, providing the concrete evaluation setup in which the NeurIPS paper measures and manipulates self-preference and links it to self-recognition."
    },
    {
      "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
      "authors": "Liu et al.",
      "year": 2023,
      "role": "Prompting/rubrics for LLM evaluators",
      "relationship_sentence": "Showed that carefully prompted LLM judges can align with humans, which the new work qualifies by revealing a hidden failure mode\u2014systematic self-preference driven by self-recognition\u2014even under strong evaluator prompts."
    },
    {
      "title": "Self-Refine: Iterative Refinement with Self-Feedback",
      "authors": "Madaan et al.",
      "year": 2023,
      "role": "Self-evaluation for self-improvement",
      "relationship_sentence": "Demonstrated loops where a model critiques and revises its own outputs, directly motivating the NeurIPS paper\u2019s examination of whether such setups inherently bias evaluators to favor self-generated text."
    },
    {
      "title": "A Watermark for Large Language Models",
      "authors": "Kirchenbauer et al.",
      "year": 2023,
      "role": "Source identification via watermarking",
      "relationship_sentence": "Framed the problem of attributing text to generators, against which the NeurIPS paper\u2019s key insight stands out: modern LLMs can self-recognize their outputs without explicit watermarks, and this recognition causally amplifies self-preference."
    },
    {
      "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
      "authors": "Mitchell et al.",
      "year": 2023,
      "role": "Model-based detection of generated text",
      "relationship_sentence": "Suggested that a model\u2019s probability landscape contains telltale signatures of generation, informing the NeurIPS paper\u2019s hypothesis that LLMs can exploit such internal signals to recognize (and then prefer) their own outputs."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014showing that LLM evaluators both recognize and systematically prefer their own outputs, and establishing a causal link between self-recognition and self-preference\u2014builds directly on three intertwined strands of prior work. First, preference-based alignment and evaluation pipelines (Ouyang et al., 2022) and AI-feedback paradigms (Bai et al., 2022) normalized replacing or augmenting human judgments with LLM critics. This mainstreamed scenarios where the evaluator and the generatee are the same or closely related models, making evaluator-identity bias a practical concern. Second, the LLM-as-a-judge literature (Zheng et al., 2023; Liu et al., 2023) provided standardized protocols, prompts, and pairwise-comparison setups that the present work adopts to expose and quantify self-preference under realistic evaluation conditions. Third, research on attributing or detecting LLM-generated text (Kirchenbauer et al., 2023; Mitchell et al., 2023) seeded the idea that models carry recognizable generation footprints, inspiring the hypothesis that an LLM can identify its own style or token-probability patterns. Building on self-refinement systems (Madaan et al., 2023), the authors then connect this recognition capacity to practical risks in self-critique and AI-feedback loops. Their empirical results extend these foundations by (i) demonstrating non-trivial out-of-the-box self-recognition among frontier models, (ii) showing a linear relationship between increased self-recognition (via fine-tuning) and stronger self-preference, and (iii) using controlled experiments to support a causal interpretation. Together, these prior works directly shaped the problem formulation, experimental protocol, and the mechanistic hypothesis tested here.",
  "analysis_timestamp": "2026-01-06T23:33:35.539436"
}
{
  "prior_works": [
    {
      "title": "Improving generalization for temporal difference learning: The successor representation",
      "authors": "Peter Dayan et al.",
      "year": 1993,
      "role": "Foundation",
      "relationship_sentence": "MR.Q\u2019s core idea of learning representations that make value functions approximately linear directly builds on the successor representation, which factorizes value into successor features and reward weights."
    },
    {
      "title": "Successor Features for Transfer in Reinforcement Learning",
      "authors": "Andr\u00e9 Barreto et al.",
      "year": 2017,
      "role": "Extension",
      "relationship_sentence": "MR.Q extends the successor-features perspective from Barreto et al. by learning feature representations that support near-linear value prediction across tasks without explicitly planning, aligning with SFs\u2019 value decomposition for transfer."
    },
    {
      "title": "Deep Successor Reinforcement Learning",
      "authors": "Tejas D. Kulkarni et al.",
      "year": 2016,
      "role": "Extension",
      "relationship_sentence": "By demonstrating that deep networks can learn successor-style representations from high-dimensional inputs, this work provides the concrete template MR.Q follows to learn value-linearizing features in a model-free deep RL setting."
    },
    {
      "title": "Data-Efficient Reinforcement Learning with Self-Predictive Representations",
      "authors": "Max Schwarzer et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "SPR showed that model-based predictive objectives can densify and stabilize value learning without rollouts; MR.Q leverages this insight to use model-based representation learning to aid Q-learning while avoiding planning."
    },
    {
      "title": "Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning",
      "authors": "Vitchyr H. Feinberg et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "MVE demonstrated that short model rollouts yield denser, lower-variance targets but at added complexity; MR.Q explicitly seeks the same value-estimation benefits by using representations that linearize value rather than simulated trajectories."
    },
    {
      "title": "Mastering Diverse Domains through World Models",
      "authors": "Danijar Hafner et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "DreamerV3 established strong cross-domain generality using learned world models but with planning and runtime overhead; MR.Q is motivated to capture these generalization benefits via model-based representations while remaining model-free and fast."
    },
    {
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation",
      "authors": "Chi Jin et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "The linear MDP/linear value function framework theoretically grounds MR.Q\u2019s objective of learning features that render value approximately linear, enabling efficient policy evaluation and improvement without explicit planning."
    }
  ],
  "synthesis_narrative": "MR.Q\u2019s core contribution\u2014using model-based representations to make value functions approximately linear while remaining model-free\u2014sits at the intersection of classic value factorization and modern predictive representation learning. The conceptual foundation comes from the successor representation (Dayan), which factorizes value into successor features and reward weights. This idea was operationalized for transfer via successor features (Barreto et al.) and shown to be learnable with deep networks (Kulkarni et al.), establishing that learned representations can linearize value in practice. Concurrently, model-based RL showed that predictive structure yields denser, lower-variance training signals: MVE demonstrated short-rollout value expansion improves sample efficiency, and DreamerV3 showed world models can deliver broad cross-domain generality\u2014yet both incur planning or simulation overhead. SPR then provided a key bridge by using predictive (model-based) objectives purely to shape representations that stabilize model-free value learning, avoiding explicit rollouts. MR.Q integrates these strands: it targets the linear-MDP/linear-value regime formalized by Jin et al., but attains it via model-based representation learning akin to SPR, capturing the densification benefits highlighted by MVE and DreamerV3 without their computational burden. The result is a unifying model-free algorithm intended to generalize across diverse domains with a single hyperparameter setting, directly extending successor-style value linearization into a practical, general-purpose deep RL method.",
  "analysis_timestamp": "2026-01-06T23:09:26.606289"
}
{
  "prior_works": [
    {
      "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
      "authors": "Geva et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "By formalizing MLP layers as discrete key\u2013value memory slots, this work provides the mechanistic basis for treating a language model\u2019s internal modules as distinct \u201cmemory sources,\u201d which the current paper aggregates into its knowledge-entropy measure."
    },
    {
      "title": "Language Models as Knowledge Bases?",
      "authors": "Petroni et al.",
      "year": 2019,
      "arxiv_id": "1909.01066",
      "role": "Foundation",
      "relationship_sentence": "This paper established factual probing as a concrete way to evaluate parametric knowledge, underpinning the current paper\u2019s formulation of knowledge acquisition/retention metrics that are tracked as entropy evolves during pretraining."
    },
    {
      "title": "In-Context Learning and Induction Heads",
      "authors": "Olsson et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By revealing stagewise changes and specialization of transformer heads during training, this work directly motivates analyzing how the breadth of active mechanisms evolves, leading to the current paper\u2019s notion and measurement of knowledge-entropy decay."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT",
      "authors": "Meng et al.",
      "year": 2022,
      "arxiv_id": "2202.05262",
      "role": "Inspiration",
      "relationship_sentence": "Demonstrating that specific factual knowledge localizes to narrow subcircuits (and can be edited there) directly informs the hypothesis that over time models rely on fewer, more specific memory sources\u2014exactly what the current work quantifies as entropy decline."
    },
    {
      "title": "Generalization through Memorization: Nearest Neighbor Language Models",
      "authors": "Khandelwal et al.",
      "year": 2020,
      "arxiv_id": "1911.00172",
      "role": "Related Problem",
      "relationship_sentence": "Showing that augmenting LMs with an external kNN datastore broadens usable memory and boosts factual recall provides a concrete mechanism the current paper cites when testing whether increasing active memory sources reverses low-entropy-induced acquisition failures."
    },
    {
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP",
      "authors": "Lewis et al.",
      "year": 2020,
      "arxiv_id": "2005.11401",
      "role": "Related Problem",
      "relationship_sentence": "By explicitly adding a non-parametric memory to complement parametric knowledge, this work operationalizes the idea of activating more memory sources, which the current paper leverages to validate that higher knowledge entropy improves acquisition and retention."
    },
    {
      "title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks",
      "authors": "Gururangan et al.",
      "year": 2020,
      "arxiv_id": "2004.10964",
      "role": "Gap Identification",
      "relationship_sentence": "Observations from continued pretraining about domain adaptation and emergent forgetting highlight the stability\u2013plasticity tension, a gap the current paper addresses by identifying knowledge-entropy decay as a mechanistic correlate of reduced plasticity."
    }
  ],
  "synthesis_narrative": "Feed-forward layers in transformers have been shown to operate as key\u2013value memories, implying that knowledge can be viewed as distributed across discrete parametric memory slots rather than being homogeneously smeared across weights. Factual probing revealed that language models store and retrieve real-world facts directly from these parameters, establishing a concrete evaluation paradigm for knowledge access and change. Training-time analyses uncovered stagewise emergence and specialization of mechanisms such as induction heads, indicating that the set of active computational pathways evolves over training. Complementarily, causal editing of factual associations showed that specific facts often localize to narrow subcircuits, suggesting that models can come to rely on increasingly specific loci of memory. On the augmentation side, nearest-neighbor LMs demonstrated that adding an external datastore substantially expands usable memory and improves factual recall, while retrieval-augmented generation explicitly combines parametric and non-parametric sources to broaden the evidence pool for answering knowledge-intensive queries. Meanwhile, continued pretraining studies documented both gains and instances of forgetting, foregrounding a stability\u2013plasticity trade-off without a clear mechanistic account. Taken together, these works reveal that (i) parametric knowledge resides in identifiable memory-like structures, (ii) training induces mechanism specialization, and (iii) expanding accessible memory sources improves factual performance. The natural next step is to quantify how widely a model engages its available memory sources over pretraining and to test whether that breadth governs plasticity. By defining and tracking knowledge entropy, and by experimentally increasing active memory sources via retrieval-style augmentation, the present work synthesizes these insights into a mechanistic explanation for knowledge acquisition and forgetting.",
  "target_paper": {
    "title": "Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition",
    "authors": "Jiyeon Kim, Hyunji Lee, Hyowon Cho, Joel Jang, Hyeonbin Hwang, Seungpil Won, Youbin Ahn, Dohaeng Lee, Minjoon Seo",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "knowledge entropy, knowledge acquisition and forgetting, evolving behavior during LLM pretraining",
    "abstract": "In this work, we investigate how a model's tendency to broadly integrate its parametric knowledge evolves throughout pretraining, and how this behavior affects overall performance, particularly in terms of knowledge acquisition and forgetting. We introduce the concept of knowledge entropy, which quantifies the range of memory sources the model engages with; high knowledge entropy indicates that the model utilizes a wide range of memory sources, while low knowledge entropy suggests reliance on specific sources with greater certainty. Our analysis reveals a consistent decline in knowledge entropy as pretraining advances. We also find that the decline is closely associated with a reduction in the model's ability to acquire and retain knowledge, leading us to conclude that diminishing knowledge entropy (smaller number of active memory sources) impairs the model's knowledge acquisition and retention capabilities. We find further support for this by demonstrating that increasing the activity",
    "openreview_id": "eHehzSDUFp",
    "forum_id": "eHehzSDUFp"
  },
  "analysis_timestamp": "2026-01-06T06:10:19.327256"
}
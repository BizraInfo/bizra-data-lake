{
  "prior_works": [
    {
      "title": "Plug-and-Play Priors for Model Based Reconstruction",
      "authors": "S. Venkatakrishnan et al.",
      "year": 2013,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "InverseBench operationalizes the plug-and-play paradigm of swapping explicit regularizers with powerful denoisers by systematically evaluating diffusion-denoiser\u2013based PnP algorithms across diverse scientific forward operators."
    },
    {
      "title": "Regularization by Denoising: Clarifications and New Interpretations",
      "authors": "Y. Romano et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "InverseBench leverages the RED perspective that a denoiser implicitly defines a regularizer to interpret and compare diffusion priors as implicit regularization mechanisms across physics-driven inverse problems."
    },
    {
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "authors": "Y. Song et al.",
      "year": 2021,
      "arxiv_id": "2011.13456",
      "role": "Foundation",
      "relationship_sentence": "The benchmarked PnP diffusion solvers rely on the score-based SDE formulation and samplers introduced here, and InverseBench evaluates how these score priors perform when coupled with measurement-consistency terms in scientific inverse operators."
    },
    {
      "title": "Denoising Diffusion Restoration Models",
      "authors": "B. Kawar et al.",
      "year": 2022,
      "arxiv_id": "2201.11793",
      "role": "Baseline",
      "relationship_sentence": "InverseBench includes DDRM as a principal baseline and probes its closed-form SVD-based data-consistency mechanism beyond natural-image degradations, assessing its strengths and failure modes on scientific forward models."
    },
    {
      "title": "Diffusion Posterior Sampling for General Noisy Inverse Problems",
      "authors": "H. Chung et al.",
      "year": 2022,
      "arxiv_id": "2206.00927",
      "role": "Baseline",
      "relationship_sentence": "InverseBench benchmarks DPS as a primary likelihood-guided diffusion sampler, directly testing how its gradient-of-log-likelihood guidance extends to the five scientific inverse problems in the suite."
    },
    {
      "title": "Compressed Sensing using Generative Models",
      "authors": "A. Bora et al.",
      "year": 2017,
      "arxiv_id": "1703.03208",
      "role": "Inspiration",
      "relationship_sentence": "By showing that learned generative models can act as powerful priors for inverse problems, this work catalyzed the learned-prior paradigm that InverseBench evaluates at scale with diffusion priors across scientific operators."
    }
  ],
  "synthesis_narrative": "Plug-and-play priors established that one can replace hand-crafted regularizers with powerful denoisers inside model-based solvers, opening a general recipe for physics-constrained reconstruction (Venkatakrishnan et al.). Regularization by Denoising formalized how a denoiser implicitly defines a regularizer, providing a principled lens to analyze denoiser-driven reconstruction behavior (Romano et al.). Score-based generative modeling through SDEs then made it practical to train and sample from high-quality image priors via learned scores and reverse-time SDE samplers (Song et al.), which became the backbone for diffusion-based priors. Building on these ingredients, Denoising Diffusion Restoration Models introduced an analytic SVD-domain data-consistency integration with diffusion denoisers for general linear inverse problems without task-specific retraining (Kawar et al.). In parallel, Diffusion Posterior Sampling injected the measurement likelihood as a gradient guidance term into diffusion sampling to target posteriors for broad noisy inverse problems (Chung et al.). Earlier, Compressed Sensing using Generative Models demonstrated the efficacy of learned generative priors for ill-posed recovery, motivating a shift from handcrafted to learned priors in inverse problems (Bora et al.). Together, these works yielded a family of plug-and-play diffusion solvers with distinct data-consistency mechanisms and sampling philosophies, but evaluations largely centered on natural-image restoration. This landscape created a clear opportunity to systematically test whether diffusion priors\u2014and their RED/PnP interpretations, SVD-based consistency (DDRM), and likelihood guidance (DPS)\u2014transfer to scientific forward models with different structure and physics. InverseBench synthesizes these strands into a rigorous, multi-domain benchmark, contrasting PnP diffusion baselines against strong domain-specific solvers to reveal where each prior and consistency strategy succeeds or breaks in physical-science inverse problems.",
  "target_paper": {
    "title": "InverseBench: Benchmarking Plug-and-Play Diffusion Priors for Inverse Problems in Physical Sciences",
    "authors": "Hongkai Zheng, Wenda Chu, Bingliang Zhang, Zihui Wu, Austin Wang, Berthy Feng, Caifeng Zou, Yu Sun, Nikola Borislavov Kovachki, Zachary E Ross, Katherine Bouman, Yisong Yue",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "inverse problem, benchmark, diffusion model",
    "abstract": "Plug-and-play diffusion priors (PnPDP) have emerged as a promising research direction for solving inverse problems. \n However, current studies primarily focus on natural image restoration, leaving the performance of these algorithms in scientific inverse problems largely unexplored. To address this gap, we introduce \\textsc{InverseBench}, a framework that evaluates diffusion models across five distinct scientific inverse problems. These problems present unique structural challenges that differ from existing benchmarks, arising from critical scientific applications such as optical tomography, medical imaging, black hole imaging, seismology, and fluid dynamics. With \\textsc{InverseBench}, we benchmark 14 inverse problem algorithms that use plug-and-play diffusion priors against strong, domain-specific baselines, offering valuable new insights into the strengths and weaknesses of existing algorithms. To facilitate further research and development, we open-source the codebase, along with d",
    "openreview_id": "U3PBITXNG6",
    "forum_id": "U3PBITXNG6"
  },
  "analysis_timestamp": "2026-01-06T12:26:04.971063"
}
{
  "prior_works": [
    {
      "title": "Deterministic Policy Gradient Algorithms",
      "authors": "David Silver et al.",
      "year": 2014,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "The analysis assumes a continuous-action actor\u2013critic with deterministic policy updates, directly adopting the deterministic policy gradient framework to define the semi-gradient training dynamics that induce the attainable-state set."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot et al.",
      "year": 2018,
      "arxiv_id": "1806.07572",
      "role": "Extension",
      "relationship_sentence": "The proof strategy leverages NTK-style linearization of two-layer networks to characterize the gradient-flow subspace of policy updates, enabling a dimension bound on the induced manifold of attainable states."
    },
    {
      "title": "Controllability of Nonlinear Systems",
      "authors": "Velimir Jurdjevic et al.",
      "year": 1972,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Geometric control results on local reachability and the rank of control-induced vector fields underpin the argument that the tangent space of attainable states is spanned by action-direction fields, linking manifold dimension to action dimension."
    },
    {
      "title": "Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator",
      "authors": "Maryam Fazel et al.",
      "year": 2018,
      "arxiv_id": "1801.05039",
      "role": "Gap Identification",
      "relationship_sentence": "By restricting to linear dynamics and quadratic costs, this work highlighted the lack of theory for general continuous-state/action RL, motivating a geometric analysis for nonlinear systems with neural policies."
    },
    {
      "title": "Proto-Value Functions: A Laplacian Framework for Learning Representation",
      "authors": "Sridhar Mahadevan",
      "year": 2005,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "The Laplacian view that MDP state spaces possess exploitable geometric structure inspired adopting a geometric lens on state space, here focused on the subset of states attainable under policy learning."
    },
    {
      "title": "Eigenoption Discovery through the Deep Successor Representation",
      "authors": "Marlos C. Machado et al.",
      "year": 2018,
      "arxiv_id": "1710.11089",
      "role": "Related Problem",
      "relationship_sentence": "By using spectral structure of dynamics to define behaviors, this work informed the idea that policy-induced dynamics carve specific low-dimensional structures in state space, motivating analysis of the attainable-state manifold under learning."
    }
  ],
  "synthesis_narrative": "Deterministic Policy Gradient established a precise actor\u2013critic framework for continuous actions in which policy updates deterministically shape the state visitation distribution, grounding subsequent analyses of how learning transforms reachable sets in continuous spaces. Neural Tangent Kernel theory showed that two-layer networks trained by gradient flow evolve within a linearized function space, effectively constraining updates to a low-dimensional subspace determined by the network\u2019s tangent features. Classical geometric control, epitomized by Jurdjevic and Sussmann, characterized local reachability via the span of control-induced vector fields and Lie algebraic conditions, linking the dimension of reachable sets to the number and structure of control inputs. In the continuous-control theory track, policy gradient analyses for LQR demonstrated how gradient dynamics couple to system trajectories, but only in linear-quadratic regimes. Meanwhile, representation-learning works like Proto-Value Functions and eigenoption discovery revealed that MDP dynamics impart geometric structure on state spaces, in practice yielding low-dimensional spectral structure that shapes behavior and exploration. Together these streams exposed a gap: while control theory ties reachability geometry to input dimensions and deep-learning theory constrains gradient dynamics via tangent features, RL lacked a unifying account of how semi-gradient actor\u2013critic training with neural policies sculpts the subset of states actually reached. The present synthesis marries NTK linearization with geometric control insights in the deterministic policy-gradient setting, yielding a precise characterization: training a two-layer policy induces a locally low-dimensional manifold of attainable states whose dimension scales with the action space, bridging continuous-control theory and representation geometry under neural training dynamics.",
  "target_paper": {
    "title": "Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces",
    "authors": "Saket Tiwari, Omer Gottesman, George Konidaris",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "reinforcement learning, deep learning, geometry",
    "abstract": "Advances in reinforcement learning (RL) have led to its successful application in complex tasks with continuous state and action spaces. Despite these advances in practice, most theoretical work pertains to finite state and action spaces. We propose building a theoretical understanding of continuous state and action spaces by employing a geometric lens to understand the locally attained set of states. The set of all parametrised policies learnt through a semi-gradient based approach induce a set of attainable states in RL. We show that training dynamics of a two layer neural policy induce a low dimensional manifold of attainable states embedded in the high-dimensional nominal state space trained using an actor-critic algorithm. We prove that, under certain conditions, the dimensionality of this manifold is of the order of the dimensionality of the action space. This is the first result of its kind, linking the geometry of the state space to the dimensionality of the action space. We em",
    "openreview_id": "AP0ndQloqR",
    "forum_id": "AP0ndQloqR"
  },
  "analysis_timestamp": "2026-01-06T13:50:18.702165"
}
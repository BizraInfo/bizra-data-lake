{
  "prior_works": [
    {
      "title": "Deep Anomaly Detection with Outlier Exposure",
      "authors": "Dan Hendrycks et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Outlier Exposure established the effectiveness of training with auxiliary OOD data but requires a separate OOD set; GOLD\u2019s core contribution is to remove this dependency by synthesizing OOD exposure via an implicit adversarial latent generator."
    },
    {
      "title": "Training Confidence-Calibrated Classifiers for Detecting Out-of-Distribution Samples",
      "authors": "Kimin Lee et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "BoundaryGAN showed that adversarially generating boundary samples and training the classifier against them improves OOD detection; GOLD adopts this adversarial generation idea but relocates it to a learned latent space for graphs, avoiding image-domain generators."
    },
    {
      "title": "Why ReLU Networks Yield High-Confidence Predictions Far Away from the Training Data and How to Fix It",
      "authors": "Matthias Hein et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "ACET demonstrated a minimax training that adversarially reduces confidence on synthesized off-manifold inputs; GOLD builds on this confidence-regularization principle by adversarially producing latent OOD nodes to train a graph OOD detector without external data."
    },
    {
      "title": "Energy-based Out-of-Distribution Detection",
      "authors": "Weitang Liu et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "Energy-based scoring provided a principled objective to separate ID and OOD via low/high energy; GOLD\u2019s adversarial OOD synthesis directly complements such ID\u2013OOD separation objectives by supplying challenging synthetic negatives in training."
    },
    {
      "title": "OpenMax: Recognizing Open Worlds",
      "authors": "Abhijit Bendale et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "OpenMax formalized open-set recognition, motivating methods that explicitly model unknowns; GOLD inherits this open-set/OOD formulation and operationalizes it for graphs by generating unknowns in latent space during training."
    },
    {
      "title": "Enhancing the Reliability of Out-of-distribution Image Detection in Neural Networks (ODIN)",
      "authors": "Shiyu Liang et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "ODIN is a seminal post-hoc OOD scoring baseline; GOLD improves upon such post-hoc detection by providing an explicit adversarial OOD exposure signal during training, eliminating reliance on external OOD or pre-trained generators."
    }
  ],
  "synthesis_narrative": "GOLD\u2019s core idea\u2014synthetic OOD exposure without external datasets or pre-trained generators\u2014emerges from two converging threads in OOD research. First, Outlier Exposure (Hendrycks et al.) proved that training with OOD samples is highly effective, but its dependency on an auxiliary OOD set is a practical bottleneck. Second, adversarial generation for OOD/openset training (Lee et al.\u2019s BoundaryGAN and Hein & Andriushchenko\u2019s ACET) showed one can manufacture challenging off-manifold examples and explicitly reduce model confidence on them via a minimax game. GOLD fuses these insights by moving the generation process into a learned latent space for graphs: an implicit generator adversarially produces OOD-like latent nodes, while the detector is trained to separate ID from these synthesized unknowns.\nEnergy-based OOD detection (Liu et al.) supplied a widely adopted objective for separating ID and OOD via a scalar score, which GOLD\u2019s synthetic negatives can directly strengthen. The open-set formulation underlying why unknowns must be considered at training time traces to OpenMax (Bendale & Boult), whose problem framing GOLD adopts in the graph domain. Finally, ODIN (Liang et al.) represents the influential post-hoc baseline that GOLD seeks to surpass by replacing test-time-only heuristics with training-time adversarial OOD exposure. Together, these works directly shape GOLD\u2019s alternating optimization, implicit adversarial latent generation, and OOD-aware training\u2014delivering graph OOD detection without external OOD data or pre-trained image generators.",
  "analysis_timestamp": "2026-01-06T23:09:26.630115"
}
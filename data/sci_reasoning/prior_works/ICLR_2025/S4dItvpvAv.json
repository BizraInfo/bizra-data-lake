{
  "prior_works": [
    {
      "title": "A Survey of Multi-Objective Sequential Decision-Making",
      "authors": "Diederik M. Roijers et al.",
      "year": 2013,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This survey formalized the MO-MDP problem, distinguished the Pareto Coverage Set (PCS) from the Convex Coverage Set (CCS), and explicitly highlighted that existing methods either traverse a continuous preference space or restrict to deterministic policies\u2014gaps this paper targets with an exact Pareto-front characterization."
    },
    {
      "title": "Optimistic Linear Support for Multi-Objective Reinforcement Learning",
      "authors": "Diederik M. Roijers et al.",
      "year": 2014,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "OLS exemplifies weight-space traversal to approximate the CCS by solving scalarized MDPs, directly motivating this paper\u2019s shift from continuous preference exploration to a finite, exact characterization of the full Pareto front."
    },
    {
      "title": "Multi-Objective Reinforcement Learning Using Sets of Pareto Dominating Policies",
      "authors": "K. Van Moffaert et al.",
      "year": 2014,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By constructing sets of deterministic Pareto-dominating policies, this work underscored the inability of deterministic-only approaches to recover the full Pareto front, a limitation the current paper overcomes by characterizing and computing the entire frontier including randomized policies."
    },
    {
      "title": "Multiple Objective Infinite-Horizon Discounted Markov Decision Processes",
      "authors": "D. J. White",
      "year": 1982,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "White\u2019s vector-valued dynamic programming formulation and the connection between linear scalarization and supported efficient solutions provide the theoretical basis that this paper extends to derive the exact structure of the Pareto front."
    },
    {
      "title": "Constrained Markov Decision Processes",
      "authors": "Eitan Altman",
      "year": 1999,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Altman\u2019s occupancy-measure linear programming view and the sufficiency of stationary randomized policies under discounting underpin this paper\u2019s polyhedral treatment of achievable return sets and the exact Pareto frontier."
    },
    {
      "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "authors": "Martin L. Puterman",
      "year": 1994,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Puterman\u2019s characterization of deterministic stationary policies as extreme points of the occupancy polytope grounds the finite extreme-structure insight that this paper leverages to enumerate all Pareto-extreme solutions and recover the full front."
    }
  ],
  "synthesis_narrative": "Work on multi-objective sequential decision-making established the formal backbone for vector-valued returns and multi-criteria optimality in MDPs. White showed how vector-valued dynamic programming aligns with scalarizations, clarifying that linear weights recover only supported efficient solutions. Altman\u2019s constrained MDP framework introduced the occupancy-measure linear program and proved that stationary randomized policies suffice under discounting, implying that attainable return sets are polyhedral images of an occupation-measure polytope. Puterman\u2019s classic results tied deterministic stationary policies to extreme points of this polytope, sharpening the geometric structure of policy-induced returns. On the algorithmic side, Roijers\u2019 survey distinguished the Pareto Coverage Set from the Convex Coverage Set and cataloged two dominant strategies: preference-space traversal via scalarization and deterministic set\u2013based methods. OLS instantiated the former, iteratively solving scalarized MDPs to approximate the CCS by exploring continuous weight space. In contrast, Van Moffaert and Now\u00e9 advanced the latter by constructing sets of deterministic Pareto-dominating policies, but without a path to the full frontier that includes randomized mixtures. Together these works revealed a precise opportunity: combine the polyhedral, occupancy-measure view (which guarantees convex representability with randomized policies) with the insight that linear scalarization only touches supported points, and then go beyond weight exploration and deterministic sets. The present paper synthesizes these threads to identify a finite, exact structural characterization of the entire Pareto front for MO-MDPs and provides a dynamic-programming-compatible route to compute it without traversing a continuous preference space.",
  "target_paper": {
    "title": "How to Find the Exact Pareto Front for Multi-Objective MDPs?",
    "authors": "Yining Li, Peizhong Ju, Ness Shroff",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Multi-objective optimization, Markov decision Process",
    "abstract": "Multi-Objective Markov Decision Processes (MO-MDPs) are receiving increasing attention, as real-world decision-making problems often involve conflicting objectives that cannot be addressed by a single-objective MDP. \nThe Pareto front identifies the set of policies that cannot be dominated, providing a foundation for finding Pareto optimal solutions that can efficiently adapt to various preferences.\nHowever, finding the Pareto front is a highly challenging problem. Most existing methods either (i) rely on traversing the *continuous preference space*, which is impractical and results in approximations that are difficult to evaluate against the true Pareto front, or (ii) focus solely on deterministic Pareto optimal policies, from which there are no known techniques to characterize the full Pareto front. Moreover, finding the structure of the Pareto front itself remains unclear even in the context of dynamic programming, where the MDP is fully known in advance.\nIn this work, we address the",
    "openreview_id": "S4dItvpvAv",
    "forum_id": "S4dItvpvAv"
  },
  "analysis_timestamp": "2026-01-06T13:27:54.481287"
}
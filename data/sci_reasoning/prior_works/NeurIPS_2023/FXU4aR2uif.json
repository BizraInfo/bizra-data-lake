{
  "prior_works": [
    {
      "title": "Conditional Neural Processes",
      "authors": "Marta Garnelo; Jonathan Schwarz; Dan Rosenbaum; Fabio Viola; Danilo J. Rezende; S. M. Ali Eslami; Yee Whye Teh",
      "year": 2018,
      "role": "Foundational meta-learning framework that models distributions over functions from context-target sets via amortized inference.",
      "relationship_sentence": "HNP extends the Neural Process paradigm of learning functional priors from context sets to a setting with heterogeneous tasks and episodic multi-task hierarchical Bayes."
    },
    {
      "title": "Attentive Neural Processes",
      "authors": "Hyunjik Kim; Andriy Mnih; Jonathan Schwarz; Marta Garnelo; S. M. Ali Eslami; Danilo J. Rezende; Yee Whye Teh",
      "year": 2019,
      "role": "Introduced attention/transformer mechanisms into Neural Processes to better capture context\u2013target dependencies.",
      "relationship_sentence": "HNP\u2019s transformer-structured inference modules directly build on ANP\u2019s insight that attention improves set-based inference, using it to infer meta-knowledge and inter-task relations."
    },
    {
      "title": "The Neural Statistician",
      "authors": "Harrison Edwards; Amos J. Storkey",
      "year": 2017,
      "role": "Hierarchical Bayesian generative model with a dataset-level latent variable learned by amortized inference.",
      "relationship_sentence": "HNP adopts the Neural Statistician\u2019s principle of a dataset/episode-level latent to capture meta-knowledge that can be reused across episodes."
    },
    {
      "title": "Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks",
      "authors": "Juho Lee; Yoonho Lee; Jungtaek Kim; Adam R. Kosiorek; Seungjin Choi; Yee Whye Teh",
      "year": 2019,
      "role": "Transformer architecture for permutation-invariant set encoding and aggregation via attention.",
      "relationship_sentence": "HNP leverages Set Transformer\u2013style attention to aggregate heterogeneous within-episode information in a permutation-invariant way for inference over tasks and meta-knowledge."
    },
    {
      "title": "Multi-task Gaussian Process Prediction",
      "authors": "Edwin V. Bonilla; Kian Chai; Christopher K. I. Williams",
      "year": 2008,
      "role": "Probabilistic framework modeling correlations among tasks via shared covariance structure.",
      "relationship_sentence": "HNP generalizes the MTGP notion of task-relatedness by learning neural, hierarchical Bayesian functional priors that capture inter-task correlations among heterogeneous tasks."
    },
    {
      "title": "Matching Networks for One Shot Learning",
      "authors": "Oriol Vinyals; Charles Blundell; Timothy Lillicrap; Daan Wierstra",
      "year": 2016,
      "role": "Pioneered episodic training for few-shot learning with context-target style episodes.",
      "relationship_sentence": "HNP operates in an episodic training regime and explicitly exploits information within and across episodes, following the episodic meta-learning setup popularized by Matching Networks."
    },
    {
      "title": "Task2Vec: Task Embedding for Meta-Learning",
      "authors": "Alessandro Achille; Michael Lam; Rahul Tewari; Avinash Ravichandran; Stefano Soatto",
      "year": 2019,
      "role": "Introduced task embeddings to quantify task relatedness and guide transfer across heterogeneous tasks.",
      "relationship_sentence": "HNP\u2019s explicit modeling of task-relatedness among heterogeneous tasks is motivated by Task2Vec\u2019s idea of learning task representations to enable informed transfer."
    }
  ],
  "synthesis_narrative": "Heterogeneous Neural Processes (HNPs) sit at the intersection of function-space meta-learning, hierarchical Bayes, and attention-based set processing. The Neural Process family, spearheaded by Conditional Neural Processes, established amortized inference over functions from context\u2013target sets, while Attentive Neural Processes demonstrated that attention/transformers substantially improve set-to-function inference. HNP adopts this NP foundation but extends it to heterogeneous multi-task settings, where tasks differ and relatedness must be inferred rather than assumed. The Neural Statistician provides the key hierarchical Bayesian insight: an episode- or dataset-level latent variable can capture reusable meta-knowledge; HNP operationalizes this by learning episode-level functional priors that can be updated with scarce task data. To effectively pool heterogeneous within-episode information, HNP employs transformer-structured inference, drawing on Set Transformer\u2019s permutation-invariant attention to aggregate diverse context signals and infer inter-task structure. From the probabilistic multi-task learning side, Multi-task Gaussian Processes formalized how cross-task correlations can be modeled; HNP inherits this goal but learns neural, hierarchical priors that encode task relatedness in a flexible, data-driven manner. Finally, the episodic training paradigm originates in Matching Networks, which frames learning as context-target episodes, and Task2Vec motivates explicit modeling of task relatedness via learned embeddings. Integrating these threads, HNP delivers a hierarchical Bayesian Neural Process that infers meta-knowledge across episodes and task relations within episodes, enabling robust few-shot performance under data insufficiency and task heterogeneity.",
  "analysis_timestamp": "2026-01-07T00:02:04.835739"
}
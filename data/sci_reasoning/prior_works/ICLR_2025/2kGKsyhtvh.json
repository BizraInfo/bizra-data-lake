{
  "prior_works": [
    {
      "title": "Deep Learning with Differential Privacy",
      "authors": "Abadi et al.",
      "year": 2016,
      "arxiv_id": "1607.00133",
      "role": "Foundation",
      "relationship_sentence": "This work established DP-SGD with per-example gradient clipping and Gaussian noise, the optimization framework within which the new automatic learning-rate schedule is adapted to eliminate tuning."
    },
    {
      "title": "Differentially Private Learning with Adaptive Clipping",
      "authors": "Andrew et al.",
      "year": 2019,
      "arxiv_id": "1905.03871",
      "role": "Extension",
      "relationship_sentence": "It introduced differentially private, automatic per-sample clipping via private quantile estimation, which the new method pairs with to realize a hyperparameter-free DP optimization pipeline."
    },
    {
      "title": "Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for SGD",
      "authors": "Loizou et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "This paper\u2019s parameter-free, loss/gradient-based step-size rule is directly modified to remain stable under DP-SGD\u2019s clipping and injected noise, yielding an automatic LR schedule usable with any optimizer."
    },
    {
      "title": "L4: Practical Loss-Based Stepsize Adaptation for Deep Learning",
      "authors": "Rolinek et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Its loss-based stepsize wrapper that works with arbitrary optimizers motivates the design of a plug-in, tuning-free learning-rate scheduler that can be applied to DP training broadly."
    },
    {
      "title": "An Empirical Model of Large-Batch Training",
      "authors": "McCandlish et al.",
      "year": 2018,
      "arxiv_id": "1812.06162",
      "role": "Inspiration",
      "relationship_sentence": "By quantifying the gradient noise scale and its impact on stable learning rates, it provides the key insight that added noise (as in DP-SGD) should directly inform automatic LR calibration."
    },
    {
      "title": "The Reusable Holdout: Preserving Validity in Adaptive Data Analysis",
      "authors": "Dwork et al.",
      "year": 2015,
      "arxiv_id": "1502.07690",
      "role": "Gap Identification",
      "relationship_sentence": "This work formalizes how repeated, data-dependent model selection leaks information, motivating the removal of data-dependent hyperparameter tuning in DP training."
    }
  ],
  "synthesis_narrative": "Differentially private stochastic optimization was concretized by the introduction of DP-SGD, which couples per-example gradient clipping with Gaussian noise to protect training data while retaining much of SGD\u2019s effectiveness. Adaptive, data-dependent clipping was later made practical by private quantile estimation, enabling automatic per-sample thresholding without manual tuning. In parallel, the optimization community developed tuning-free step-size rules: the stochastic Polyak step-size tied learning rates to current loss and gradient magnitude, delivering parameter-free adaptivity under stochasticity, while the L4 approach distilled a practical, loss-based stepsize wrapper compatible with a wide array of optimizers. Complementing these mechanisms, empirical analyses of large-batch training quantified the gradient noise scale and linked it to the stable learning-rate regime, underscoring that the amount of stochastic noise should directly govern step-size. Finally, adaptive data analysis theory established that data-dependent hyperparameter selection itself can leak information, particularly acute in privacy-sensitive training. Together these works expose a clear opportunity: combine automatic clipping with a truly tuning-free learning-rate schedule that explicitly accounts for the elevated stochasticity induced by DP noise and clipping. Building on DP-SGD\u2019s mechanics, borrowing loss/gradient-driven step-size control from Polyak-style and L4 schemes, and guided by noise-scale insights, the new method adapts automatic learning rates to the DP regime and, when paired with adaptive clipping, removes the need for data-dependent hyperparameter search while maintaining state-of-the-art private performance.",
  "target_paper": {
    "title": "Towards hyperparameter-free optimization with differential privacy",
    "authors": "Ruixuan Liu, Zhiqi Bu",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Differential privacy, optimization, hyper-parameter tuning",
    "abstract": "Differential privacy (DP) is a privacy-preserving paradigm that protects the training data when training deep learning models. Critically, the performance of models is determined by the training hyperparameters, especially those of the learning rate schedule, thus requiring fine-grained hyperparameter tuning on the data. In practice, it is common to tune the learning rate hyperparameters through the grid search that (1) is computationally expensive as multiple runs are needed, and (2) increases the risk of data leakage as the selection of hyperparameters is data-dependent. In this work, we adapt the automatic learning rate schedule to DP optimization for any models and optimizers, so as to significantly mitigate or even eliminate the cost of hyperparameter tuning when applied together with automatic per-sample gradient clipping. Our hyperparameter-free DP optimization is almost as computationally efficient as the standard non-DP optimization, and achieves state-of-the-art DP performanc",
    "openreview_id": "2kGKsyhtvh",
    "forum_id": "2kGKsyhtvh"
  },
  "analysis_timestamp": "2026-01-06T14:15:20.963912"
}
{
  "prior_works": [
    {
      "title": "Dynabench: Rethinking Benchmarking in NLP",
      "authors": "Douwe Kiela et al.",
      "year": 2021,
      "arxiv_id": "2104.14337",
      "role": "Inspiration",
      "relationship_sentence": "DyVal adopts the core idea of dynamic, model-in-the-loop evaluation from Dynabench but replaces human adversarial collection with automated, graph-guided synthesis that enables precise complexity control."
    },
    {
      "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding",
      "authors": "Yixin Nie et al.",
      "year": 2020,
      "arxiv_id": "1910.14599",
      "role": "Related Problem",
      "relationship_sentence": "ANLI\u2019s progressive, round-based hardening of evaluation directly informs DyVal\u2019s escalating-difficulty paradigm, which DyVal formalizes using DAG-defined complexity."
    },
    {
      "title": "CheckList: A Behavioral Testing Framework for NLP",
      "authors": "Marco Tulio Ribeiro et al.",
      "year": 2020,
      "arxiv_id": "2005.04118",
      "role": "Extension",
      "relationship_sentence": "Building on CheckList\u2019s programmatic, template-driven test generation, DyVal generalizes this notion to reasoning tasks by composing operations along a DAG to produce novel items with tunable difficulty."
    },
    {
      "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
      "authors": "Adam Besta et al.",
      "year": 2023,
      "arxiv_id": "2308.09687",
      "role": "Inspiration",
      "relationship_sentence": "By framing reasoning as traversal over general graph structures, Graph of Thoughts provides the representational insight DyVal leverages to define, measure, and control evaluation complexity via DAG topology."
    },
    {
      "title": "GSM8K: Training Verifiers to Solve Math Word Problems",
      "authors": "Karl Cobbe et al.",
      "year": 2021,
      "arxiv_id": "2110.14168",
      "role": "Foundation",
      "relationship_sentence": "GSM8K formalizes grade-school math reasoning as a core evaluation setting that DyVal targets, while DyVal addresses GSM8K\u2019s static and contamination-prone nature through on-the-fly, complexity-calibrated generation."
    },
    {
      "title": "Extracting Training Data from Large Language Models",
      "authors": "Nicholas Carlini et al.",
      "year": 2021,
      "arxiv_id": "2012.07805",
      "role": "Gap Identification",
      "relationship_sentence": "Evidence of memorization and data extraction in LMs motivates DyVal\u2019s design to mitigate contamination by generating fresh, previously unseen evaluation samples."
    },
    {
      "title": "CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text",
      "authors": "Koustuv Sinha et al.",
      "year": 2019,
      "arxiv_id": "1908.06177",
      "role": "Inspiration",
      "relationship_sentence": "CLUTRR\u2019s use of relational path length as a controllable knob for reasoning difficulty directly inspires DyVal\u2019s structural control via DAG size and topology across math, logic, and algorithmic tasks."
    }
  ],
  "synthesis_narrative": "Dynamic, model-in-the-loop benchmarking emerged with Dynabench, which advocated continually refreshed test sets that evolve alongside model capabilities, while ANLI operationalized this idea with progressive rounds that hardened NLI examples against current systems. CheckList showed that programmatic, template-based behavioral tests can systematically probe capabilities and generate targeted variants, suggesting a path to automated, repeatable evaluation construction. Graph of Thoughts introduced representing problem solving as traversals over general graph structures, revealing a natural link between the topology of reasoning artifacts and their difficulty. CLUTRR demonstrated that structural controls\u2014such as relation-path length\u2014can calibrate reasoning complexity in synthetic text tasks. In parallel, GSM8K established math word problems as a standard reasoning evaluation setting but remained static, and work by Carlini et al. revealed that memorization and data extraction can taint evaluations through contamination.\nThese strands collectively highlighted a gap: evaluations need to be dynamic to avoid contamination, yet systematically structured to control and scale reasoning difficulty. DyVal synthesizes these ideas by automating dynamic evaluation through graph-informed generation, using DAGs to encode operations and dependencies so that sample novelty and complexity are precisely tunable. This design inherits the dynamic ethos of Dynabench/ANLI, the programmatic rigor of CheckList, and the structural complexity control exemplified by CLUTRR and Graph of Thoughts, addressing the static and contamination-prone limitations in benchmarks like GSM8K and yielding a principled, adaptable protocol for reasoning evaluation.",
  "target_paper": {
    "title": "DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks",
    "authors": "Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Large Language Models, Evaluation, Data Contamination",
    "abstract": "Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns are raised about potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. \nIn this paper, we introduce DyVal, a general and flexible protocol for dynamic evaluation of LLMs. Based on our framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo and GPT-4. Experiments show that LLMs perform worse in DyVal-generated evaluation samples with different complexities, highlighting the significance of dynamic eval",
    "openreview_id": "gjfOL9z5Xr",
    "forum_id": "gjfOL9z5Xr"
  },
  "analysis_timestamp": "2026-01-06T15:50:35.927666"
}
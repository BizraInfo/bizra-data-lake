{
  "prior_works": [
    {
      "title": "Energy and Policy Considerations for Deep Learning in NLP",
      "authors": "Emma Strubell et al.",
      "year": 2019,
      "arxiv_id": "1906.02243",
      "role": "Foundation",
      "relationship_sentence": "This paper established accounting for the full development cycle (e.g., hyperparameter tuning) in NLP energy/carbon estimates, which we directly generalize to include hardware manufacturing and water use across the entire LLM creation process."
    },
    {
      "title": "Quantifying the Carbon Emissions of Machine Learning",
      "authors": "Alexandre Lacoste et al.",
      "year": 2019,
      "arxiv_id": "1910.09700",
      "role": "Foundation",
      "relationship_sentence": "We adopt and build on their core carbon-accounting formulation\u2014linking power, PUE, and location-specific grid intensity\u2014to compute emissions, then extend the scope to multi-stage development and integrate manufacturing and water impacts."
    },
    {
      "title": "Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model",
      "authors": "Sasha Luccioni et al.",
      "year": 2022,
      "arxiv_id": "2211.02001",
      "role": "Baseline",
      "relationship_sentence": "Their LLM-scale case study, including amortized embodied hardware emissions, serves as the primary methodological baseline that we extend from final training runs to the full development lifecycle while adding comprehensive water accounting."
    },
    {
      "title": "Carbon Emissions and Large Neural Network Training",
      "authors": "David Patterson et al.",
      "year": 2021,
      "arxiv_id": "2104.10350",
      "role": "Related Problem",
      "relationship_sentence": "Their analysis of datacenter efficiency, regional grid carbon intensity, and scheduling informs our parameterization of location- and infrastructure-dependent factors in life-cycle emissions and water estimates."
    },
    {
      "title": "Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning",
      "authors": "Peter Henderson et al.",
      "year": 2020,
      "arxiv_id": "2002.05651",
      "role": "Gap Identification",
      "relationship_sentence": "By highlighting the lack of lifecycle reporting and proposing standardized tracking, this work directly motivates our holistic, transparent accounting across development, training, and hardware manufacturing."
    },
    {
      "title": "Making AI Less Thirsty: Uncovering and Addressing the Secret Water Footprint of AI Models",
      "authors": "Pengfei Li et al.",
      "year": 2023,
      "arxiv_id": "2304.03271",
      "role": "Extension",
      "relationship_sentence": "We adapt their water-use estimation via WUE and cooling-water modeling and extend it from isolated training events to end-to-end LLM development and integrate it alongside carbon and manufacturing impacts."
    }
  ],
  "synthesis_narrative": "Early work on environmental impacts in NLP showed that reported training costs often ignore the broader development process; notably, Strubell et al. quantified the energy and carbon of hyperparameter tuning and ablations, establishing that the true footprint extends beyond a single final run. Lacoste et al. provided the core accounting framework translating metered energy, datacenter PUE, and regional grid carbon intensity into CO2e, enabling standardized, comparable estimates. At LLM scale, Luccioni et al. presented a concrete case study for BLOOM that went beyond runtime power to include amortized embodied emissions from hardware manufacturing, demonstrating how hardware life-cycle considerations materially affect totals. Patterson et al. analyzed how datacenter efficiency, regional grid mixes, and scheduling practices shape emissions, emphasizing the importance of infrastructure-aware parameterization. Henderson et al. called for systematic, lifecycle-oriented reporting practices and instrumentation to capture real-world energy use across experiments. Complementing carbon accounting, Li et al. introduced a practical methodology for quantifying AI\u2019s water footprint using datacenter water usage effectiveness and cooling models, revealing a significant but previously overlooked dimension. Together, these works reveal a methodological toolkit and a gap: estimates often focus on a single training run, omit water, or only partially consider embodied impacts. The natural next step is to unify these strands\u2014combining standardized carbon accounting with embodied hardware and water-use models\u2014and to apply them across the entire model development pipeline with real operational traces. By synthesizing lifecycle boundaries from BLOOM-style analyses, PUE/WUE-based estimators, and full-development accounting advocated by Strubell and Henderson, a holistic evaluation of creating language models becomes possible and policy-relevant.",
  "target_paper": {
    "title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
    "authors": "Jacob Morrison, Clara Na, Jared Fernandez, Tim Dettmers, Emma Strubell, Jesse Dodge",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "machine learning, artificial intelligence, language model, large language models, environmental impact, carbon emissions, water usage",
    "abstract": "As the performance of artificial intelligence systems has dramatically increased, so too has the environmental impact of creating these systems. While many model developers release estimates of the power consumption and carbon emissions from the final training runs for their latest models, there is comparatively little transparency into the impact of model development, hardware manufacturing, and total water usage throughout. In this work, we estimate the real-world environmental impact of developing a series of language models, ranging from 20 million to 13 billion active parameters, trained on up to 5.6 trillion tokens each. When accounting for hardware manufacturing, model development, and our final training runs, we find that our series of models released **493 metric tons** of carbon emissions, equivalent to powering about 98 homes in the United States for one year, and consumed **2.769 million liters of water**, equivalent to about 24.5 years of water usage by a person in the Uni",
    "openreview_id": "04qx93Viwj",
    "forum_id": "04qx93Viwj"
  },
  "analysis_timestamp": "2026-01-06T07:18:32.370468"
}
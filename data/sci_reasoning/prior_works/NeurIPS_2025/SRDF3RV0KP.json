{
  "prior_works": [
    {
      "title": "TAPAS: Weakly Supervised Table Parsing via Pre-training",
      "authors": "Jonathan Herzig, Pawe\u0142 Krzysztof Nowak, Thomas M\u00fcller, Francesco Piccinno, Julian Martin Eisenschlos",
      "year": 2020,
      "role": "Foundational table-LLM serialization approach",
      "relationship_sentence": "TAPAS established the paradigm of serializing table structure and cell contents for LM-based reasoning, whose token- and privacy-heavy design motivates our shift to compact logical rule interfaces."
    },
    {
      "title": "TAPEX: Table Pre-training via Learning a Neural SQL Executor",
      "authors": "Qian Liu, Bei Chen, Wenqiang Lei, Chaoyu Tang, Jian-Guo Shen, Nan Duan",
      "year": 2022,
      "role": "Methodological inspiration (executable intermediates)",
      "relationship_sentence": "By showing that an executable symbolic layer (SQL) can mediate LMs\u2019 table reasoning, TAPEX directly inspires our use of decision-tree rules as a lightweight, executable logic intermediate for tabular tasks."
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc V. Le, Denny Zhou",
      "year": 2022,
      "role": "Reasoning scaffold inspiration",
      "relationship_sentence": "CoT shows explicit intermediate reasoning improves LLM performance; our decision paths operationalize a structured, compact analogue of CoT tailored to tabular logic (feature-threshold rules)."
    },
    {
      "title": "Distilling a Neural Network Into a Soft Decision Tree",
      "authors": "Nicholas Frosst, Geoffrey Hinton",
      "year": 2017,
      "role": "Neural\u2013symbolic bridge via trees",
      "relationship_sentence": "This work demonstrates that neural predictions can be captured by decision-tree structures, underpinning our idea of interfacing LLMs with tabular data through interpretable tree rules."
    },
    {
      "title": "RuleFit: Predictive Learning via Rule Ensembles",
      "authors": "Jerome H. Friedman, Bogdan E. Popescu",
      "year": 2008,
      "role": "Interpretable rule sets for tabular prediction",
      "relationship_sentence": "RuleFit\u2019s conversion of trees into sparse rule ensembles motivates our choice of compact, human-readable rules as a token-efficient, privacy-aware representation for LLM interaction."
    },
    {
      "title": "XGBoost: A Scalable Tree Boosting System",
      "authors": "Tianqi Chen, Carlos Guestrin",
      "year": 2016,
      "role": "Practical source of high-quality decision rules",
      "relationship_sentence": "XGBoost provides robust decision-tree ensembles whose learned splits and paths can be harvested as high-signal rules, the substrate our method leverages for LLM reasoning on tabular data."
    },
    {
      "title": "Revisiting Deep Learning Models for Tabular Data",
      "authors": "Ilya O. Gorishniy, Yury Rubachev, Valentin Khrulkov, Artem Babenko",
      "year": 2021,
      "role": "Problem framing and baseline context",
      "relationship_sentence": "This study documents the persistent strength of tree-based methods and the difficulty of end-to-end neural approaches on tabular data, motivating our non-serialization, tree-rule\u2013centric integration with LLMs."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014integrating LLMs with tabular data via decision-tree rules as an intermediate\u2014directly grows from two threads: prior LM-table interfaces that serialize data into tokens and neuro-symbolic methods that operationalize logical structure. TAPAS exemplifies the dominant strategy of tokenizing table structure and cell contents for LM reasoning, but its reliance on verbose textual exposure underscores privacy and input-length limitations that the present work seeks to overcome. TAPEX advances this by interposing an executable symbolic layer (SQL), empirically validating that LMs benefit from compact, logic-based intermediates; our method generalizes this insight to decision rules that fit generic tabular prediction beyond QA.\nChain-of-Thought shows that explicit intermediate reasoning improves LLM performance; we instantiate an analogous but domain-appropriate scaffold where decision paths act as succinct, structured rationales over feature thresholds. On the modeling side, Frosst and Hinton\u2019s soft decision trees demonstrate that neural behavior can be captured in tree form, while RuleFit shows trees can be compressed into sparse, interpretable rule sets\u2014both directly informing our design of a rule-based interface that is both predictive and token-efficient. XGBoost provides a practical and powerful mechanism to derive high-quality rules at scale, serving as a rule source our framework can extract and manipulate. Finally, the empirical findings of Gorishniy et al. reinforce that end-to-end neural approaches often underperform on tabular data, motivating a hybrid that leverages tree logic for structure and LLMs for flexible reasoning without exposing raw tables or exceeding context limits.",
  "analysis_timestamp": "2026-01-07T00:05:12.538571"
}
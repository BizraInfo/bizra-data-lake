{
  "prior_works": [
    {
      "title": "Gaussian Process Optimization in the Bandit Setting: No Regret Algorithms",
      "authors": "Srinivas et al.",
      "year": 2010,
      "arxiv_id": "0912.3995",
      "role": "Foundation",
      "relationship_sentence": "This work provides the GP-UCB framework and kernel-dependent information gain analysis that the paper builds on to reason about how SE versus Mat\u00e9rn kernels interact with dimensionality in BO."
    },
    {
      "title": "Practical Bayesian Optimization of Machine Learning Algorithms",
      "authors": "Snoek et al.",
      "year": 2012,
      "arxiv_id": "1206.2944",
      "role": "Inspiration",
      "relationship_sentence": "Its empirical advocacy of Mat\u00e9rn 5/2 kernels, ARD, and careful length-scale priors directly motivates re-examining 'standard BO' kernel choices and analyzing how length-scale initialization affects training stability."
    },
    {
      "title": "Bayesian Optimization in High Dimensions via Random Embeddings (REMBO)",
      "authors": "Wang et al.",
      "year": 2013,
      "arxiv_id": "1301.1942",
      "role": "Gap Identification",
      "relationship_sentence": "By asserting that naive GP BO fails in high dimensions and proposing random embeddings, this paper cemented the prevailing belief that the present work challenges and surpasses using standard GPs with appropriate kernels."
    },
    {
      "title": "High Dimensional Bayesian Optimization and Bandits via Additive Models",
      "authors": "Kandasamy et al.",
      "year": 2015,
      "arxiv_id": "1503.01673",
      "role": "Gap Identification",
      "relationship_sentence": "This method\u2019s reliance on additive kernel structure to circumvent dimensionality embodies the structural assumptions the current paper shows are unnecessary when standard BO uses Mat\u00e9rn kernels and proper hyperparameter handling."
    },
    {
      "title": "TuRBO: Trust-Region Bayesian Optimization",
      "authors": "Eriksson et al.",
      "year": 2019,
      "arxiv_id": "1910.01739",
      "role": "Baseline",
      "relationship_sentence": "As a leading high-dimensional BO approach that replaces a global GP with local trust-region models to avoid surrogate pathologies, TuRBO serves as a primary comparator that the paper systematically reevaluates against standard GPs."
    },
    {
      "title": "Scalable Bayesian Optimization in High Dimensions via Sparse Axis-Aligned Subspace Priors (SAASBO)",
      "authors": "Eriksson et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "By imposing sparsity-inducing priors over ARD length-scales to discover low-dimensional subspaces, SAASBO represents the dominant specialized alternative that the paper shows can be matched or exceeded by standard Mat\u00e9rn GPs without sparsity priors."
    }
  ],
  "synthesis_narrative": "Kernel-dependent learning behavior in Gaussian process bandits was formalized by Srinivas et al., who tied regret to information gain, making explicit how kernel smoothness and hyperparameters interact with dimensionality. Snoek et al. demonstrated in practice that Mat\u00e9rn 5/2 kernels with ARD and informed priors on length-scales often outperform squared exponential choices, and emphasized marginal-likelihood training and initialization details that make GP BO robust. REMBO crystallized the notion that naive GP BO fails in high dimensions and proposed random low-dimensional embeddings as a remedy, catalyzing a wave of embedding-based methods. Kandasamy et al. argued that additivity assumptions are key to overcoming dimensionality, constructing GP-UCB on additive kernels with regret scaling in group sizes rather than ambient dimension. Eriksson et al.\u2019s TuRBO sidestepped global surrogate issues by using local trust regions with GPs, establishing a powerful high-dimensional baseline. SAASBO introduced sparsity-inducing priors on ARD length-scales to automatically select a small active subspace, becoming a standard for high-dimensional problems.\nTogether, these works fostered a consensus that specialized structure or locality is required for high-dimensional BO, yet they also hinted\u2014through kernel choice, hyperparameter priors, and theory\u2014that the kernel and its training are pivotal. This paper synthesizes those cues by revisiting plain GP BO, showing empirically that Mat\u00e9rn kernels suffice in high dimensions and theoretically that common SE length-scale initializations induce vanishing gradients, clarifying why SE underperforms and why Mat\u00e9rn avoids the failure mode.",
  "target_paper": {
    "title": "Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization",
    "authors": "Zhitong Xu, Haitao Wang, Jeff M. Phillips, Shandian Zhe",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Gaussian Process, Bayesian Optimization, High Dimensional Bayesian Optimization",
    "abstract": "A long-standing belief holds that Bayesian Optimization (BO) with standard Gaussian processes (GP) --- referred to as standard BO --- underperforms in high-dimensional optimization problems. While this belief seems plausible, it lacks both robust empirical evidence and theoretical justification. To address this gap, we present a systematic investigation. First, through a comprehensive evaluation across twelve benchmarks, we found that while the popular Square Exponential (SE) kernel often leads to poor performance, using Mat\\'ern kernels enables standard BO to consistently achieve top-tier results, frequently surpassing methods specifically designed for high-dimensional optimization. Second, our theoretical analysis reveals that the SE kernel\u2019s failure primarily stems from improper initialization of the length-scale parameters, which are commonly used in practice but can cause gradient vanishing in training. We provide a probabilistic bound to characterize this issue, showing that Mat\\",
    "openreview_id": "kX8h23UG6v",
    "forum_id": "kX8h23UG6v"
  },
  "analysis_timestamp": "2026-01-06T09:31:26.070381"
}
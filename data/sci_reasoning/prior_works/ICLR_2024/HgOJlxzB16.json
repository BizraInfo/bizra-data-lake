{
  "prior_works": [
    {
      "title": "On the Power and Limitations of Random Features for Learning",
      "authors": "Sivan Yehudai et al.",
      "year": 2019,
      "arxiv_id": "1908.01000",
      "role": "Gap Identification",
      "relationship_sentence": "They prove that random-features/NTK-style training cannot learn parity/XOR under isotropic distributions with polynomial resources, directly motivating a feature-learning (non-lazy) analysis to overcome this barrier."
    },
    {
      "title": "On Lazy Training in Differentiable Programming",
      "authors": "L\u00e9na\u00efc Chizat et al.",
      "year": 2019,
      "arxiv_id": "1812.07956",
      "role": "Inspiration",
      "relationship_sentence": "This work formalizes the lazy vs feature-learning regimes and conditions for significant parameter movement, inspiring the paper\u2019s two-phase view where neurons first move non-lazily to find features before entering a tuning phase."
    },
    {
      "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks",
      "authors": "Kaifeng Lyu et al.",
      "year": 2019,
      "arxiv_id": "1906.05890",
      "role": "Extension",
      "relationship_sentence": "Their directional convergence and layer-balancing results for homogeneous networks under logistic loss are extended to the signal-heavy phase to show SGD maintains and balances the discovered XOR features."
    },
    {
      "title": "The Implicit Bias of Gradient Descent on Separable Data",
      "authors": "Daniel Soudry et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This establishes that gradient descent on logistic loss converges directionally to max-margin classifiers on separable data, providing the foundational tool to characterize the tuning/balancing dynamics once XOR features render the problem separable."
    },
    {
      "title": "Kernel and Rich Regimes in Overparameterized Models",
      "authors": "Blake Woodworth et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "By delineating kernel (lazy) versus rich (feature-learning) regimes and their learnability implications, this work motivates operating in (and analyzing) a rich regime necessary to capture XOR beyond NTK limitations."
    },
    {
      "title": "Gradient Descent Learns One-Hidden-Layer Convolutional Neural Networks",
      "authors": "Simon S. Du et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "Their analysis showing neurons specialize and evolve (approximately) independently from random initialization informs the signal-finding phase where many neurons independently align to the quadratic (pairwise) XOR features."
    }
  ],
  "synthesis_narrative": "Random-features and NTK analyses have shown stark limits for learning simple but non-linear interactions like parity/XOR: Yehudai and Shamir proved that with isotropic inputs, such kernelized approaches cannot succeed with polynomial resources, making explicit that learning must rely on feature movement rather than fixed features. Chizat, Oyallon, and Bach formalized the lazy versus feature-learning dichotomy, identifying when parameters undergo significant motion and thus can create new features, laying conceptual groundwork for a phase-based view of training dynamics. In parallel, the implicit-bias literature for logistic loss provides precise descriptions of the tuning dynamics once data become separable: Soudry et al. established directional convergence to max-margin classifiers, and Lyu and Li extended this to deep homogeneous networks, showing margin maximization accompanied by inter-layer weight balancing. Complementing these, Woodworth et al. delineated kernel versus rich regimes in overparameterized models, underscoring that rich (feature-learning) dynamics are essential for tasks beyond kernels. Finally, Du et al. demonstrated in a related shallow setting that neurons can specialize and evolve largely independently from random initialization, a mechanism that supports feature discovery.\n\nTaken together, these works expose both the necessity and the mechanism of feature learning to overcome NTK barriers on XOR, and they provide the tools to analyze what happens after features are found. The rich/lazy dichotomy and neuron specialization insights suggest an initial feature-finding phase where many units independently latch onto pairwise interactions, while the implicit-bias results for logistic loss naturally describe a subsequent signal-heavy phase that tunes and balances those features. This synthesis enables a rigorous two-phase analysis showing that standard minibatch SGD on a standard two-layer ReLU network can learn XOR with near-optimal sample complexity.",
  "target_paper": {
    "title": "SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem",
    "authors": "Margalit Glasgow",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "optimization, stochastic gradient descent, two-layer neural network, sample complexity",
    "abstract": "In this work, we consider the optimization process of minibatch stochastic gradient descent (SGD) on a 2-layer neural network with data separated by a quadratic ground truth function. We prove that with data drawn from the Boolean hypercube labeled by the quadratic ``XOR'' function $y = -x_ix_j$ , it is possible to train to a population error $o(1)$\n with $\\Theta(d\\text{polylog}(d))$ samples. Our result considers simultaneously training both layers of the two-layer-neural network with ReLU activations via standard minibatch SGD on the logistic loss. To our knowledge, this work is the first to give a sample complexity of \n for efficiently learning the XOR function on isotropic data on a standard neural network with standard training. Our main technique is showing that the network evolves in two phases: a \\em signal-finding \\em phase where the network is small and many of the neurons evolve independently to find features, and a \\em signal-heavy \\em phase, where SGD maintains and balances",
    "openreview_id": "HgOJlxzB16",
    "forum_id": "HgOJlxzB16"
  },
  "analysis_timestamp": "2026-01-06T15:48:23.393402"
}
{
  "prior_works": [
    {
      "title": "Deep generative modeling for single-cell transcriptomics (scVI)",
      "authors": "Romain Lopez et al.",
      "year": 2018,
      "role": "Foundational single-cell generative model and widely used batch-correction baseline",
      "relationship_sentence": "scVI provided the canonical specialized single-cell framework against which scSSL-Bench calibrated uni-modal batch correction performance and evaluation protocols."
    },
    {
      "title": "CLAIRE: Contrastive learning for single-cell RNA-seq batch correction",
      "authors": "Anonymous et al.",
      "year": 2023,
      "role": "Domain-specific contrastive SSL for scRNA-seq integration",
      "relationship_sentence": "As a specialized single-cell SSL method designed for batch correction, CLAIRE directly motivated the benchmark\u2019s task design and enabled head-to-head comparison between domain-specific and generic SSL paradigms."
    },
    {
      "title": "scGPT: Toward building a foundation model for single-cell multi-omics using generative pretraining",
      "authors": "Anonymous et al.",
      "year": 2023,
      "role": "Transformer-based foundation model for single-cell with self-supervised pretraining and fine-tuning",
      "relationship_sentence": "scGPT exemplifies finetuned, large-scale pretraining for single-cell data; scSSL-Bench incorporated it to assess whether foundation-model pretraining confers advantages on batch correction and modality tasks."
    },
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations (SimCLR)",
      "authors": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton",
      "year": 2020,
      "role": "Generic contrastive SSL framework",
      "relationship_sentence": "SimCLR\u2019s augmentation-driven contrastive paradigm informed the inclusion and adaptation of generic SSL baselines in scSSL-Bench and underpinned findings that generic SSL can excel at cell typing and multi-modal integration."
    },
    {
      "title": "VICReg: Variance-Invariance-Covariance Regularization",
      "authors": "Adrien Bardes, Jean Ponce, Yann LeCun",
      "year": 2022,
      "role": "Generic non-contrastive SSL objective",
      "relationship_sentence": "VICReg\u2019s redundancy-reduction objective provided a strong, augmentation-friendly baseline; scSSL-Bench leveraged it to demonstrate that non-contrastive SSL can outperform domain-specific methods on annotation and integration tasks."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners (MAE)",
      "authors": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, Ross Girshick",
      "year": 2022,
      "role": "Foundational masked reconstruction SSL and random masking strategy",
      "relationship_sentence": "MAE popularized random masking as an SSL signal; scSSL-Bench systematically evaluated masking-based augmentations and found random masking to be the most robust across single-cell tasks."
    },
    {
      "title": "Benchmarking atlas-level data integration in single-cell genomics",
      "authors": "Malte D. Luecken et al.",
      "year": 2022,
      "role": "Design precedent for multi-dataset, multi-task single-cell benchmarking",
      "relationship_sentence": "This benchmark established rigorous evaluation settings, datasets, and metrics for integration and batch correction, directly informing scSSL-Bench\u2019s dataset curation, task selection, and evaluation protocol."
    }
  ],
  "synthesis_narrative": "scSSL-Bench\u2019s core contribution\u2014a systematic, task-driven benchmark of self-supervised learning for single-cell data\u2014rests on two converging lines of prior work. From single-cell\u2013specific modeling, scVI established the de facto baseline for probabilistic representation learning and batch correction, while CLAIRE introduced a contrastive paradigm tailored to scRNA-seq integration; together they motivated scSSL-Bench\u2019s focus on uni-modal batch correction and provided specialized comparators. The emergence of scGPT extended this line to foundation-model pretraining and fine-tuning in single-cell, prompting the benchmark to probe whether large-scale generative pretraining yields consistent gains across tasks. From generic SSL, SimCLR and VICReg offered augmentation-driven contrastive and non-contrastive objectives that are architecture- and domain-agnostic; including these methods enabled a principled test of whether general SSL principles transfer to biological data and, indeed, revealed strengths in cell type annotation and multi-modal integration. MAE\u2019s masked reconstruction popularized random masking as an information-efficient pretext signal; scSSL-Bench operationalized this insight by systematically evaluating augmentation schemes and demonstrating the cross-task superiority of random masking over domain-specific augmentations. Finally, the design and rigor of the evaluation were directly guided by the atlas-scale integration benchmark of Luecken et al., informing dataset selection, task definitions, and metrics. Together, these works directly shaped scSSL-Bench\u2019s method portfolio, task suite, and augmentation study, enabling clear, task-specific conclusions about when specialized versus generic SSL methods prevail.",
  "analysis_timestamp": "2026-01-07T00:29:42.078191"
}
{
  "prior_works": [
    {
      "title": "TD-MPC: Temporal-Difference Learning for Model Predictive Control",
      "authors": "Nicklas Hansen et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "TD-MPC2 directly extends TD-MPC\u2019s decoder-free latent world model and latent-space MPC, modifying the TD-driven training objectives, architecture, and scaling regimen while keeping the same planning-in-latent framework."
    },
    {
      "title": "PlaNet: Learning Latent Dynamics for Planning from Pixels",
      "authors": "Danijar Hafner et al.",
      "year": 2019,
      "arxiv_id": "1811.04551",
      "role": "Foundation",
      "relationship_sentence": "TD-MPC2 builds on PlaNet\u2019s core idea of CEM-based local trajectory optimization in a learned latent dynamics model, while replacing reconstruction-trained RSSM with value-centric, decoder-free learning."
    },
    {
      "title": "DreamerV3: Mastering Diverse Domains through World Models",
      "authors": "Danijar Hafner et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By showing that robust, single-hyperparameter world-model agents can scale across many domains, DreamerV3 set the bar and exposed limitations of reconstruction-heavy, actor-critic pipelines that TD-MPC2 addresses with an MPC-based, decoder-free alternative."
    },
    {
      "title": "PETS: Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models",
      "authors": "Kurtland Chua et al.",
      "year": 2018,
      "arxiv_id": "1805.12114",
      "role": "Foundation",
      "relationship_sentence": "PETS pioneered uncertainty-aware model ensembles and sampling-based MPC (CEM) that inform TD-MPC2\u2019s robust planning and ensemble design for continuous control."
    },
    {
      "title": "DrQ-v2: Stronger, Better, and Faster",
      "authors": "Denis Yarats et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "DrQ\u2011v2 serves as the principal model-free vision baseline with strong data augmentation and stable hyperparameters that TD\u2011MPC2 consistently benchmarks against and aims to surpass under a single configuration."
    },
    {
      "title": "Value-Aware Model Learning",
      "authors": "Amir-massoud Farahmand et al.",
      "year": 2017,
      "arxiv_id": "1701.06676",
      "role": "Inspiration",
      "relationship_sentence": "The value-aware principle\u2014that models should preserve value-relevant predictions rather than reconstruct observations\u2014directly motivates TD\u2011MPC2\u2019s TD-trained, decoder-free world model objective."
    }
  ],
  "synthesis_narrative": "Planning-based model-based RL from pixels crystallized with latent world models. PlaNet introduced learning a recurrent latent dynamics model and performing CEM-based local trajectory optimization entirely in latent space, showing that planning need not occur in pixel space. PETS established the effectiveness of sampling-based MPC with uncertainty-aware ensembles, providing a recipe for robust control in continuous domains that many later world-model methods inherited. DreamerV3 demonstrated that scaling latent world models and adhering to a single hyperparameter set can produce strong, general-purpose agents across diverse domains, although it achieved this with actor-critic policies and reconstruction-heavy training objectives. In parallel, value-aware model learning argued that models should be optimized for task-relevant predictive fidelity (value consistency) instead of pixel reconstruction, motivating decoder-free objectives for world models. TD\u2011MPC operationalized these ideas by discarding decoders, training latent dynamics via temporal-difference signals, and performing latent-space MPC, delivering strong sample efficiency and robustness on visual control. Strong model-free methods like DrQ\u2011v2 set competitive baselines and training practices for vision-based control, emphasizing simple, stable pipelines. Building on this, the next step was to marry PlaNet\u2019s latent-space planning with value-aware, decoder-free training, adopt ensemble-aware robust MPC from PETS, and embrace DreamerV3\u2019s scale and single-configuration ethos. This synthesis naturally leads to a scalable, robust, decoder-free world-model control framework that retains the planning advantages of MPC while achieving cross-domain generality with a single set of hyperparameters.",
  "target_paper": {
    "title": "TD-MPC2: Scalable, Robust World Models for Continuous Control",
    "authors": "Nicklas Hansen, Hao Su, Xiaolong Wang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "reinforcement learning, model-based reinforcement learning, world models",
    "abstract": "TD-MPC is a model-based reinforcement learning (RL) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoder-free) world model. In this work, we present TD-MPC2: a series of improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves significantly over baselines across 104 online RL tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters. We further show that agent capabilities increase with model and data size, and successfully train a single 317M parameter agent to perform 80 tasks across multiple task domains, embodiments, and action spaces. We conclude with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents.\n\nExplore videos, models, data, code, and more at https://tdmpc2.com",
    "openreview_id": "Oxh5CstDJU",
    "forum_id": "Oxh5CstDJU"
  },
  "analysis_timestamp": "2026-01-06T23:46:12.140188"
}
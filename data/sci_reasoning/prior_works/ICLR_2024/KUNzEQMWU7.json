{
  "prior_works": [
    {
      "title": "DVQA: Understanding Data Visualizations via Question Answering",
      "authors": "Kushal Kafle et al.",
      "year": 2018,
      "arxiv_id": "1801.08163",
      "role": "Foundation",
      "relationship_sentence": "DVQA introduced chart question answering with numeric, textual, and structural reasoning over plots, providing one of the core task formulations and data sources that MathVista aggregates and systematically evaluates within a unified math-in-vision benchmark."
    },
    {
      "title": "ChartQA: A Benchmark for Question Answering on Charts",
      "authors": "Mohamed Masry et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "ChartQA extended chart QA toward semantic and logical operations on real charts, and MathVista directly incorporates this style of chart reasoning to test compositional mathematical inference over visualizations at scale."
    },
    {
      "title": "RAVEN: A Dataset for Relational and Analogical Visual Reasoning",
      "authors": "Chi Zhang et al.",
      "year": 2019,
      "arxiv_id": "1903.02741",
      "role": "Foundation",
      "relationship_sentence": "RAVEN formalized abstract IQ-style matrix reasoning with visual patterns, directly informing MathVista\u2019s IQTest subset design to probe analogical and relational visual reasoning with mathematical structures."
    },
    {
      "title": "IconQA: A New Benchmark for Abstract Diagram Understanding and Reasoning",
      "authors": "Pan Lu et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "IconQA demonstrated that LMMs struggle with abstract diagrammatic reasoning (including math-like patterns), inspiring MathVista to broaden and standardize such visual-math challenges and include analogous IQ-style items."
    },
    {
      "title": "DocVQA: A Dataset for Document Visual Question Answering",
      "authors": "Minesh Mathew et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "DocVQA established document-centric VQA where understanding text, tables, and layout is essential, a formulation MathVista adapts in its PaperQA subset to evaluate mathematical reasoning over figures and scientific documents."
    },
    {
      "title": "ScienceQA: A Large Dataset of Multi-Modal Science Questions and Answers with Explanations",
      "authors": "Pan Lu et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "ScienceQA showed the value of multi-modal reasoning and explanations across science topics but lacked a focused, rigorous math-in-vision evaluation, a gap MathVista explicitly targets with specialized tasks and diagnostics."
    },
    {
      "title": "Measuring Mathematical Problem Solving With the MATH Dataset",
      "authors": "Dan Hendrycks et al.",
      "year": 2021,
      "arxiv_id": "2103.03874",
      "role": "Gap Identification",
      "relationship_sentence": "The MATH dataset catalyzed text-only mathematical reasoning evaluation, and its omission of visual contexts directly motivated MathVista\u2019s formulation to assess math reasoning that requires fine-grained visual understanding."
    }
  ],
  "synthesis_narrative": "Early chart understanding benchmarks like DVQA established question answering over data visualizations, requiring numeric, textual, and structural reasoning from plots. ChartQA advanced this direction by emphasizing logical and compositional queries on real-world charts, pushing beyond synthetic setups toward realistic chart semantics. Abstract reasoning datasets such as RAVEN defined matrix-style analogical reasoning over visual patterns, crystallizing IQ-like relational structures vital for probing generalization. IconQA brought abstract diagram understanding into a QA setting, revealing persistent weaknesses of multimodal systems on symbol- and pattern-centric reasoning akin to mathematical thinking. In document understanding, DocVQA formalized question answering over visually rich documents\u2014text, tables, and layout\u2014highlighting the need for models that integrate OCR, structure parsing, and reasoning. ScienceQA broadened multi-modal QA across science domains with explanations, demonstrating the promise of cross-modal reasoning but without a dedicated, rigorous focus on mathematical competence. Meanwhile, the MATH dataset sharpened evaluation for text-only mathematical problem solving, setting expectations for reasoning depth while leaving visual perception unaddressed.\nTogether, these works exposed a clear opportunity: despite strong progress in chart/diagram QA, document understanding, and text-only math benchmarks, there was no unified, systematic assessment of mathematical reasoning that inherently depends on visual inputs. MathVista synthesizes chart, abstract diagram, and document-centric formulations into a comprehensive benchmark, introduces new subsets tailored to mathematical visual reasoning (IQTest, FunctionQA, PaperQA), and standardizes evaluation across state-of-the-art LMMs to diagnose fine-grained visual and compositional math skills.",
  "target_paper": {
    "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
    "authors": "Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "large language models, large multimodal models, mathematical reasoning, vision-language reasoning, foundation models and their evaluations",
    "abstract": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of",
    "openreview_id": "KUNzEQMWU7",
    "forum_id": "KUNzEQMWU7"
  },
  "analysis_timestamp": "2026-01-06T09:26:44.836295"
}
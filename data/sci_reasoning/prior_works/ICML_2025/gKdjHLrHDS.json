{
  "prior_works": [
    {
      "title": "Classification and Geometry of General Perceptual Manifolds",
      "authors": "SueYeon Chung et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Introduced manifold capacity theory and concrete geometric metrics (e.g., manifold radius, dimension, capacity) that this paper directly employs to quantify how task-relevant representational manifolds change during feature learning."
    },
    {
      "title": "Separability and geometry of object manifolds in deep neural networks",
      "authors": "Uri Cohen et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "Demonstrated that deep networks progressively untangle object manifolds across layers; the present work extends this framework from static layer-wise comparisons to temporal learning dynamics, revealing distinct feature-learning regimes via manifold geometry."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Formalized the lazy (linearized) training regime that underpins the \u2018lazy\u2019 side of the lazy\u2013rich dichotomy, providing the theoretical baseline this paper critiques and moves beyond using representational geometry."
    },
    {
      "title": "On Lazy Training in Differentiable Programming",
      "authors": "L\u00e9na\u00efc Chizat et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Characterized conditions leading to lazy training and the absence of feature learning, highlighting a limitation that directly motivates this paper\u2019s need to dissect multiple feature-learning behaviors beyond a simple dichotomy."
    },
    {
      "title": "Mean-field theory of two-layer neural networks: dimension-free bounds on population risk",
      "authors": "Song Mei et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Provided the contrasting mean-field (feature-learning/rich) regime and training dynamics, forming the other pole of the dichotomy that this paper refines by uncovering diverse sub-regimes via manifold geometry."
    },
    {
      "title": "Kernel and Rich Regimes in Overparameterized Models",
      "authors": "Blake E. Woodworth et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "Codified the prevailing kernel (lazy) versus rich taxonomy; the current work directly improves upon this baseline categorization by proposing a geometry-based framework that reveals a richer taxonomy of feature learning."
    },
    {
      "title": "How Does the Brain Solve Visual Object Recognition?",
      "authors": "James J. DiCarlo et al.",
      "year": 2012,
      "role": "Inspiration",
      "relationship_sentence": "Articulated the \u2018untangling\u2019 principle for object manifolds in the ventral stream; this conceptual lens directly inspires the paper\u2019s core idea that learning manifests as the untangling of task-relevant manifolds."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014probing feature learning through the geometry of task-relevant manifolds to move beyond the lazy\u2013rich dichotomy\u2014rests on two converging intellectual lines. First, manifold-based representational theory from neuroscience and deep learning supplied the measurement language. Chung et al. (2018) established manifold capacity theory and quantitative geometric metrics for classification on object manifolds, while Cohen et al. (2020) showed that deep networks progressively untangle manifolds across layers. DiCarlo et al. (2012) provided the conceptual anchor of \u2018untangling\u2019 as the hallmark of effective representations. Together, these works directly enable the present paper\u2019s shift from inspecting individual features to tracking geometry of task-relevant manifolds as learning unfolds.\nSecond, modern training-theory formalized the lazy\u2013rich dichotomy that this work refines. Jacot et al. (2018) introduced the NTK framework capturing lazy, random-feature-like training, while Mei et al. (2018) developed mean-field analyses capturing feature-learning (rich) dynamics. Chizat and Bach (2019) clarified the conditions and limitations of lazy training, and Woodworth et al. (2020) codified the kernel-versus-rich taxonomy that became the field\u2019s de facto baseline. The present paper directly addresses the gap these works leave\u2014an overly coarse dichotomy\u2014by using manifold geometry to reveal distinct learning behaviors shaped by algorithm, architecture, and data. In synthesis, prior manifold-geometry methods furnish the tools, and lazy\u2013rich theory poses the motivating limitation; their intersection yields a principled, geometry-first framework that uncovers a richer taxonomy of feature learning.",
  "analysis_timestamp": "2026-01-06T23:07:19.568364"
}
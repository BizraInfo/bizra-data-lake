{
  "prior_works": [
    {
      "title": "Theory of Games and Economic Behavior",
      "authors": "John von Neumann, Oskar Morgenstern",
      "year": 1944,
      "role": "Theoretical foundation for representing coherent preferences with a utility function",
      "relationship_sentence": "The paper\u2019s core test\u2014whether LLM preferences admit a consistent utility representation\u2014rests on the VNM link between axiomatic coherence of preferences and the existence of an underlying utility function."
    },
    {
      "title": "The Construction of a Utility Function from Demand Data",
      "authors": "S. N. Afriat",
      "year": 1967,
      "role": "Methodological basis via revealed preference (Afriat\u2019s theorem/GARP) to test internal consistency and recover utilities",
      "relationship_sentence": "Utility Engineering adapts revealed-preference logic to AI by checking global consistency of observed choices and reconstructing utility orderings, directly echoing Afriat\u2019s framework."
    },
    {
      "title": "Algorithms for Inverse Reinforcement Learning",
      "authors": "Andrew Y. Ng, Stuart Russell",
      "year": 2000,
      "role": "Conceptual and algorithmic precedent for inferring latent reward/utility from behavior",
      "relationship_sentence": "Treating an AI\u2019s \u2018values\u2019 as latent utilities inferred from its expressed preferences follows the IRL paradigm introduced by Ng and Russell."
    },
    {
      "title": "Deep Reinforcement Learning from Human Preferences",
      "authors": "Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei",
      "year": 2017,
      "role": "Operational machinery for mapping pairwise preferences to learned utility/reward models",
      "relationship_sentence": "The paper\u2019s control agenda\u2014shaping and optimizing learned utilities\u2014builds on preference-based reward modeling established by Christiano et al."
    },
    {
      "title": "Emergent Abilities of Large Language Models",
      "authors": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Denny Zhou, Donald Metzler, et al.",
      "year": 2022,
      "role": "Empirical framing of emergent properties with scale",
      "relationship_sentence": "The finding that preference coherence strengthens with model scale is positioned as an emergent property, directly motivated by the emergence literature."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, S\u00f6ren Mindermann, Jacob Steinhardt, et al.",
      "year": 2022,
      "role": "Precedent for principled, normative control of LLM behavior",
      "relationship_sentence": "Utility Engineering\u2019s \u2018control\u2019 component\u2014steering utilities toward desired value systems\u2014extends the idea of imposing high-level principles pioneered by Constitutional AI."
    },
    {
      "title": "Aligning AI with Shared Human Values",
      "authors": "Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Dawn Song, Jacob Steinhardt",
      "year": 2021,
      "role": "Early articulation and benchmarking of values in AI systems",
      "relationship_sentence": "This work motivates measuring and improving model values; the present paper deepens it by moving from surface moral judgments to internally coherent utility structures and their controllability."
    }
  ],
  "synthesis_narrative": "Utility Engineering\u2019s central move\u2014testing whether large language models exhibit internally coherent \u2018values\u2019 and then shaping those values\u2014draws directly from utility theory, preference inference, and recent alignment practice. Von Neumann\u2013Morgenstern provides the foundational representation result that coherent preferences can be captured by a utility function; Afriat\u2019s revealed-preference program turns that idea into testable consistency criteria and constructive utility recovery, which the authors adapt from economics to the LLM setting. On the machine learning side, Ng and Russell\u2019s inverse reinforcement learning reframes behavior as evidence about latent rewards/utilities, while Christiano et al. operationalize preference-based learning to build and optimize such utilities from pairwise comparisons\u2014machinery that underpins the paper\u2019s \u2018control\u2019 side of engineering utilities.\n\nThe empirical claim that coherence strengthens with model size is situated within the emergence literature, most notably Wei et al., who document qualitative capability shifts with scale; here, value coherence is treated as a new emergent property. For steering those emergent utilities, Constitutional AI demonstrates how explicit normative principles can systematically shape assistant behavior, and Utility Engineering generalizes this toward engineering the underlying utility landscape rather than only surface outputs. Finally, prior work by Hendrycks and colleagues on aligning AI with shared human values supplies both motivation and benchmarks for value-sensitive evaluation; the present paper advances from measuring moral judgments to probing whether models possess structured, utility-like value systems and how those systems can be analyzed and controlled.",
  "analysis_timestamp": "2026-01-06T23:42:48.141522"
}
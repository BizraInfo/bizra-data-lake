{
  "prior_works": [
    {
      "title": "Action understanding as inverse planning",
      "authors": "Chris L. Baker et al.",
      "year": 2009,
      "role": "Foundation",
      "relationship_sentence": "AutoToM directly instantiates Baker et al.\u2019s framing of Theory-of-Mind as Bayesian inverse planning\u2014inferring latent goals and beliefs from observed actions\u2014but automates the construction and refinement of the agent model rather than handcrafting it."
    },
    {
      "title": "Bayesian Theory of Mind: Modeling joint belief\u2013desire attribution",
      "authors": "Chris L. Baker et al.",
      "year": 2011,
      "role": "Extension",
      "relationship_sentence": "AutoToM extends the BToM belief\u2013desire framework by letting an LLM propose and refine the set of mental state variables (and temporal context) used for inference, automating what BToM specified manually."
    },
    {
      "title": "Reasoning about reasoning by nested conditioning: Modeling theory of mind with probabilistic programs",
      "authors": "Andreas Stuhlm\u00fcller et al.",
      "year": 2014,
      "role": "Foundation",
      "relationship_sentence": "AutoToM inherits the probabilistic-programming view of ToM from Stuhlm\u00fcller & Goodman, using nested conditioning over explicit agent models and coupling it with an LLM-backed, automated model-revision loop."
    },
    {
      "title": "Learning the preferences of others by inverse planning",
      "authors": "Owain Evans et al.",
      "year": 2016,
      "role": "Extension",
      "relationship_sentence": "AutoToM builds on Evans et al.\u2019s Bayesian inverse planning for preference/goal inference, but generalizes it by automatically discovering which mental variables are needed for a given task and iteratively refining them based on uncertainty."
    },
    {
      "title": "Machine Theory of Mind",
      "authors": "Neil C. Rabinowitz et al.",
      "year": 2018,
      "role": "Related Problem",
      "relationship_sentence": "Targeting the same goal of agent modeling, AutoToM departs from ToMnet\u2019s learned black-box embeddings by returning to explicit generative models and improving transferability and interpretability via uncertainty-guided model revision."
    },
    {
      "title": "DreamCoder: Growing generalizable, interpretable knowledge with wake\u2013sleep program learning",
      "authors": "Kevin Ellis et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "AutoToM is inspired by DreamCoder\u2019s automated, interpretable program-structure growth, adapting the idea to discover and refine symbolic agent models for ToM while performing Bayesian inverse planning with an LLM proposal mechanism."
    },
    {
      "title": "Theory of mind may have spontaneously emerged in large language models",
      "authors": "Mikolaj P. Kosinski",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "AutoToM addresses the prompting-only ToM approach exemplified by Kosinski by embedding LLMs inside a model-based inverse-planning pipeline, thereby overcoming the systematic errors of pure prompting with uncertainty-calibrated inference and model refinement."
    }
  ],
  "synthesis_narrative": "AutoToM\u2019s core innovation\u2014automated agent modeling for scalable, robust Theory-of-Mind via Bayesian inverse planning\u2014emerges from a direct lineage in model-based cognitive science and programmatic model discovery. The foundation is Baker et al.\u2019s inverse-planning paradigm and the Bayesian Theory-of-Mind formulation, which cast mental inference as Bayesian inversion of an explicit agent model with belief\u2013desire variables. Stuhlm\u00fcller and Goodman\u2019s probabilistic-programming view of ToM further anchored the idea that nested reasoning and conditioning over explicit agent models can support interpretable mental-state inference. Building on this base, Evans et al. extended inverse planning to preference/goal inference, which AutoToM generalizes by automatically selecting which mental variables matter for each task. In contrast to learned black-box predictors like ToMnet (Rabinowitz et al.), AutoToM returns to explicit generative models but makes them scalable by using an LLM to propose model structure and by iteratively revising the model based on inference uncertainty\u2014an idea inspired by DreamCoder\u2019s automated, interpretable program growth. Finally, AutoToM directly responds to the recent practice of prompting LLMs for ToM (e.g., Kosinski) by moving beyond surface prompting to a model-based pipeline, thereby mitigating systematic errors and providing calibrated uncertainty and interpretability. Together, these works converge on AutoToM\u2019s key insight: combine explicit inverse-planning models with automated, uncertainty-guided structure discovery to achieve robust, generalizable mental inference.",
  "analysis_timestamp": "2026-01-06T23:08:23.950565"
}
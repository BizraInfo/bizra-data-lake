Data Science for LLMs
https://github.com/weAIDB/awesome-data-llm

Data: The Fuel of LLM Development
2
Ø LLMs have been widely used, which require vast, high-
quality data, distributing across diverse domains
General Domains
Specific Domains
LLM Datasets
Data Size
↑
Data Quality
↑
LLM Applications
DISC-Law-SFT
Qwen-2VL à Qwen-2.5VL
1.2 à 4 trillion tokens
(with over 10,000 object categories for 
visual grounding)
A Survey of LLM × DATA. https://github.com/weAIDB/awsome-data-llm 
!

Question to Ask:
3
Question to Ask:
What data is required 
for different LLM stages?
Pre-Training
Continual Pre-
Training
Supervised 
Fine-Tuning
Reinforcement 
Learning
Retrieval 
Augmentation
LLM Agent

Data: The Fuel of LLM Development
4
Ø Different LLM stages have divese dataset requirements
1. Pre-Training
• 1T+ unlabeled samples
2. Continual Pre-Training
• 10M+ unlabeled samples
3. Supervised Finetuning
• 10k+ labeled samples
4. Reinforcement Learning
• 1k+ labeled samples
5. RAG
6. Agentic LLM

Data: The Fuel of LLM Development
5
Ø Different LLM stages have divese dataset requirements
1. Pre-Training (1T+ unlabeled samples)
• Data Demand:
• Acquire broad language (and cross-modality)
understanding;
• Reduce the risk of overfitting.
• Sources:
• Web crawls (e.g., HTML, WARC)
• Open code repository
• Books (text, EPUB)
• Academic papers
• Interleaved image-text

Data: The Fuel of LLM Development
6
Ø Different LLM stages have divese dataset requirements
2. Continual Pre-Training (10M+ unlabeled samples)
• Data Demand:
• Fill knowledge gaps
• Adapt the model to specific domains
• Examples:
• BBT-FinCorpus: 300 GB of finance data
• Medical-pt: 360,000 Chinese-English entries from medical 
encyclopedias

Data: The Fuel of LLM Development
7
Ø Different LLM stages have divese dataset requirements
3. Supervised Finetuning (10k+ labeled samples)
• Data Demand: Guide the model in learning a 
specific, narrower set of tasks
• General Instruction Following
• Databricks-dolly-15K with 7 types of tasks, e.g., closed
QA, summarization, information extraction
• Specific Task Resolving
• DISC-Law-SFT: Legal information extraction (32k), 
legal judgment prediction (16k), legal event detection 
(27k), and legal question-answering (93k)

Data: The Fuel of LLM Development
8
Ø Different LLM stages have divese dataset requirements
4. Reinforcement Learning (1k+ labeled samples)
• RLHF (Reinforcement Learning with Human 
Feedback)
• Data: Smaller than SFT’s with more complex data
annotations (compare and rank multiple candidate 
responses by human preference)
• Reasoning-oriented Reinforcement 
Learning (RoRL)
• Data: Only feedback on whether the answer is 
correct or not (for long-term reasoning)

Data: The Fuel of LLM Development
9
Ø Different LLM stages have divese dataset requirements
5. RAG
• Data Demand: Strictly reviewed to ensure 
authenticity and validity, while dynamic data 
requires real-time updates
• Examples
• Medical: Data from over 65,000 ICU patients and more than 200,000 
patients treated in emergency department
• Legal: 800+ national and local laws, regulations, and rules, aswell 
as 24,000 legal-related exam questions.
• User-personalized LLM

Data: The Fuel of LLM Development
10
Ø Different LLM stages have divese dataset requirements
6. Agent
• Data Demand: Require training data for
advanced capabilities such as planning, tool 
orchestration, and multi-turn dialogue capability
• Examples
• Reasoning (UltraInteract): Instruction as the root 
node; Both the correct actions and corresponding 
incorrect actions as nodes to construct a trajectory 
tree of human preference
• Tool (AutoTools): Finetune on tool data
(<python>code</python>)
• Dialogue (UltraChat): Add an LLM to simulate
user instructions and conversational content

Data: The Fuel of LLM Development
11
Ø Different LLM stages have divese dataset requirements
7. Evaluation
• Data: Representative data samples that 
reflect different aspects of an LLM’s 
capabilities.
• General Domain
• MMMU: Perception, knowledge, and reasoning
• Specific Domain
• HumanEval: 164 programming problems
• MedQA: 61,097 medical exam questions from 
various regions

Question to Ask:
12
Inclusiveness
Question to Ask:
What Makes a Dataset Truly 
“Good” for LLMs?
Abundance
Articulation
Sanitization

Ø The IaaS Concept
A good dataset is a purposefully balanced and
rigorously sanitized collection of broad, 
diverse, and well-articulated data that,
when used for training, yields
high-performing, safe, and 
resource-efficient
large language models.
13
The IaaS Concept of DATA4LLM
Data-Centric
Training

The IaaS Concept can be categorized into four keywords:
1. Inclusiveness
• Domains; Task Types; Sources; Languages;
• Expression Styles; Data Modalities
2. Abundance
• Enhance domain-specific ability
3. Articulation
• Well-Formatted; Instructive;
• Step-by-step Reasoning
4. Sanitization
• Privacy; Ethical; Risk Removal
14
The IaaS Concept of DATA4LLM
Data-Centric
Training

Ø The IaaS Concept covers most LLM stages
The IaaS Concept of DATA4LLM
Characters \ Stages
Pre-Training
Continual
Pre-Training
Finetuning
RL
RAG
Agent
Evaluation
Inclusiveness
Diverse 
Domains: 
General Knowledge,
Coding, STEM, 
Multilingual ...
Specific
Domain:
Finance, Medicine, ...
Specific
Domain:
QA, Summarization, 
Writing, 
Classification...
Specific
Domain:
RLHF: General, 
Medicine, Law...
RoRL:  Math, 
Coding, ...
Specific
Domain:
Finance, 
Medicine, Law...
Planning;
Tool;
Multi-Turn 
Dialogue
Domain-
Specific:
Legal Consultation,
Code Synthesis 
and Translation...
Abundance
Over 1 TB;
Tend to Increase 
Ratios like Math and 
Coding
10M-100B;
Need General Data
10k-10M
1k-1M
---
10k-10M
0.1k-100k
Articulation
Format: Clear, 
Fluent, and 
Unambiguous 
Language;
Well Formatted
Format: Clear, 
Fluent, and 
Unambiguous 
Language;
Well Formatted
Instruction-
Response 
Pairs;
Format:
Task-Oriented 
Response
Step-By-Step 
Reasoning;
Fine-Grained 
Annotation
Real-time;
Extensive 
Info
Reasoning;
Interaction 
Format:
Insturction+
Action+
Observation...
Granular 
Evaluation;
Free from 
Data 
Leakage
Sanitization
Privacy Compliance; Toxicity-Free; Ethical Consistency; Risk Mitigation

Ø For an IaaS-compliant dataset, use the data processing techniques
16
The IaaS Concept of DATA4LLM
Inclusiveness
Data Acquisition
Abundance
Articulation
Sanitization
Data Synthesis
Data Deduplication
Data Selection
Data Mixing
Data Filtering

Ø The IaaS Concept
17
The IaaS Concept of DATA4LLM
Inclusiveness
Data Acquisition
Abundance
Articulation
Sanitization
Data Synthesis
Data Deduplication
Data Selection
Data Mixing
Data Filtering

Ø The IaaS Concept
18
The IaaS Concept of DATA4LLM
Inclusiveness
Data Acquisition
Abundance
Articulation
Sanitization
Data Synthesis
Data Deduplication
Data Selection
Data Mixing
Data Filtering

Ø The IaaS Concept
19
The IaaS Concept of DATA4LLM
Data Acquisition
Abundance
Articulation
Sanitization
Data Synthesis
Data Deduplication
Data Selection
Data Mixing
Data Filtering
Inclusiveness

Ø The IaaS Concept
20
The IaaS Concept of DATA4LLM
Inclusiveness
Data Acquisition
Abundance
Articulation
Sanitization
Data Synthesis
Data Deduplication
Data Selection
Data Mixing
Data Filtering

Ø The IaaS Concept
21
The IaaS Concept of DATA4LLM
Inclusiveness
Data Acquisition
Abundance
Articulation
Sanitization
Data Synthesis
Data Deduplication
Data Selection
Data Mixing
Data Filtering

Ø Required Techniques under IaaS
22
The IaaS Concept of DATA4LLM

Ø Required Techniques under IaaS
23
The IaaS Concept of DATA4LLM

24
Data Acquisition —— Data Sources
• Motivation: Pretraining needs abundant data, ensuring greater 
coverage, stronger abilities and less hallucination
• Challenge: Where to collect the massive data?
•
Public Data: Publicly available on the Internet with free access
•
Examples: Webpages (CommonCrawl, C4), digital books, code repositories (GitHub), etc.

25
Data Acquisition —— Data Sources
• Motivation: Pretraining needs abundant data, ensuring greater 
coverage, stronger abilities and less hallucination
• Challenge: Where to collect the massive data?
•
Private Data: Privately owned by companies or organizations with limited access
•
Examples: Internal documents, event logs, subscriber-only content.
•
Constraints:
•
Prone to contain sensitive content, requiring inspecting and cleaning (e.g., anonymization)
•
Restricted use due to regional privacy laws (GDPR)

26
Data Acquisition —— Techniques

27
Data Acquisition —— Website Crawling
• Motivation: Pretraining needs abundant data, ensuring greater 
coverage, stronger abilities and less hallucination
• Challenge: How to identify and extract informative content inside the 
raw HTML data?
•
Rule-based Crawling
•
Analyze and extract HTML pages through tags (head, body, footer, navbar, etc.), or
•
Find the elements with the largest region.
•
ML-based Crawling: Analyze whether a DOM node contains textual content using a 
HTML tag classifier
•
Available features include text density and word frequencies in “id” and “class” attributes.
Barbaresi, A. (2021, August). Trafilatura: A web scraping library and command-line tool for text discovery and extraction. In Proceedings of the 59th Annual Meeting of the Association 
for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations (pp. 122-131).

28
Data Acquisition —— Layout Analysis
• Challenge: How to handle various types of documents with different 
elements.
• Model Pipeline: Converts raw data (e.g., scanned books) into machine-
readable formats in a pipeline manner, which consist of multiple small models.
Bin Wang, et al. MinerU: An Open-Source Solution for Precise Document Content Extraction [J]. arXiv preprint arXiv:2409.18839, 2024

29
Data Acquisition —— Layout Analysis
• Challenge: How to handle various types of documents with different 
elements.
• Multimodal LLM: Adopts multimodal LLMs for end-to-end text acquisition. 
Haoran Wei, et al. General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model[J]. arXiv preprint arXiv:2409.01704, 2024

30
Data Acquisition —— Layout Analysis
• Challenge: How to handle various types of documents with different 
elements.
布局检测挑战
p 文档类型多样，而可用数据集单一
p 文档元素尺度多样
p 多模态框架（LayoutLMv3等）实时性差

31
Data Acquisition —— Layout Analysis
表格元素
公式元素
带格式⽂本
图⽚
⽂本/标题
多样性元素展示

32
Data Acquisition —— Layout Analysis
文字识别挑战
p 不同语言
p 不同尺度
p 不同清晰度
p 不同字体
p 不同排版
p ……
多样性文字展示

33
Data Acquisition —— Layout Analysis
多样性公式展示
公式识别挑战
p 简短公式Vs 复杂长公式/多行公式
p 打印体公式Vs 手写体公式/扫描件公式
p 公式识别多关注简短公式，忽略复杂长
公式。

34
Data Acquisition —— Layout Analysis
表格识别挑战
p 不同风格（有线、无线、三线…）
p 不同旋转角度
p 合并单元格、复杂长表格
p 空单元格表格
p 超长文字表格
p ……

35
Data Acquisition —— Layout Analysis
⽣成式
模型

36
Data Acquisition —— Layout Analysis
• Challenge: Existing dataset for layout analysis has limited layout 
types and volume.
• Generate diverse document images by searching for the best match between 
candidate elements (Candidates) and idle blocks (Mesh).
Zhiyuan Zhao, et al. DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and 
Global-to-Local Adaptive Perception[J]. arXiv preprint arXiv:2410.12628, 2024
ü 基于YOLO框架，精度高、
速度快
ü 基于多样性文档预训练，
鲁棒性强
ü Global-local模型优化，
适配多尺度元素。

37
Data Acquisition —— Layout Analysis

38
Data Acquisition —— Layout Analysis
1. Bestfit算法合成多样性数据

39
Data Acquisition —— Layout Analysis

40
Data Acquisition —— Layout Analysis
2. （公式识别）UniMERNet网络结构

41
Data Acquisition —— Layout Analysis
2. （公式识别）UniMERNet网络结构

42
Data Acquisition —— Layout Analysis

43
Data Acquisition —— Layout Analysis
化学式
统计图表
表单
几何图
代码段

44
Data Acquisition —— Layout Analysis
01
当前文档解析的两种主流方案：基于小模型的流程线方案（pipeline-based）及基于端到端大模型
方案(vlm-based)均存在各自优势，是否有更易于拓展，高效推理且精准解析的方案仍需探索。
复杂排版下的阅读顺序，复杂元素解析（复杂表格、表单及流程图）需要逻辑推理能
力，仅依赖简单的识别能力不足以精准解析。
02
03
文档解析作为大模型时代的基础技术，需要质量足够高，推理足够快，成本足够小

Data Acquisition
45
• Takeaways
• Data acquisition for large language models (LLMs) relies primarily on large-scale, unsupervised 
web scraping across diverse domains, contrasting with traditional ML’s focus on labeled, 
domain-specific data.
• Key data sources include public data (e.g., webpages, books, code repositories) and private data 
(e.g., internal logs, subscriber content), with the latter requiring strict privacy safeguards and 
secure handling.
• Main acquisition methods are:
• Website crawling: Use rule-based (e.g., Trafilatura, BET) or ML-based techniques (e.g., Dragnet) to extract 
clean text from HTML, aided by tools like Beautiful Soup, Selenium, and Playwright.
• Layout analysis: Extract text from non-digital or structured documents using pipeline systems (e.g., 
PaddleOCR, MinerU) or end-to-end multimodal LLMs (e.g., GOT2.0, Fox), the latter offering higher 
versatility but lower efficiency.
• Entity recognition and linking: Derive structured knowledge (e.g., triples) from text, using models like 
ReFinED and UMIE, though recent LLMs may implicitly learn such relationships, reducing the need for 
explicit linking.
• Ensuring data quality, translation consistency (e.g., via AACTRANS), and multimodal integration 
(e.g., text-image alignment in UMIE) are critical for robust LLM pretraining.
• Efficiency, scalability, and ethical compliance remain central challenges in large-scale data 
acquisition.

46
• Motivation: Pretraining prefers to remove duplicates,
ensuring greater coverage with less redundancy
• Data Deduplication: Remove duplicates to enhance training or sometimes 
improve accuracy
• Utility Function: (1) Exactly match strings or documents or (2) Use 
approximate matching methods calculated according to similarity measures 
Data Deduplication

47
Data Deduplication

48
• Motivation: Pretraining prefers to remove duplicates, 
ensuring greater coverage with less redundancy
• Data Deduplication: Remove duplicates to enhance training performance
• Exact Matching Techniques:
•
1. URL Deduplication: Remove data that shares the same URL
•
Individual web page may appear in multiple datasets.
•
The samples sourced from the same URL will be identified as duplicates
Data Deduplication

49
• Motivation: Pretraining prefers to remove duplicates, 
ensuring greater coverage with less redundancy
• Data Deduplication: Remove duplicates to enhance training performance
• Exact Matching Techniques:
•
2. Exact Substring Deduplication: Remove data that shares the same substring
•
Individual content may be referenced by multiple samples.
•
The samples with the same substring will be identified as duplicates.
•
Suffix Array: Find the duplicates through the common prefix of the suffice
•
S1: Combine all the samples into one single string
•
S2: Build the Suffix Array of the string
•
S3: Find duplicates through common prefix
ABCABCAB
•
ABCABCAB
•
BCABCAB
•
CABCAB
•
ABCAB
•
BCAB
•
CAB
•
AB
•
B
String
Build Suffix Array
Find Common Prefix 
(Threshold = 5)
•
ABCABCAB
•
ABCAB
Lower threshold 
finds more 
duplicates but 
produces more 
false positive
Lee, K., Ippolito, D., Nystrom, 
A., Zhang, C., Eck, D., 
Callison-Burch, C., & Carlini, 
N. (2021). Deduplicating 
training data makes language 
models better.
Data Deduplication

Data Deduplication
50
• Motivation: Pretraining prefers to remove duplicates,
ensuring greater coverage with less redundancy
• Data Deduplication: Remove duplicates to enhance training performance
• Exact Hashing Techniques:
•
1. Hash Functions: Guarantee to find all exact matches
•
(1) Initialize a Set for Hashes
•
A set ~ The hashes of encountered text entries.
•
(2) Hash Each Text Entry
•
For each text entry, compute a simple hash (e.g., the sum of ASCII values of its characters).
•
(3) Check for Duplicates
•
If the hash of the current entry is already in the set, it is a duplicate and will be ignored.
•
If the hash is not in the set, add the hash to the set and keep the entry.
Efficient and Fast,  but may find false positives due to hash 
collisions and remove non-matching documents

Data Deduplication
51
• Motivation: Pretraining prefers to remove duplicates,
ensuring greater coverage with less redundancy
• Data Deduplication: Remove duplicates to enhance training or sometimes 
improve accuracy
•
Approximate Hashing Techniques (MinHash):
•
S1: One-hot encode the samples by, e.g., n-grams, forming a matrix with row as token and 
column as sample
•
S2: Apply a series of hashing functions to row indices to shuffle the rows. For each sample:
•
After applying each hashing function,
look for the lowest row index with value 1.
•
The sample signature is a vector of
the lowest row indices.
•
Signature also works: Samples with similar
set of tokens produce similar signatures.
Manku, G. S., Jain, A., & Das Sarma, A. (2007, May). Detecting near-duplicates for web crawling. In Proceedings of the 16th international conference on World Wide Web.
Row Indices
Samples
Tokens
Hashing Function #1
Hashing Function #2
Hashing Function #3

Data Deduplication
52
• Motivation: Pretraining prefers to remove duplicates,
ensuring greater coverage with less redundancy
• Data Deduplication: Remove duplicates to enhance training or sometimes 
improve accuracy
•
Approximate Hashing Techniques (MinHash):
•
S1: One-hot encode the samples by, e.g., n-grams, forming a matrix with row as token 
and column as sample
•
S2: Apply a series of hashing functions to the row indices to obtain random row indices.
•
S3: Compute Jaccard Index between fingerprints.
•
Less computation by computing Jaccard Index
on short fingerprints instead of long original data.
•
The more the hashing functions, the better the
signature similarity approximates to the original one

Data Deduplication
53
• Motivation: Pretraining prefers to remove duplicates,
ensuring greater coverage with less redundancy
• Data Deduplication: Remove duplicates to enhance training or sometimes 
improve accuracy
• Approximate Hashing Techniques (SimHash):
•
S1: Hash each token in the document into a fixed-dimension vector of {0, 1}d, weighted by 
pre-defined or TF-IDF weight w that is positive for 1 and negative for 0.
•
S2: Add up weighted vector elements of each token to a single value,
forming a new vector of the same dimension d
•
S3: Map the new vector to another vector of {0, 1}d,
which is the fingerprint of each sample
•
S5: Compute Hamming distance, the number of
different elements between their vectors.

Data Deduplication
54
• Motivation: Pretraining prefers to remove duplicates,
ensuring greater coverage with less redundancy
• Data Deduplication: Remove duplicates to enhance training or sometimes 
improve accuracy
• Approximate Hashing Techniques
维度
SimHash
MinHash
基本原理
将每个token 哈希为固定维度的向量，加权累
加后对每⼀位取符号（正为1，负为0）
对每个样本⽣成哈希签名，签名表示最早“命
中”的token 位置
最终表示
固定⻓度的⼆进制向量（指纹）
多个哈希函数下的最⼩值组成的签名向量
相似度计算⽅式
汉明距离（Hamming Distance）
Jaccard 相似度估计（签名重合程度）
适合的数据结构
⾼维稀疏向量（如TF-IDF ⽂本）
集合（如n-gram 集合、token 集合）
签名⽣成速度
⼀次遍历即可，速度快
需要多次哈希（多轮），速度稍慢
签名⼤⼩
较⼩，通常为64 位或128 位
可调⼤⼩（根据哈希函数个数）
鲁棒性
对⽂本顺序和轻微变动不太敏感
对token 顺序不敏感，但对token 改动较敏感
常⻅应⽤
⽂档去重（如搜索引擎）、海量⽂本相似检测
⽹⻚去重、⼤规模集合聚类、重复检测
相似性误差
会有⼀定信息损失，可能⾼估相似度
准确估计Jaccard，但需要较多哈希函数才能稳
定

Data Deduplication
55
• Motivation: Pretraining prefers to remove duplicates,
ensuring greater coverage with less redundancy
• Data Deduplication: Remove duplicates to enhance training or sometimes 
improve accuracy
• Approximate Frequency Techniques (SoftDeDup): Deduplicate by reweighting 
samples instead of pruning samples, preventing the loss of potentially valuable 
information
•
S1: Compute the frequency of each n-gram across all the samples
•
S2: Calculate the commonness of each sample by multiplying the frequencies of all the
n-grams that appear in the document.
•
S3: Samples with higher commonness are more likely to be duplicates and thus be down-
weighted.
He, N., Xiong, W., Liu, H., et al. SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training. ACL, 2024.

Data Deduplication
56
• Motivation: Pretraining prefers to remove duplicates,
ensuring greater coverage with less redundancy
• Data Deduplication: Remove duplicates to enhance training or sometimes 
improve accuracy
• Embedding-Based Clustering Techniques:
•
Use pretrained models for semantic deduplication
•
S1: Embed each data point into a vector in the embedding space using existing LLM.
The vector contains the tokens and the context between them,
thus providing the semantic information.
•
S2: Cluster data points using k-means
•
S3: Within each cluster, pairwise cosine similarities between
     data points are calculated.
•
S4: For identified duplicates within a cluster, only the point with 
     the lowest cosine similarity to the cluster centroid is kept, and the others are removed.
Abbas, A., Tirumala, K., Simig, D., Ganguli, S., & Morcos, A. S. (2023). Semdedup: Data-efficient learning at web-scale through semantic deduplication.

Data Deduplication
57
• Takeaways
•
Redundant data negatively impacts LLM performance by reducing generalization ability and 
increasing overfitting.
•
Deduplication improves training efficiency, prevents memorization of repeated patterns, and 
mitigates bias.
•
Challenges include efficiently encoding semantic content for comparison and scalability of 
deduplication methods for large datasets.
•
Future Directions: 1) Enhancing accuracy in detecting semantically similar but structurally 
different duplicates; 2) Developing fair deduplication strategies that preserve underrepresented 
groups.

Data Filtering
58
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove low-quality or noisy samples and ensure diversity 
in the selected subset.

59
Data Filtering

Data Filtering
60
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove low-quality or noisy samples and ensure diversity in 
the selected subset.
• Perplexity Measuring: Filter by sample response generation difficulty.
•
Higher perplexity score indicates higher difficulty for model to generate sample response.
•
Filter samples by generating their responses using an individual model and assessing 
their perplexity scores.
•
Perplexity score is calculated by conditional probabilities:
•
Given a sentence “I love machine learning”, its perplexity score is calculated by
!(i) = 0.2, !(love ∣i) = 0.1, !(machine ∣i,love) = 0.05, !(learning ∣i,love,machine) = 0.01
!!8 learning i,love,machine = exp −1
4 log 0.2 + log 0.1 + log 0.05 + log 0.01
=  17.78279
PPL # $ = exp
−1
+ ,
!"#
$
log 0 #! $, ##, … , #!%#

Data Filtering
61
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove low-quality or noisy samples and ensure diversity in 
the selected subset.
• Perplexity Measuring: Filter by sample response generation difficulty.
•
S1: Compute perplexity score with a surrogate model (smaller size for faster computing).
•
S2: Rank samples by perplexity values and select subsets based on the criteria (e.g., 
top-ranked domains or medium/high perplexity samples).
•
S3: Train larger models on the optimal subset.
!!"#$#
%>?()>*+DE
F()
/)0I230(
L5N#OP*?(
!(?9+(RI0S
!!"
!!"
!!"
! "
!?(TI*=)#0*>(?)
@(R0#0*>(?
Y?IBI?3+#N303)(0
5I+0(?(D#N303)(0
C(2*T(D#N303)(0
PPL # $ = exp
−1
+ ,
!"#
$
log 0 #! $, ##, … , #!%#
Ankner, Z., Blakeney, C., Sreenivasan, K., Marion, M., Leavitt, M. L., & Paul, M. (2024). Perplexed by perplexity: Perplexity-based data pruning with small reference models.

Data Filtering
62
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove low-quality or noisy samples and ensure diversity in 
the selected subset.
• Perplexity Measuring: Filter by sample response generation difficulty.
•
Learning Percentage (LP): Models learn easier samples first and harder ones later. 
Assess sample difficulty by measuring its perplexity drop ratio during training.
•
S1: Train a model to track sample perplexity across epochs.
•
S2: Calculate the learning percentage after the first epoch !"(1).
•
S3: Rank samples and split them into three parts
(hardest, medium, easiest).
•
S4: Train larger models on the hardest subset.
ℒC D = C!"# −C!
C$ −C%
measures the perplexity drop ratio of 
a sample between the specific epoch i 
and the whole training procedure.
Mekala, D., Nguyen, A., & Shang, J. (2024). Smaller language models are capable of selecting instruction-tuning training data for larger language models.

Data Filtering
63
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove low-quality or noisy samples and ensure diversity in 
the selected subset.
• Perplexity Measuring: Filter by sample response generation difficulty.
•
Instruction-Following Difficulty (IFD): Measures how much the instruction + input part 
of the sample would affect sample perplexity by comparing perplexity w/o the part.
•
S1: Compute IFD score
using an existing model.
•
S2: Rank samples by
IFD scores.
•
S3: Train model on the
samples with higher IFD
scores.
!"#$=$
>?()*?+DEF
/)*
0*I234I)
!"#$5N+()
O)(PD)92IR
!"#$S
T=>
!"#$S
T=?
!"#$S
T=@
! "
O()Y2+B*$I+C)D*
E)9I$I+C)D
!"#$5N+()
F(2G2D4D$#4I4*)I
"2DI)()E$#4I4*)I
H)3+Y)E$#4I4*)I
PPL # $ = exp
−1
+ ,
!"#
$
log 0 #! $, ##, … , #!%#
IFD! ), + = --. + )
--. +
Li, M., Zhang, Y., Li, Z. et al.: From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning.
Li, M., Zhang, Y., He, S. et al.: Superfiltering: Weak-to-strong data filtering for fast instruction-tuning.

Data Filtering
64
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove low-quality or noisy samples and ensure diversity in 
the selected subset.
• Influence Assessment: Filter by how much a sample would affect model 
parameters or model performance (e.g., accuracy, fairness).
•
DEALRec: Estimate the influence of a sample when removing/upweighting samples.
•
Estimate model parameter change through !
θ!" −$θ ≈#
$ &%&
!#∇&ℒ(, $θ , and extend it to
•
Estimate model performance change through *remove, loss (, + = #
$ ∑(
#
$ ∇&ℒ((, $θ T &%&
!#∇&ℒ(, $θ
•
SHED: Assess the influence of a sample on model performance using Shapley value.
•
Iteratively removing n samples to measure their contribution to model performance by 
comparing model performance w/o this subset.
Lin, X., Wang, W., Li, Y., Yang, S., Feng, F., Wei, Y., & Chua, T. S. (2024, July). Data-efficient Fine-tuning for LLM-based Recommendation. In Proceedings of the 47th 
international ACM SIGIR conference on research and development in information retrieval (pp. 365-374).
He, Y., Wang, Z., Shen, Z., Sun, G., Dai, Y., Wu, Y., ... & Li, A. (2024). Shed: Shapley-based automated dataset refinement for instruction fine-tuning. arXiv preprint 
arXiv:2405.00705.

Data Filtering
65
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove low-quality or noisy samples and ensure diversity in 
the selected subset.
• Clustering: Groups similar samples together, allowing selection within clusters 
to reduce redundancy and across clusters to increase diversity.
•
S1: Encode samples into embeddings and cluster similar samples using cosine similarity.
!"#$%&'($
!A*&%%C"D
EF/0(&I0
2I/"&'*3
EF/0(&I
E$A4F&5C(3
E$A4F&5C(3
6O&IPD&'9"(&IR
#F/0(&I';C0(P"#&
6O&IPD&'9"(IPR
#F/0(&I';C0(P"#&
<ICDC"PF';P(P0&(
=&A$O&%';P(P0&(
>CF(&I&%';P(P0&(
/ = 0"#$%&×0"#$'%
Abbas, A., Rusak, E., Tirumala, K., Brendel, W., Chaudhuri, K., & Morcos, A. S. (2024). Effective pruning of web-scale datasets based on complexity of concept clusters.

Data Filtering
66
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove low-quality or noisy samples and ensure diversity in 
the selected subset.
• Clustering: Groups similar samples together, allowing selection within clusters 
to reduce redundancy and across clusters to increase diversity.
•
S1: Encode samples into embeddings and cluster similar samples using cosine similarity.
•
S2: Calculate cluster complexity based on intra-cluster and inter-cluster distances:
& = (!"#$%×(!"#&$
Abbas, A., Rusak, E., Tirumala, K., Brendel, W., Chaudhuri, K., & Morcos, A. S. (2024). Effective pruning of web-scale datasets based on complexity of concept clusters.
!"#$%&'($
!A*&%%C"D
EF/0(&I0
2I/"&'*3
EF/0(&I
E$A4F&5C(3
E$A4F&5C(3
6O&IPD&'9"(&IR
#F/0(&I';C0(P"#&
6O&IPD&'9"(IPR
#F/0(&I';C0(P"#&
<ICDC"PF';P(P0&(
=&A$O&%';P(P0&(
>CF(&I&%';P(P0&(
/ = 0"#$%&×0"#$'%

Data Filtering
67
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove low-quality or noisy samples and ensure diversity in 
the selected subset.
• Clustering: Groups similar samples together, allowing selection within clusters 
to reduce redundancy and across clusters to increase diversity.
•
S1: Encode samples into embeddings and cluster similar samples using cosine similarity.
•
S2: Calculate cluster complexity based on intra-cluster and inter-cluster distances:
& = (!"#$%×(!"#&$
•
S3: Resample clusters
probabilistically to 
balance diversity 
and quality.
Abbas, A., Rusak, E., Tirumala, K., Brendel, W., Chaudhuri, K., & Morcos, A. S. (2024). Effective pruning of web-scale datasets based on complexity of concept clusters.
!"#$%&'($
!A*&%%C"D
EF/0(&I0
2I/"&'*3
EF/0(&I
E$A4F&5C(3
E$A4F&5C(3
6O&IPD&'9"(&IR
#F/0(&I';C0(P"#&
6O&IPD&'9"(IPR
#F/0(&I';C0(P"#&
<ICDC"PF';P(P0&(
=&A$O&%';P(P0&(
>CF(&I&%';P(P0&(
/ = 0"#$%&×0"#$'%

Data Filtering
68
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove low-quality or noisy samples and ensure diversity in 
the selected subset.
• Model Scoring: Use LLMs to evaluate sample quality explicitly or implicitly 
through prompt engineering or human-labeled data.
•
S1: Prompt GPT-3.5-turbo to compare pairs of samples along four quality criteria (writing 
style, fact & trivia amount, educational value, and the expertise required to understand)
Wettig, A., Gupta, A., Malik, S., & Chen, D. (2024). Qurating: Selecting high-quality data for training language models.

Data Filtering
69
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove low-quality or noisy samples and ensure diversity in 
the selected subset.
• Model Scoring: Use LLMs to evaluate sample quality explicitly or implicitly 
through prompt engineering or human-labeled data.
•
S1: Prompt GPT-3.5-turbo to compare pairs of samples along four quality criteria (writing 
style, fact & trivia amount, educational value, and the expertise required to understand)
•
S2: Record binary confidence E&≻( ∈0,1  and translate it into probabilistic sample quality 
rating E&≻( = σ H& −H(  through Bradley-Terry model.
Wettig, A., Gupta, A., Malik, S., & Chen, D. (2024). Qurating: Selecting high-quality data for training language models.

Data Filtering
70
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove low-quality or noisy samples and ensure diversity in 
the selected subset.
• Model Scoring: Use LLMs to evaluate sample quality explicitly or implicitly 
through prompt engineering or human-labeled data.
•
S1: Prompt GPT-3.5-turbo to compare pairs of samples along four quality criteria (writing 
style, fact & trivia amount, educational value, and the expertise required to understand)
•
S2: Record binary confidence E&≻( ∈0,1  and translate it into probabilistic sample quality 
rating E&≻( = σ H& −H(  through Bradley-Terry model.
•
S3: Train a rating model on these quality ratings.
Wettig, A., Gupta, A., Malik, S., & Chen, D. (2024). Qurating: Selecting high-quality data for training language models.

Data Filtering
71
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove low-quality or noisy samples and ensure diversity in 
the selected subset.
• Model Scoring: Use LLMs to evaluate sample quality explicitly or implicitly 
through prompt engineering or human-labeled data.
•
S1: Prompt GPT-3.5-turbo to compare pairs of samples along four quality criteria (writing 
style, fact & trivia amount, educational value, and the expertise required to understand)
•
S2: Record binary confidence E&≻( ∈0,1  and translate it into probabilistic sample quality 
rating E&≻( = σ H& −H(  through Bradley-Terry model.
•
S3: Train a rating model on these quality ratings.
•
S4: Use the rating model predict quality ratings for new samples on each criterion.
Wettig, A., Gupta, A., Malik, S., & Chen, D. (2024). Qurating: Selecting high-quality data for training language models.

Data Filtering
72
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove low-quality or noisy samples and ensure diversity in 
the selected subset.
• Model Scoring: Use LLMs to evaluate sample quality explicitly or implicitly 
through prompt engineering or human-labeled data.
•
S1: Prompt GPT-3.5-turbo to compare pairs of samples along four quality criteria (writing 
style, fact & trivia amount, educational value, and the expertise required to understand)
•
S2: Record binary confidence E&≻( ∈0,1  and translate it into probabilistic sample quality 
rating E&≻( = σ H& −H(  through Bradley-Terry model.
•
S3: Train a rating model on these quality ratings.
•
S4: Use the rating model predict quality ratings for new samples on each criterion.
•
S5: Resample new samples by the predicted ratings.
Wettig, A., Gupta, A., Malik, S., & Chen, D. (2024). Qurating: Selecting high-quality data for training language models.

Data Filtering
73
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove low-quality or noisy samples and ensure diversity in 
the selected subset.
• Model Scoring: Use LLMs to evaluate sample quality explicitly or implicitly 
through prompt engineering or human-labeled data.
•
S1: Prompt ChatGPT to evolve the samples along instruction complexity and response 
quality, and to score these evolved samples
•
S2: Train two scorers (complexity and quality) on the evolved samples with their scores.
•
S3: Use both scorers
to score new samples.
Multiply both scores
to form the final score
•
S4: Resample samples
by the final score
Liu, W., Zeng, W., He, K., Jiang, Y., & He, J. (2023). What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning.

Data Filtering
74
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove noise or sensitive information within samples.
•
Clean text by removing invalid characters, unnecessary texts, or harmful content (e.g., 
bias ranging from gender and racial stereotypes to cultural and socioeconomic 
prejudices).
•
SEAL: Train a selector based on a safety-aligned model (e.g., Merlinite-7b) using bi-
level optimization:
•
Minimize the safety loss on the safe dataset
•
Minimize the fine-tuning loss on the filtered dataset during training to make the 
selector prioritize safe and high-quality samples in selecting data
Navigli, R., Conia, S., & Ross, B. (2023). Biases in large language models: origins, inventory, and discussion. ACM Journal of Data and Information Quality, 15(2), 1-21.
Shen, H., Chen, P. Y., Das, P., & Chen, T. (2024). Seal: Safety-enhanced aligned llm fine-tuning via bilevel data selection.

Data Filtering
75
• Motivation: Pretraining prefers to remove low-quality or noisy 
samples, simplifying training while retaining performance
• Data Filtering: Remove noise or sensitive information within samples.
•
Clean by removing invalid characters, unnecessary texts, harmful content.
•
Detect and anonymize private/sensitive information while preserving semantics.
•
Use NER models (spaCy, Flair, etc.) to tag personally identifiable information 
(e.g., names, addresses) and replace tagged entities with placeholders (e.g., 
[NAME], [LOCATION]), or
•
DeID-GPT: Prompt LLM to redact PII within the given text:
Please de-identify the following clinical notes by 
replacing any terms that could be a name, an 
address, a date, or an ID with the term ‘[redacted]’.
[content]
Lukas, N., Salem, A., Sim, R., Tople, S., Wutschitz, L., & Zanella-Béguelin, S. (2023, May). Analyzing leakage of personally identifiable information in language models. In 
2023 IEEE Symposium on Security and Privacy (SP) (pp. 346-363). IEEE.
Liu, Z., Huang, Y., Yu, X., Zhang, L., Wu, Z., Cao, C., ... & Li, X. (2023). Deid-gpt: Zero-shot medical text de-identification by gpt-4.

Data Filtering
76
• Takeaways
•
Data filtering aims to remove low-quality or sensitive samples from training datasets, reducing 
computational overhead while maintaining or improving model performance.
•
Challenges include balancing data reduction with model performance and ensuring diversity 
while filtering out redundant or noisy samples.
•
Sample-level filtering focuses on removing entire low-quality samples based on metrics like 
perplexity, influence assessment, clustering, entropy, and model scoring.
•
Content-level filtering targets partial noise or sensitive content within samples rather than 
removing entire entries.
•
Future Directions include improving efficiency for massive datasets, enhancing accuracy in 
detecting low-quality or noisy text and developing better algorithms for locating potential 
sensitive information.

77
Data Selection

Data Selection
78
• Motivation: Adapt LLMs to specific domains (e.g., medical or 
legal).
• Data Selection: Select subsets of already well-cleaned data samples to align 
LLMs with target tasks while maintaining generalization.
• Lexicon Set Overlap: Measures relevance of a dataset to a specific domain 
using overlap between lexicons.
•
Break strings into lexicon sets, and select samples with large lexicon intersection to the 
target task
Data filtering removes noisy samples
Relevant
Data selection selects relevant samples
Compute QR decomposition for matrix
[data, sample, select, relevant]
[data, filter, noisy, remove, sample]
[compute, decompose, matrix, QR]
Strings
Lexicons
Qin, R., Xia, J., Jia, Z., Jiang, M., Abbasi, A., Zhou, P., ... & Shi, Y. (2024, June). Enabling on-device large language model personalization with self-supervised data 
selection and synthesis. In Proceedings of the 61st ACM/IEEE Design Automation Conference (pp. 1-6).

Data Selection
79
• Motivation: Adapt LLMs to specific domains (e.g., medical or 
legal).
• Data Selection: Select subsets of already well-cleaned data samples to align 
LLMs with target tasks while maintaining generalization.
• Bag-of-Words Similarity: Utilize bag-of-words to compute feature distributions 
for raw and target data. 
•
S1: Represent raw and target data as bag-of-words features.
•
S2: Estimate importance weights .( =
)
*feat +2
)
,feat +2 .
•
S3: Resample raw data with probability 
-2
∑234
5
-2 
to match the target distribution.
Xie, S. M., Santurkar, S., Ma, T., & Liang, P. S. (2023). Data selection for language models via importance resampling. Advances in Neural Information Processing 
Systems, 36, 34201-34227.
“Data selection selects
relevant samples”
p(data) = 1/5
p(sample) = 1/5
p(select) = 2/5
p(relevant) = 1/5

Data Selection
80
• Motivation: Adapt LLMs to specific domains (e.g., medical or 
legal).
• Data Selection: Select subsets of already well-cleaned data samples to align 
LLMs with target tasks while maintaining generalization.
• Cosine Similarity: Compare embeddings of task-specific labeled data and 
unlabeled data.
•
S1: Encode both labeled and unlabeled data into embeddings
•
S2: Measure similarity between embeddings using cosine similarity.
•
S3: Select unlabeled samples that align closely
with the task's embedding distribution.
80
Xie, Y., Aggarwal, K., & Ahmad, A. (2024, August). Efficient continual pre-training for building domain specific large language models. In Findings of the Association for 
Computational Linguistics ACL 2024 (pp. 10184-10201).

Data Selection
81
• Motivation: Adapt LLMs to specific domains (e.g., medical or 
legal).
• Data Selection: Select subsets of already well-cleaned data samples to align 
LLMs with target tasks while maintaining generalization.
• Optimization-based Selection: Select subsets towards reducing model loss 
and improving model performance on the target tasks. 
•
Approach 1: Minimizes model loss on target tasks using linear datamodels.
•
S1: Use a linear datamodel τ63 17  to estimate how training samples affect model loss.
•
S2: Adjust characteristic vector 1/ to reflect the subset and estimate parameters θ0 via 
regression.
•
S3: Select the subset 5 of size 6 that minimizes the loss 4
58targ 6 .
Engstrom, L., Feldmann, A., & Madry, A. (2024). Dsdm: Model-aware dataset selection with datamodels.

Data Selection
82
• Motivation: Adapt LLMs to specific domains (e.g., medical or 
legal).
• Data Selection: Select subsets of already well-cleaned data samples to align 
LLMs with target tasks while maintaining generalization.
• Optimization-based Selection: Select subsets towards reducing model loss 
and improving model performance on the target tasks. 
•
Approach 2: Identifies impactful data by analyzing gradient similarities.
•
S1: Fine-tune the model on a random subset using LoRA.
•
S2: Compute Adam LoRA gradients for each sample and project them into lower-dimensional 
features.
•
S3: For downstream tasks, calculate gradient features of validation samples.
•
S4: Estimate influence using cosine similarity and select top-scoring samples.
Xia, M., Malladi, S., Gururangan, S., Arora, S., & Chen, D. (2024). Less: Selecting influential data for targeted instruction tuning.

Data Selection
83
• Motivation: Adapt LLMs to specific domains (e.g., medical or 
legal).
• Data Selection: Select subsets of already well-cleaned data samples to align 
LLMs with target tasks while maintaining generalization.
• Model-Based: Leverages LLMs themselves to evaluate and select high-
quality samples.
•
S1: Prompt the LLM to assess relevance and educational value of each sample.
•
S2: Extract logits for responses ("Yes"/"No") and compute LM-Score
•
S3: Calculate composite score and select samples with highest scores.
•
Is it mathematically relevant?
•
Is it educationally valuable?
Yes/No?
Zhang, Y., Luo, Y., Yuan, Y., & Yao, A. C. (2024). Autonomous data selection with language models for mathematical texts. In ICLR 2024 Workshop on Navigating and 
Addressing Data Problems for Foundation Models.
p(Yes) = 0.7
p(No) = 0.2
LM−Score ⋅=
exp logit `YES′
exp logit `YES′
+ exp logit `NO′

Data Selection
84
• Takeaways
•
Data selection involves choosing subsets of already cleaned data to adapt large language 
models (LLMs) to specific domains, such as medical or legal fields.
•
Three main types of filtering are discussed: Similarity-Based, Optimization-Based and Model-
Based selections:
•
Similarity-based methods select samples with similar feature to the target task.
•
Optimization-based methods select samples that improve model performance on the target task.
•
Model-based methods prompt LLM models to select relevant samples to the target task.
•
Challenges include computational efficiency and robust generalization across tasks.
•
Future Directions include improving efficiency for massive datasets, enhancing accuracy in 
extracting domain patterns and developing better algorithms that generalizes well with the 
incoming data.

86
Data Mixing

Data Mixing For LLM training
87
• Challenge: How to optimize dataset ratios for better 
performance and training efficiency.
•
Before Training(Human Experience): Empirically set and experiment different ratios of 
datasets based on various factors (e.g., complexity and diversity of the datasets) that 
likely improve LLMs’ abilities.
•
e.g., Two-phase training, which focuses diversity in data in phase 1, high quality data such as math 
and code in phase 2, explores their effect with 5 blends each.
Feng, S., Prabhumoye, S., Kong, K. et al.: Maximize your data’s potential: Enhancing llm accuracy with two-phase 
pretraining. arXiv preprint arXiv:2412.15285 (2024)

Data Mixing For LLM training
88
• Challenge: How to optimize dataset ratios for better 
performance and training efficiency.
• Before Training(Model-Based Optimazation): Model the relationship that depicts the 
relation between (i) the distribution of each domain or datasets, (ii) validation loss, and (iii) 
some other variables like training steps. Then find the optimal ratio based on the models.
e.g., Based on the observation of  data from 
experiments for scaling behaviour, Bivariate 
Data Mixing Law depicts the relation among 
domain's proportion, training steps and 
validation loss.
Ge, C., Ma, Z., Chen, D. et al.: Bimix: A bivariate data 
mixing law for language model pretraining (2025)

Data Mixing For LLM training
89
• Challenge: How to optimize dataset ratios for better 
performance and training efficiency.
• During Training (Bilevel Optimization): Use a closed-loop optimization 
technique for two nested optimazation problem that ensures model 
parameters are optimized.
•
e.g., By measuring the contribution of a domain to other domains, which is 
calculated by the dot product of the gradient of the domain and the sum of gradients 
from all other domains and the amount of gradients, dynamically adjust the domain by 
the contributions.
•
Train a larger model using the optimized domain ratios
Minimize the maximum loss across all domains
Fan, S., Pagliardini, M., Jaggi, M.: Doge: Domain reweighting with generalization estimation. arXiv 
preprint arXiv:2310.15393 (2023)

Data Mixing For LLM training
90
• Challenge: How to optimize dataset ratios for better 
performance and training efficiency.
• During Training (Distributionally Robust Optimization): Adopt Distributionally 
Robust Optimization (DRO) for a robust data mixing strategy (which can be 
sub-optimal but with low uncertainty).      
Ma, G., Ma, Y., Wu, X. et al.: Task-level distributionally robust optimization for large language model-
based dense retrieval. arXiv preprint arXiv:2408.10613(2024)
e.g., Optimize By DRO using a small proxy 
model, which first trains a reference model, we 
have the validation loss of each domain for 
reference, then trains proxy model, by 
measuring the loss difference to reference, 
which indicates the improvement potential,  
dynamically adjust the domain weights, and tilt 
to domains with larger loss difference.

Takeaways
• Human experience mixing takes least amount of cost to get a better data ratio for 
training by using the ratios given by these works, by experimenting like this still 
needs quite amount of training.
• For now, almost all mixing methods need a small proxy model for experiments then 
scale up to larger one.
• Model-based methods provide other variables like training step that help us see how 
these variables affect LLM performance with domain ratios. 
• Optimize methods like Bilevel Optimization and DRO performs very well but find 
difficulties for applying on larger models (Largest model with 34B from recent works), 
as it’s done during training.
91

92
Data Synthesis and Distillation For LLM training

Data Synthesis and Distillation For LLM training
93
• Motivation: Synthesize abundant and high-quality data 
for specific domains or use-cases
•
Distillation: Design paradigms to prompt LLM to generate high-quality data to train 
a student LLM with less parameters to mimic the target model’s generation ability.

Data Synthesis and Distillation For LLM training
94
• Pretraining Data Augmentation: Design paradigms to prompt LLM to generate 
synthetic pre-training data for training performant LLM.
• Rephrasing: Rephrase raw corpora into styled corpora
Maini, P., Seto, S., Bai, H. et al.: Rephrasing the web: A recipe for compute and data-efficient language modeling.
Rephrase raw corpora into various styles, from easier 
to harder, e.g.:
1.
Naïve vocabulary and sentence structures.
2.
Standardized encyclopedia-style expression.
3.
Complex academic-style expression. 
4.
Multi-turn dialogue.
Raw corpora from 
web-crawled datasets, 
e.g. C4
Mix original and rephrased corpora 
to increase diversity and reflect the 
real-world scenarios (e.g., typos).
Example
Original: The stock rose $2.11, or about 11 percent, to close Friday at 
$21.51 on the New York Stock Exchange. Revenue in the first quarter 
of the year dropped 15 percent from the same period a year earlier.
Medium: The stock experienced an increase of approximately 11 
percent, closing at $21.51 on the New York Stock Exchange on Friday, 
with a rise of $2.11. During the initial three months of the current year, 
there was a 15 percent decrease in revenue compared to the 
corresponding quarter of the previous year.

Data Synthesis and Distillation For LLM training
95
• Pretraining Data Augmentation: Design paradigms to prompt LLM to generate 
synthetic pre-training data for training performant LLM.
• Augmentation: Turn raw corpora into instruction-augmented corpora.
Fine-tune an Instruction Synthesizer LLM that outputs various instruction-response pairs 
given the raw corpora for LLM pre-training
1.
Curate a diverse set of existing NLP datasets where each example includes a context 
(raw text) and associated tasks (e.g., QA, sentiment, reasoning).
2.
Fine-tune an instruction synthesizer on the curated data.
3.
Convert raw corpora into instruction-augmented corpora by interleaving raw text with its 
synthesized instruction-response pairs
Cheng, D., Gu, Y., Huang, S., Bi, J., Huang, M., & Wei, F. (2024). Instruction pre-training: Language models are supervised multitask learners.

Data Synthesis and Distillation For LLM training
96
• Pretraining Data Augmentation: Design paradigms to prompt LLM to generate 
synthetic pre-training data for training performant LLM.
• Domain Data Synthesis: Turn raw corpora into instruction-augmented corpora.
Synthesize Q&A pairs based on domain and language:
1.
For scientific QA data, prompt LLM with scientific data
(e.g., Mathematics Stack Exchange)
to generate QA pairs (a self-contained problem and
comprehensive, step-by-step solution).
2.
For code QA data, prompt with examples from LeetCode
to generate new, high-quality coding problems and
their corresponding solutions.
Mix and sample Chinese, English and synthetic data to
enhance model’s scientific ability.
Chen, J., Chen, Z., Wang, J., Zhou, K., Zhu, Y., Jiang, J., ... & Wen, J. R. (2024). Towards effective and efficient continual pre-training of large language models. arXiv 
preprint arXiv:2407.18743.

Data Synthesis and Distillation For LLM training
97
• Pretraining Data Augmentation: Design paradigms to prompt LLM to generate 
synthetic pre-training data for training performant LLM.
• Image Caption Synthesis: Augment image captions for uncaptioned or simple-
captioned source images via multimodal models
•
Rewrite existing captions using
textual LLM for diversity
•
Write a detailed visual description of
the image content (e.g., color, shape,
surroundings, etc.) using multimodal
LLM and fuse them with the original
caption for more comprehensive
final caption
Fan, L., Krishnan, D., Isola, P., Katabi, D., & Tian, Y. (2023). Improving clip training with language rewrites. Advances in Neural Information Processing Systems, 36, 35544-35575.
Lai, Z., Zhang, H., Zhang, B., Wu, W., Bai, H., Timofeev, A., ... & Cao, M. (2024, September). Veclip: Improving clip training via visual-enriched captions. In European Conference 
on Computer Vision (pp. 111-127). Cham: Springer Nature Switzerland.

Data Synthesis and Distillation For LLM training
98
• SFT Task Data Augmentation: Design paradigms to prompt LLM to generate 
fine-tuning data to improve specific domains (math, medicine, etc.), align 
LLM’s knowledge to instructions, etc.
• Domain Data Synthesis: Improve specific domains (math, medicine) and 
enhance data diversity
Synthesize domain data gradually and by domains in a course outline way for
systematic coverage of knowledge
1.
Classify data into disciplines (e.g., math, chemistry, computer science)
2.
Generate syllabus (e.g., “Introduction to Calculus”) and key concepts
(e.g., “Limits”) for a specific subject
3.
Create diverse questions and answers based on the concepts using LLM.
Li, H., Dong, Q., Tang, Z. et al.: Synthetic data (almost) from scratch: Generalized instruction tuning for language models. arXiv preprint arXiv:2402.13064 (2024) 

Data Synthesis and Distillation For LLM training
99
• SFT Task Data Augmentation: Design paradigms to prompt LLM to generate 
fine-tuning data to improve specific domains (math, medicine, etc.), align 
LLM’s knowledge to instructions, etc.
• Agentic Data Synthesis: Make LLM solve complex tasks with unfamiliar tools
Generate agentic training data that mimics the real-world tool-use scenarios:
1.
Build a diverse tool repository with MCP tools fetched from GitHub and synthetically 
generated tools created by domains (e.g., financial trading, robotics).
2.
Synthesize agents via system prompts for sampled toolsets and generates tasks paired 
with explicit success rubrics, covering a range from simple to complex operations.
3.
Produce realistic multi-turn
interaction trajectories
where agents use tools to
complete tasks using
multi-agent simulation.
Team, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., ... & Zhang, H. (2025). Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534.

Data Synthesis and Distillation For LLM training
100
•
SFT Task Data Augmentation: Design paradigms to prompt LLM to generate fine-
tuning data to improve specific domains (math, medicine, etc.), align LLM’s 
knowledge to instructions, etc.
•
Self-Synthesis of Instructed LLM: Make LLM self-synthesize user input by 
prompting the model with partial instruction template right before the user input.
•
Instructed LLMs were fine-tuned on templates like ...[INST]Instruction[/INST]..., 
enabling LLM autoregressive output.
Xu, Z., Jiang, F., Niu, L., Deng, Y., Poovendran, R., Choi, Y., & Lin, B. Y. (2024). Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. 
arXiv preprint arXiv:2406.08464.
1. Input the model 
with only [INST]
1. LLM self-synthesize 
instructions without 
any prompts
2. Prompt the model 
with the obtained 
instructions
2. LLM produce various 
responses based on 
the instruction
3. The instructions and responses form the dataset

Data Synthesis and Distillation For LLM training
101
• SFT Reasoning Data Augmentation: Design paradigms to prompt LLM to 
generate fine-tuning data to enhance LLM’s reasoning ability.
• LLM Exploring: Make models perform self-reflection, error correction, and 
alternative solution exploration during reasoning.
Multiple LLM agents generate “Continue-Reflect-Explore” COAT-formatted reasoning chains to fine-
tune a base model for COAT-formatted syntax mastery.
Shen, M., Zeng, G., Qi, Z. et al.: Satori: Reinforcement learning with chain-of-action-thought enhances llm reasoning via autoregressive search. arXiv preprint 
arXiv:2502.02508 (2025)

Data Synthesis and Distillation For LLM training
102
•
SFT Reasoning Data Augmentation: Design paradigms to prompt LLM to 
generate fine-tuning data to enhance LLM’s reasoning ability.
•
Student Model Collaboration Distillation: Improve data labeling by using multiple 
student models with generalized knowledge learned from teacher model.
1.
Generate initial pseudo-labels for unlabeled data using GPT-3.5.
2.
Split data with pseudo-labels into two subsets. Iteratively train two student models on the 
corresponding subset and use these models to generate new pseudo-labels for the other 
subset, achieving less noise and higher generalizability.
3.
Train a single model on all the data with refined labels, achieving near-supervised 
performance with 50 labeled samples (vs. 500 required traditionally).
Zhao, J., Zhao, W., Drozdov, A., Rozonoyer, B., Sultan, M. A., Lee, J. Y., ... & McCallum, A. (2023). Multistage collaborative knowledge distillation from a large language 
model for semi-supervised sequence generation. arXiv preprint arXiv:2311.08640.

Data Synthesis and Distillation For LLM training
103
•
SFT Reasoning Data Augmentation: Design paradigms to prompt LLM to 
generate fine-tuning data to enhance LLM’s reasoning ability.
•
Self-Instruct: Improve instruction following using the model’s generated data.
•
Prompt model to generate novel instructions from in-context examples sampled from 
seed task pool and determine its task type with few-shot labeled examples.
•
Input-first approach for non-classification tasks:
generates an input based on the instruction,
then produces the corresponding output.
•
Output-first approach for classification tasks:
picks a class label, then generates an input that
matches that label, improving label balance.
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., & Hajishirzi, H. (2023). Self-instruct: Aligning language models with self-generated instructions. arXiv 
preprint arXiv:2212.10560.

Data Synthesis and Distillation For LLM training
104
•
SFT Reasoning Data Augmentation: Design paradigms to prompt LLM to 
generate fine-tuning data to enhance LLM’s reasoning ability.
•
Chain-of-Thought Distillation: Enable CoT-like reasoning abilities
for smaller models by decomposing problems into smaller ones
and guiding reasoning steps through subproblems.
1.
Generate step-by-step reasoning traces (full CoT explanations or
subquestion-solution pairs) for problems using a larger model.
2.
Decompose problems into subquestion and frame each reasoning
step as a subquestion followed by its solution.
3.
Train two separate models: a Question Generator (QG) to
produce subquestions and a Question Answerer (QA) to solve them step by step.
Shridhar, K., Stolfo.. Distilling reasoning capabilities into smaller language models. Findings of the Association for Computational Linguistics: ACL 2023, 7059-7073.

Data Synthesis and Distillation For LLM training
105
•
SFT Reasoning Data Augmentation: Design paradigms to prompt LLM to 
generate fine-tuning data to enhance LLM’s reasoning ability.
•
Self-Consistent Chain-of-Thought Distillation: Make teacher model generate 
rationales consistent with correct answer and student model focus on rationales.
•
For teacher model, compare token likelihoods when the correct answer is provided 
versus when a perturbed (e.g., empty or incorrect) answer is given and prefer tokens that 
are more plausible only under the correct answer.
•
The student model is trained on two objectives:
•
factual reasoning, where it learns to generate a rationale and predict the correct answer; and 
•
counterfactual reasoning, where it learns to predict the (incorrect) answer associated with a 
counterfactual rationale.
This teaches the student to base its predictions on the content of the rationale rather than shortcuts.
Wang, P., Wang, Z., Li, Z., Gao, Y., Yin, B., & Ren, X. (2023). Scott: Self-consistent chain-of-thought distillation. arXiv preprint arXiv:2305.01879.

Data Synthesis and Distillation For LLM training
106
•
SFT Reasoning Data Augmentation: Design paradigms to prompt LLM to 
generate fine-tuning data to enhance LLM’s reasoning ability.
•
Knowledge-Augmented Reasoning Distillation: Adapt small models to knowledge-
intensive reasoning tasks with complex reasoning and extensive knowledge.
•
Reasoning Distillation: Prompt teacher model to generate multiple reasoning steps 
(rationales) for each training question, along with correct answers.
•
RAG: During training, retrieve passages relevant to each rationale from a knowledge 
base using the rationale as a query.
Kang, M., Lee, S., Baek, J., Kawaguchi, K., & Hwang, S. J. (2023). Knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks. 
Advances in Neural Information Processing Systems, 36, 48573-48602.

• Takeaways
• Data synthesis using large language models (LLMs) is a powerful approach to generate high-
quality, diverse, and privacy-preserving training data, addressing challenges like data scarcity, 
imbalance, and sensitive information exposure.
• Data Distillation enables LLMs with less parameters has similar performance of larger LLM.
• Synthetic data for pretraining has to be controlled under certain ratio, should be mixed with 
authentic data, otherwise it will even does harm to performance. 
• High quality and well-formatted reasoning data are keys to high reasoning performance.
107
Data Synthesis and Distillation For LLM training

108
End-to-End Pipelines
•
End-to-End Pipelines: Orchestrate data processing operations that 
transform raw data into high-quality LLM pre-training data.

•
RefinedWeb Pipeline
1.
Data acquisition
•
URL filtering: blocklist & URL score
•
Lang identification: FastText (like Word2Vec)
•
Text extraction: Regx Lib (Trafilatura)
2.
Data filtering
•
Document-level filtering: Rule-based.
•
Line-level filtering: Rule-based.
3.
Data deduplication
•
Fuzzy deduplication: Minhash.
•
Exact deduplication: Suffix array.
109
End-to-End Pipelines
The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data only[J]. arXiv, 2023.

110
End-to-End Pipelines
•
DCLM-Baseline Pipeline
•
Adopt RefinedWeb’s heuristic filtering.
•
Use Bloom filter deduplication, offering comparable performance to MinHash with 
higher efficiency on large-scale datasets.
•
Compared to RefinedWeb, additionally apply model-based filtering, retaining only 
1.4% of raw data (vs. 11.67% in RefinedWeb).
Datacomp-lm: In search of the next generation of training sets for language models[J]. NIPS, 2024.
A fastText classifier trained on 
instruction-formatted data, 
including diverse data formats 
(OpenHermes 2.5) and QA
samples (ExplainLikeImFive).

111
End-to-End Pipelines
The fineweb datasets: Decanting the web for the finest text data at scale[J]. NIPS, 2024.
•
FineWeb Pipeline
•
Adopts heuristic filtering from MassiveText and C4 Dataset.
•
Conducts individual Minhash deduplication for each CommonCrawl snapshot.
•
Develops additional custom heuristic filters (e.g. fraction of lines ending with 
punctuation) through a systematic process for better performance.
•
PII (email addresses and public IP addresses) is anonymized using regex patterns.

112
End-to-End Pipelines
•
Designing Principles
•
The trade-off between data quality and quantity.
•
Dependencies across the processing operations (e.g., text extraction necessarily 
preceding operations like deduplication and filtering).
•
Efficiency optimization (e.g., conducting computationally intensive steps like 
model-based filtering after lightweight processing steps like URL filtering).

Data Packing For Data-Centric Training
113
• Challenge: How to conduct data-centric training on the basis 
of a high-quality dataset.
Data Packing Is Required During Pre-Training: Data Packing combines texts to 
ensure uniform input 
lengths in pre-training, 
improving coherence 
and reducing padding 
and truncation.
e.g., Best-fit Packing
Hantian Ding, Zijian Wang, et al. Fewer 
Truncations Improve Language 
Modeling. ICML, 2024
Bin Packing 
Problem
Greedy Algorithm
with Segment 
Tree

Training Strategy For Data-Centric Training
114
• Challenge: How to conduct data-centric training on the basis 
of a high-quality dataset.
    During Training Need Appropriate Strategy: Different training strategies lead to 
different training results.
Guanting Dong, et al. How Abilities in Large Language Models 
are Affected by Supervised Fine-tuning Data Composition. ACL, 
2024
To balance general and 
specialized abilities:
• 1) Fine-tunes on specific 
datasets. 
• 2) Fine-tunes on mixed data. 

Data Shuffling For Data-Centric Training
115
• Challenge: How to conduct data-centric training on the basis 
of a high-quality dataset.
Data Shuffling To Help Training: Data shuffling means that different data needs to be 
selected and provided to LLMs at various stages. (e.g., in different epochs for SFT).
e.g., Velocitune
Zheheng Luo, Xin Zhang,et al. Velocitune: A Velocity-based Dynamic Domain 
Reweighting Method for Continual Pre-training. ACL, 2025

Takeaways
• Data Packing is essentially a bin packing problem in the field of optimization, 
necessitating the use of efficient algorithms with low time complexity.
• Training Strategy requires an appropriate workflow that explores suitable domain 
compositions and mixing ratios at each stage.
• Data Shuffling involves monitoring training signals such as loss, gradients... allowing 
for dynamic adjustment of data sampling ratios throughout the training process.
• Open problems:
• More explainable Training Strategy
• More unified metrics for monitoring training status in Data Shuffling
116

117
Future Opportunities
•
Task-Specific Data Selection for Efficient Pretraining
•
Inclusion of irrelevant data not only increases training time but also impedes the 
model’s adaptability to specific tasks à Adaptive data selection strategies
Align with Human Preference à Specific Tasks
SCAR: Data Selection via Style Consistency-Aware Response Ranking for Efficient Instruction-Tuning of 
Large Language Models. arXiv, 2024.

118
•
Predictive Pipeline Selection / Processing Agent Design
•
Experimentally decide the pipeline is resource-intensive à Predict optimal 
preprocessing configurations in advance or design agentic processing method
Empirical Pipelines à Data(-agent) driven Pipelines
DecorateLM: Data Engineering through Corpus Rating, Tagging, and Editing with Language Models.
EMNLP, 2024.
Future Opportunities

Thanks

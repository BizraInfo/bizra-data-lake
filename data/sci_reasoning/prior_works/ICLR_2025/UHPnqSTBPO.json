{
  "prior_works": [
    {
      "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena",
      "authors": "Lianmin Zheng et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "Established the LLM-as-a-judge paradigm and pairwise evaluation protocols that this paper adopts, while exposing the practical need to decide when to trust a judge\u2019s verdict versus defer."
    },
    {
      "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Correlation",
      "authors": "Shuyang Liu et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "Serves as a strong prompt-based LLM judge baseline that this work explicitly improves upon by adding calibrated confidence and provable guarantees of agreement with humans."
    },
    {
      "title": "Selective Classification for Deep Neural Networks",
      "authors": "Yonatan Geifman et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Introduced the selective prediction (abstain) framework that directly underpins the paper\u2019s selective evaluation idea of trusting a judge only when sufficiently confident."
    },
    {
      "title": "Conformal Risk Control",
      "authors": "Anastasios N. Angelopoulos et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "Provides distribution-free procedures to control risk on selected predictions, which this paper adapts to guarantee a user-specified human disagreement rate for LLM judges."
    },
    {
      "title": "Confident Adaptive Language Modeling",
      "authors": "Tal Schuster et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrates confidence-based adaptive computation and early exits, inspiring the paper\u2019s cascaded selective evaluation that escalates from cheaper to stronger judges only when needed."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "Shows that multiple stochastic samples can be aggregated for more reliable decisions, directly motivating the paper\u2019s Simulated Annotators approach for confidence estimation and calibration of judges."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contributions\u2014selective evaluation with provable guarantees, Simulated Annotators for calibrated confidence, and cascaded trust-or-escalate judging\u2014stem from two converging lines of work. First, the LLM-as-a-judge paradigm was crystallized by Zheng et al. with MT-Bench and Chatbot Arena, and operationalized in strong prompt-based evaluators such as G-Eval. These works defined the problem setting and showed promising human correlation, but they did not endow judges with calibrated uncertainty or any formal agreement guarantees, creating the central gap this paper targets. Second, classical selective prediction by Geifman and El-Yaniv introduced the abstain/coverage framework that maps naturally to the decision of trusting an LLM judge or deferring. To move from heuristic abstention to provable alignment with humans, the paper leverages and adapts Conformal Risk Control to bound disagreement on the selected set at a user-specified level, yielding distribution-free guarantees.\nComplementing this theory, two ideas inform the practical mechanisms. Self-Consistency demonstrates that aggregating multiple stochastic samples yields more reliable signals, directly inspiring Simulated Annotators to estimate judge confidence and improve calibration without human labels. Finally, confidence-driven adaptive computation from Confident Adaptive Language Modeling motivates the cascaded design: use inexpensive judges by default and escalate only when confidence is insufficient, while keeping the conformal-style guarantee intact. Together, these works directly enable the paper\u2019s principled, efficient, and guarantee-bearing framework for trustworthy LLM-based evaluation.",
  "analysis_timestamp": "2026-01-06T23:09:26.635907"
}
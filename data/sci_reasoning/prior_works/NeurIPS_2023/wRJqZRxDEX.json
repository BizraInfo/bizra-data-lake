{
  "prior_works": [
    {
      "title": "Deep Neural Networks as Gaussian Processes",
      "authors": "Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein",
      "year": 2018,
      "role": "Foundational theory for the infinite-width Gaussian process (NNGP) limit used to justify mean-field analyses and hyperparameter selection at initialization.",
      "relationship_sentence": "The paper\u2019s criticality diagnostics rely on the infinite-width GP approximation to compute layerwise statistics, directly building on the NNGP framework established by Lee et al."
    },
    {
      "title": "Exponential expressivity in deep neural networks through transient chaos",
      "authors": "Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, Surya Ganguli",
      "year": 2016,
      "role": "Introduced edge-of-chaos analysis via mean-field signal propagation, linking weight/bias variances and activation functions to trainability.",
      "relationship_sentence": "Doshi et al. extend edge-of-chaos style criteria by moving from forward correlation maps to partial Jacobian norm recurrences that directly diagnose criticality across depths."
    },
    {
      "title": "Deep Information Propagation",
      "authors": "Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, Jascha Sohl-Dickstein",
      "year": 2017,
      "role": "Established variance and correlation recursions for deep random networks and formalized critical initialization conditions.",
      "relationship_sentence": "The new partial Jacobian recurrences generalize the Schoenholz et al. forward-propagation framework to derivatives between arbitrary layers, yielding a practical numerical criticality test."
    },
    {
      "title": "Random Walk Initialization for Training Very Deep Feedforward Networks",
      "authors": "David Sussillo, L.F. Abbott",
      "year": 2015,
      "role": "Showed how to balance gradient norms at initialization (avoiding explosion/vanishing) via principled variance choices.",
      "relationship_sentence": "The proposed partial Jacobian norms target the same goal\u2014stable gradients\u2014but provide a more general, layer-to-layer diagnostic that systematizes critical initialization beyond earlier random-walk heuristics."
    },
    {
      "title": "Resurrecting the Sigmoid: The Effect of Normalization and Residual Connections on Deep Neural Networks",
      "authors": "Jeffrey Pennington, Samuel S. Schoenholz, Surya Ganguli",
      "year": 2017,
      "role": "Mean-field analysis of how residual connections and normalization affect signal propagation and trainability.",
      "relationship_sentence": "Doshi et al. leverage and extend these insights by deriving partial-Jacobian recurrences that explicitly handle LayerNorm and residual architectures to set weight/bias variances and learning rates."
    },
    {
      "title": "The Emergence of Spectral Universality in Deep Networks",
      "authors": "Jeffrey Pennington, Samuel S. Schoenholz, Surya Ganguli",
      "year": 2018,
      "role": "Analyzed the spectrum of the input\u2013output Jacobian and introduced the notion of dynamical isometry for stable training.",
      "relationship_sentence": "By focusing on partial (intermediate-layer) Jacobians rather than only the global input\u2013output Jacobian, the paper provides a finer-grained spectral/norm diagnostic aligned with dynamical isometry principles."
    },
    {
      "title": "On the Impact of the Activation Function on Deep Neural Networks",
      "authors": "Saad Hayou, Arnaud Doucet, Judith Rousseau",
      "year": 2019,
      "role": "Refined edge-of-chaos conditions across activations, giving precise criticality parameters for deep nets.",
      "relationship_sentence": "The authors\u2019 partial-Jacobian framework subsumes activation-dependent criticality conditions, turning them into computable recurrences that guide practical hyperparameter choices (including biases) under various activations."
    }
  ],
  "synthesis_narrative": "Doshi, He, and Gromov build on the mean-field and infinite-width program that connects initialization to trainability. The Gaussian-process perspective (Lee et al., 2018) grounds their use of layerwise statistics in the wide-limit, while edge-of-chaos analyses (Poole et al., 2016; Schoenholz et al., 2017) supply the conceptual link between criticality and stable information flow. Earlier work stabilized training via variance tuning (Sussillo & Abbott, 2015) and revealed how normalization and residual connections reshape signal propagation (Pennington et al., 2017). Complementarily, spectral studies of the input\u2013output Jacobian (Pennington et al., 2018) formalized dynamical isometry as a target for robust gradient flow. \n\nThe key advance here is to shift from global or purely forward-propagation metrics to partial Jacobians\u2014derivatives from preactivations at layer l0 to those at layer l\u2014which directly probe the stability of gradients and signals between arbitrary depths. The authors derive recurrence relations for norms of these partial Jacobians and turn them into a simple numerical test for criticality. This unifies and extends prior criteria by (i) providing a local-in-depth diagnostic tighter than input\u2013output Jacobian analyses, (ii) encompassing architectures with residual connections and LayerNorm within the same framework, and (iii) using the recurrences to select weight and bias variances and learning rates in practice. In short, the work operationalizes the edge-of-chaos/dynamical-isometry intuition at a finer granularity and for modern architectures, delivering a practical and theoretically grounded initialization procedure.",
  "analysis_timestamp": "2026-01-06T23:42:49.113923"
}
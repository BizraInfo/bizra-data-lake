{
  "prior_works": [
    {
      "title": "Emergent Abilities of Large Language Models",
      "authors": "Jason Wei et al.",
      "year": 2022,
      "arxiv_id": "2206.07682",
      "role": "Foundation",
      "relationship_sentence": "This work crystallized the notion of \u201cemergent abilities\u201d from scale on benchmarks, providing the specific phenomenon that the present paper re-examines and explains via the degree of training on the test task."
    },
    {
      "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models (BIG-bench)",
      "authors": "Abhishek Srivastava et al.",
      "year": 2022,
      "arxiv_id": "2206.04615",
      "role": "Foundation",
      "relationship_sentence": "BIG-bench supplied the task suite where sharp performance jumps were first documented, furnishing the concrete test tasks on which the paper studies how exposure to task-relevant data drives measured gains."
    },
    {
      "title": "Measuring Massive Multitask Language Understanding",
      "authors": "Dan Hendrycks et al.",
      "year": 2020,
      "arxiv_id": "2009.03300",
      "role": "Foundation",
      "relationship_sentence": "MMLU established a central benchmark used to compare model families, which this paper analyzes under the lens of training-on-the-test-task and adjusts by equalizing task-relevant fine-tuning across models."
    },
    {
      "title": "Are Emergent Abilities of Large Language Models a Mirage?",
      "authors": "Rylan Schaeffer et al.",
      "year": 2023,
      "arxiv_id": "2304.15004",
      "role": "Gap Identification",
      "relationship_sentence": "By attributing emergence to metric and scaling artifacts, this paper exposed a gap that the present work addresses by demonstrating a distinct confound\u2014training on the test task\u2014and providing an adjustment procedure."
    },
    {
      "title": "Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks",
      "authors": "Suchin Gururangan et al.",
      "year": 2020,
      "arxiv_id": "2004.10964",
      "role": "Inspiration",
      "relationship_sentence": "Domain- and task-adaptive pretraining showed that pretraining on task-relevant corpora boosts downstream scores, directly motivating the paper\u2019s formalization of \u201ctraining on the test task\u201d and its controlled adjustment via shared task-relevant tuning."
    },
    {
      "title": "Finetuned Language Models are Zero-Shot Learners (FLAN)",
      "authors": "Jason Wei et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Instruction tuning on mixtures of tasks demonstrated large zero-shot gains via exposure to task formulations, providing the concrete mechanism the paper identifies as training on the test task that can confound benchmark comparisons."
    },
    {
      "title": "Holistic Evaluation of Language Models (HELM)",
      "authors": "Percy Liang et al.",
      "year": 2022,
      "arxiv_id": "2211.09110",
      "role": "Gap Identification",
      "relationship_sentence": "HELM highlighted confounds and comparability issues in LM evaluation, motivating the paper\u2019s proposal to standardize evaluation by fine-tuning all compared models on the same task-relevant data."
    }
  ],
  "synthesis_narrative": "Work on large language model evaluation documented striking scale-related performance patterns on diverse, compositional tasks, with BIG-bench providing a broad suite where sharp capability jumps were observed and the Emergent Abilities paper coining the term for such discontinuities. MMLU established a widely adopted multitask benchmark for ranking models, becoming a focal point for claims about general knowledge and reasoning ability. A counterpoint argued that emergence could be a mirage induced by metric discretization and scaling choices, suggesting artifacts rather than genuine step-changes. In parallel, lines of research demonstrated that exposing models to task-relevant distributions during pretraining or fine-tuning materially boosts benchmark performance: domain- and task-adaptive pretraining showed gains from continued pretraining on task/domain text, while instruction tuning (e.g., FLAN) achieved large zero-shot improvements by training on mixtures of tasks that mirror evaluation formats. Holistic evaluation efforts emphasized comparability, surfacing confounds from differing data, prompts, and setups across model families. Together, these strands revealed a plausible mechanism behind apparent superiority and emergence: models often differ in how much they have already been trained on the very tasks or formats used for evaluation. The natural next step is to make this mechanism explicit and controlled\u2014operationalizing fairness by equalizing task-relevant exposure across models via a shared fine-tuning phase\u2014and to test whether emergence persists once this confound is removed, thereby reframing both model comparisons and claims of emergent capabilities.",
  "target_paper": {
    "title": "Training on the Test Task Confounds Evaluation and Emergence",
    "authors": "Ricardo Dominguez-Olmedo, Florian E. Dorner, Moritz Hardt",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "language models, benchmarking, emergence",
    "abstract": "We study a fundamental problem in the evaluation of large language models that we call training on the test task. Unlike wrongful practices like training on the test data, leakage, or data contamination, training on the test task is not a malpractice.  Rather, the term describes a growing set of techniques to include task-relevant data in the pretraining stage of a language model. We demonstrate that training on the test task confounds both relative model evaluations and claims about emergent capabilities. We argue that the seeming superiority of one model family over another may be explained by a different degree of training on the test task. To this end, we propose an effective method to adjust for the effect of training on the test task on benchmark evaluations. Put simply, to fine-tune each model under comparison on the same task-relevant data before evaluation. Lastly, we show that instances of emergent behavior disappear gradually as models train on the test task. Our work promot",
    "openreview_id": "jOmk0uS1hl",
    "forum_id": "jOmk0uS1hl"
  },
  "analysis_timestamp": "2026-01-06T16:52:57.600025"
}
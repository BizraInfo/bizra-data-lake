{
  "prior_works": [
    {
      "title": "Multicalibration: Calibration for the (Computationally-Identifiable) Masses",
      "authors": "Hebert-Johnson et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "This paper introduced multicalibration with respect to a hypothesis class H and oracle-based algorithms, and left open when multicalibration guarantees Bayes-optimal prediction\u2014questions this work resolves via a swap-regret characterization and a weak-learning condition."
    },
    {
      "title": "Multiaccuracy: Black-Box Post-Processing for Fairness in Classification",
      "authors": "Kim et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Their audit-and-correct procedure over a class H directly inspires the algorithmic template here; the present work strengthens it to squared-error regression and shows the same updates are boosting steps that provably reach Bayes optimality under a weak learning assumption."
    },
    {
      "title": "Boosting with the L2 loss: Regression and classification",
      "authors": "B\u00fchlmann et al.",
      "year": 2003,
      "role": "Baseline",
      "relationship_sentence": "L2Boosting\u2019s iterative residual-fitting with a squared-error regression oracle is the baseline mechanism that this paper reinterprets as enforcing multicalibration, unifying boosting and multicalibration under a single analysis."
    },
    {
      "title": "From External to Internal Regret",
      "authors": "Blum et al.",
      "year": 2007,
      "role": "Foundation",
      "relationship_sentence": "This work formalized swap/internal regret; the present paper\u2019s key technical step is a swap-regret\u2013like characterization of multicalibration for squared error, directly borrowing this notion to structure the analysis."
    },
    {
      "title": "Asymptotic Calibration",
      "authors": "Foster et al.",
      "year": 1998,
      "role": "Inspiration",
      "relationship_sentence": "By linking calibration to no-internal-regret dynamics, this paper motivates viewing calibration constraints through an internal/swap-regret lens, which the current work adapts to multicalibration under squared loss."
    },
    {
      "title": "Omnipredictors",
      "authors": "Gopalan et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "Omnipredictors connected multicalibration-like conditions to near-optimality across tasks; this work sharpens that connection for squared error by giving necessary and sufficient weak-learning conditions under which multicalibration implies Bayes-optimal regression."
    },
    {
      "title": "Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness",
      "authors": "Kearns et al.",
      "year": 2018,
      "role": "Related Problem",
      "relationship_sentence": "Introduced the auditor\u2013learner framework over a concept class H; the present algorithm follows the same audit-and-correct paradigm but instantiates it with a squared-error regression oracle and interprets it as a boosting procedure."
    }
  ],
  "synthesis_narrative": "The core innovation in \u201cMulticalibration as Boosting for Regression\u201d crystallizes two intellectual threads: multicalibration from algorithmic fairness and boosting for squared-error regression. Hebert-Johnson et al. established multicalibration with respect to a class H and oracle-based procedures, but left open when such constraints ensure Bayes-optimal prediction. Kim et al.\u2019s Multiaccuracy showed how to audit and correct predictors via correlations with H, foreshadowing a boosting-like residual-correction view but focused on classification and without Bayes-optimality guarantees. In parallel, L2Boosting (B\u00fchlmann & Yu) provided the squared-loss residual-fitting mechanism using a regression oracle\u2014the very update rule this paper shows also enforces multicalibration, thereby unifying boosting and multicalibration in a single, simple algorithm. The paper\u2019s key technical tool is a swap-regret\u2013style characterization of multicalibration for squared error, directly grounded in the internal/swap-regret framework developed by Foster & Vohra and Blum & Mansour. Finally, recent work on Omnipredictors connected multicalibration-like guarantees to downstream optimality; the present paper sharpens this lens for regression by giving a weak-learning condition that is both necessary and sufficient for multicalibration to imply Bayes optimality, delivering an agnostic boosting result without realizability. The result is a clean lineage: calibration \u2194 internal regret, instantiated as multicalibration; residual-fitting \u2194 L2 boosting, instantiated via a regression oracle; and their synthesis yields a principled, agnostic path to Bayes-optimal regression.",
  "analysis_timestamp": "2026-01-06T23:09:26.538350"
}
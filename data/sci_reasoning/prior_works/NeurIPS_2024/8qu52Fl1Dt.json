{
  "prior_works": [
    {
      "title": "Reconstructing visual experiences from brain activity evoked by natural movies",
      "authors": "Shinji Nishimoto; An T. Vu; Thomas Naselaris; Yuval Benjamini; Bin Yu; Jack L. Gallant",
      "year": 2011,
      "role": "Foundational fMRI-to-video decoding",
      "relationship_sentence": "Established the feasibility of reconstructing dynamic naturalistic videos from fMRI and highlighted the importance of temporal coherence\u2014directly motivating NeuroClips\u2019 focus on smoothness over time."
    },
    {
      "title": "Deep image reconstruction from human brain activity",
      "authors": "Guohua Shen; Tomoyasu Horikawa; Kei Majima; Yukiyasu Kamitani",
      "year": 2019,
      "role": "Deep generative prior for brain decoding",
      "relationship_sentence": "Showed that mapping fMRI to hierarchical DNN features and using a generative prior yields high-fidelity reconstructions, informing NeuroClips\u2019 two-stage design that aligns high-level semantics and low-level perceptual signals before generation."
    },
    {
      "title": "High-resolution image reconstruction with latent diffusion models from human brain activity",
      "authors": "Yu Takagi; Shinji Nishimoto",
      "year": 2023,
      "role": "Stable Diffusion-based fMRI-to-image",
      "relationship_sentence": "Demonstrated that LDMs enable photorealistic image reconstructions from fMRI via semantic-space alignment, a direct precursor to NeuroClips\u2019 use of diffusion priors and keyframe-driven semantic guidance."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford; Jong Wook Kim; Chris Hallacy; Aditya Ramesh; Gabriel Goh; Sandhini Agarwal; Girish Sastry; Amanda Askell; Pamela Mishkin; Jack Clark; Gretchen Krueger; Ilya Sutskever",
      "year": 2021,
      "role": "Semantic embedding for alignment",
      "relationship_sentence": "Provided the multimodal embedding space widely used to map brain activity to high-level semantics; NeuroClips\u2019 semantics reconstructor leverages this paradigm to ensure keyframe semantic accuracy and consistency."
    },
    {
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "authors": "Robin Rombach; Andreas Blattmann; Dominik Lorenz; Patrick Esser; Bj\u00f6rn Ommer",
      "year": 2022,
      "role": "Generative diffusion backbone",
      "relationship_sentence": "Introduced LDMs as controllable, efficient priors; NeuroClips builds on LDM-based T2V models and injects keyframes/low-level signals into the diffusion process to steer video synthesis."
    },
    {
      "title": "Two-Stream Convolutional Networks for Action Recognition in Videos",
      "authors": "Karen Simonyan; Andrew Zisserman",
      "year": 2014,
      "role": "Appearance\u2013motion factorization",
      "relationship_sentence": "Popularized decoupling spatial appearance (semantics) from temporal motion (optical-flow-like cues), directly inspiring NeuroClips\u2019 split into a semantics reconstructor (keyframes) and a perception reconstructor (low-level flows) for smoothness."
    },
    {
      "title": "ControlNet: Adding Conditional Control to Text-to-Image Diffusion Models",
      "authors": "Lvmin Zhang; Maneesh Agrawala",
      "year": 2023,
      "role": "Conditioned diffusion for structural guidance",
      "relationship_sentence": "Showed how to inject external structural signals into diffusion generation; NeuroClips analogously injects keyframes and low-level perceptual cues into a T2V diffusion model to enforce fidelity and temporal consistency."
    }
  ],
  "synthesis_narrative": "NeuroClips\u2019 core contribution\u2014high-fidelity and smooth fMRI-to-video reconstruction by jointly decoding high-level semantics and low-level perceptual flows\u2014sits at the confluence of three lines of work. First, foundational brain-to-video decoding by Nishimoto et al. demonstrated that dynamic visual experiences can be reconstructed from fMRI, underscoring the need for temporal coherence. Subsequent advances in brain decoding with deep generative priors, notably Shen et al., established that mapping fMRI into hierarchical feature spaces and leveraging a powerful image prior markedly improves fidelity. Takagi and Nishimoto then showed that latent diffusion models (LDMs/Stable Diffusion) are particularly effective for fMRI-to-image, validating the strategy of aligning brain signals to semantic embeddings and using a diffusion prior for photorealism. Second, CLIP provided the semantic embedding space that enables robust alignment of fMRI with high-level visual-language representations; NeuroClips exploits this via a semantics reconstructor that produces keyframes to anchor content accuracy and consistency. Third, progress in generative modeling\u2014LDMs as an efficient, controllable backbone and ControlNet-style conditioning\u2014introduced mechanisms to inject structural guidance into diffusion sampling. Complementing this, the two-stream paradigm from video recognition crystallized the separation of appearance (semantics) and motion (perceptual flow), directly shaping NeuroClips\u2019 dual reconstructor design. Together, these works enable NeuroClips to inject both semantically accurate keyframes and low-level motion cues into a pre-trained text-to-video diffusion model, achieving reconstructions that are simultaneously high-fidelity and temporally smooth.",
  "analysis_timestamp": "2026-01-06T23:33:35.582266"
}
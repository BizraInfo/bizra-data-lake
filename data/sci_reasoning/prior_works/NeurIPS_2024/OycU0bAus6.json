{
  "prior_works": [
    {
      "title": "Extracting and Composing Robust Features with Denoising Autoencoders",
      "authors": "Pascal Vincent, Hugo Larochelle, Yoshua Bengio, Pierre-Antoine Manzagol",
      "year": 2008,
      "role": "Conceptual foundation: denoising as a representation-learning objective",
      "relationship_sentence": "Introduced denoising as a training signal to learn robust features, which DenoiseRep generalizes from input-level denoising to treating every embedding layer as a denoising layer within a discriminative backbone."
    },
    {
      "title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion",
      "authors": "Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol",
      "year": 2010,
      "role": "Methodological precedent: layerwise/cascaded denoising",
      "relationship_sentence": "Showed that stacking denoising modules yields progressively more abstract representations, directly informing DenoiseRep\u2019s view of a deep network as a cascade of recursive denoising steps aligned with feature extraction."
    },
    {
      "title": "Semi-Supervised Learning with Ladder Networks",
      "authors": "Antti Rasmus, Mathias Valpola, Mikko Honkala, Harri Aila, Tapani Raiko",
      "year": 2015,
      "role": "Architectural precursor: denoising at every layer for discriminative tasks",
      "relationship_sentence": "Performed layerwise denoising of latent activations to improve classification, a core idea that DenoiseRep adopts and systematizes by unifying backbone feature extraction and denoising and by fusing their parameters."
    },
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho, Ajay Jain, Pieter Abbeel",
      "year": 2020,
      "role": "Algorithmic inspiration: iterative denoising as a principled process",
      "relationship_sentence": "Framed generation as a sequence of denoising steps; DenoiseRep maps this iterative denoising perspective onto network depth, interpreting each embedding layer as one denoising stage to enhance discriminative representations."
    },
    {
      "title": "What Regularized Auto-Encoders Learn from the Data-Generating Distribution",
      "authors": "Guillaume Alain, Yoshua Bengio",
      "year": 2014,
      "role": "Theoretical grounding: denoising connects to score estimation",
      "relationship_sentence": "Established that denoising objectives estimate the score/structure of the data distribution, underpinning DenoiseRep\u2019s theoretical claim that recursive denoising improves feature discrimination."
    },
    {
      "title": "Learning Fast Approximations of Sparse Coding (LISTA)",
      "authors": "Karla Gregor, Yann LeCun",
      "year": 2010,
      "role": "Design pattern: unrolling iterative inference into deep layers with parameter sharing",
      "relationship_sentence": "Demonstrated interpreting network layers as iterations of an optimization/denoising procedure with shared or fused parameters, mirroring DenoiseRep\u2019s treatment of depth as recursive denoising and its parameter fusion between extraction and denoising."
    }
  ],
  "synthesis_narrative": "DenoiseRep\u2019s core contribution\u2014viewing each embedding layer as a denoising layer to unify feature extraction with recursive denoising and then fusing their parameters\u2014sits at the intersection of classic denoising-based representation learning and modern iterative-denoising paradigms. The foundational idea comes from denoising autoencoders (Vincent et al., 2008), which established denoising as a powerful objective for learning robust features. Stacked denoising autoencoders (Vincent et al., 2010) extended this to cascaded layers, suggesting that progressively deeper representations can be formed by sequential denoising, a premise DenoiseRep operationalizes across all embedding layers of a discriminative backbone.\nLadder Networks (Rasmus et al., 2015) provided a direct architectural precedent by applying denoising at every layer to aid classification, thereby showing that denoising can enhance discriminative performance, not just generative modeling. The theoretical link that denoising objectives estimate the score of the data distribution (Alain & Bengio, 2014) supports DenoiseRep\u2019s claim that recursive denoising sharpens feature discrimination.\nFrom the generative side, Denoising Diffusion Probabilistic Models (Ho et al., 2020) popularized the view of generation as iterative denoising steps; DenoiseRep translates this temporal denoising notion into spatial depth within a classifier. Finally, the deep-unfolding perspective of LISTA (Gregor & LeCun, 2010) inspires interpreting layers as iterations of a denoising/inference process, and motivates parameter fusion/sharing across steps\u2014echoed in DenoiseRep\u2019s fusion of feature extraction and denoising parameters. Together, these works directly shape DenoiseRep\u2019s unified, layerwise denoising framework for discriminative representation learning.",
  "analysis_timestamp": "2026-01-06T23:33:35.556565"
}
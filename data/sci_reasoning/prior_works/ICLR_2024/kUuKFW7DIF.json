{
  "prior_works": [
    {
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "authors": "Wei-Ning Hsu et al.",
      "year": 2021,
      "arxiv_id": "2106.07447",
      "role": "Baseline",
      "relationship_sentence": "The proposed method directly adopts and extends HuBERT\u2019s masked hidden-unit prediction objective, generalizing it to multiple temporal resolutions within a hierarchical encoder."
    },
    {
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": "Alexei Baevski et al.",
      "year": 2020,
      "arxiv_id": "2006.11477",
      "role": "Foundation",
      "relationship_sentence": "wav2vec 2.0 established the fixed 20 ms frame-rate SSL paradigm for speech that this work explicitly relaxes by introducing multi-resolution processing."
    },
    {
      "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
      "authors": "Sanyuan Chen et al.",
      "year": 2022,
      "arxiv_id": "2110.13900",
      "role": "Gap Identification",
      "relationship_sentence": "Although WavLM strengthens HuBERT with denoising and large-scale training, it still models speech at a single 20 ms resolution, highlighting the gap that multi-resolution SSL aims to fill."
    },
    {
      "title": "Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing",
      "authors": "Zihang Dai et al.",
      "year": 2020,
      "arxiv_id": "2006.03236",
      "role": "Inspiration",
      "relationship_sentence": "The hierarchical Transformer with progressive downsampling in Funnel-Transformer motivates using a multi-stage encoder that reduces sequence length while enriching higher-level representations."
    },
    {
      "title": "Listen, Attend and Spell",
      "authors": "William Chan et al.",
      "year": 2016,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "The pyramidal subsampling strategy introduced for speech sequences provides the core idea of multi-resolution temporal modeling that is adapted into a modern SSL, Transformer-based setting."
    },
    {
      "title": "SUPERB: Speech processing Universal PERformance Benchmark",
      "authors": "Shuo-Wei (Steven) Yang et al.",
      "year": 2021,
      "arxiv_id": "2105.01051",
      "role": "Foundation",
      "relationship_sentence": "SUPERB furnishes the standardized downstream evaluation setting that defines the task suite and metrics used to assess the benefits of multi-resolution SSL representations."
    }
  ],
  "synthesis_narrative": "Masked prediction of hidden units established a powerful non-contrastive recipe for speech self-supervision, with HuBERT showing that pseudo-labels derived from clustering can drive strong representations when models are trained on masked spans. wav2vec 2.0 framed the modern speech SSL pipeline with a fixed 20 ms frame rate and masking, cementing the assumption of a single temporal resolution as the default processing granularity. WavLM demonstrated that scaling data and adding denoising tasks further improves HuBERT-style pretraining, yet it retained the single-resolution setup, underscoring that robustness gains were possible without changing temporal granularity. In parallel, the NLP community proposed hierarchical Transformers such as the Funnel-Transformer, which progressively reduces sequence length to improve efficiency while preserving semantics at coarser layers. Earlier in speech, Listen, Attend and Spell popularized pyramidal subsampling to shorten acoustic sequences and expose models to longer linguistic contexts, providing an early template for multi-resolution temporal processing. SUPERB standardized the evaluation of learned speech representations across many downstream tasks, enabling consistent assessment of architectural changes.\n\nTogether, these works reveal a clear opportunity: powerful masked-unit SSL remained locked to a single 20 ms resolution despite longstanding evidence that hierarchical, downsampled encoders capture longer-range structure efficiently. The current work fuses HuBERT\u2019s masked unit prediction with a hierarchical Transformer that operates at multiple temporal resolutions, directly addressing the fixed-resolution limitation while inheriting the efficiency benefits of pyramidal and funnel architectures, and validating the gains under the SUPERB framework.",
  "target_paper": {
    "title": "Multi-resolution HuBERT: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction",
    "authors": "Jiatong Shi, Hirofumi Inaguma, Xutai Ma, Ilia Kulikov, Anna Sun",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Speech Representation Learning, Self-supervised Learning, Multi-resolution",
    "abstract": "Existing Self-Supervised Learning (SSL) models for speech typically process speech signals at a fixed resolution of 20 milliseconds. This approach overlooks the varying informational content present at different resolutions in speech signals. In contrast, this paper aims to incorporate multi-resolution information into speech self-supervised representation learning. We introduce an SSL model that leverages a hierarchical Transformer architecture, complemented by HuBERT-style masked prediction objectives, to process speech at multiple resolutions. Experimental results indicate that the proposed model not only achieves more efficient inference but also exhibits superior or comparable performance to the original HuBERT model over various tasks. Specifically, significant performance improvements over the original HuBERT have been observed in fine-tuning experiments on the LibriSpeech speech recognition benchmark as well as in evaluations using the Speech Universal PERformance Benchmark (SU",
    "openreview_id": "kUuKFW7DIF",
    "forum_id": "kUuKFW7DIF"
  },
  "analysis_timestamp": "2026-01-06T18:04:42.622716"
}
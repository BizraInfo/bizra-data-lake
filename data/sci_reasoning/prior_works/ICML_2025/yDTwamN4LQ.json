{
  "prior_works": [
    {
      "title": "Characteristic Functions of Measures on Geometric Rough Paths",
      "authors": "Ilya Chevyrev, Terry Lyons",
      "year": 2016,
      "role": "Foundational theory of expected signatures as law-characterizing features",
      "relationship_sentence": "Established that the expected signature (under mild conditions) uniquely determines the law of a path-valued random variable, providing the probabilistic foundation the paper relies on to interpret expected-signature-based learning and to target the continuous-time expected signature with consistent estimators."
    },
    {
      "title": "Differential Equations Driven by Rough Signals",
      "authors": "Terry Lyons",
      "year": 1998,
      "role": "Core rough path framework and convergence of discrete approximations",
      "relationship_sentence": "Provided the rough path continuity/Wong\u2013Zakai paradigm showing that signatures of piecewise-linear (discrete) approximations converge to the continuous-time (Stratonovich) signature, a key technical backbone for proving convergence from empirical discrete-time signature estimators to their continuous-time expected limits."
    },
    {
      "title": "Multidimensional Stochastic Processes as Rough Paths: Theory and Applications",
      "authors": "Peter K. Friz, Nicolas B. Victoir",
      "year": 2010,
      "role": "Technical machinery for signature convergence and estimates",
      "relationship_sentence": "Developed quantitative rough path estimates and approximation results that the paper leverages implicitly to control discretization error and pass to limits when establishing consistency and convergence rates of expected-signature estimators."
    },
    {
      "title": "The Expected Signature of a L\u00e9vy Process",
      "authors": "Ilya Chevyrev",
      "year": 2018,
      "role": "Structure of expected signatures for martingales/independent-increment processes",
      "relationship_sentence": "Gave explicit tensor-exponential formulas and structural properties (e.g., vanishing of certain levels, cumulant structure) for expected signatures of L\u00e9vy processes, directly motivating the paper\u2019s martingale-specific modification that reduces MSE by exploiting these moment/tensor constraints."
    },
    {
      "title": "Kernels for Sequentially Ordered Data",
      "authors": "Tam\u00e1s Kiss T. Gy. Kir\u00e1ly, Harald Oberhauser",
      "year": 2019,
      "role": "Signature-based kernel methods and distributional embeddings",
      "relationship_sentence": "Connected signatures to characteristic/universal kernels and kernel mean embeddings, reinforcing the distributional viewpoint the paper adopts and motivating convergence results that justify using expected signatures as model-free features in probabilistic ML."
    },
    {
      "title": "A Primer on the Signature Method in Machine Learning",
      "authors": "Ilya Chevyrev, Alexander Kormilitzin",
      "year": 2016,
      "role": "Methodological bridge from signature theory to practical estimators",
      "relationship_sentence": "Surveyed and formalized the use of (expected) signatures as features estimated from discretely sampled data, directly highlighting the gap between empirical discrete-time estimators and continuous-time objects that this paper closes with rigorous convergence guarantees."
    },
    {
      "title": "Deep Signature Transforms",
      "authors": "Emmanuel Bonnier, Patrick Kidger, Imanol Perez Arribas, Cristopher Salvi, Terry Lyons",
      "year": 2019,
      "role": "Practical use of empirical signature statistics in learning pipelines",
      "relationship_sentence": "Demonstrated effective learning with signature features computed from discrete streams, underscoring the need for theoretical results on estimator consistency and motivating the paper\u2019s analysis and improved martingale estimator to enhance downstream predictive performance."
    }
  ],
  "synthesis_narrative": "The paper\u2019s main contribution\u2014establishing convergence from empirical discrete-time estimators to continuous-time expected signatures and proposing a lower-MSE estimator for martingale data\u2014rests on two intertwined lines of prior work. First, the probabilistic meaning of the expected signature as a distribution-determining object is grounded in Chevyrev and Lyons (2016), which treats the expected signature as an analogue of a moment/characteristic function. This viewpoint motivates targeting the continuous-time expected signature as the population quantity that learning algorithms should estimate. Second, the technical bridge from discrete observations to continuous-time signatures is supplied by rough path theory. Lyons (1998) and Friz\u2013Victoir (2010) provide the Wong\u2013Zakai-type convergence and quantitative control needed to pass from signatures of discretely sampled, piecewise-linear paths to their continuous-time limits, enabling the paper\u2019s consistency and convergence claims for expected-signature estimators.\nOn the modeling and application side, the use of expected/signature features in machine learning\u2014articulated in Chevyrev\u2013Kormilitzin (2016) and operationalized in works like Bonnier et al. (2019)\u2014created the practical demand for the paper\u2019s theoretical guarantees. Meanwhile, Kir\u00e1ly\u2013Oberhauser (2019) connected signatures to characteristic kernels and distributional embeddings, reinforcing the model-free, distribution-centric framing. Finally, structural results on expected signatures for processes with independent increments, particularly Chevyrev (2018) for L\u00e9vy processes, illuminate martingale-specific tensor constraints that the paper exploits to design a simple variance-reducing modification of the estimator, yielding improved mean-squared error and better predictive performance in practice.",
  "analysis_timestamp": "2026-01-07T00:21:32.375471"
}
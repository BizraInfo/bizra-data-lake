{
  "prior_works": [
    {
      "title": "Hyperparameter Optimization with Approximate Gradient",
      "authors": "Fabian Pedregosa",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "Established the bilevel formulation for hyperparameter optimization and the implicit-gradient (inverse-Hessian) hypergradient, which F2SA targets while explicitly avoiding any Hessian computations."
    },
    {
      "title": "Bilevel Programming for Hyperparameter Optimization and Meta-Learning",
      "authors": "Luca Franceschi et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Popularized the bilevel formulation in modern ML and reinforced the implicit-differentiation route to hypergradients, providing the problem setup that F2SA adopts but solves with fully first-order oracles."
    },
    {
      "title": "Approximation Methods for Bilevel Programming",
      "authors": "Saeed Ghadimi et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Provided one of the first rigorous stochastic bilevel methods and non-asymptotic guarantees but with explicit reliance on second-order information (Hessian/Hvps), the computational bottleneck that F2SA removes while delivering finite-time rates."
    },
    {
      "title": "Bilevel Optimization: Nonconvex\u2013Strongly-Convex Case",
      "authors": "Kaiyi Ji et al.",
      "year": 2021,
      "role": "Baseline",
      "relationship_sentence": "Gave provable algorithms and rates for stochastic bilevel problems under nonconvex\u2013strongly-convex structure using Hessian-based hypergradient estimators, whose second-order dependence F2SA replaces with a first-order stochastic approximation scheme."
    },
    {
      "title": "Meta-Learning with Implicit Gradients",
      "authors": "Aravind Rajeswaran et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrated the effectiveness of implicit-gradient hyperparameter/meta-learning but required Hessian\u2013vector products, highlighting the cost that motivates F2SA\u2019s fully first-order design with provable iteration complexity."
    },
    {
      "title": "Momentum-Based Variance Reduction in Non-Convex SGD (STORM)",
      "authors": "Ashok Cutkosky et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Introduced momentum-assisted gradient estimators with O(1) samples per iteration, which F2SA adapts to the bilevel setting to sharpen iteration complexities beyond its basic first-order estimator."
    }
  ],
  "synthesis_narrative": "The core innovation of F2SA is to obtain provable, non-asymptotic convergence for stochastic bilevel optimization while using only first-order gradient oracles, thereby eliminating any dependence on Hessians or Hessian\u2013vector products. This advances the implicit-differentiation paradigm initiated for hyperparameter tuning by Pedregosa (2016) and widely adopted in ML by Franceschi et al. (2018), where the upper-level gradient requires an inverse Hessian of the lower-level objective. While these works crystallized the bilevel formulation and hypergradient, they either lacked finite-time guarantees or entailed second-order computations. Subsequent theoretical progress in stochastic bilevel optimization\u2014exemplified by Ghadimi et al. (2018) and Ji et al. (2021)\u2014provided non-asymptotic guarantees but retained explicit second-order operations as a central ingredient, leaving a clear computational gap. Practical meta-learning approaches using implicit gradients (Rajeswaran et al., 2019) further underscored the importance of the hypergradient route while highlighting the burden of Hessian\u2013vector products in large-scale stochastic regimes. F2SA addresses this gap by constructing a fully first-order stochastic approximation scheme with rigorous rates under different noise models. Moreover, by integrating momentum-assisted gradient estimators in the spirit of STORM (Cutkosky et al., 2019), F2SA attains improved iteration complexities with O(1) samples per iteration. In sum, F2SA directly builds on the bilevel/implicit-gradient foundation and the latest stochastic bilevel theory, while replacing their second-order dependencies with a principled, provably convergent first-order alternative.",
  "analysis_timestamp": "2026-01-06T23:09:26.570416"
}
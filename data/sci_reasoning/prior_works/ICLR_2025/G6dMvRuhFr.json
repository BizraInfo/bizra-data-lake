{
  "prior_works": [
    {
      "title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos",
      "authors": "Bowen Baker et al.",
      "year": 2022,
      "arxiv_id": "2206.11795",
      "role": "Gap Identification",
      "relationship_sentence": "VPT operationalizes learning from internet videos via a vision-based inverse dynamics model trained with embodiment-specific labels, whose data demands and embodiment dependence are precisely the limitations this paper removes by replacing inverse-dynamics labeling with goal-conditioned self-exploration."
    },
    {
      "title": "Reinforcement Learning with Imagined Goals",
      "authors": "Ashvin Nair et al.",
      "year": 2018,
      "arxiv_id": "1807.04742",
      "role": "Foundation",
      "relationship_sentence": "RIG introduced using a generative model to sample visual goals for goal-conditioned learning and exploration, a principle this paper adopts by sourcing those goals from a pretrained internet-scale video model."
    },
    {
      "title": "Skew-Fit: State-Covering Self-Supervised Reinforcement Learning",
      "authors": "Vitchyr Pong et al.",
      "year": 2019,
      "arxiv_id": "1903.03698",
      "role": "Inspiration",
      "relationship_sentence": "Skew-Fit showed that biasing goal sampling toward underrepresented states expands exploration coverage, motivating the paper\u2019s use of video-generated goals to drive coverage beyond the agent\u2019s initial embodiment data."
    },
    {
      "title": "Diffuser: Diffusion Models for Offline Reinforcement Learning",
      "authors": "Michael Janner et al.",
      "year": 2022,
      "arxiv_id": "2205.09991",
      "role": "Extension",
      "relationship_sentence": "Diffuser demonstrated trajectory-level action generation via diffusion conditioned on goals, which this paper extends by conditioning trajectory generation on video-guided visual goals to directly ground video to continuous actions."
    },
    {
      "title": "Visual Foresight: Deep Predictive Models for Planning in Robotic Manipulation",
      "authors": "Frederik Ebert et al.",
      "year": 2018,
      "arxiv_id": "1812.00568",
      "role": "Related Problem",
      "relationship_sentence": "Visual Foresight showed that predicting future visual states can guide control toward goal images, informing this paper\u2019s use of generated video states as guidance targets for action generation."
    },
    {
      "title": "Hindsight Experience Replay",
      "authors": "Marcin Andrychowicz et al.",
      "year": 2017,
      "arxiv_id": "1707.01495",
      "role": "Foundation",
      "relationship_sentence": "HER established the goal-conditioned RL formulation and relabeling mechanism, which underpins learning from self-exploration episodes when pursuing video-specified visual goals."
    }
  ],
  "synthesis_narrative": "Learning from internet videos has been catalyzed by methods that infer actions from pixels, with VPT showing at scale that a vision-based inverse dynamics model can convert raw videos into training signals\u2014while incurring the costs and brittleness of embodiment-specific labeling. In parallel, goal-conditioned frameworks matured: HER introduced relabeling and the formalism of conditioning on goals, and RIG showed a powerful twist\u2014sampling imagined visual goals from a generative model to both drive exploration and supervise goal-reaching, laying a template for leveraging generative targets rather than explicit action labels. Skew-Fit refined this direction by biasing goal sampling toward undercovered states to widen exploration. On the control side, Visual Foresight established that predicting or specifying future images can guide visuomotor behavior by matching to desired visual states. More recently, Diffuser demonstrated that diffusion models can generate coherent action trajectories conditioned on goals, making trajectory-level control a practical mechanism for goal-directed behavior.\n\nTogether these works reveal a gap and an opportunity: inverse-dynamics labeling from videos is expensive and tied to embodiments, while goal-conditioned exploration and trajectory-level generators can learn to reach visual targets without explicit action labels. The present paper synthesizes these strands by using internet video models to produce plausible future visual goals and coupling them with trajectory-level action generation, enabling an agent to self-explore and directly ground video knowledge into continuous actions\u2014eliminating embodiment-specific inverse dynamics while leveraging generative goals to expand coverage.",
  "target_paper": {
    "title": "Grounding Video Models to Actions through Goal Conditioned Exploration",
    "authors": "Yunhao Luo, Yilun Du",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Embodied AI, Decision Making, Robotics, Video Model",
    "abstract": "Large video models, pretrained on massive quantities of amount of Internet video,  provide a rich source of physical knowledge about the dynamics and motions of objects and tasks.\nHowever, video models are not grounded in the embodiment of an agent, and do not describe how to actuate the world to reach the visual states depicted in a video.\nTo tackle this problem, current methods use a separate vision-based inverse dynamic model trained on embodiment-specific data to map image states to actions. \nGathering data to train such a model is often expensive and challenging, and this model is limited to visual settings similar to the ones in which data is available.\nIn this paper, we investigate how to directly  ground video models to continuous actions through self-exploration in the embodied environment -- using generated video states as visual goals for exploration.\nWe propose a framework that uses trajectory level action generation in combination with video guidance to\nenable an agent to ",
    "openreview_id": "G6dMvRuhFr",
    "forum_id": "G6dMvRuhFr"
  },
  "analysis_timestamp": "2026-01-06T19:37:22.141652"
}
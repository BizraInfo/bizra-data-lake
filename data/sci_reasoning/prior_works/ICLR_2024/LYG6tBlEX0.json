{
  "prior_works": [
    {
      "title": "MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control",
      "authors": "Yicheng Luo et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "MoCapAct provided the large-scale, task-diverse humanoid state\u2013action trajectories and benchmark tasks that H-GAP trains on and evaluates with MPC, defining the data and problem setting H-GAP directly builds upon."
    },
    {
      "title": "Planning with Diffusion for Control",
      "authors": "Michael Janner et al.",
      "year": 2022,
      "arxiv_id": "2205.09991",
      "role": "Extension",
      "relationship_sentence": "This work established the core idea of using a learned generative trajectory prior for planning, which H-GAP extends to high-DoF humanoids by replacing diffusion with a trajectory autoencoder and embedding it within MPC for task-conditioned control."
    },
    {
      "title": "Trajectory Transformer",
      "authors": "Michael Janner et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Trajectory Transformer showed that sequence models of state\u2013action trajectories can be used for planning in offline RL, inspiring H-GAP\u2019s generalist trajectory-model-as-planner paradigm for humanoid control."
    },
    {
      "title": "Learning Latent Plans from Play",
      "authors": "Corey Lynch et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "The latent-plan autoencoding framework for long-horizon behavior directly motivates H-GAP\u2019s trajectory autoencoder and latent-space search, enabling tractable planning for 56-DoF humanoids."
    },
    {
      "title": "AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control",
      "authors": "Xue Bin Peng et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "AMP demonstrated the power of motion priors from MoCap for humanoid control but required online RL and task-specific training, a limitation H-GAP addresses by using an offline-trained generative prior with MPC to adapt to new tasks without online interaction."
    },
    {
      "title": "DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills",
      "authors": "Xue Bin Peng et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "DeepMimic established MoCap-driven physics-based humanoid control via RL, laying the groundwork for using motion data as a prior that H-GAP repurposes through a generalist generative model and planning."
    },
    {
      "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
      "authors": "Lili Chen et al.",
      "year": 2021,
      "arxiv_id": "2106.01345",
      "role": "Baseline",
      "relationship_sentence": "As a primary sequence-modeling baseline for offline control on diverse tasks, Decision Transformer frames generalist control from datasets that H-GAP improves upon by performing model-based MPC with a trajectory prior rather than return-conditioned action prediction."
    }
  ],
  "synthesis_narrative": "MoCapAct assembled human motion capture into standardized humanoid state\u2013action trajectories across many tasks, establishing both the data modality and multi-task evaluation regime for simulated humanoid control. Planning with Diffusion for Control showed that a learned generative model over trajectories can serve as a powerful planner by sampling task-consistent rollouts, demonstrating how data-driven priors can replace explicit dynamics in planning. Trajectory Transformer further validated the notion that sequence models trained on offline trajectories can drive planning by searching in model space rather than optimizing raw actions. Learning Latent Plans from Play introduced autoencoding long-horizon behaviors into compact latent plans that can be sampled or optimized at test time, providing a way to make search tractable over complex behaviors. DeepMimic and AMP established MoCap as an effective motion prior for physics-based humanoids, with AMP in particular using an adversarial motion prior to enforce realism, but both relied on online RL and often per-task training.\nTogether, these works reveal a gap: motion priors from MoCap enable realism and robustness, and generative trajectory models enable data-driven planning, but high-DoF humanoids demand a tractable search space without online RL. H-GAP synthesizes these insights by training a trajectory autoencoder on MoCapAct to obtain a latent motion prior and performing MPC by optimizing over latent plans, thereby combining the realism of MoCap priors with the flexibility of generative-model planning for generalist humanoid control without online interaction.",
  "target_paper": {
    "title": "H-GAP: Humanoid Control with a Generalist Planner",
    "authors": "zhengyao jiang, Yingchen Xu, Nolan Wagener, Yicheng Luo, Michael Janner, Edward Grefenstette, Tim Rockt\u00e4schel, Yuandong Tian",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Generative Modelling, Humanoid Control, Model Predictive Control, Model-based Reinforcement Learning, Offline Reinforcement Learning",
    "abstract": "Humanoid control is an important research challenge offering avenues for integration into human-centric infrastructures and enabling physics-driven humanoid animations.\nThe daunting challenges in this field stem from the difficulty of optimizing in high-dimensional action spaces and the instability introduced by the bipedal morphology of humanoids. \nHowever, the extensive collection of human motion-captured data and the derived datasets of humanoid trajectories, such as MoCapAct, paves the way to tackle these challenges. In this context, we present Humanoid Generalist Autoencoding Planner (H-GAP), a state-action trajectory generative model trained on humanoid trajectories derived from human motion-captured data, capable of adeptly handling downstream control tasks with Model Predictive Control (MPC).\nFor 56 degrees of freedom humanoid, we empirically demonstrate that H-GAP learns to represent and generate a wide range of motor behaviors. Further, without any learning from online intera",
    "openreview_id": "LYG6tBlEX0",
    "forum_id": "LYG6tBlEX0"
  },
  "analysis_timestamp": "2026-01-06T08:44:49.004702"
}
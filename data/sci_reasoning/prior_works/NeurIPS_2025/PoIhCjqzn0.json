{
  "prior_works": [
    {
      "title": "Learning from Labeled and Unlabeled Data with Label Propagation",
      "authors": "Xiaojin Zhu, Zoubin Ghahramani",
      "year": 2002,
      "role": "Foundation for teacher-side label propagation",
      "relationship_sentence": "L2DGCN\u2019s teacher model builds on classical label propagation to diffuse labels across the graph, extending it to enable remote dissemination beyond local neighborhoods to counter degree-induced information sparsity."
    },
    {
      "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank (APPNP)",
      "authors": "Johannes Klicpera, Aleksandar Bojchevski, Stephan G\u00fcnnemann",
      "year": 2019,
      "role": "Long-range propagation mechanism",
      "relationship_sentence": "APPNP\u2019s decoupled transformation-and-propagation with PPR inspires L2DGCN\u2019s improved propagation in the teacher, allowing label information to reach distant nodes without oversmoothing low-degree neighborhoods."
    },
    {
      "title": "Combining Label Propagation and Simple Models outperforms Graph Neural Networks (Correct and Smooth)",
      "authors": "Johannes Klicpera, Stefan Wei\u00dfenberger, Stephan G\u00fcnnemann",
      "year": 2020,
      "role": "Label propagation on predictions/pseudolabel refinement",
      "relationship_sentence": "C&S demonstrates that post-hoc label propagation can correct and smooth predictions, motivating L2DGCN\u2019s teacher-driven propagation and its label selection strategy for reliable pseudo-label dissemination."
    },
    {
      "title": "DropEdge: Towards Deep Graph Convolutional Networks on Node Classification",
      "authors": "Yu Rong, Yatao Bian, Tingyang Xu, Junzhou Huang",
      "year": 2020,
      "role": "Edge perturbation as an effective training augmentation",
      "relationship_sentence": "L2DGCN generalizes the idea of edge perturbation from random DropEdge to a dynamically learnable edge enhancement that specifically facilitates information exchange for low-degree nodes."
    },
    {
      "title": "Learning Discrete Structures for Graph Neural Networks",
      "authors": "Luca Franceschi, Mathias Niepert, Massimiliano Pontil, Xiao He",
      "year": 2019,
      "role": "Learnable adjacency/graph structure optimization",
      "relationship_sentence": "The bilevel optimization of discrete graph structure informs L2DGCN\u2019s student-side learnable graph enhancement, enabling targeted edge modifications while preserving overall structure."
    },
    {
      "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications",
      "authors": "Uri Alon, Eran Yahav",
      "year": 2021,
      "role": "Theoretical basis for long-range information flow and rewiring",
      "relationship_sentence": "By formalizing over-squashing and the need for graph rewiring to enable distant information flow, this work motivates L2DGCN\u2019s remote label propagation and degree-bias-aware edge enhancement."
    },
    {
      "title": "Pseudo-Label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks",
      "authors": "Dong-Hyun Lee",
      "year": 2013,
      "role": "Confidence-based pseudo-label selection",
      "relationship_sentence": "L2DGCN\u2019s label selection module draws on pseudo-labeling principles to choose reliable labels from teacher predictions, reducing error amplification when expanding supervision to unlabeled, low-degree nodes."
    }
  ],
  "synthesis_narrative": "L2DGCN addresses degree bias by combining a teacher-driven, long-range label propagation mechanism with a student-side, learnable graph enhancement and principled label selection. The teacher component is grounded in classical label propagation, leveraging Zhu and Ghahramani\u2019s framework to diffuse label information beyond immediate neighborhoods. APPNP further informs the design by decoupling transformation and propagation and using Personalized PageRank to enable stable, long-range dissemination, while Correct-and-Smooth demonstrates that post-hoc label propagation on predictions can refine and stabilize pseudo-label signals. These propagation-centric insights directly support L2DGCN\u2019s goal of remote label dissemination to reach low-degree nodes.\nOn the structural side, DropEdge shows that perturbing edges can improve training and combat oversmoothing, but L2DGCN advances this by making perturbations learnable and targeted. Franceschi et al.\u2019s learnable adjacency via bilevel optimization provides the blueprint for optimizing edges to enhance information flow while preserving global structure. The theoretical motivation for both remote propagation and structural enhancement traces to Alon and Yahav\u2019s over-squashing analysis, which argues for rewiring to alleviate information bottlenecks that disproportionately harm low-degree nodes. Finally, L2DGCN\u2019s label selection draws on pseudo-labeling (Lee), using confidence-aware selection to prevent error propagation as unlabeled nodes receive supervision. Together, these works converge to enable L2DGCN\u2019s teacher-student architecture that mitigates degree bias through remote label dissemination, adaptive edge enhancement, and robust pseudo-label selection.",
  "analysis_timestamp": "2026-01-07T00:29:41.029166"
}
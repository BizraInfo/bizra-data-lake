{
  "prior_works": [
    {
      "title": "UCB-Advantage (reference-advantage decomposition for model-free Q-learning)",
      "authors": "Zhang et al.",
      "year": 2020,
      "role": "Algorithmic precursor introducing reference-advantage decomposition and variance-based bonuses for on-policy Q-learning",
      "relationship_sentence": "The present paper targets UCB-Advantage directly, supplying the missing gap-dependent regret guarantees by building a refined error decomposition tailored to its reference-advantage structure and variance estimators."
    },
    {
      "title": "Q-EarlySettled-Advantage (Q-ESA)",
      "authors": "Li et al.",
      "year": 2021,
      "role": "Algorithmic refinement of RAD-based Q-learning achieving near-optimal worst-case regret",
      "relationship_sentence": "This work extends the theoretical picture for Q-ESA by establishing the first gap-dependent regret bounds, leveraging a novel decomposition that captures how early-settled states interact with advantage-based variance reduction."
    },
    {
      "title": "Minimax Regret Bounds for Reinforcement Learning (UCBVI with Bernstein bonuses)",
      "authors": "Azar, Osband, Munos",
      "year": 2017,
      "role": "Foundational variance-aware exploration via empirical/Bernstein bonuses",
      "relationship_sentence": "The paper\u2019s variance-estimator bonuses and their analysis inherit from the Bernstein-style confidence paradigms introduced in UCBVI, and the new gap-dependent bounds adapt this variance-aware machinery to model-free RAD-based Q-learning."
    },
    {
      "title": "Q-learning with UCB Exploration is Sample Efficient for Tabular MDPs",
      "authors": "Jin et al.",
      "year": 2018,
      "role": "Model-free on-policy Q-learning with Hoeffding-type bonuses and worst-case guarantees",
      "relationship_sentence": "This baseline established the UCB-style Q-learning framework; the current paper builds on its analysis traditions while moving from Hoeffding-type to variance-aware bonuses and from worst-case to gap-dependent guarantees."
    },
    {
      "title": "Tighter Problem-Dependent Regret Bounds in Reinforcement Learning",
      "authors": "Zanette, Brunskill",
      "year": 2019,
      "role": "Problem-dependent (gap-based) analysis for episodic RL",
      "relationship_sentence": "Their gap-dependent viewpoint and techniques motivate seeking suboptimality-gap improvements; the current work adapts this problem-dependent lens to the RAD setting, crafting an error decomposition that isolates gap-driven terms under variance reduction."
    },
    {
      "title": "Near-Optimal Regret Bounds for Reinforcement Learning (UCRL2)",
      "authors": "Jaksch, Ortner, Auer",
      "year": 2010,
      "role": "Classical optimism-based analysis framework and problem-dependent motivation in MDPs",
      "relationship_sentence": "The optimism and confidence-set methodology from UCRL2 underpins modern regret proofs; this paper\u2019s decomposition-based proof inherits that lineage while targeting model-free Q-learning with variance-aware bonuses and explicit gap dependence."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014gap-dependent regret bounds for on-policy Q-learning algorithms that use variance-aware bonuses with reference-advantage decomposition (RAD)\u2014sits at the intersection of three strands of prior work. First, the algorithms under study, UCB-Advantage (Zhang et al., 2020) and Q-EarlySettled-Advantage (Li et al., 2021), introduced RAD to reduce variance in Q-learning and achieved near-optimal worst-case O(\u221aT) regret with variance-estimator (Bernstein-style) bonuses. Second, the variance-aware exploration paradigm originates in model-based UCBVI (Azar et al., 2017), whose empirical/Bernstein bonuses inspired analogous variance estimators in model-free Q-learning; the present paper must reconcile these bonuses with RAD\u2019s control of temporal uncertainty. Third, the motivation and tools for problem-dependent (gap-based) reinforcement learning analyses were advanced by works on gap-dependent regret in episodic MDPs (e.g., Zanette & Brunskill, 2019) and the broader optimism framework (Jaksch et al., 2010), while classical model-free UCB Q-learning with Hoeffding-type bonuses (Jin et al., 2018) provided precedents for gap-dependent improvements in simpler bonus schemes. The novelty here is an error decomposition that simultaneously tracks reference-value and advantage estimation errors under variance-aware bonuses, enabling the first gap-dependent bounds for RAD-based Q-learning. By integrating variance-sensitive confidence control (\u00e0 la UCBVI) with RAD\u2019s structure and a gap-focused decomposition, the paper bridges the gap between prior Hoeffding-based gap-dependent results and the more refined variance-based, model-free Q-learning algorithms.",
  "analysis_timestamp": "2026-01-06T23:42:48.091408"
}
{
  "prior_works": [
    {
      "title": "SVM Classifiers for Learning from Label Proportions",
      "authors": "Tobias R\u00fcping",
      "year": 2010,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work formalized learning from bags with known class proportions (label proportions), directly matching the setting of multiple unlabeled datasets with provided class priors that this paper adopts as its problem formulation."
    },
    {
      "title": "Learning with Noisy Labels",
      "authors": "Nagarajan Natarajan et al.",
      "year": 2013,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "It established risk-consistent learning under class-conditional noise via loss correction using a noise-transition matrix, providing the theoretical basis for the paper\u2019s transition-matrix view and risk-consistency objective."
    },
    {
      "title": "Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach",
      "authors": "Giorgio Patrini et al.",
      "year": 2017,
      "arxiv_id": "1609.03683",
      "role": "Gap Identification",
      "relationship_sentence": "Their forward/backward corrections with a label-noise transition matrix revealed that forward correction is classifier-consistent but not risk-consistent, a key limitation that the paper\u2019s CCM/RCM explicitly addresses."
    },
    {
      "title": "Learning from Complementary Labels",
      "authors": "Takashi Ishida et al.",
      "year": 2017,
      "arxiv_id": "1705.07541",
      "role": "Extension",
      "relationship_sentence": "By constructing an unbiased risk via a probabilistic transition matrix from complementary labels, this work provides the transition-matrix technique generalized here to multiple unlabeled datasets guided by class priors."
    },
    {
      "title": "Positive-Unlabeled Learning with Non-Negative Risk Estimator",
      "authors": "Ryotaro Kiryo et al.",
      "year": 2017,
      "arxiv_id": "1703.00593",
      "role": "Inspiration",
      "relationship_sentence": "nnPU introduced importance-weighted unbiased risk estimation from unlabeled data using known class priors, directly inspiring the paper\u2019s risk-consistent estimator and its stabilization via progressive purification."
    },
    {
      "title": "Covariate shift adaptation by importance weighted empirical risk minimization",
      "authors": "Masashi Sugiyama et al.",
      "year": 2007,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper supplies the core principle of importance-weighted ERM to match risks across distributions, which the paper leverages to reweight and progressively purify supervision among unlabeled datasets."
    },
    {
      "title": "Adjusting the Outputs of a Classifier to New a Priori Probabilities: A Simple Procedure",
      "authors": "Michel Saerens et al.",
      "year": 2002,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "It formalized prior-shift correction using known class priors (via EM), echoing the probability-transition calibration across datasets that underlies the paper\u2019s classifier-consistent transition-matrix formulation."
    }
  ],
  "synthesis_narrative": "Learning from label proportions established that accurate classifiers can be trained when only bag-level class proportions are known, crystallizing the scenario where multiple unlabeled sets come with class priors. Risk correction under class-conditional noise then showed that if one can relate observed supervision to true labels through a transition matrix, risk-consistent learning is possible via appropriate loss transformation. Subsequent loss-correction methods with forward and backward mappings operationalized this transition-matrix view for deep models, but also surfaced a crucial trade-off: forward correction tends to be classifier-consistent yet not risk-consistent, while backward correction is unbiased but often unstable. Complementary-label learning pushed the transition-matrix idea to another weak-supervision form, deriving unbiased risks directly from probabilistic relationships between observed weak labels and true classes. In parallel, positive\u2013unlabeled learning with a non-negative risk estimator demonstrated how class priors and unlabeled data enable importance-weighted, risk-consistent training in practice. Finally, importance-weighted ERM under distribution shift and classic prior-shift adjustment provided the general machinery for reweighting risks and calibrating predictions using known priors.\nTaken together, these works reveal both the opportunity and the gap: known class priors across unlabeled datasets define a transition structure that can drive learning, but achieving risk consistency and stable training requires principled importance weighting rather than purely classifier-consistent corrections. The current paper synthesizes these insights by formulating a transition-matrix-based objective for multi-class learning from multiple unlabeled datasets, then enforcing risk consistency through importance-weighted estimation while progressively purifying the supervision, a natural evolution of unbiased risk estimation and loss-correction ideas tailored to the multi-dataset prior setting.",
  "target_paper": {
    "title": "Consistent Multi-Class Classification from Multiple Unlabeled Datasets",
    "authors": "Zixi Wei, Senlin Shu, Yuzhou Cao, Hongxin Wei, Bo An, Lei Feng",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "mutli-class classification, multiple unlabeled datasets, learning consistency",
    "abstract": "Weakly supervised learning aims to construct effective predictive models from imperfectly labeled data. The recent trend of weakly supervised learning has focused on how to learn an accurate classifier from completely unlabeled data, given little supervised information such as class priors. In this paper, we consider a newly proposed weakly supervised learning problem called multi-class classification from multiple unlabeled datasets, where only multiple sets of unlabeled data and their class priors (i.e., the proportions of each class) are provided for training the classifier. To solve this problem, we first propose a classifier-consistent method (CCM) based on a probability transition matrix. However, CCM cannot guarantee risk consistency and lacks of purified supervision information during training. Therefore, we further propose a risk-consistent method (RCM) that progressively purifies supervision information during training by importance weighting. We provide comprehensive theoret",
    "openreview_id": "fW7DOHDQvF",
    "forum_id": "fW7DOHDQvF"
  },
  "analysis_timestamp": "2026-01-06T13:36:50.677929"
}
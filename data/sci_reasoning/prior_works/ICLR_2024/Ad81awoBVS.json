{
  "prior_works": [
    {
      "title": "Deep Anomaly Detection Using Geometric Transformations",
      "authors": "Izhak Golan and Ran El-Yaniv",
      "year": 2019,
      "arxiv_id": "1805.10917",
      "role": "Inspiration",
      "relationship_sentence": "This work established the discriminative OCC paradigm of classifying geometric transformations (notably rotations) as surrogate labels to create pseudo-anomalies, directly motivating the present paper\u2019s focus on why rotation-based transformation classification so strongly benefits one-class detection."
    },
    {
      "title": "Unsupervised Representation Learning by Predicting Image Rotations",
      "authors": "Spyros Gidaris et al.",
      "year": 2018,
      "arxiv_id": "1803.07728",
      "role": "Foundation",
      "relationship_sentence": "By introducing rotation prediction as a self-supervised pretext task and showing it induces semantic, orientation-aware features, this paper provides the core mechanism whose predictive accuracy the current work correlates with OCC performance."
    },
    {
      "title": "CSI: Novelty Detection via Contrastive Learning",
      "authors": "Jihoon Tack et al.",
      "year": 2020,
      "arxiv_id": "2007.08176",
      "role": "Baseline",
      "relationship_sentence": "CSI treats distribution-shifted augmentations such as rotations as shifted instances in a contrastive framework for novelty detection, serving as a primary baseline whose rotation-driven gains the present work analyzes and explains."
    },
    {
      "title": "Deep One-Class Classification",
      "authors": "Lukas Ruff et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This paper formalized deep one-class classification (Deep SVDD) and established the modern OCC evaluation protocol that the current study adopts to systematically assess how rotation-based augmentations affect one-class performance."
    },
    {
      "title": "Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty Estimates",
      "authors": "Dan Hendrycks et al.",
      "year": 2019,
      "arxiv_id": "1906.12340",
      "role": "Related Problem",
      "relationship_sentence": "By showing that an auxiliary rotation-prediction objective improves OOD/novelty detection, this work provided the empirical clue that rotation self-supervision enhances anomaly sensitivity, which the current paper quantifies via a strong linear correlation with OCC."
    }
  ],
  "synthesis_narrative": "Classifying geometric transformations as surrogate labels for anomaly detection was crystallized by the transformation-classification approach of Golan and El-Yaniv, who demonstrated that rotations and related transforms can serve as pseudo-anomaly generators that sharply separate in-distribution from out-of-distribution samples. Gidaris, Singh, and Komodakis introduced rotation prediction as a self-supervised pretext that reliably elicits orientation-aware, semantic features, implying that success on rotation classification reflects meaningful representation learning. Tack and colleagues\u2019 CSI further operationalized distribution shifts like rotations within a contrastive novelty-detection framework, revealing that \u2018shifted instances\u2019 can be systematically leveraged as negatives for stronger OCC. Meanwhile, Ruff et al. defined deep one-class classification through Deep SVDD and standardized evaluation settings for OCC, providing the base formulation on which augmentation effects could be rigorously measured. Hendrycks et al. showed that adding a rotation-prediction objective improves robustness and novelty detection, hinting that rotation learning carries anomaly-relevant signals.\nTogether, these works established rotation as a powerful, distribution-shifting signal within self-supervised OCC but left the mechanism largely unaccounted for. Building on the pretext-task semantics of rotation prediction, the pseudo-anomaly framing of transformation classification, and standardized OCC protocols, the current paper isolates rotation\u2019s specific contribution and shows that rotation-prediction accuracy tightly tracks OCC performance. By probing this linkage across strong baselines like CSI and classical one-class setups (Deep SVDD), it clarifies why rotation \u2018works,\u2019 turning a widely used but poorly understood augmentation into a measurable predictor of one-class effectiveness.",
  "target_paper": {
    "title": "Rotation Has Two Sides: Evaluating Data Augmentation for Deep One-class Classification",
    "authors": "Guodong Wang, Yunhong Wang, Xiuguo Bao, Di Huang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "self-supervised learning, deep one-class cilassification",
    "abstract": "One-class classification (OCC) involves predicting whether a new data is normal or anomalous based solely on the data from a single class during training. Various attempts have been made to learn suitable representations for OCC within a self-supervised framework. Notably, discriminative methods that use geometric visual transformations, such as rotation, to generate pseudo-anomaly samples have exhibited impressive detection performance. Although rotation is commonly viewed as a distribution-shifting transformation and is widely used in the literature, the cause of its effectiveness remains a mystery. In this study, we are the first to make a surprising observation: there exists a strong linear relationship (Pearson's Correlation, $r > 0.9$) between the accuracy of rotation prediction and the performance of OCC. This suggests that a classifier that effectively distinguishes different rotations is more likely to excel in OCC, and vice versa. The root cause of this phenomenon can be attr",
    "openreview_id": "Ad81awoBVS",
    "forum_id": "Ad81awoBVS"
  },
  "analysis_timestamp": "2026-01-06T06:27:36.769241"
}
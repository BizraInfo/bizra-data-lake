{
  "prior_works": [
    {
      "title": "Learning from Partial Labels",
      "authors": "Cour et al.",
      "year": 2011,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized the partial label learning setting and ambiguity degree that LS-PLL adopts to derive expected-risk bounds and analyze how candidate-set ambiguity interacts with training."
    },
    {
      "title": "Rethinking the Inception Architecture for Computer Vision",
      "authors": "Szegedy et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "Introduced label smoothing as a simple distributional regularizer; LS-PLL directly adapts this mechanism to the partial-label target distribution and centers its theory on the LS mixing rate."
    },
    {
      "title": "When Does Label Smoothing Help?",
      "authors": "M\u00fcller et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Provided mechanisms and evidence for how label smoothing improves generalization/calibration; LS-PLL extends this line by proving risk bounds and deriving an optimal smoothing rate specifically under partial-label ambiguity."
    },
    {
      "title": "Does Label Smoothing Mitigate Label Noise?",
      "authors": "Lukasik et al.",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "Analyzed LS under label noise and identified conditions where it improves robustness; LS-PLL generalizes these insights to ambiguous candidate-label supervision and uses them to motivate smoothing against false-positive labels in PLL."
    },
    {
      "title": "PRODEN: Progressive Identification of True Labels for Partial-Label Learning",
      "authors": "Lv et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "A leading deep PLL method that progressively sharpens pseudo-labels but is vulnerable to propagating false positives; LS-PLL targets this weakness by replacing overconfident targets with smoothed ones and serves as a primary baseline for improvement."
    },
    {
      "title": "PiCO: Contrastive Label Disambiguation for Partial Label Learning",
      "authors": "Wang et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "Highlighted that deep PLL can overfit false candidate labels and proposed contrastive remedies; LS-PLL addresses this persistent gap via principled label-space smoothing and provides the missing theory linking ambiguity degree to risk."
    }
  ],
  "synthesis_narrative": "This work sits at the intersection of partial label learning (PLL) and label smoothing (LS). The core problem formulation and the notion of ambiguity degree trace directly to Cour et al., whose setup underpins LS-PLL\u2019s risk analysis. On the modeling side, Szegedy et al. introduced label smoothing as a simple yet powerful distributional regularizer; this paper adopts LS as the central tool but transposes it to the PLL setting where targets are candidate sets rather than one-hot labels. Prior analyses by M\u00fcller et al. clarified why LS can improve generalization and calibration, while Lukasik et al. examined when LS mitigates label noise\u2014both lines of evidence directly inspire LS-PLL\u2019s theoretical contribution: proving bounds for expected risk under partial-label ambiguity and deriving the optimal smoothing rate that specifies when LS helps deep PLL.\n\nEmpirically, the paper is motivated by limitations in state-of-the-art deep PLL methods that tend to overfit false-positive labels as training progresses. PRODEN, which progressively sharpens pseudo-labels, provides a strong baseline but is susceptible to error reinforcement; LS-PLL explicitly tempers this overconfidence via smoothing. PiCO exposed similar fragility and sought relief in contrastive representation learning; LS-PLL complements this with a label-space regularization principle and, crucially, supplies formal guarantees linking ambiguity degree, noise, and performance. Together, these prior works furnished the problem framing, the regularization mechanism, the noise-robustness intuition, and the baseline behaviors that LS-PLL theoretically explains and practically improves.",
  "analysis_timestamp": "2026-01-06T23:09:26.508969"
}
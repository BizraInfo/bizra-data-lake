{
  "prior_works": [
    {
      "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?",
      "authors": "Jimenez et al.",
      "year": 2023,
      "role": "dataset/benchmark",
      "relationship_sentence": "Established an end-to-end, repository-level software engineering benchmark with dockerized environments and test-based verification that SWE-Lancer generalizes from bug fixing to broader freelance tasks and reuses in spirit via a unified Docker evaluation harness."
    },
    {
      "title": "SWE-bench (Verified/Lite) extensions",
      "authors": "SWE-bench team",
      "year": 2024,
      "role": "evaluation protocol/verification",
      "relationship_sentence": "Strengthened correctness verification and curated public evaluation splits\u2014directly informing SWE-Lancer\u2019s triple-verified tests and its emphasis on a transparent, public split."
    },
    {
      "title": "Evaluating Large Language Models Trained on Code (HumanEval)",
      "authors": "Chen et al.",
      "year": 2021,
      "role": "benchmark design",
      "relationship_sentence": "Popularized unit-test\u2013based automatic grading for code generation, a core evaluation mechanism that SWE-Lancer scales to end-to-end freelance engineering tasks."
    },
    {
      "title": "APPS: A Benchmark for Code Generation and Problem Solving",
      "authors": "Hendrycks et al.",
      "year": 2021,
      "role": "dataset/benchmark",
      "relationship_sentence": "Demonstrated large-scale, diverse programming problems with execution-based scoring, motivating SWE-Lancer\u2019s broad task coverage and rigorous, test-driven assessment."
    },
    {
      "title": "Defects4J: A Database of Real-World Java Bugs",
      "authors": "Just, Jalali, and Ernst",
      "year": 2014,
      "role": "dataset/benchmark",
      "relationship_sentence": "Pioneered real-world bug benchmarks with regression-test\u2013based correctness, a paradigm SWE-Lancer adopts for authentic, end-to-end verification of code changes."
    },
    {
      "title": "EvalPlus: Better Testing for Code Generation Evaluation",
      "authors": "Liu et al.",
      "year": 2023,
      "role": "evaluation methodology",
      "relationship_sentence": "Showed that stronger and augmented hidden tests improve the reliability of automatic code evaluation, directly motivating SWE-Lancer\u2019s triple verification by experienced engineers."
    }
  ],
  "synthesis_narrative": "SWE-Lancer\u2019s core contribution\u2014an end-to-end benchmark of real freelance software engineering tasks with rigorous, test-driven grading and market-aligned outcomes\u2014builds directly on the evolution of code evaluation benchmarks. HumanEval established the viability of execution-based, unit-test grading for code generation, which APPS expanded to a large, diverse set of problems, demonstrating the scalability of automated assessment. Defects4J laid the groundwork for evaluating real-world software changes via regression tests, anchoring correctness in practical project contexts rather than synthetic toy problems.\n\nSWE-bench then bridged to repository-scale, real-world engineering by operationalizing dockerized environments, issue-level tasks, and patch validation. Its extensions (e.g., Verified/Lite) emphasized robust correctness criteria and reproducible public splits. These works collectively shaped SWE-Lancer\u2019s decision to provide a unified Docker image, a public evaluation split, and strict, execution-based correctness checks. In parallel, EvalPlus highlighted that na\u00efve test suites can overstate model capability; its methodology for strengthening tests directly informs SWE-Lancer\u2019s triple-verification process by experienced engineers to ensure reliability.\n\nSWE-Lancer extends beyond prior art along two axes: task realism and outcome alignment. It moves from repository bug fixes to a broader spectrum of freelance tasks\u2014including implementation and managerial decision-making\u2014and uniquely maps model performance to monetary value using real Upwork payouts, creating a market-grounded lens on the economic impact of frontier LLMs.",
  "analysis_timestamp": "2026-01-07T00:05:12.563745"
}
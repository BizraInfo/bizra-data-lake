{
  "prior_works": [
    {
      "title": "Neural networks and principal component analysis: Learning from examples without local minima",
      "authors": "Paolo Baldi and Kurt Hornik",
      "year": 1989,
      "role": "Baseline",
      "relationship_sentence": "This classic work characterizes optimal linear autoencoders as PCA solutions; the present paper explicitly generalizes that linear baseline to non-linear two-layer autoencoders in the proportional regime and proves analogous optimality/achievability results."
    },
    {
      "title": "Nonlinear principal component analysis using autoassociative neural networks",
      "authors": "Michelle A. Kramer",
      "year": 1991,
      "role": "Foundation",
      "relationship_sentence": "Kramer introduced non-linear autoencoders (nonlinear PCA) as a formal problem setting; the current paper analyzes exactly this model class and supplies the first sharp population-risk characterization and gradient-method achievability in the challenging proportional scaling."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": "Andrew M. Saxe et al.",
      "year": 2013,
      "role": "Extension",
      "relationship_sentence": "Saxe et al. showed that gradient descent converges to PCA in (deep) linear autoencoders by analyzing learning dynamics; this paper extends the gradient-optimality story to non-linear two-layer autoencoders, proving that gradient methods attain the population minimizers identified here."
    },
    {
      "title": "From Principal Subspaces to Principal Components with Linear Autoencoders",
      "authors": "Eli Plaut",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Plaut sharpened the linear autoencoder\u2013PCA equivalence and training behavior, making clear that rigorous understanding existed mainly for the linear case; the present work addresses the explicit gap by moving to non-linear activations and proportional compression rates."
    },
    {
      "title": "Coding theorems for a discrete source with a fidelity criterion",
      "authors": "Claude E. Shannon",
      "year": 1959,
      "role": "Foundation",
      "relationship_sentence": "Shannon\u2019s rate\u2013distortion theory establishes the fundamental limits for lossy compression of Gaussian sources; in the special case of sign activation, this paper\u2019s analysis meets those limits, directly tying its autoencoder optimality to the classical R\u2013D bound."
    },
    {
      "title": "Optimal Shrinkage of Singular Values",
      "authors": "Matan Gavish and David L. Donoho",
      "year": 2017,
      "role": "Inspiration",
      "relationship_sentence": "The shrinkage perspective on optimal low-rank reconstruction in proportional regimes informs the structure of the minimizers found here; this paper generalizes the shrinkage principle from linear denoising to learned non-linear encoder\u2013decoder mappings."
    }
  ],
  "synthesis_narrative": "The intellectual lineage of this work begins with the linear autoencoder\u2013PCA equivalence established by Baldi and Hornik, which provided a complete characterization of optimal linear encoders/decoders under reconstruction loss. Kramer then formalized non-linear autoencoders (nonlinear PCA) as a vehicle for dimensionality reduction, defining the exact model class the present paper studies but leaving its population-risk landscape and training guarantees unresolved. Subsequent analyses of training dynamics in linear networks\u2014most notably Saxe et al.\u2014showed that gradient descent provably recovers PCA in deep/linear autoencoders, crystallizing a template for proving that gradient methods can achieve globally optimal representations in the linear setting. Plaut further clarified the precise conditions under which linear autoencoders recover principal components, highlighting that rigorous theory was largely confined to linear architectures and did not address modern high-dimensional proportional scaling.\n\nTwo additional pillars directly shape the paper\u2019s core contributions. First, Shannon\u2019s rate\u2013distortion theory provides the fundamental limits for compressing Gaussian sources; the paper\u2019s sign-activation case achieves these limits, explicitly connecting non-linear two-layer autoencoders to classical R\u2013D optimality. Second, the optimal singular-value shrinkage viewpoint of Gavish and Donoho informs the structure of the population minimizers uncovered here; the authors\u2019 characterization can be viewed as a learned, non-linear generalization of shrinkage operating in the proportional regime. Together, these works lead directly to the paper\u2019s main advances: a precise description of optimal non-linear two-layer autoencoders in proportional dimensions and a proof that gradient methods attain these fundamental limits.",
  "analysis_timestamp": "2026-01-06T23:09:26.584902"
}
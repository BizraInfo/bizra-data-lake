{
  "prior_works": [
    {
      "title": "Reasoning about Generalization via Conditional Mutual Information",
      "authors": "Thomas Steinke et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "This paper introduces the conditional mutual information (CMI) framework that the ICML 2024 work adopts to quantify memorization; the lower bounds and formalization of \u201cinformation revealed about the data\u201d are stated exactly in terms of Steinke\u2013Zakynthinou\u2019s CMI."
    },
    {
      "title": "Information-Theoretic Generalization Bounds for Learning Algorithms via Mutual Information",
      "authors": "Aolin Xu et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Xu and Raginsky initiated the mutual information viewpoint on generalization that directly precedes and motivates CMI; the present paper\u2019s information-complexity lens builds on this lineage by proving necessary (lower bound) information requirements."
    },
    {
      "title": "Stochastic Convex Optimization",
      "authors": "Shai Shalev-Shwartz et al.",
      "year": 2009,
      "role": "Foundation",
      "relationship_sentence": "This work formalized the SCO setting (Lipschitz losses, strong convexity, excess risk) that the ICML 2024 paper uses as its problem template for deriving tight CMI\u2013accuracy tradeoffs."
    },
    {
      "title": "Open Problem: Information Complexity of Stochastic Convex Optimization",
      "authors": "Roi Livni",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "Livni explicitly asked whether achieving excess error \u03b5 in SCO necessitates large CMI; the ICML 2024 paper resolves this open question with tight \u03a9(1/\u03b52) and \u03a9(1/\u03b5) lower bounds."
    },
    {
      "title": "Fingerprinting Codes and the Price of Approximate Differential Privacy",
      "authors": "Mark Bun et al.",
      "year": 2016,
      "role": "Inspiration",
      "relationship_sentence": "Techniques from fingerprinting/tracing used to link utility to the ability to identify participants inspire the paper\u2019s adversarial tracing construction that recovers a significant fraction of training samples in SCO."
    },
    {
      "title": "Interactive Fingerprinting Codes and the Hardness of Releasing Information about Individuals",
      "authors": "Thomas Steinke et al.",
      "year": 2015,
      "role": "Related Problem",
      "relationship_sentence": "This foundational tracing framework underlies the idea that informative outputs enable identification of contributors; the ICML 2024 paper adapts this logic to SCO to exhibit concrete tracing attacks."
    },
    {
      "title": "Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds",
      "authors": "Raef Bassily et al.",
      "year": 2014,
      "role": "Related Problem",
      "relationship_sentence": "Privacy\u2013utility tradeoffs for (strongly) convex ERM provide a closely related template where reduced information leakage (via DP) imposes accuracy costs; the ICML 2024 work translates this intuition into CMI\u2013accuracy lower bounds for SCO."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014tight lower bounds linking the information a learner reveals about its data (measured via CMI) to the accuracy achievable in stochastic convex optimization\u2014sits squarely on the information-theoretic generalization lineage and the canonical SCO formulation. Xu and Raginsky\u2019s mutual-information perspective framed generalization through information usage, which Steinke and Zakynthinou sharpened by introducing conditional mutual information (CMI), an operational measure the present paper adopts as its definition of memorization. The SCO environment and performance criteria (L2-Lipschitz losses, strong convexity, excess risk) trace directly to Shalev-Shwartz, Srebro, and Sridharan\u2019s formulation, providing the exact setting in which the new lower bounds are proved.\nAnswering Livni\u2019s 2023 open problem, the paper establishes that achieving excess error \u03b5 necessarily entails CMI at least \u03a9(1/\u03b52) (Lipschitz) and \u03a9(1/\u03b5) (strongly convex). The proof strategy and interpretive thrust are informed by privacy-utility paradigms: Bassily, Smith, and Thakurta\u2019s tight error bounds for private ERM demonstrate how constraining information leakage imposes accuracy costs in convex learning. To underscore the essential role of memorization, the paper designs tracing attacks that identify many training points\u2014an idea inspired by fingerprinting/tracing techniques developed by Bun, Nissim, Stemmer, Vadhan, and by Steinke and Ullman\u2014thereby operationalizing how informative outputs entail identifiable data dependence. Together, these works directly shaped the paper\u2019s problem statement, measurement of memorization, target lower bounds, and the tracing adversary that illustrates the necessity of information usage in SCO.",
  "analysis_timestamp": "2026-01-06T23:09:26.480181"
}
{
  "prior_works": [
    {
      "title": "Hyena Hierarchy: Efficient Multiscale Architectures for Long-Range Sequence Modeling",
      "authors": "Michael Poli, Stefano Massaroli, Eric Nguyen, Armin W. Thomas, Yoshua Bengio, Stefano Ermon, Christopher R\u00e9",
      "year": 2023,
      "role": "Primary methodological backbone",
      "relationship_sentence": "HyenaDNA directly adopts the Hyena implicit long-convolution operator to replace attention, enabling much longer contexts with competitive quality\u2014this is the core mechanism that makes single-nucleotide, long-range genomic modeling feasible."
    },
    {
      "title": "Efficiently Modeling Long Sequences with Structured State Spaces",
      "authors": "Albert Gu, Karan Goel, Christopher R\u00e9",
      "year": 2021,
      "role": "Theoretical/methodological precursor",
      "relationship_sentence": "Hyena builds on S4-style structured state-space ideas for long-range dependencies with subquadratic complexity; HyenaDNA leverages this lineage to scale genomic models beyond attention limits."
    },
    {
      "title": "Effective gene expression prediction from sequence by integrating long-range interactions (Enformer)",
      "authors": "\u017diga Avsec et al.",
      "year": 2021,
      "role": "Domain SOTA showing value of long-range genomic context",
      "relationship_sentence": "Enformer established the importance of 100kb-scale sequence context for regulatory genomics and highlighted attention\u2019s scaling bottleneck, motivating HyenaDNA\u2019s shift to Hyena operators for even longer contexts at base resolution."
    },
    {
      "title": "Basenji: Predicting regulatory activity across chromosomes with convolutional neural networks",
      "authors": "David R. Kelley et al.",
      "year": 2018,
      "role": "Early long-range genomics with dilated CNNs",
      "relationship_sentence": "Basenji demonstrated that distal regulatory elements over tens to hundreds of kilobases are crucial and can be captured with convolutional architectures, informing HyenaDNA\u2019s emphasis on long-range dependencies without sacrificing resolution."
    },
    {
      "title": "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome",
      "authors": "Ji et al.",
      "year": 2021,
      "role": "Genomic foundation model baseline with k-mer tokenization",
      "relationship_sentence": "DNABERT showed the promise of self-supervised pretraining for DNA but relied on k-mers and short contexts; HyenaDNA explicitly addresses these limitations by modeling single nucleotides over far longer sequences."
    },
    {
      "title": "Rethinking Attention with Performers",
      "authors": "Krzysztof Choromanski et al.",
      "year": 2021,
      "role": "Efficient attention baseline for long contexts",
      "relationship_sentence": "Performer provided a scalable attention approximation used in Enformer; HyenaDNA positions Hyena\u2019s implicit convolutions as a more scalable alternative for genomics, enabling longer context at lower cost."
    },
    {
      "title": "ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models",
      "authors": "Linting Xue et al.",
      "year": 2022,
      "role": "Tokenizer-free sequence modeling precedent",
      "relationship_sentence": "ByT5 validated that character/byte-level modeling can rival subword tokenization; HyenaDNA analogously commits to single-nucleotide (character-level) modeling to preserve SNP-level resolution."
    }
  ],
  "synthesis_narrative": "HyenaDNA\u2019s core contribution\u2014single-nucleotide, long-range genomic sequence modeling\u2014sits at the intersection of two lines of progress: (1) efficient long-context sequence architectures and (2) domain evidence that distal genomic context matters while fine-grained base resolution must be preserved. On the architectural side, Hyena Hierarchy introduced implicit long convolutions that match attention quality while scaling to far longer contexts; this capability is grounded in the structured state-space lineage exemplified by S4, which formalized subquadratic long-range dependency modeling. Performer provided a practical benchmark for scalable attention, widely adopted in genomics, and sets the efficiency/quality bar HyenaDNA seeks to surpass with Hyena operators.\n\nOn the genomics side, Basenji first underscored that regulatory signals span tens to hundreds of kilobases and can be captured with convolutional mechanisms. Enformer cemented this by showing substantial gains from ~200 kb receptive fields, but also exposed attention\u2019s quadratic scaling limits in practice. In parallel, DNABERT showed the utility of large-scale pretraining on genomes, yet its reliance on k-mer tokenization and short contexts sacrifices single-nucleotide fidelity and distal interactions. Insights from ByT5 in NLP strengthened the case for token-free, character-level modeling, directly mirroring HyenaDNA\u2019s choice to operate at nucleotide resolution. Together, these works shaped HyenaDNA\u2019s design: replace attention with Hyena implicit convolutions to unlock longer contexts, and model raw nucleotides to preserve SNP-level signal\u2014advancing both scale and resolution in genomic sequence modeling.",
  "analysis_timestamp": "2026-01-06T23:42:49.070836"
}
{
  "prior_works": [
    {
      "title": "Object-Centric Learning with Slot Attention",
      "authors": "Francesco Locatello et al.",
      "year": 2020,
      "arxiv_id": "2006.15055",
      "role": "Baseline",
      "relationship_sentence": "Introduces the slot-based object-centric autoencoder with per-slot decoding and masking that serves as the canonical architecture whose structural decoder assumptions are formalized and theoretically analyzed for compositional generalization."
    },
    {
      "title": "MONet: Unsupervised Scene Decomposition and Representation",
      "authors": "Christopher P. Burgess et al.",
      "year": 2019,
      "arxiv_id": "1901.11390",
      "role": "Foundation",
      "relationship_sentence": "Establishes the spatial mixture-of-objects decoder (per-object reconstructions combined via masks) that matches the structural decoder form assumed in the identifiability and compositionality guarantees."
    },
    {
      "title": "Multi-Object Representation Learning with Iterative Variational Inference (IODINE)",
      "authors": "Klaus Greff et al.",
      "year": 2019,
      "arxiv_id": "1903.00450",
      "role": "Foundation",
      "relationship_sentence": "Provides a latent-variable generative model with multiple object slots and a per-slot decoder-masking composition, supplying the precise object-centric encoder\u2013decoder decomposition that the theory requires."
    },
    {
      "title": "Variational Autoencoders and Nonlinear ICA: A Unifying Framework",
      "authors": "Ilyes Khemakhem et al.",
      "year": 2020,
      "arxiv_id": "1907.04809",
      "role": "Inspiration",
      "relationship_sentence": "Shows that identifiability can be achieved in VAEs under explicit auxiliary conditions, motivating an identifiability-based lens and the need for explicit structural and consistency assumptions that are adapted here to the object-centric, compositional setting."
    },
    {
      "title": "Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations",
      "authors": "Francesco Locatello et al.",
      "year": 2019,
      "arxiv_id": "1811.12359",
      "role": "Gap Identification",
      "relationship_sentence": "Proves impossibility of unsupervised disentanglement without inductive biases, directly motivating the structural decoder and encoder\u2013decoder consistency biases used to obtain identifiability and compositional generalization."
    },
    {
      "title": "On the Transfer of Disentangled Representations in Realistic Settings",
      "authors": "Andrea Dittadi et al.",
      "year": 2021,
      "arxiv_id": "2010.14407",
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrates that learned disentangled/object-centric representations often fail under OOD shifts, highlighting the lack of compositional generalization that the present theory addresses with provable guarantees."
    },
    {
      "title": "Recurrent Independent Mechanisms",
      "authors": "Anirudh Goyal et al.",
      "year": 2021,
      "arxiv_id": "1909.10893",
      "role": "Related Problem",
      "relationship_sentence": "Argues that modular, independent components facilitate combinatorial generalization, providing the key intuition that per-object modular decoders can yield compositional generalization when appropriately constrained."
    }
  ],
  "synthesis_narrative": "Slot-based object-centric models established an architectural bias in which scenes are decomposed into per-object latents, each decoded separately and combined via masks. MONet introduced the spatial mixture-of-objects decoder, where object-wise reconstructions are alpha-composited by masks, and IODINE cast this as an explicit multi-slot generative model with amortized iterative inference, making the encoder\u2013decoder decomposition operational. Slot Attention then became the canonical autoencoding instantiation, using competitive attention to allocate slots and a per-slot decoder, effectively enforcing the mask-based additive structure during reconstruction. In parallel, identifiability theory for latent-variable models showed that guarantees require explicit assumptions: Identifiable VAEs demonstrated that structural or auxiliary conditions can render latent factors recoverable. Complementing this, impossibility results for unsupervised disentanglement proved that without inductive biases such recovery is not achievable, and empirical analyses of transfer exposed that even seemingly disentangled representations often fail under OOD shifts. Finally, the modularity perspective of Recurrent Independent Mechanisms argued that independent components facilitate systematic recombination, suggesting that object-wise modular decoders could support combinatorial generalization.\nTogether, these works reveal both the right inductive bias\u2014object-wise, mask-composed decoders\u2014and the missing ingredient: conditions that make the learned factors identifiable and robustly recomposable. The present work synthesizes these insights by formalizing the slot-style decoder as a structural assumption and adding an encoder\u2013decoder consistency constraint, then proving that under these conditions the learned object-centric codes are identifiable and provably generalize to novel compositions.",
  "target_paper": {
    "title": "Provable Compositional Generalization for Object-Centric Learning",
    "authors": "Thadd\u00e4us Wiedemer, Jack Brady, Alexander Panfilov, Attila Juhos, Matthias Bethge, Wieland Brendel",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "compositional generalization, identifiability, object-centric learning, generalization, OOD generalization, unsupervised learning, slot attention, disentanglement, autoencoders, representation learning",
    "abstract": "Learning representations that generalize to novel compositions of known concepts is crucial for bridging the gap between human and machine perception. One prominent effort is learning object-centric representations, which are widely conjectured to enable compositional generalization. Yet, it remains unclear when this conjecture will be true, as a principled theoretical or empirical understanding of compositional generalization is lacking. In this work, we investigate when compositional generalization is guaranteed for object-centric representations through the lens of identifiability theory. We show that autoencoders that satisfy structural assumptions on the decoder and enforce encoder-decoder consistency will learn object-centric representations that provably generalize compositionally. We validate our theoretical result and highlight the practical relevance of our assumptions through experiments on synthetic image data.",
    "openreview_id": "7VPTUWkiDQ",
    "forum_id": "7VPTUWkiDQ"
  },
  "analysis_timestamp": "2026-01-06T07:02:42.246593"
}
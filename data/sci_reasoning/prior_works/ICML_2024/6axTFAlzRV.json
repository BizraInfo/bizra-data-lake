{
  "prior_works": [
    {
      "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization",
      "authors": "Pierre Foret et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "FedLESAM directly modifies SAM\u2019s perturb-then-descend procedure by replacing the locally computed perturbation direction with a locally estimated global direction, preserving SAM\u2019s min\u2013max objective while altering how the perturbation vector is obtained."
    },
    {
      "title": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
      "authors": "H. Brendan McMahan et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "FedLESAM is built on the standard FedAvg federated learning protocol (multi-step local updates followed by global averaging), and its global-perturbation estimate is computed from consecutive global models produced by FedAvg-style aggregation."
    },
    {
      "title": "Adaptive Federated Optimization",
      "authors": "Sashank J. Reddi et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "FedLESAM\u2019s key idea\u2014using differences between consecutive global models as a proxy for a global descent signal\u2014echoes FedAdam\u2019s use of aggregated model deltas to approximate global gradients, motivating FedLESAM\u2019s use of global-model differences to steer perturbations."
    },
    {
      "title": "SCAFFOLD: Stochastic Controlled Averaging for Federated Learning",
      "authors": "Sai Praneeth Karimireddy et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "SCAFFOLD\u2019s control variates explicitly correct client drift by injecting an estimate of the global-gradient signal into local updates; FedLESAM applies the same principle to the SAM perturbation step by aligning perturbations with an estimated global direction."
    },
    {
      "title": "Federated Optimization in Heterogeneous Networks (FedProx)",
      "authors": "Tian Li et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "FedProx formalized the objective inconsistency caused by client heterogeneity, a core limitation that underlies why locally computed SAM perturbations can misalign with the global landscape\u2014precisely the mismatch FedLESAM addresses."
    },
    {
      "title": "Averaging Weights Leads to Wider Optima in Deep Learning",
      "authors": "Pavel Izmailov et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "This work established the generalization benefits of flatter minima, providing the flatness motivation that SAM operationalizes and that FedLESAM seeks to target specifically at the global (not merely local) loss landscape in FL."
    }
  ],
  "synthesis_narrative": "FedLESAM sits at the intersection of federated optimization and sharpness-aware training. The conceptual motivation for pursuing flat minima comes from the flatness\u2013generalization link demonstrated by Averaging Weights Leads to Wider Optima, which SAM later operationalized via perturb-then-descend updates. However, standard federated learning is conducted under the FedAvg paradigm, whose multi-step local updates and aggregation create a global model sequence; under heterogeneity, this induces objective inconsistency, a gap formalized by FedProx. That gap explains why naively applying SAM locally can be misaligned: local sharpness need not reflect the global landscape relevant to the aggregated model. Prior algorithms such as SCAFFOLD showed that injecting an estimate of the global gradient into local updates can correct drift, highlighting the value of aligning local computation with global signals. Adaptive Federated Optimization (FedAdam/FedYogi) provided a concrete mechanism: treat consecutive global model differences as a proxy for the global gradient. FedLESAM fuses these threads by modifying SAM\u2019s perturbation step\u2014replacing the locally computed direction with a client-side estimate of the global perturbation direction computed from consecutive global models. This directly targets global flatness, mitigating heterogeneity-induced misalignment, and, by fixing the perturbation direction, reduces SAM\u2019s two-backprop overhead to a single backprop per iteration.",
  "analysis_timestamp": "2026-01-06T23:09:26.486363"
}
{
  "prior_works": [
    {
      "title": "What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?",
      "authors": "Alex Kendall et al.",
      "year": 2017,
      "arxiv_id": "1703.04977",
      "role": "Foundation",
      "relationship_sentence": "This paper formalized the aleatoric vs. epistemic uncertainty distinction that ValUES operationalizes by creating controlled settings to test their practical separability in semantic segmentation."
    },
    {
      "title": "Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty under Dataset Shift",
      "authors": "Yarin Ovadia et al.",
      "year": 2019,
      "arxiv_id": "1906.02530",
      "role": "Extension",
      "relationship_sentence": "Its protocol for assessing uncertainty under distribution shift is directly generalized by ValUES to dense prediction with controlled, segmentation-specific shifts and ambiguity factors."
    },
    {
      "title": "Uncertainty Baselines: Benchmarks for Uncertainty & Robustness",
      "authors": "Andrew Nado et al.",
      "year": 2021,
      "arxiv_id": "2106.04015",
      "role": "Inspiration",
      "relationship_sentence": "The idea of standardized, comparable baselines and systematic ablations in uncertainty estimation inspired ValUES to provide segmentation-focused ablations of method components to reveal what truly matters."
    },
    {
      "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles",
      "authors": "Balaji Lakshminarayanan et al.",
      "year": 2017,
      "arxiv_id": "1612.01474",
      "role": "Baseline",
      "relationship_sentence": "Deep ensembles serve as a primary baseline whose components (e.g., ensemble size and diversity) are systematically ablated within ValUES to quantify their real impact on segmentation uncertainty."
    },
    {
      "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
      "authors": "Yarin Gal et al.",
      "year": 2016,
      "arxiv_id": "1506.02142",
      "role": "Baseline",
      "relationship_sentence": "Monte Carlo Dropout provides the canonical model-uncertainty baseline that ValUES scrutinizes through controlled experiments to disentangle method effects from data ambiguity and shift."
    },
    {
      "title": "A Probabilistic U-Net for Segmentation of Ambiguous Images",
      "authors": "Simon A. A. Kohl et al.",
      "year": 2018,
      "arxiv_id": "1806.05034",
      "role": "Inspiration",
      "relationship_sentence": "By demonstrating that multiple plausible segmentations can legitimately exist, this work motivated ValUES\u2019s controlled ambiguity setting to evaluate data-related uncertainty in a principled way."
    },
    {
      "title": "Confidence Calibration and Predictive Uncertainty in Deep Learning for Medical Image Segmentation",
      "authors": "Alireza Mehrtash et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Their finding that standard classification-style calibration metrics can be misleading for segmentation directly prompted ValUES\u2019s pitfall analysis and adoption of segmentation-appropriate evaluation protocols."
    }
  ],
  "synthesis_narrative": "Kendall and Gal established the crucial distinction between aleatoric and epistemic uncertainty and introduced practical mechanisms (e.g., heteroscedastic modeling) to estimate them in vision, grounding later attempts to separate data- from model-related uncertainty. Gal and Ghahramani showed that Monte Carlo Dropout provides a tractable Bayesian approximation, becoming a default model-uncertainty baseline. Lakshminarayanan and colleagues demonstrated the effectiveness of Deep Ensembles and revealed how ensemble diversity can drive uncertainty quality. Kohl and co-authors highlighted that segmentation often admits multiple valid annotations, providing a precise notion of inherent ambiguity that uncertainty methods should reflect. Ovadia et al. proposed evaluating uncertainty under dataset shift, emphasizing that reliability must hold beyond the training distribution. Nado et al. systematized benchmarking by curating uncertainty baselines and ablations, underscoring the need for comparable, methodical evaluations. Mehrtash and collaborators specifically showed that naively ported classification calibration metrics can mischaracterize segmentation uncertainty, calling for domain-appropriate measures.\nTogether, these works reveal a gap: despite powerful uncertainty methods and shift-aware evaluations, segmentation lacks a controlled, comprehensive framework to probe ambiguity vs. shift, compare method components fairly, and apply suitable metrics. ValUES synthesizes these insights by constructing controlled ambiguity and distribution-shift regimes inspired by Kendall\u2013Gal and Ovadia, benchmarking canonical baselines like MC Dropout and Deep Ensembles following Nado, and addressing Mehrtash\u2019s critique with segmentation-tailored evaluation and systematic ablations to isolate which components genuinely deliver reliable uncertainty in practice.",
  "target_paper": {
    "title": "ValUES: A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation",
    "authors": "Kim-Celine Kahl, Carsten T. L\u00fcth, Maximilian Zenk, Klaus Maier-Hein, Paul F Jaeger",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "uncertainty, segmentation, validation",
    "abstract": "Uncertainty estimation is an essential and heavily-studied component for the reliable application of semantic segmentation methods. While various studies exist claiming methodological advances on the one hand, and successful application on the other hand, the field is currently hampered by a gap between theory and practice leaving fundamental questions unanswered: Can data-related and model-related uncertainty really be separated in practice? Which components of an uncertainty method are essential for real-world performance? Which uncertainty method works well for which application? In this work, we link this research gap to a lack of systematic and comprehensive evaluation of uncertainty methods. Specifically, we identify three key pitfalls in current literature and present an evaluation framework that bridges the research gap by providing 1) a controlled environment for studying data ambiguities as well as distribution shifts, 2) systematic ablations of relevant method components, an",
    "openreview_id": "yV6fD7LYkF",
    "forum_id": "yV6fD7LYkF"
  },
  "analysis_timestamp": "2026-01-06T12:23:24.329700"
}
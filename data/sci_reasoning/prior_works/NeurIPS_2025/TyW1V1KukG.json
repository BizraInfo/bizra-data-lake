{
  "prior_works": [
    {
      "title": "Universal Adversarial Perturbations",
      "authors": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard",
      "year": 2017,
      "role": "Foundational concept and method for model- and data-agnostic perturbations",
      "relationship_sentence": "The paper\u2019s goal of generating model/prompt-agnostic perturbations inherits the UAP idea of learning image-agnostic directions that transfer broadly, but extends it to LVLMs and prompt semantics via an information-theoretic objective."
    },
    {
      "title": "Adversarial Patch",
      "authors": "Tom B. Brown, Dandelion Man\u00e9, Aurko Roy, Mart\u00edn Abadi, Justin Gilmer",
      "year": 2017,
      "role": "Universal, input-independent attack paradigm",
      "relationship_sentence": "The pursuit of prompt-transferable, broadly effective perturbations echoes the adversarial patch\u2019s universality, while this work generalizes the notion to non-patch, fine-grained perturbations guided by mutual information in LVLMs."
    },
    {
      "title": "Boosting Adversarial Attacks with Momentum",
      "authors": "Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, Jianguo Li",
      "year": 2018,
      "role": "Transferability-enhancing gradient method (MI-FGSM)",
      "relationship_sentence": "As a core baseline for transfer attacks, MI-FGSM\u2019s insight\u2014stabilizing optimization to avoid overfitting a source model\u2014motivates this paper\u2019s move beyond heuristics to principled MI-driven objectives that yield cross-model LVLM transfer."
    },
    {
      "title": "Improving Transferability of Adversarial Examples with Input Diversity",
      "authors": "Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, Alan L. Yuille",
      "year": 2019,
      "role": "Data augmentation strategy to improve attack transfer (DI^2-FGSM)",
      "relationship_sentence": "The idea of perturbation generalization via input diversity informs the paper\u2019s aim to avoid overfitting; here, generalization is achieved by manipulating mutual information with disentangled benign/adversarial patterns rather than augmentations alone."
    },
    {
      "title": "Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks",
      "authors": "Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu",
      "year": 2019,
      "role": "Smoothing-based transferability via translation invariance (TI-FGSM)",
      "relationship_sentence": "This work\u2019s translation-invariant principle\u2014favoring perturbations aligned with shared, model-invariant features\u2014directly precedes the present paper\u2019s formalization of such invariance as maximizing output\u2013adversarial-pattern dependence while minimizing benign dependence via MI."
    },
    {
      "title": "Adversarial Examples Are Not Bugs, They Are Features",
      "authors": "Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry",
      "year": 2019,
      "role": "Conceptual framework of non-robust vs. robust features underpinning transfer",
      "relationship_sentence": "The paper operationalizes Ilyas et al.\u2019s view by explicitly disentangling benign (robust) and adversarial (non-robust) patterns and steering LVLM outputs\u2019 reliance via mutual information, thereby engineering transferability across models and prompts."
    },
    {
      "title": "Mutual Information Neural Estimation (MINE)",
      "authors": "Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, Devon Hjelm",
      "year": 2018,
      "role": "Practical estimator for mutual information objectives",
      "relationship_sentence": "By enabling differentiable MI estimates between inputs and model outputs, MINE provides the technical foundation for the paper\u2019s core objective of enlarging/decreasing MI with disentangled adversarial/benign patterns to craft transferable LVLM attacks."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014an information-theoretic attacker that is transferable across both models and prompts in LVLMs\u2014stands on two converging lines of prior work. First, the transferability literature in vision established that perturbations can generalize beyond a single model through careful objective design and optimization. Universal Adversarial Perturbations and Adversarial Patch showed that model- and input-agnostic perturbations exist, inspiring the present work\u2019s aim for agnostic behavior with respect to both model architecture and prompt semantics. Successive advances like MI-FGSM, DI^2-FGSM, and TI-FGSM demonstrated concrete mechanisms\u2014momentum stabilization, input diversity, and translation-invariant smoothing\u2014to reduce overfitting to a source model and align perturbations with broadly shared features, directly motivating a search for a more principled criterion for generalization.\nSecond, the conceptual reframing by Ilyas et al. that adversarial examples exploit non-robust yet predictive features provided the semantic lens for the paper\u2019s disentanglement of benign versus adversarial patterns. Rather than implicitly encouraging transfer via heuristics, the authors explicitly modulate a model\u2019s dependency on these patterns. This is enabled technically by mutual information optimization, for which MINE supplies a tractable estimator, allowing the attack to increase output dependence on adversarial patterns while suppressing dependence on benign ones. By marrying universal/transferable attack principles with an MI-based dependency control grounded in the non-robust features perspective, the paper advances from image-only CNN settings to LVLMs, achieving both model-transferability and prompt-transferability\u2014two dimensions previously handled only implicitly or separately.",
  "analysis_timestamp": "2026-01-07T00:21:32.305748"
}
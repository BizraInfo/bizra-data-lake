{
  "prior_works": [
    {
      "title": "Language Models as Knowledge Bases?",
      "authors": "Fabio Petroni et al.",
      "year": 2019,
      "role": "Probing/elicitation methodology",
      "relationship_sentence": "Provided the foundational idea of eliciting latent \"beliefs\" from LMs via carefully designed prompts, directly inspiring this paper\u2019s survey-based elicitation of model-encoded moral beliefs."
    },
    {
      "title": "The Moral Machine Experiment",
      "authors": "Edmond Awad et al.",
      "year": 2018,
      "role": "Moral survey design and large-scale evaluation",
      "relationship_sentence": "Established the large-scale, scenario-based survey paradigm for measuring moral choices under ambiguity, which this work adapts to LLMs to compare choices across models and scenarios."
    },
    {
      "title": "Social Chemistry 101: Learning to Reason about Social and Moral Norms",
      "authors": "Maxwell Forbes et al.",
      "year": 2020,
      "role": "Normative knowledge and rule annotations",
      "relationship_sentence": "Introduced structured representations of everyday moral norms and rule violations, informing this paper\u2019s use of auxiliary labels (e.g., violated rules) to analyze LLM moral choices."
    },
    {
      "title": "Aligning AI with Shared Human Values (ETHICS)",
      "authors": "Dan Hendrycks et al.",
      "year": 2021,
      "role": "Ethical judgment benchmarks",
      "relationship_sentence": "Provided benchmark tasks and datasets for evaluating ethical judgments in LMs, motivating the need for rigorous, systematic measurement of moral preferences across models."
    },
    {
      "title": "Delphi: Towards Machine Ethics through Moral Judgments",
      "authors": "Lisa Anne Hendricks (Jiang) et al.",
      "year": 2021,
      "role": "Modeling and evaluating moral judgments",
      "relationship_sentence": "Demonstrated that models can make stable moral judgments over everyday scenarios, directly motivating the paper\u2019s focus on eliciting and quantifying LLM moral beliefs via surveys."
    },
    {
      "title": "Language Models (Mostly) Know What They Know",
      "authors": "Saurav Kadavath et al.",
      "year": 2022,
      "role": "Uncertainty estimation and calibration for LMs",
      "relationship_sentence": "Introduced techniques to quantify LMs\u2019 confidence and uncertainty, influencing this paper\u2019s statistical measures for probability of choice and associated uncertainty."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang et al.",
      "year": 2023,
      "role": "Aggregation and consistency via sampling",
      "relationship_sentence": "Popularized sampling-based aggregation to assess stability of model outputs, informing this work\u2019s consistency metric across multiple samples/paraphrases of moral prompts."
    }
  ],
  "synthesis_narrative": "This paper\u2019s core contribution\u2014rigorously eliciting and quantifying the moral beliefs encoded in LLMs\u2014builds on two methodological threads: probing latent model beliefs and evaluating moral judgments at scale. Petroni et al. pioneered prompt-based probing to surface internal knowledge, providing the conceptual template for treating LLM responses as measurable beliefs. Awad et al.\u2019s Moral Machine delivered the survey paradigm for moral decision-making under ambiguity, directly inspiring the paper\u2019s scenario design and cross-system comparison. To interpret choices, Social Chemistry 101 contributed structured normative knowledge and explicit rule annotations, which the paper extends into auxiliary labels (e.g., violated rules) to dissect moral dimensions of responses. ETHICS and Delphi established that LMs can be benchmarked on ethical judgments and that stable moral judgments can be elicited from models, motivating a principled and comprehensive evaluation framework for moral beliefs rather than ad hoc prompting. Methodologically, the authors\u2019 statistical measures for probability of making a choice and uncertainty are grounded in recent advances on LM uncertainty and calibration (Kadavath et al.), while their consistency metric echoes self-consistency sampling (Wang et al.). Together, these works converge to enable a survey-based, statistically principled approach: define controlled moral scenarios, elicit multiple samples, and quantify choice rates, uncertainty, and consistency to compare moral tendencies across diverse LLMs and scenario ambiguities.",
  "analysis_timestamp": "2026-01-06T23:42:49.079435"
}
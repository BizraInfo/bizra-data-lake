{
  "prior_works": [
    {
      "title": "Supervised Learning with Tensor Networks",
      "authors": "Edward Stoudenmire, David J. Schwab",
      "year": 2016,
      "role": "Brought tensor-network formalisms (especially MPS) into classical supervised learning and used entanglement entropy of label functions as a complexity metric.",
      "relationship_sentence": "This work directly motivates the paper\u2019s entanglement-based suitability criterion by showing that learnability with locally structured models hinges on low entanglement across input partitions."
    },
    {
      "title": "Deep Learning and Quantum Entanglement: Fundamental Connections with Tensor Networks",
      "authors": "Yoav Levine, Or Sharir, Nadav Cohen, Amnon Shashua",
      "year": 2017,
      "role": "Established a formal bridge between deep neural architectures (notably convolutional arithmetic circuits) and tensor networks, relating network depth/structure to entanglement capacity.",
      "relationship_sentence": "The paper builds on this mapping to argue that locally connected networks have bounded entanglement across specific cuts, enabling necessary-and-sufficient characterizations in terms of data entanglement."
    },
    {
      "title": "Convolutional Rectifier Networks as Generalized Tensor Decompositions",
      "authors": "Nadav Cohen, Amnon Shashua",
      "year": 2016,
      "role": "Cast CNNs as hierarchical tensor decompositions and introduced separation rank across input partitions as an expressivity measure.",
      "relationship_sentence": "Separation rank is the classical analog of entanglement rank, underpinning the paper\u2019s identification of canonical partitions and the corresponding entanglement limits of locally connected networks."
    },
    {
      "title": "Inductive Bias of Deep Convolutional Networks through Pooling Geometry",
      "authors": "Nadav Cohen, Amnon Shashua",
      "year": 2017,
      "role": "Showed how pooling geometry selects which input partitions a CNN can model correlations across, via separation-rank analysis.",
      "relationship_sentence": "This directly informs the paper\u2019s \u2018canonical partitions\u2019 concept, determining the cuts over which data must exhibit low entanglement for LCNN suitability."
    },
    {
      "title": "Matrix Product State Representations",
      "authors": "David Perez-Garcia, Frank Verstraete, Michael M. Wolf, J. Ignacio Cirac",
      "year": 2007,
      "role": "Characterized MPS, including how bond dimension bounds entanglement entropy across cuts and dictates what correlations can be represented.",
      "relationship_sentence": "The necessity/sufficiency proof relies on these MPS/TN fundamentals: LCNNs correspond to TNs with bounded bond dimensions, so they can represent exactly those distributions with sufficiently low entanglement across the relevant partitions."
    },
    {
      "title": "Efficient Classical Simulation of Slightly Entangled Quantum Computations",
      "authors": "Guifr\u00e8 Vidal",
      "year": 2003,
      "role": "Introduced the idea that low entanglement enables efficient classical representations via tensor networks, formalizing entanglement as the key resource.",
      "relationship_sentence": "This principle underlies the paper\u2019s core message that low entanglement (over canonical cuts) is the resource condition making data learnable by locally connected networks."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014an if-and-only-if characterization of when a locally connected neural network (LCNN) can accurately model a data distribution\u2014emerges by unifying two lines of work: (i) the mapping of neural architectures to tensor networks and (ii) entanglement-based capacity limits from quantum many-body theory. On the neural side, the tensor-decomposition view of CNNs and separation-rank analysis established by Cohen and Shashua, alongside the pooling-geometry result, identify specific \u2018canonical partitions\u2019 of the input that a locally structured architecture can effectively couple. Levine et al. strengthened this bridge by explicitly tying deep networks\u2019 expressive capacity to tensor-network entanglement, setting the stage to translate architectural constraints into entanglement bounds. On the physics side, foundational results by Vidal and by Perez-Garcia et al. formalize how tensor networks with bounded bond dimension impose strict limits on entanglement across cuts, precisely determining representable correlations. Stoudenmire and Schwab then demonstrated that entanglement is a practically meaningful complexity measure for classical learning tasks with local tensor-network models. Synthesizing these insights, the paper proves necessity (LCNNs cannot realize functions with high entanglement across the canonical cuts they induce) and sufficiency (low entanglement guarantees representability by an appropriately sized LCNN). This lens also yields a practical preprocessing strategy: reconfigure inputs to reduce entanglement across the canonical partitions dictated by the chosen LCNN, thereby enhancing learnability. The result is a crisp, physics-grounded criterion that unifies theory and practice for CNNs, RNNs, and local self-attention models.",
  "analysis_timestamp": "2026-01-07T00:02:04.851693"
}
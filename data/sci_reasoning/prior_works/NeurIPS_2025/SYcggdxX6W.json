{
  "prior_works": [
    {
      "title": "Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis",
      "authors": "Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, et al.",
      "year": 2018,
      "role": "Foundational expressive control mechanism",
      "relationship_sentence": "WeSCon\u2019s dynamic emotional attention bias extends the style-attention idea introduced by GST, biasing attention to invoke desired emotional attributes at specific textual spans rather than only utterance-level styles."
    },
    {
      "title": "Towards End-to-End Prosody Transfer for Expressive Speech Synthesis",
      "authors": "RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, et al.",
      "year": 2018,
      "role": "Prosody representation and transfer",
      "relationship_sentence": "The notion of transferring and interpolating prosodic embeddings directly informs WeSCon\u2019s transition-smoothing strategy for handling multi-emotion transitions across words without annotated intra-sentence labels."
    },
    {
      "title": "FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech",
      "authors": "Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu",
      "year": 2021,
      "role": "Controllable duration/speed and prosodic factors",
      "relationship_sentence": "WeSCon\u2019s dynamic speed control at the word level builds on FastSpeech 2\u2019s explicit duration (and pitch/energy) controllability, adapting it to fine-grained, intra-sentence rate manipulation in a zero-shot setting."
    },
    {
      "title": "Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention",
      "authors": "Hideyuki Tachibana, Katsuya Uenoyama, Shunsuke Aihara",
      "year": 2018,
      "role": "Attention alignment shaping",
      "relationship_sentence": "WeSCon\u2019s emotional attention bias mechanism follows the principle of attention guidance from Guided Attention, but injects emotion-aware, word-targeted biases during inference to stabilize and steer alignments for localized expression."
    },
    {
      "title": "Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis",
      "authors": "Ye Jia, Ron J. Weiss, Fadi Biadsy, et al.",
      "year": 2018,
      "role": "Zero-shot/multi-speaker TTS foundation",
      "relationship_sentence": "By leveraging a pretrained zero-shot/multispeaker backbone as in this line of work, WeSCon can decouple speaker identity from prosodic control and apply its self-training procedure without needing word-level emotion annotations."
    },
    {
      "title": "Self-Training with Noisy Student Improves ImageNet Classification",
      "authors": "Qizhe Xie, Eduard Hovy, Minh-Thang Luong, Quoc V. Le",
      "year": 2020,
      "role": "General self-training/pseudo-labeling paradigm",
      "relationship_sentence": "WeSCon adapts the Noisy-Student style self-training idea to TTS, using multi-round inference to generate pseudo-supervision for word-level emotion and speed, enabling control learning without intra-sentence labeled data."
    }
  ],
  "synthesis_narrative": "WeSCon\u2019s core contribution\u2014word-level control of both emotion and speaking rate in a pretrained zero-shot TTS model without intra-sentence annotations\u2014emerges by unifying advances in expressive modeling, controllable generation, attention shaping, and self-training. The groundwork for expressive control stems from GST and end-to-end prosody transfer, which established attention-mediated style embeddings and reference-based prosody manipulation. WeSCon leverages these insights but moves from utterance-level style to localized control by introducing an emotional attention bias that selectively steers attention toward target words, and by smoothing transitions between emotional states\u2014conceptually akin to prosody interpolation.\nFastSpeech 2 provides the practical mechanism for controllable duration and prosodic factors. WeSCon adapts this to dynamic, word-level rate control, enabling fine-grained speed adjustments alongside emotional changes. Stability and precise targeting of intra-sentence control further draw on guided attention, whose alignment-biasing principle is reinterpreted here as an emotion-aware, inference-time attention bias.\nCrucially, the absence of word-level emotion annotations is addressed through a self-training framework inspired by Noisy Student: WeSCon performs multi-round inference to generate reliable pseudo labels for word-level emotion and speed, iteratively refining the model\u2019s control signals. All of this is layered atop a pretrained zero-shot/multispeaker TTS backbone (as typified by transfer from speaker verification), ensuring robust speaker generalization while the new mechanisms focus on localized expressive control. Together, these strands directly enable WeSCon\u2019s novel capability: smooth, precise, word-level emotional and rate manipulation in zero-shot TTS without specialized annotated datasets.",
  "analysis_timestamp": "2026-01-07T00:21:32.262585"
}
{
  "prior_works": [
    {
      "title": "Attention-based Deep Multiple Instance Learning",
      "authors": "Maximilian Ilse et al.",
      "year": 2018,
      "arxiv_id": "1802.04712",
      "role": "Extension",
      "relationship_sentence": "CAMIL directly extends Ilse et al.\u2019s attention pooling by constraining each tile\u2019s attention weight using its k-nearest neighbors, injecting spatial dependencies into the ABMIL mechanism."
    },
    {
      "title": "CLAM: Clustering-constrained Attention Multiple Instance Learning for Whole Slide Image Classification",
      "authors": "Lu et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "CAMIL borrows CLAM\u2019s idea of imposing priors on attention for weakly supervised WSI learning but replaces CLAM\u2019s clustering regularizer with an explicit neighborhood prior to encode tissue context."
    },
    {
      "title": "TransMIL: Transformer-based Multiple Instance Learning for Whole Slide Image Classification",
      "authors": "Shao et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "TransMIL showed that explicitly modeling inter-patch relations improves WSI MIL, motivating CAMIL to encode such dependencies via locally neighbor-constrained attention rather than global self-attention."
    },
    {
      "title": "PatchGCN: Weakly Supervised Graph Convolutional Networks for Whole-Slide Image Classification",
      "authors": "Chen et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "PatchGCN\u2019s formulation of a WSI as a k-NN graph of patch embeddings directly informs CAMIL\u2019s use of k-nearest-neighbor graphs as a prior to guide attention weights."
    },
    {
      "title": "Clinical-grade computational pathology using weakly supervised deep learning on whole-slide images",
      "authors": "Campanella et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "By establishing the slide-level MIL paradigm for WSIs and highlighting over-reliance on a few top tiles, this work sets up the weakness that CAMIL addresses by distributing attention within spatial neighborhoods."
    },
    {
      "title": "Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer (CAMELYON16/17)",
      "authors": "B. E. Bejnordi et al.",
      "year": 2017,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "The CAMELYON challenges defined the weakly supervised metastasis-detection task and datasets that CAMIL targets, grounding the context-aware MIL problem setting."
    },
    {
      "title": "Classification and mutation prediction from non\u2013small cell lung cancer histopathology images using deep learning",
      "authors": "Nicolas Coudray et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work established the LUAD vs LUSC subtyping benchmark on TCGA-NSCLC that CAMIL uses, anchoring the evaluation of context-aware MIL in NSCLC subtyping."
    }
  ],
  "synthesis_narrative": "Attention-based Deep Multiple Instance Learning introduced a learnable attention pooling mechanism for MIL, enabling slide-level prediction from sets of instance embeddings. CLAM advanced this by constraining attention with a clustering prior to improve discriminative patch discovery and interpretability in weakly supervised WSIs, yet still treated instances largely independently. TransMIL showed that explicitly modeling relationships among instances via transformer self-attention boosts WSI performance, underscoring the value of inter-patch dependencies. In parallel, PatchGCN framed a whole slide as a k-nearest-neighbor graph of patches and learned over this adjacency, demonstrating that spatial neighborhood structure is an effective inductive bias for WSI aggregation. Campanella et al. established clinically scaled weakly supervised WSI MIL and exposed the pitfall of over-reliance on a few top-scoring patches, hinting at the need to incorporate broader contextual evidence. The CAMELYON challenges and Coudray et al.\u2019s NSCLC study defined the core metastasis detection and LUAD/LUSC subtyping tasks that anchor progress in weakly supervised WSI learning. Together, these works reveal a gap: attention-MIL excels at selecting instances but neglects spatial context, while graph and transformer models capture relations but lack the simplicity and interpretability of attention pooling. The natural next step is to inject neighborhood structure as prior knowledge directly into the attention mechanism, retaining MIL\u2019s efficiency and interpretability while encoding local dependencies. CAMIL synthesizes these insights by imposing neighbor-constrained attention over a k-NN graph, distributing evidence across spatially coherent tiles and addressing the identified limitations.",
  "target_paper": {
    "title": "CAMIL: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images",
    "authors": "Olga Fourkioti, Matt De Vries, Chris Bakal",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Multiple Instance Learning, Histopathology, Nearest Neighbors, Graph Representation",
    "abstract": "The visual examination of tissue biopsy sections is fundamental for cancer diagnosis, with pathologists analyzing sections at multiple magnifications to discern tumor cells and their subtypes. However, existing attention-based multiple instance learning (MIL) models used for analyzing Whole Slide Images (WSIs) in cancer diagnostics often overlook the contextual information of tumor and neighboring tiles, leading to misclassifications. To address this, we propose the Context-Aware Multiple Instance Learning (CAMIL) architecture. CAMIL incorporates neighbor-constrained attention to consider dependencies among tiles within a WSI and integrates contextual constraints as prior knowledge into the MIL model. We evaluated CAMIL on subtyping non-small cell lung cancer (TCGA-NSCLC) and detecting lymph node (CAMELYON16 and CAMELYON17) metastasis, achieving test AUCs of 97.5\\%, 95.9\\%, and 88.1\\%, respectively, outperforming other state-of-the-art methods. Additionally, CAMIL enhances model interp",
    "openreview_id": "rzBskAEmoc",
    "forum_id": "rzBskAEmoc"
  },
  "analysis_timestamp": "2026-01-06T11:21:33.697835"
}
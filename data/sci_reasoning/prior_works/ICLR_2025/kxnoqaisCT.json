{
  "prior_works": [
    {
      "title": "Visual Instruction Tuning (LLaVA)",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "arxiv_id": "2304.08485",
      "role": "Extension",
      "relationship_sentence": "The current paper directly adapts the LLaVA architecture and training paradigm to a coordinate-prediction setup for GUI elements, leveraging its instruction-following visual-language backbone for grounding."
    },
    {
      "title": "Shikra: Unleashing Multimodal LLMs with Region-Level Capabilities",
      "authors": "Chen et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Shikra\u2019s idea of using normalized box/point tokens to elicit region-level grounding from MLLMs informs this paper\u2019s simple recipe for making a LLaVA-style model output pixel coordinates given referring expressions."
    },
    {
      "title": "Pix2Struct: Screen Understanding via Image-to-Text Pretraining",
      "authors": "Kenton Lee et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Pix2Struct demonstrated that large-scale, web-based synthetic screen data with programmatic annotations is highly effective for screen understanding, a data strategy this paper adopts for GUI grounding."
    },
    {
      "title": "Mind2Web: Towards a Generalist Agent for the Web",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "Mind2Web\u2019s reliance on HTML/DOM trees and its documented brittleness/noise motivate this paper\u2019s shift to purely visual grounding that avoids text-based environment representations."
    },
    {
      "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
      "authors": "Zhou et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "WebArena established the modern web-agent setting (typically using accessibility/HTML trees), providing the primary benchmark and problem context that this work rethinks with pixel-only grounding."
    },
    {
      "title": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Vocabulary Detection",
      "authors": "Liu et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "As the prevailing open-vocabulary text-to-box grounding baseline, Grounding DINO is a direct comparator that this paper aims to surpass on GUI referring expressions with an MLLM-based approach."
    },
    {
      "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
      "authors": "Zhang et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "Kosmos-2\u2019s grounded MLLM showed that LLMs can output spatial pointers tied to language, an approach that informs the coordinate-formatting and grounding supervision used here for GUI targets."
    }
  ],
  "synthesis_narrative": "Instruction-following multimodal LLMs such as LLaVA showed that a lightweight vision-language alignment plus dialogue-style decoding can reliably connect images to natural-language intents. Building on this, Shikra introduced simple tokenization schemes for normalized coordinates, demonstrating that the same class of models can be coaxed to point to regions with referring expressions. In parallel, Pix2Struct established that synthetic, programmatically labeled web screenshots are a powerful substrate for screen understanding, indicating that scalable browser-based generation can cover diverse UI layouts and semantics. Web agents matured with environments like WebArena, where systems typically access accessibility/HTML trees to act, while Mind2Web documented that such textual structures can be noisy, incomplete, and brittle for generalization. For grounding as a capability, Grounding DINO supplied a strong open-vocabulary text-to-box detector, and Kosmos-2 generalized grounded outputs within MLLMs, reinforcing that language and spatial predictions can be unified.\nTogether these threads expose a clear opening: leverage synthetic, web-rendered supervision to teach MLLMs to point, but target GUI elements directly from pixels to avoid DOM noise and to generalize across platforms. Adapting a LLaVA-style backbone with Shikra/Kosmos-2-like coordinate formatting provides a minimal architectural change while keeping instruction-following intact. Grounding DINO offers a rigorous baseline for referring expressions, and WebArena/Mind2Web contextualize the shift away from text-based representations. The resulting synthesis\u2014a visual-only, universally grounded GUI model trained on scalable web-based synthetic data\u2014emerges as the natural next step.",
  "target_paper": {
    "title": "Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents",
    "authors": "Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, Yu Su",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "GUI Agents, Visual Grounding, Multimodal Large Language Models, GUI Grounding, Large Language Model",
    "abstract": "Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the robustness of their grounding capability. Current GUI agents predominantly utilize text-based representations such as HTML or accessibility trees, which, despite their utility, often introduce noise, incompleteness, and increased computational overhead. In this paper, we advocate a human-like embodiment for GUI agents that perceive the environment entirely visually and directly perform pixel-level operations on the GUI. The key is visual grounding models that can accurately map diverse referring expressions of GUI elements to their coordinates on the GUI across different platforms. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture",
    "openreview_id": "kxnoqaisCT",
    "forum_id": "kxnoqaisCT"
  },
  "analysis_timestamp": "2026-01-06T14:10:54.491997"
}
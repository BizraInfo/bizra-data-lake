{
  "prior_works": [
    {
      "title": "Ridge Regression and Asymptotic Minimax Estimation",
      "authors": "Lee H. Dicker et al.",
      "year": 2016,
      "role": "Baseline",
      "relationship_sentence": "Established the in-distribution optimal tuning of ridge and its asymptotic risk under minimal assumptions, providing the baseline notion of \u201coptimal ridge\u201d that this paper generalizes to out-of-distribution settings (and to possibly negative regularization)."
    },
    {
      "title": "High-dimensional asymptotics of prediction: Ridge regression and classification",
      "authors": "Edgar Dobriban et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Derived precise high-dimensional risk characterizations and optimal tuning for ridge under matched train/test distributions, forming the theoretical framework that the current work extends to covariate and regression shifts."
    },
    {
      "title": "Surprises in High-Dimensional Ridgeless Least Squares Interpolation",
      "authors": "Trevor Hastie et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Revealed when ridgeless solutions and double descent arise in-distribution, highlighting a gap on how tuning behaves under distribution shift that this paper fills by proving when even negative regularization is optimal OOD."
    },
    {
      "title": "Benign Overfitting in Linear Regression",
      "authors": "Peter L. Bartlett et al.",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "Introduced alignment-based conditions between signal and covariance for benign interpolation, directly inspiring this paper\u2019s alignment criteria that determine the sign of optimal ridge under train\u2013test mismatch."
    },
    {
      "title": "Harmless Interpolation in Regression and Classification",
      "authors": "Vidya Muthukumar et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "Showed that spectral alignment can make interpolation optimal in-distribution; the present work translates this alignment perspective to cross-distribution alignment and extends it to allow optimal negative ridge."
    },
    {
      "title": "Improving predictive inference under covariate shift by weighting the log-likelihood function",
      "authors": "Hidetoshi Shimodaira et al.",
      "year": 2000,
      "role": "Foundation",
      "relationship_sentence": "Formalized the covariate shift setting that underpins this paper\u2019s OOD framework; the current results are developed explicitly for covariate shift and regression shift as defined in this lineage."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014characterizing the sign of optimal ridge regularization and the monotonicity of optimally tuned risk under out-of-distribution (OOD) prediction\u2014builds directly on the modern theory of ridge risk and alignment, while extending it beyond the matched train/test regime. Foundational analyses of optimal ridge and its risk in high dimensions by Dicker (2016) and Dobriban & Wager (2018) supplied the precise in-distribution framework and tuning baseline that this work generalizes to arbitrary covariate and regression shifts. The empirical and theoretical revelations around ridgeless solutions and double descent by Hastie et al. (2019) exposed a gap: existing insights were largely restricted to in-distribution settings. Bartlett et al. (2020) crystallized the role of alignment between the signal and the feature covariance in determining when interpolation is benign; this alignment viewpoint directly informs the present paper\u2019s cross-distribution alignment criteria that determine whether optimal regularization is positive, zero, or negative under OOD. Complementing this, Muthukumar et al. (2020) further developed alignment-based explanations for harmless interpolation, which the current work extends by showing that OOD misalignment can flip the optimal sign of ridge to negative. Finally, the classical formulation of covariate shift by Shimodaira (2000) provides the problem setting the authors analyze. Together, these works enabled the paper\u2019s key insights: principled, distribution-agnostic conditions for the optimal ridge sign under OOD and a monotonicity result for optimally tuned risk across aspect ratios\u2014even when permitting negative regularization.",
  "analysis_timestamp": "2026-01-06T23:09:26.439436"
}
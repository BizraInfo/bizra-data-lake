{
  "prior_works": [
    {
      "title": "Network Dissection: Quantifying Interpretability of Deep Visual Representations",
      "authors": [
        "David Bau",
        "Bolei Zhou",
        "Aditya Khosla",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "year": 2017,
      "role": "Methodological foundation for quantifying mechanistic interpretability",
      "relationship_sentence": "This paper established a quantitative, unit-level benchmark for interpretability that Zimmermann et al. seek to complement with a psychophysical, model-level metric, enabling a principled comparison across architectures and scales."
    },
    {
      "title": "Feature Visualization",
      "authors": [
        "Chris Olah",
        "Alexander Mordvintsev",
        "Ludwig Schubert"
      ],
      "year": 2017,
      "role": "Mechanistic interpretability agenda and tools",
      "relationship_sentence": "Feature visualization catalyzed the field\u2019s focus on mechanistic understanding of internal features, which Zimmermann et al. evaluate at scale using behaviorally grounded tests rather than visualization heuristics."
    },
    {
      "title": "Sanity Checks for Saliency Maps",
      "authors": [
        "Julius Adebayo",
        "Justin Gilmer",
        "Michael Muelly",
        "Ian Goodfellow",
        "Moritz Hardt",
        "Been Kim"
      ],
      "year": 2018,
      "role": "Evaluation rigor for interpretability claims",
      "relationship_sentence": "By revealing that popular explanation methods can be insensitive to model parameters, this work motivates Zimmermann et al.\u2019s use of a psychophysical paradigm that yields falsifiable, quantitative interpretability measurements."
    },
    {
      "title": "Adversarial Examples Are Not Bugs, They Are Features",
      "authors": [
        "Andrew Ilyas",
        "Shibani Santurkar",
        "Dimitris Tsipras",
        "Logan Engstrom",
        "Brandon Tran",
        "Aleksander Madry"
      ],
      "year": 2019,
      "role": "Conceptual link between accuracy and non-human-aligned features",
      "relationship_sentence": "The identification of non-robust, non-human-aligned features directly informs Zimmermann et al.\u2019s hypothesis that scaling may not improve mechanistic interpretability, and motivates their behavioral tests of what features models actually use."
    },
    {
      "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias in CNNs using stylized ImageNet",
      "authors": [
        "Robert Geirhos",
        "Patricia Rubisch",
        "Claudio Michaelis",
        "Matthias Bethge",
        "Felix A. Wichmann",
        "Wieland Brendel"
      ],
      "year": 2019,
      "role": "Psychophysical cue-conflict paradigm for probing model features",
      "relationship_sentence": "Geirhos et al.\u2019s psychophysics-inspired cue-conflict methodology provides a template for behaviorally probing model feature usage that Zimmermann et al. adapt to quantify interpretability across scales."
    },
    {
      "title": "Approximating CNNs with Bag-of-local-features models works surprisingly well on ImageNet",
      "authors": [
        "Wieland Brendel",
        "Matthias Bethge"
      ],
      "year": 2019,
      "role": "Evidence for local/texture reliance and interpretable baselines",
      "relationship_sentence": "By showing strong performance from simple local-feature models, this work underpins Zimmermann et al.\u2019s comparison to older architectures (e.g., GoogLeNet) and their conclusion that newer, larger models are not more mechanistically interpretable."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": 2021,
      "role": "Scaling catalyst and modern architecture target",
      "relationship_sentence": "The success of ViT as a scaled architecture frames Zimmermann et al.\u2019s central question and supplies key model families for testing whether scale improves mechanistic interpretability."
    }
  ],
  "synthesis_narrative": "Zimmermann, Klein, and Brendel interrogate whether the modern recipe of scaling models and datasets has improved our mechanistic understanding of vision models. Their approach is grounded in a trajectory of work that made interpretability measurable and falsifiable. Network Dissection pioneered quantitative unit-level semantics, while Feature Visualization popularized mechanistic probing of internal features; together they framed interpretability as an empirical science rather than anecdotal visualization. Adebayo et al.\u2019s sanity checks exposed brittleness in explanation methods, motivating the authors\u2019 pivot to a psychophysical, behavioral paradigm that can be rigorously quantified. Conceptually, Ilyas et al. showed that high accuracy can rest on non-human-aligned, non-robust features, directly inspiring the paper\u2019s hypothesis that scale may not fix the misalignment that hinders mechanistic interpretability. Geirhos et al. introduced cue-conflict psychophysics for CNNs, offering a principled way to probe which cues models use; Zimmermann et al. adapt this spirit to quantify interpretability across architectures and training regimes. Brendel and Bethge\u2019s BagNets demonstrated that simple, local features can suffice for strong performance, providing a historically interpretable baseline and context for the surprising finding that older models (e.g., GoogLeNet) are as interpretable as, or more interpretable than, today\u2019s giants. Finally, the advent of ViT and scale-first training established the practical need and experimental substrate for the paper\u2019s cross-scale comparison. Collectively, these works shaped a rigorous, behaviorally anchored evaluation that reveals scaling alone does not yield more mechanistically interpretable vision systems.",
  "analysis_timestamp": "2026-01-06T23:42:49.048827"
}
{
  "prior_works": [
    {
      "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
      "authors": "Mor Geva et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Building on the finding that FFNs implement key\u2013value memories, this paper formalizes and proves linear storage capacity for MLP associative memories and constructs a shallow transformer that solves factual recall by using the MLP as the memory."
    },
    {
      "title": "Locating and Editing Factual Associations in GPT",
      "authors": "Kevin Meng et al.",
      "year": 2022,
      "arxiv_id": "2202.05262",
      "role": "Gap Identification",
      "relationship_sentence": "Because this work showed factual associations localize in MLP layers and can be edited, it motivated the present paper\u2019s analysis and its key result that both MLPs and the attention value matrices can alternatively store facts with near\u2013parameter-optimal capacity."
    },
    {
      "title": "Hopfield Networks is All You Need",
      "authors": "Hubert Ramsauer et al.",
      "year": 2020,
      "arxiv_id": "2008.02217",
      "role": "Foundation",
      "relationship_sentence": "By casting attention as a modern Hopfield associative memory, this work provided the conceptual and mathematical lens used here to treat attention value matrices as associative memories and analyze their capacity."
    },
    {
      "title": "Neural networks and physical systems with emergent collective computational abilities",
      "authors": "John J. Hopfield",
      "year": 1982,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Classical associative memory capacity results from Hopfield networks underlie this paper\u2019s linear-in-parameters capacity benchmarks, which are adapted and proved for linear and MLP memories inside transformers."
    },
    {
      "title": "In-context Learning and Induction Heads",
      "authors": "Catherine Olsson et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "The demonstration that a single attention head performs key\u2013value associative retrieval directly informs the constructive proof that a one-layer attention module can store and recall factual pairs via its value matrices."
    },
    {
      "title": "Language Models as Knowledge Bases?",
      "authors": "Fabio Petroni et al.",
      "year": 2019,
      "arxiv_id": "1909.01066",
      "role": "Foundation",
      "relationship_sentence": "By framing factual recall as retrieving stored knowledge from language models, this work motivates the formal synthetic factual-recall task used here to enable precise capacity analysis."
    }
  ],
  "synthesis_narrative": "Feed-forward layers in transformers were shown to act as key\u2013value memories, with learned keys and values implementing direct associative lookup (Geva et al.). This mechanistic view connected the FFN to explicit storage of input\u2013output associations rather than mere feature transformation. Concurrently, practical knowledge-editing studies demonstrated that factual associations are often localized in MLP layers and can be altered with targeted, low-rank updates (Meng et al.), underscoring that factual knowledge can be weight-encoded and suggesting a concrete locus of memory. From the attention side, the induction-head mechanism illustrated that even a single attention head can perform key\u2013value retrieval by matching keys and copying corresponding values (Olsson et al.), highlighting attention as an associative lookup primitive. The associative-memory perspective was formalized by modern Hopfield theory, which recasts attention as energy-based retrieval over stored patterns (Ramsauer et al.), and rooted in classical results showing linear-in-neuron storage capacity for associative memories (Hopfield, 1982). Finally, the notion of factual recall as a knowledge-retrieval problem for language models was crystallized by probing setups like LAMA (Petroni et al.), providing a clean recall objective. Taken together, these works revealed two plausible loci for storing facts\u2014FFN key\u2013value memories and attention value matrices\u2014yet left open principled capacity guarantees and their tradeoff. The present paper synthesizes these insights by proving linear storage capacity for linear and MLP associative memories, and by constructing a shallow transformer that achieves perfect factual recall using either the value matrices or the MLP, thereby characterizing when and how transformers can reach near\u2013parameter-optimal factual storage and trade memory between modules.",
  "target_paper": {
    "title": "Understanding Factual Recall in Transformers via Associative Memories",
    "authors": "Eshaan Nichani, Jason D. Lee, Alberto Bietti",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "transformers, associative memories, factual recall, storage capacity, training dynamics",
    "abstract": "Large language models have demonstrated an impressive ability to perform factual recall. Prior work has found that transformers trained on factual recall tasks can store information at a rate proportional to their parameter count. In our work, we show that shallow transformers can use a combination of associative memories to obtain such near optimal storage capacity. We begin by proving that the storage capacities of both linear and MLP associative memories scale linearly with parameter count. We next introduce a synthetic factual recall task, and prove that a transformer with a single layer of self-attention followed by an MLP can obtain 100\\% accuracy on the task whenever either the total number of self-attention parameters or MLP parameters scales (up to log factors) linearly with the number of facts. In particular, the transformer can trade off between using the value matrices or the MLP as an associative memory to store the dataset of facts. We complement these expressivity result",
    "openreview_id": "hwSmPOAmhk",
    "forum_id": "hwSmPOAmhk"
  },
  "analysis_timestamp": "2026-01-06T09:28:06.916664"
}
{
  "prior_works": [
    {
      "title": "The Information Bottleneck Method",
      "authors": "Naftali Tishby et al.",
      "year": 1999,
      "arxiv_id": "physics/0004057",
      "role": "Foundation",
      "relationship_sentence": "The paper explicitly positions its \"information retention\" objective as a counterpoint to the Information Bottleneck\u2019s compression trade-off, reframing the goal from squeezing representations to keeping multiple relevant signals for prediction under shift."
    },
    {
      "title": "Deep Variational Information Bottleneck",
      "authors": "Alexander A. Alemi et al.",
      "year": 2017,
      "arxiv_id": "1612.00410",
      "role": "Gap Identification",
      "relationship_sentence": "By operationalizing IB in deep networks and encouraging compression that prunes redundant cues, VIB highlights the very mechanism this work seeks to avoid through a supplemental-feature pathway and staged training that preserves redundant-but-useful signals."
    },
    {
      "title": "Gradient Starvation: A Learning Proclivity in Neural Networks",
      "authors": "Mohammad Pezeshki et al.",
      "year": 2021,
      "arxiv_id": "2011.09468",
      "role": "Gap Identification",
      "relationship_sentence": "The observation that early-learned dominant features suppress gradients for alternative predictive features directly motivates the paper\u2019s three-stage design to relieve suppression so that supplemental features can be effectively learned."
    },
    {
      "title": "Just Train Twice: Improving Group Robustness Without Training Group Information",
      "authors": "Evan Z. Liu et al.",
      "year": 2021,
      "arxiv_id": "2107.09044",
      "role": "Inspiration",
      "relationship_sentence": "The staged residual-learning idea in JTT\u2014first train a standard model, then focus subsequent training on what it misses\u2014inspires the paper\u2019s sequence of learning mainline features first and then targeting residual signals as supplemental features."
    },
    {
      "title": "Learning from Failure: De-biasing Neural Networks by Learning to Identify Failure",
      "authors": "Junho Nam et al.",
      "year": 2020,
      "arxiv_id": "2007.02561",
      "role": "Inspiration",
      "relationship_sentence": "LfF\u2019s complementary-training setup (bias-only versus debiased learner) informs the explicit separation and coordinated training of \"mainline\" and \"supplemental\" feature pathways that the paper uses to encourage diverse, complementary cues."
    },
    {
      "title": "Invariant Risk Minimization",
      "authors": "Martin Arjovsky et al.",
      "year": 2019,
      "arxiv_id": "1907.02893",
      "role": "Baseline",
      "relationship_sentence": "IRM serves as the principal OOD baseline that enforces invariance by removing spurious features, against which this paper contrasts by retaining such signals as supplemental features and combining them with mainline cues."
    },
    {
      "title": "Shortcut Learning in Deep Neural Networks",
      "authors": "Robert Geirhos et al.",
      "year": 2020,
      "arxiv_id": "2004.07780",
      "role": "Related Problem",
      "relationship_sentence": "Evidence that networks default to easy, shortcut cues motivates intentionally learning additional, diverse cues rather than suppressing them, which this paper operationalizes via its supplemental-feature learning framework."
    }
  ],
  "synthesis_narrative": "The Information Bottleneck framework formalized representation learning as a trade-off between compression and prediction, providing a powerful rationale for discarding redundant input information. Its deep instantiation, the Variational Information Bottleneck, concretely implements this pressure to compress, encouraging networks to prune features deemed superfluous for in-domain accuracy. However, the phenomenon of gradient starvation shows that early, dominant features suppress gradients needed to learn alternative predictive cues, leaving potentially useful signals underexploited. Empirical studies of shortcut learning further reveal that deep models gravitate to easy proxies, often overlooking diverse cues that could help under distribution shift. In parallel, staged and complementary training ideas emerged in robustness work: Just Train Twice introduced a two-stage procedure that first learns a strong model and then concentrates on residual errors to uncover overlooked signals, while Learning from Failure trains complementary predictors (bias-only versus debiased) to encourage learning features that the other misses. Invariant Risk Minimization, a standard OOD baseline, instead removes non-invariant cues to seek robust predictors. Together, these works expose a tension: compression and invariance can discard redundant-but-relevant cues, while learning dynamics suppress alternative features; yet staged, complementary training can recover missed signals. The current paper naturally integrates these insights by replacing compression with retention and designing a three-stage supervised framework that first learns mainline features, then deliberately learns supplemental, complementary features without being suppressed, and finally combines them to improve robustness in low-resource and shifted settings.",
  "target_paper": {
    "title": "Information Retention via Learning Supplemental Features",
    "authors": "Zhipeng Xie, Yahe Li",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Information Retention, Few-shot Learning, Deep Neural Network",
    "abstract": "The information bottleneck principle provides an information-theoretic method for learning a good representation as a trade-off between conciseness and predictive ability, which can reduce information redundancy, eliminate irrelevant and superfluous features, and thus enhance the in-domain generalizability. However, in low-resource or out-of-domain scenarios where the assumption of i.i.d does not necessarily hold true, superfluous (or redundant) relevant features may be supplemental to the mainline features of the model, and be beneficial in making prediction for test dataset with distribution shift. Therefore, instead of squeezing the input information by information bottleneck, we propose to keep as much relevant information as possible in use for making predictions. A three-stage supervised learning framework is designed and implemented to jointly learn the mainline and supplemental features, relieving supplemental features from the suppression of mainline features. Extensive experi",
    "openreview_id": "o83eu4H9Mb",
    "forum_id": "o83eu4H9Mb"
  },
  "analysis_timestamp": "2026-01-06T12:59:04.274177"
}
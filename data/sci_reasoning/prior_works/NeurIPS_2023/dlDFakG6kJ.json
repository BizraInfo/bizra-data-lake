{
  "prior_works": [
    {
      "title": "Bayesian Persuasion",
      "authors": "Emir Kamenica, Matthew Gentzkow",
      "year": 2011,
      "role": "Belief-based representation of information structures",
      "relationship_sentence": "Their splitting-lemma view that signal structures correspond to distributions over posterior beliefs underpins modeling with samples of reported posteriors and enables constructing indistinguishable posterior distributions for hard lower-bound instances."
    },
    {
      "title": "Bayes correlated equilibrium and the comparison of information structures",
      "authors": "Dirk Bergemann, Stephen Morris",
      "year": 2016,
      "role": "Feasibility constraints for joint posterior profiles",
      "relationship_sentence": "Characterization of feasible joint distributions over multiple agents\u2019 posteriors (Bayes-plausibility/martingale constraints) provides the multi-expert consistency conditions used to design families of posterior-profile distributions that are hard to distinguish yet yield different optimal aggregators."
    },
    {
      "title": "Strictly Proper Scoring Rules, Prediction, and Estimation",
      "authors": "Tilmann Gneiting, Adrian E. Raftery",
      "year": 2007,
      "role": "Loss metric and optimality benchmark",
      "relationship_sentence": "Establishes squared error (Brier score) as a strictly proper scoring rule and identifies the Bayes-optimal forecast as the conditional expectation, which is exactly the optimality notion the paper\u2019s aggregation objective seeks to approximate."
    },
    {
      "title": "Combining Probability Distributions: A Critique and an Annotated Bibliography",
      "authors": "Christian Genest, James V. Zidek",
      "year": 1986,
      "role": "Foundations of forecast aggregation",
      "relationship_sentence": "Provides canonical opinion-pooling approaches and clarifies the role of dependence among experts, motivating a distribution-free, data-driven view of aggregation whose learnability (sample complexity) the paper investigates."
    },
    {
      "title": "Reaching a Consensus",
      "authors": "Morris H. DeGroot",
      "year": 1974,
      "role": "Classical Bayesian consensus model under quadratic loss",
      "relationship_sentence": "Supplies the foundational Bayesian framework of experts with private signals and quadratic loss aggregation, which the paper adopts and extends by asking how many samples suffice to achieve near-optimal aggregation."
    },
    {
      "title": "Introduction to Nonparametric Estimation",
      "authors": "Alexandre B. Tsybakov",
      "year": 2009,
      "role": "Information-theoretic lower-bound toolkit (Fano/Assouad methods)",
      "relationship_sentence": "Provides the standard minimax lower-bound techniques used to construct packings of distributions and translate indistinguishability into sample complexity lower bounds, adapted here to posterior-profile distributions."
    },
    {
      "title": "Verification of Forecasts Expressed in Terms of Probability",
      "authors": "Glenn W. Brier",
      "year": 1950,
      "role": "Origin of squared-error (Brier) score for probabilistic forecasts",
      "relationship_sentence": "Introduces the evaluation metric minimized in the paper, directly linking the aggregation task to Brier-risk minimization."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014tight sample-complexity lower bounds for learning near-optimal forecast aggregation from samples of reported beliefs\u2014rests on two intellectual pillars: a belief-based representation of information and information-theoretic lower-bound techniques. Kamenica and Gentzkow\u2019s Bayesian Persuasion reframes signal structures as distributions over posteriors, while Bergemann and Morris extend this to multi-agent environments via Bayes-correlated feasibility constraints. Together, these works justify treating each sample as a posterior-profile/outcome tuple and provide the consistency constraints needed to construct hard families of posterior distributions across experts. On the objective side, Brier (1950) and Gneiting\u2013Raftery (2007) ground the choice of squared loss and characterize the Bayes-optimal aggregator as the conditional expectation, furnishing the precise optimality benchmark for the learning task. Classical aggregation frameworks from DeGroot (quadratic-loss consensus) and Genest\u2013Zidek (opinion pooling under dependence) supply the canonical Bayesian and statistical context, clarifying why one cannot rely on a fixed pooling rule when dependence and information structure are unknown, hence motivating a distribution-free learning viewpoint. The lower-bound methodology is powered by Tsybakov\u2019s Fano/Assouad toolkit: by crafting packs of posterior-profile distributions that satisfy Bayes-plausibility (multi-expert martingale constraints) yet induce distinct optimal aggregators, the authors translate indistinguishability into a minimax lower bound that scales as ~\u03a9(m^{n\u22122}/\u03b5). This synthesis of belief-based representation, proper scoring, and information-theoretic techniques directly enables the paper\u2019s core sample-complexity result.",
  "analysis_timestamp": "2026-01-06T23:42:49.137206"
}
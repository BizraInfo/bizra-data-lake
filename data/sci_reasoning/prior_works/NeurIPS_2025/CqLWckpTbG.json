{
  "prior_works": [
    {
      "title": "WebGPT: Browser-assisted question-answering with human feedback",
      "authors": "Reiichiro Nakano et al.",
      "year": 2021,
      "role": "RLHF for web browsing QA",
      "relationship_sentence": "WebGPT established an RL paradigm over a browser action space with citation-aware rewards, directly informing DeepDiver\u2019s use of reinforcement learning to control live web search actions and reward correctness with evidence."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "Reason\u2013act interleaving for tool use",
      "relationship_sentence": "ReAct\u2019s interleaving of thoughts with tool calls underpins DeepDiver\u2019s policy of alternating reflection with search, enabling the agent to decide when to issue additional queries versus continue reasoning."
    },
    {
      "title": "Self-Ask: A Simple Framework for Multi-Step Reasoning (with Search)",
      "authors": "Ofir Press et al.",
      "year": 2022,
      "role": "Question decomposition with external search",
      "relationship_sentence": "Self-Ask\u2019s strategy of decomposing questions and invoking web search for sub-questions motivates DeepDiver\u2019s escalation of search frequency and depth when single-pass evidence is insufficient."
    },
    {
      "title": "Self-RAG: Learning to Retrieve, Generate, and Critique for Language Models",
      "authors": "Akari Asai et al.",
      "year": 2023,
      "role": "Learned retrieval scheduling with critique loops",
      "relationship_sentence": "Self-RAG\u2019s learned gating for retrieval and self-critique informs DeepDiver\u2019s SIS policy, where the model learns when to retrieve more evidence and when to proceed, guided by verifiability-oriented rewards."
    },
    {
      "title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
      "authors": "Noah Shinn et al.",
      "year": 2023,
      "role": "Self-reflection to improve multi-step agents",
      "relationship_sentence": "Reflexion demonstrates how retrospective signals can guide future actions, inspiring DeepDiver\u2019s reflective reasoning to trigger deeper search when earlier attempts yield uncertain or unsupported answers."
    },
    {
      "title": "Adaptive Computation Time for Recurrent Neural Networks",
      "authors": "Alex Graves",
      "year": 2016,
      "role": "Adaptive halting/dynamic depth",
      "relationship_sentence": "ACT provides the conceptual basis for dynamically allocating computation, directly paralleling DeepDiver\u2019s Search Intensity Scaling that adaptively increases the number and depth of search steps based on uncertainty."
    },
    {
      "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
      "authors": "Timo Schick et al.",
      "year": 2023,
      "role": "Learning when to call external tools",
      "relationship_sentence": "Toolformer\u2019s mechanism for learning tool-call triggers influences DeepDiver\u2019s action-space design and the learning of calibrated decisions about when to issue live web queries versus rely on internal knowledge."
    }
  ],
  "synthesis_narrative": "DeepDiver\u2019s core idea\u2014Search Intensity Scaling (SIS) learned via reinforcement learning to adaptively escalate web-search frequency and depth\u2014sits at the intersection of three lines of prior work: browser-based QA with RL, reasoning\u2013tool interleaving, and adaptive computation. WebGPT pioneered reinforcement learning over a browser environment with citation-sensitive rewards, demonstrating that RL can govern open-web actions for QA. Building on this, ReAct provided the control structure to interleave reasoning and tool use, enabling policies that decide when to think versus act, while Self-Ask showed that complex questions benefit from iterative decomposition and repeated search calls when single-shot retrieval is inadequate.\nSelf-RAG advanced this further by learning retrieval scheduling and critique loops, offering a template for verifiability-driven decision gates\u2014precisely the kind of learned trigger DeepDiver needs to scale search intensity under uncertainty. Reflexion contributed the idea that agents should use reflective feedback from prior steps to guide future exploration, which naturally supports DeepDiver\u2019s decision to deepen searches after weak or unsupported intermediate findings. Complementing these, Adaptive Computation Time gave the foundational principle for dynamically allocating computational steps\u2014transposed here into dynamically allocating web-search steps. Finally, Toolformer\u2019s demonstration that models can learn when to invoke external tools informs DeepDiver\u2019s calibrated action-space for live web queries. Together, these works directly enable DeepDiver\u2019s RL-trained policy that adaptively increases search depth and frequency on the live internet, moving beyond fixed prompts or static SFT corpora.",
  "analysis_timestamp": "2026-01-07T00:21:32.244327"
}
{
  "prior_works": [
    {
      "title": "A Mean-Field Model for Transformers",
      "authors": "Boris Geshkovski et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "The present work directly builds on the continuous-depth, unit-sphere mean-field PDE for token evolution introduced here, extending that framework by analyzing its long-time behavior to prove persistence near meta-stable structured manifolds and clustering."
    },
    {
      "title": "The Variational Formulation of the Fokker\u2013Planck Equation",
      "authors": "Richard Jordan et al.",
      "year": 1998,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This paper provides the variational/Wasserstein gradient-flow formulation of diffusion-type PDEs that the current work leverages to interpret the transformer mean-field equation and to use energy/entropy methods in its analysis."
    },
    {
      "title": "Gradient Flows: In Metric Spaces and in the Space of Probability Measures",
      "authors": "Luigi Ambrosio et al.",
      "year": 2005,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "The metric-space gradient-flow theory developed here underpins the well-posedness, stability, and Lyapunov framework the current paper uses to study the transformer mean-field PDE as a Wasserstein gradient flow."
    },
    {
      "title": "On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport",
      "authors": "L\u00e9na\u00efc Chizat et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "By casting neural network training dynamics as Wasserstein gradient flows over measures, this work provided the measure-theoretic calculus and variational techniques that inspired the current paper\u2019s perturbative and energy-based analysis of the attention mean-field PDE."
    },
    {
      "title": "A Mean Field View of the Landscape of Two-Layer Neural Networks",
      "authors": "Song Mei et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This paper\u2019s derivation of interacting-particle PDEs for neural networks and its linearization/perturbative analysis around isotropic initialization inform the present work\u2019s perturbative approach around i.i.d. uniform initialization and large-token mean-field limit."
    },
    {
      "title": "Mean Field Analysis of Neural Networks: A Law of Large Numbers",
      "authors": "Justin Sirignano et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Their rigorous propagation-of-chaos and LLN results justify passing to the mean-field limit for large populations, directly enabling the present paper\u2019s asymptotic analysis as the number of tokens grows."
    },
    {
      "title": "Metastability: A Potential-Theoretic Approach",
      "authors": "Anton Bovier et al.",
      "year": 2015,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "This monograph\u2019s techniques for characterizing metastable behavior in interacting systems guide the current work\u2019s identification and persistence analysis of meta-stable manifolds in the transformer mean-field dynamics."
    }
  ],
  "synthesis_narrative": "A recent line of work formalized self-attention as an interacting-particle system, with Geshkovski et al. introducing a continuous-depth mean-field PDE on the unit sphere for token evolution and exposing its variational structure. The variational interpretation traces to Jordan, Kinderlehrer, and Otto, who showed how Fokker\u2013Planck dynamics arise as gradient flows of entropy in Wasserstein space, providing an energy/entropy lens to study long-time behavior. Ambrosio, Gigli, and Savar\u00e9 extended this into a general metric-space theory, yielding conditions for well-posedness, contraction, and Lyapunov functionals that are essential when treating the attention-induced PDE as a Wasserstein gradient flow. In parallel, Chizat and Bach cast neural-network training dynamics as optimal-transport gradient flows, supplying measure-theoretic calculus and variational techniques that carry over to attention\u2019s mean-field setting. Mei, Montanari, and Nguyen developed interacting-particle PDEs for neural nets and analyzed perturbations around isotropic initializations, offering a template for linearization and spectral arguments in the mean-field regime. Sirignano and Spiliopoulos supplied rigorous law-of-large-numbers and propagation-of-chaos results to justify the passage from finite particles to mean-field limits. Finally, the potential-theoretic perspective on metastability synthesized by Bovier and den Hollander offers tools to detect and quantify metastable behavior in high-dimensional interacting systems. Together, these works define a precise attention mean-field PDE, equip it with a Wasserstein gradient-flow structure, and provide perturbative and asymptotic toolkits, yet stop short of characterizing metastable manifolds and clustering in transformer dynamics. Building on that foundation, the present paper extends the Geshkovski framework with a perturbative analysis around uniform initialization, using the optimal-transport gradient-flow toolkit and mean-field limit theory to prove persistence near structured (e.g., periodic) metastable manifolds and to explicitly identify the clustering structures that emerge at large token counts.",
  "target_paper": {
    "title": "Emergence of meta-stable clustering in mean-field transformer models",
    "authors": "Giuseppe Bruno, Federico Pasqualotto, Andrea Agazzi",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Mean-field limits, Transformers, Meta-stability, Clustering",
    "abstract": "We model the evolution of tokens within a deep stack of Transformer layers as a continuous-time flow on the unit sphere, governed by a mean-field interacting particle system, building on the framework introduced in Geshkovski et al. (2023). Studying the corresponding mean-field Partial Differential Equation (PDE), which can be interpreted as a Wasserstein gradient flow, in this paper we provide a mathematical investigation of the long-term behavior of this system, with a particular focus on the emergence and persistence of meta-stable phases and clustering phenomena, key elements in applications like next-token prediction. More specifically, we perform a perturbative analysis of the mean-field PDE around the iid uniform initialization and prove that, in the limit of large number of tokens, the model remains close to a meta-stable manifold of solutions with a given structure (e.g., periodicity). Further, the structure characterizing the meta-stable manifold is explicitly identified, as ",
    "openreview_id": "eBS3dQQ8GV",
    "forum_id": "eBS3dQQ8GV"
  },
  "analysis_timestamp": "2026-01-06T16:00:12.295518"
}
{
  "prior_works": [
    {
      "title": "LLaVA: Large Language and Vision Assistant",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "Spatial-MLLM adopts the LLaVA-style vision-to-LLM alignment (connector) but departs from its single CLIP-based semantic encoder by introducing a second, geometry-initialized spatial encoder to address spatial reasoning."
    },
    {
      "title": "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models",
      "authors": "Muhammad Maaz et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "As a representative CLIP-based video MLLM optimized for semantics, Video-ChatGPT exhibits limited fine-grained spatial reasoning, directly motivating Spatial-MLLM\u2019s shift to explicitly encode 3D structure features rather than relying solely on semantic embeddings."
    },
    {
      "title": "3D-LLM: Injecting the 3D World into Large Language Models",
      "authors": "Wang et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "3D-LLM achieves spatial understanding by consuming explicit 3D/2.5D inputs, and Spatial-MLLM is designed specifically to remove this dependency by extracting 3D structure priors from 2D observations via a geometry foundation model."
    },
    {
      "title": "DUSt3R: Geometric 3D Vision Made Easy",
      "authors": "Jerome Revaud et al.",
      "year": 2024,
      "role": "Inspiration",
      "relationship_sentence": "DUSt3R demonstrated that feed-forward visual geometry models learn strong 3D structure from 2D images; Spatial-MLLM leverages this insight by repurposing a geometry model\u2019s backbone as a 3D spatial encoder to inject structural priors into an MLLM."
    },
    {
      "title": "MASt3R: A Unified Pre-training for Matching, Alignment and Reconstruction",
      "authors": "Vincent Leroy et al.",
      "year": 2024,
      "role": "Extension",
      "relationship_sentence": "Spatial-MLLM initializes its 3D spatial encoder from the backbone of a visual geometry foundation model in the MASt3R/DUSt3R family, extending it to produce language-alignable 3D structure features for spatial reasoning from 2D inputs."
    },
    {
      "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
      "authors": "Justin Johnson et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "CLEVR formalized core spatial relation reasoning (e.g., left/right, behind/in front), providing the foundational problem formulation that Spatial-MLLM targets in realistic image/video settings using geometry-aware features."
    }
  ],
  "synthesis_narrative": "Spatial-MLLM\u2019s core innovation\u2014fusing semantic and geometry-aware features to enable spatial reasoning from purely 2D inputs\u2014emerged from two converging lines of prior work. First, LLaVA and its video counterparts (e.g., Video-ChatGPT) established the now-standard connector-based MLLM pipeline but relied on CLIP-style encoders that excel at semantics while struggling with spatial relations. Their limitations crystallized the need for an explicit spatial pathway, directly motivating Spatial-MLLM\u2019s dual-encoder design. Second, the recent surge of feed-forward visual geometry foundation models, exemplified by DUSt3R and MASt3R, showed that robust 3D structure priors can be learned from 2D imagery at scale. Spatial-MLLM repurposes this geometry backbone as a 3D spatial encoder, integrating its structure features with semantic embeddings inside the MLLM\u2014thereby capturing 3D-aware cues without requiring depth, point clouds, or meshes at inference. In parallel, 3D-LLM demonstrated that injecting explicit 3D data into LLMs boosts spatial understanding, but its dependence on 3D/2.5D inputs limited applicability; Spatial-MLLM addresses precisely this gap by extracting 3D structure from 2D views. Finally, CLEVR\u2019s canonical formulation of spatial relations provides the conceptual foundation for the kinds of queries Spatial-MLLM aims to solve. Together, these works directly shaped Spatial-MLLM\u2019s insight: marry semantic vision features with a geometry-initialized encoder to unlock spatial intelligence in MLLMs using only 2D observations.",
  "analysis_timestamp": "2026-01-06T23:08:23.961350"
}
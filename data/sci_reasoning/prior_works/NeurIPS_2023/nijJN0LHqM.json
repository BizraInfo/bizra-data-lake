{
  "prior_works": [
    {
      "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization",
      "authors": "Pierre Foret, Ariel Kleiner, Hossein Mobahi, Behnam Neyshabur",
      "year": 2021,
      "role": "Algorithm origin",
      "relationship_sentence": "Introduced SAM with the practical recipe of perturbing in the normalized-gradient direction with a fixed radius, which is exactly the configuration whose convergence (or lack thereof) this paper analyzes."
    },
    {
      "title": "Towards Understanding Sharpness-Aware Minimization",
      "authors": "Maximilian Andriushchenko, Nicolas Flammarion",
      "year": 2022,
      "role": "Early SAM theory and analysis",
      "relationship_sentence": "Provided theoretical insights and analyses of SAM under simplified/modified settings (e.g., alternative perturbation rules or radii), helping frame the gap that this paper closes by proving (non-)convergence results for the practical constant-radius, normalized-perturbation SAM."
    },
    {
      "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima",
      "authors": "Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang",
      "year": 2017,
      "role": "Motivation via sharpness-generalization link",
      "relationship_sentence": "Established the connection between sharpness of minima and generalization, motivating SAM\u2019s objective of controlling sharpness and thereby motivating a careful study (as in this paper) of what the practical SAM dynamics actually converge to."
    },
    {
      "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
      "authors": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu",
      "year": 2018,
      "role": "Min\u2013max/robust optimization viewpoint",
      "relationship_sentence": "Framed training as a min\u2013max optimization over worst-case perturbations; SAM adopts a parameter-space worst-case perturbation heuristic, and this paper\u2019s analysis leverages that min\u2013max/robust perspective to study convergence under a fixed-radius inner maximization proxy."
    },
    {
      "title": "How to Escape Saddle Points Efficiently",
      "authors": "Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M. Kakade, Michael I. Jordan",
      "year": 2017,
      "role": "Perturbation-based convergence baseline",
      "relationship_sentence": "Showed that (randomly) perturbed gradient methods can reach second-order stationary points, serving as a foil for this paper\u2019s negative results that a deterministic, normalized perturbation (as in practical SAM) may still fail to converge to stationary points."
    },
    {
      "title": "Stochastic First- and Zeroth-Order Methods for Nonconvex Stochastic Programming",
      "authors": "Saeed Ghadimi, Guanghui Lan",
      "year": 2013,
      "role": "Stochastic optimization rates baseline",
      "relationship_sentence": "Provided canonical convergence rates and stationarity guarantees for stochastic first-order methods, which this paper contrasts with by showing that stochastic SAM (with practical constant radius and normalization) suffers degraded convergence/steady-state behavior."
    }
  ],
  "synthesis_narrative": "The key contribution of Si and Yun (NeurIPS 2023) is a principled convergence analysis of practical SAM\u2014specifically, the variant that perturbs parameters in the normalized gradient direction with a fixed radius\u2014revealing fundamental limitations: in many smooth settings it does not converge all the way to global minima or even stationary points. This contribution is anchored in the original SAM formulation by Foret et al. (2021), whose practical design choices (constant radius and gradient normalization) are precisely the focus of the new analysis. Earlier theoretical efforts, notably Andriushchenko and Flammarion (2022), analyzed SAM-like procedures but typically under altered assumptions (e.g., decaying radii or modified perturbations), leaving a gap between theory and practice; the present work closes this gap by proving sharp (non-)convergence statements for the truly practical configuration.\nConceptually, SAM\u2019s min\u2013max flavor traces to adversarial/robust optimization (Madry et al., 2018), where inner worst-case perturbations shape the outer descent step; the paper formalizes how fixing the perturbation radius and using a normalized direction changes the dynamics and can preclude convergence. The results are further contextualized by classic baselines: perturbation-based guarantees for escaping saddles (Jin et al., 2017) underscore that not all perturbations are equal\u2014deterministic, normalized SAM perturbations may still stagnate\u2014while standard stochastic first-order theory (Ghadimi and Lan, 2013) provides reference rates that highlight how stochastic SAM\u2019s convergence degrades. Finally, the broader motivation from sharpness\u2013generalization (Keskar et al., 2017) explains why practitioners adopted SAM, making it especially important to understand the precise convergence behavior of the practical variant analyzed here.",
  "analysis_timestamp": "2026-01-07T00:02:04.781535"
}
{
  "prior_works": [
    {
      "title": "Embodied Question Answering",
      "authors": "Abhishek Das et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work defined the EQA problem\u2014an agent must navigate a 3D environment to answer a question\u2014which EQA-MX directly extends by incorporating deictic nonverbal cues (gaze/pointing) and multi-perspective inputs into the same embodied QA formulation."
    },
    {
      "title": "Neural Modular Control for Embodied Question Answering",
      "authors": "Daniel Gordon et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "As a leading EQA modeling approach, this paper\u2019s modular perception\u2013navigation\u2013answering pipeline serves as a primary baseline framework that EQA-MX augments by injecting explicit gesture channels and multimodal expression reasoning."
    },
    {
      "title": "ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes",
      "authors": "Panos Achlioptas et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By showing that referring expressions in 3D are viewpoint-sensitive and require grounding to scene geometry, this work motivates EQA-MX\u2019s multi-view visual perspectives and deictic language (\u201cthis/that\u201d) disambiguation in embodied QA."
    },
    {
      "title": "ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language",
      "authors": "Dave Zhenyu Chen et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "This paper\u2019s technique for aligning natural language to 3D objects underscores the need for precise cross-modal grounding that EQA-MX generalizes to embodied question answering with additional nonverbal signals."
    },
    {
      "title": "Where are they looking?",
      "authors": "Adria Recasens et al.",
      "year": 2015,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By establishing gaze as a reliable cue for locating referents in images, this work directly inspires EQA-MX\u2019s use of eye gaze as a nonverbal deictic signal to resolve ambiguous references in embodied questions."
    },
    {
      "title": "SQA3D: Situated Question Answering in 3D Scenes",
      "authors": "Fei Xia et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work formalized QA grounded in reconstructed 3D scenes, providing the situated-3D QA framing that EQA-MX scales to interactive embodied settings with multi-view and multimodal expressions."
    },
    {
      "title": "ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks",
      "authors": "Mohit Shridhar et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By demonstrating embodied language understanding with only verbal instructions and no nonverbal cues, ALFRED highlights the limitation EQA-MX addresses by introducing gestures and gaze into embodied QA."
    }
  ],
  "synthesis_narrative": "Embodied Question Answering introduced the core idea of an agent navigating a 3D world to answer questions, crystallizing the embodied QA formulation that subsequent methods adopted. Neural Modular Control for EQA operationalized this with a modular perception\u2013navigation\u2013answering pipeline, a practical template for building embodied QA agents. In parallel, ReferIt3D showed that referring expressions in 3D are intrinsically viewpoint-sensitive and demand explicit grounding to geometric context, while ScanRefer demonstrated effective alignment of language to objects in RGB-D scans, reinforcing the need for precise cross-modal representations in 3D. Earlier, Where are they looking? established human gaze as a strong deictic cue for localizing targets, pointing toward the utility of nonverbal signals in resolving ambiguous references like \u201cthat.\u201d SQA3D formalized situated QA over reconstructed 3D scenes, shifting focus from flat images to spatially coherent environments. ALFRED expanded embodied language understanding to longer-horizon tasks but remained text-only, revealing a missing piece: nonverbal communication.\nTogether these works expose an opportunity: embodied QA agents that reason not just over language and 3D scenes, but also over deictic gestures and multiple visual perspectives to disambiguate references. EQA-MX emerges as the natural synthesis\u2014extending the EQA and situated 3D QA formulations with gaze/pointing signals and multi-view inputs, and learning representations tailored to deictic, viewpoint-dependent expressions\u2014while benchmarking against modular EQA pipelines and 3D grounding techniques they directly build upon.",
  "target_paper": {
    "title": "EQA-MX: Embodied Question Answering using Multimodal Expression",
    "authors": "Md Mofijul Islam, Alexi Gladstone, Riashat Islam, Tariq Iqbal",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "multimodal representation learning, visual-language models, embodied question answering",
    "abstract": "Humans predominantly use verbal utterances and nonverbal gestures (e.g., eye gaze and pointing gestures) in their natural interactions. For instance, pointing gestures and verbal information is often required to comprehend questions such as \"what object is that?\" Thus, this question-answering (QA) task involves complex reasoning of multimodal expressions (verbal utterances and nonverbal gestures). However, prior works have explored QA tasks in non-embodied settings, where questions solely contain verbal utterances from a single verbal and visual perspective. In this paper, we have introduced 8 novel embodied question answering (EQA) tasks to develop learning models to comprehend embodied questions with multimodal expressions. We have developed a novel large-scale dataset, EQA-MX, with over 8 million diverse embodied QA data samples involving multimodal expressions from multiple visual and verbal perspectives. To learn salient multimodal representations from discrete verbal embeddings a",
    "openreview_id": "7gUrYE50Rb",
    "forum_id": "7gUrYE50Rb"
  },
  "analysis_timestamp": "2026-01-06T11:10:48.140290"
}
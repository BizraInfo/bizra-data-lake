{
  "prior_works": [
    {
      "title": "A Theory of Regularized Markov Decision Processes",
      "authors": "Matthieu Geist et al.",
      "year": 2019,
      "arxiv_id": "1901.08508",
      "role": "Foundation",
      "relationship_sentence": "Dual RL directly builds on the convex-dual view of RL from Geist et al., using f-regularized MDP duality to express RL/IL as unconstrained dual objectives over value functions and policies."
    },
    {
      "title": "Generative Adversarial Imitation Learning",
      "authors": "Jonathan Ho et al.",
      "year": 2016,
      "arxiv_id": "1606.03476",
      "role": "Foundation",
      "relationship_sentence": "Dual RL generalizes GAIL\u2019s occupancy-measure matching objective\u2014originally implemented via a discriminator-driven min\u2013max\u2014and identifies this adversarial machinery as unnecessary under the dual formulation that ReCOIL exploits."
    },
    {
      "title": "DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections",
      "authors": "Ofir Nachum et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Dual RL adopts DualDICE\u2019s density-ratio\u2013based dual lens on off-policy objectives to analyze when off-policy IL is sound and to formalize the support-overlap assumption at the heart of ratio-learning approaches."
    },
    {
      "title": "ValueDICE: Stabilizing Off-Policy Imitation Learning",
      "authors": "Ilya Kostrikov et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "By relying on learning discounted occupancy ratios from arbitrary off-policy data, ValueDICE exposes the restrictive coverage assumption that Dual RL pinpoints as the failure mode and eliminates with ReCOIL\u2019s discriminator-free objective."
    },
    {
      "title": "Conservative Q-Learning for Offline Reinforcement Learning",
      "authors": "Aviral Kumar et al.",
      "year": 2020,
      "arxiv_id": "2006.04779",
      "role": "Baseline",
      "relationship_sentence": "Dual RL shows CQL\u2019s pessimistic value regularization is a special case of its dual objective with a particular regularizer, anchoring the paper\u2019s unification of offline RL methods."
    },
    {
      "title": "Offline Reinforcement Learning with Implicit Q-Learning",
      "authors": "Ilya Kostrikov et al.",
      "year": 2022,
      "arxiv_id": "2110.06169",
      "role": "Baseline",
      "relationship_sentence": "Dual RL casts IQL\u2019s expectile value learning and advantage-weighted policy extraction as optimizing a divergence-regularized dual objective, subsuming the method in the unified framework that informs ReCOIL\u2019s updates."
    },
    {
      "title": "Accelerating Online Reinforcement Learning with Offline Datasets",
      "authors": "Ashvin Nair et al.",
      "year": 2020,
      "arxiv_id": "2006.09359",
      "role": "Related Problem",
      "relationship_sentence": "AWAC\u2019s exponentiated advantage-weighted policy update arises as a KL-regularized dual solution, which Dual RL identifies as a canonical instance within its family and uses to connect IL-style weighting to dual RL structure."
    }
  ],
  "synthesis_narrative": "Regularized MDP theory established that many policy optimization procedures can be viewed through a convex-dual lens, where primal occupancy-measure problems correspond to unconstrained dual objectives over value functions and policies; this f-regularized perspective precisely links divergences and policy updates. Building on occupancy-measure matching, Generative Adversarial Imitation Learning framed imitation as distribution alignment via a discriminator optimizing a JS-divergence, operationalizing the idea but at the cost of unstable min\u2013max training. DualDICE advanced a behavior-agnostic, dual formulation by estimating discounted occupancy ratios to enable off-policy evaluation/control, clarifying that correctness hinges on support overlap between data and target distributions. ValueDICE adapted this density-ratio view to stabilize off-policy imitation with value-based learning, yet still critically depended on coverage through ratio learning. In offline RL, Conservative Q-Learning introduced pessimism via conservative value regularization to mitigate extrapolation error from out-of-distribution actions. Implicit Q-Learning showed that expectile value learning with advantage-weighted extraction can avoid explicit behavior modeling while remaining robust offline. AWAC further connected advantage-weighted updates to KL-regularized objectives, tying policy updates to divergence-regularized dual solutions. Together, these works reveal that both imitation and offline RL can be expressed as dual objectives over values/policies, while highlighting a central weakness of off-policy IL rooted in density-ratio/coverage assumptions. The natural next step is to formalize a unifying dual RL framework that subsumes these algorithms, diagnose the coverage-driven failures of ratio/discriminator approaches, and introduce a discriminator-free imitation method that borrows the stable, divergence-regularized updates seen in offline RL\u2014precisely the synthesis that enables effective imitation from arbitrary off-policy data.",
  "target_paper": {
    "title": "Dual RL: Unification and New Methods for Reinforcement and Imitation Learning",
    "authors": "Harshit Sikchi, Qinqing Zheng, Amy Zhang, Scott Niekum",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Robot Learning, Offline Imitation Learning, Offline Reinforcement Learning, Deep Reinforcement Learning",
    "abstract": "The goal of reinforcement learning (RL) is to find a policy that maximizes the expected cumulative return. It has been shown that this objective can be represented as an optimization problem of state-action visitation distribution under linear constraints. The dual problem of this formulation, which we refer to as *dual RL*, is unconstrained and easier to optimize. In this work, we first cast several state-of-the-art offline RL and offline imitation learning (IL) algorithms as instances of dual RL approaches with shared structures. Such unification allows us to identify the root cause of the shortcomings of prior methods. For offline IL, our analysis shows that prior methods are based on a restrictive coverage assumption that greatly limits their performance in practice. To fix this limitation, we propose a new discriminator-free method ReCOIL that learns to imitate from arbitrary off-policy data to obtain near-expert performance. For offline RL, our analysis frames a recent offline RL",
    "openreview_id": "xt9Bu66rqv",
    "forum_id": "xt9Bu66rqv"
  },
  "analysis_timestamp": "2026-01-06T08:10:53.123563"
}
{
  "prior_works": [
    {
      "title": "Deep Reinforcement Learning from Human Preferences",
      "authors": "Paul F. Christiano; Jan Leike; Tom B. Brown; Miljan Martic; Shane Legg; Dario Amodei",
      "year": 2017,
      "role": "Introduced preference-based reward models and the notion of optimizing policies against imperfect proxy rewards, highlighting risks like reward hacking.",
      "relationship_sentence": "The paper\u2019s core problem\u2014overoptimization of misspecified reward models at inference time\u2014directly builds on the preference-modeling paradigm and reward-misspecification concerns introduced by Christiano et al."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang; Jeff Wu; Xu Jiang; Diogo Almeida; Carroll L. Wainwright; Pamela Mishkin; Chong Zhang; Sandhini Agarwal; Katarina Slama; Alex Ray; et al.",
      "year": 2022,
      "role": "Established KL-regularized RLHF for LLMs and popularized the reward\u2013KL divergence objective with a reference policy.",
      "relationship_sentence": "The paper\u2019s analysis of the optimal reward\u2013KL policy and its inference-time approximation (BoP) explicitly leverages the KL-regularized objective structure operationalized by InstructGPT."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai; Andy Jones; Kamal Ndousse; Amanda Askell; Anna Chen; Nova DasSarma; Dawn Drain; Saurav Kadavath; Jackson Kernion; Tom Henighan; et al.",
      "year": 2022,
      "role": "Demonstrated inference-time alignment via rejection sampling and re-ranking with a preference/reward model (a practical Best-of-n scheme).",
      "relationship_sentence": "The characterization of reward hacking under Best-of-n and the mitigation strategies in this paper directly extend the inference-time reranking approach used in Constitutional AI."
    },
    {
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "authors": "Rafael Rafailov; Aviral Kumar; Tianhe Yu; Chelsea Finn",
      "year": 2023,
      "role": "Showed that optimizing a reward\u2013KL objective yields a closed-form exponential tilting of the reference policy, connecting preferences to an implicit optimal policy.",
      "relationship_sentence": "This paper\u2019s Best-of-Poisson (BoP) is motivated by the same exponential-tilted optimal policy implied by reward\u2013KL, seeking a near-exact inference-time approximation of that distribution."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang; Jason Wei; Dale Schuurmans; Quoc V. Le; Ed H. Chi; Sharan Narang; Aakanksha Chowdhery; Denny Zhou",
      "year": 2023,
      "role": "Popularized sampling multiple candidates and selecting/aggregating at inference time to boost performance (an operational Best-of-n framing).",
      "relationship_sentence": "The paper formalizes how Best-of-n and its soft variant behave under proxy rewards, extending self-consistency/BoN-style inference-time selection to a reward-hacking analysis."
    },
    {
      "title": "The Gumbel-Top-k Trick for Discrete Distributions",
      "authors": "Wouter Kool; Herke van Hoof; Max Welling",
      "year": 2019,
      "role": "Connected maxima/selection among samples to extreme-value/perturb-and-MAP views via Gumbel tricks, enabling principled approximations to top-k selection.",
      "relationship_sentence": "The Best-of-Poisson method leverages similar extreme-value/point-process intuitions for modeling the distribution of top-reward samples, grounding a Poisson-based approximation to Best-of-n."
    },
    {
      "title": "Concrete Problems in AI Safety",
      "authors": "Dario Amodei; Chris Olah; Jacob Steinhardt; Paul Christiano; John Schulman; Dan Man\u00e9",
      "year": 2016,
      "role": "Framed specification gaming and reward hacking as central safety failures when optimizing imperfect proxies.",
      "relationship_sentence": "The paper\u2019s formal characterization of inference-time reward hacking and its hedging-based mitigations directly address the specification-gaming failure modes highlighted by Amodei et al."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014formalizing reward hacking in inference-time alignment and introducing Best-of-Poisson (BoP) as a near-exact approximation to the optimal reward\u2013KL policy\u2014rests on three converging lines of prior work. First, Christiano et al. and Ouyang et al. established the modern alignment stack: learning reward models from preferences and optimizing a KL-regularized objective against a reference policy. This framework both enables inference-time reranking and creates the conditions for Goodhart-style failures, as flagged by Amodei et al. Second, practical inference-time alignment methods like Constitutional AI and self-consistency popularized Best-of-n style selection\u2014either via reward-model rejection sampling or multi-sample reranking\u2014demonstrating large empirical gains while implicitly exposing sensitivity to mis-specified proxies. This paper systematizes those procedures, analyzing Best-of-n and a soft variant (SBoN) to reveal characteristic reward-hacking patterns and motivate principled hedging on the proxy reward. Third, theory on the optimal policy under reward\u2013KL (as in DPO) shows that the ideal solution is an exponential tilt of the reference policy. Bridging this theory to practical decoding, the paper draws on extreme-value/perturb-and-select insights typified by the Gumbel-Top-k literature to craft Best-of-Poisson\u2014a Poisson point-process-based sampler that closely matches the exponential-tilted target at inference time. Together, these works directly inform the paper\u2019s diagnosis of reward hacking under inference-time selection and its BoP-based, theoretically grounded mitigation strategy.",
  "analysis_timestamp": "2026-01-06T23:42:48.122441"
}
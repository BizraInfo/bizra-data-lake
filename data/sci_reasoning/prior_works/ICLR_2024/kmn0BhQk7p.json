{
  "prior_works": [
    {
      "title": "Extracting Training Data from Large Language Models",
      "authors": "Nicholas Carlini et al.",
      "year": 2021,
      "arxiv_id": "2012.07805",
      "role": "Gap Identification",
      "relationship_sentence": "By centering LLM privacy risk on memorized training-data extraction, this work defined the prevailing threat model that the current paper explicitly challenges by shifting the focus to inference-based privacy violations from user-provided text."
    },
    {
      "title": "Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures",
      "authors": "Matt Fredrikson et al.",
      "year": 2015,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This paper introduced attribute inference as a concrete privacy harm\u2014recovering sensitive attributes from model outputs\u2014which the current work translates to the LLM setting with natural-language inputs and black-box access."
    },
    {
      "title": "Overlearning Reveals Sensitive Attributes",
      "authors": "Nicholas Carlini et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "By showing models can inadvertently learn and reveal sensitive attributes unrelated to their primary task, this work directly motivates testing whether general-purpose LLMs infer private traits from users\u2019 text at inference time."
    },
    {
      "title": "Private traits and attributes are predictable from digital records of human behavior",
      "authors": "Michal Kosinski et al.",
      "year": 2013,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This study established that rich digital traces enable accurate prediction of personal attributes, providing the foundational insight that the present work operationalizes using LLMs on free-form Reddit text."
    },
    {
      "title": "Classifying Latent User Attributes in Twitter",
      "authors": "Delip Rao et al.",
      "year": 2010,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "It formulated the problem of inferring latent demographics from short social media text, a formulation the current paper adopts while evaluating modern pretrained LLMs across many attributes."
    },
    {
      "title": "Studying User Income through Language, Behaviour and Social Connections on Twitter",
      "authors": "Daniel Preo\u0163iuc-Pietro et al.",
      "year": 2015,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "By demonstrating that income can be predicted from user-level language and behavior, this work directly informs the inclusion and evaluation of income as a sensitive attribute in the LLM-based inference setting."
    },
    {
      "title": "Hierarchical Discriminative Classification for Text-Based Geolocation",
      "authors": "Benjamin P. Wing and Jason Baldridge",
      "year": 2014,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "This paper provided methods and evidence for inferring location from textual signals, which the current paper incorporates as a key attribute when benchmarking LLMs\u2019 inference capabilities."
    }
  ],
  "synthesis_narrative": "Work on model inversion crystallized attribute inference as a concrete privacy harm by showing sensitive features can be reconstructed from model outputs, even when they are not the prediction target. Complementing this, overlearning results revealed that models often internalize and expose private attributes unrelated to their primary task, indicating that inference risks can arise without explicit supervision. Social computing and NLP studies demonstrated that personal traits are predictable from behavioral traces and language: digital records such as Facebook Likes enable accurate profiling of private attributes, short social media posts contain enough linguistic signal to classify latent demographics, income is inferable from user-level language and social behavior, and textual cues alone can reveal a user\u2019s geographic location. In parallel, LLM privacy research largely centered on memorization and extraction of training data, shaping a dominant threat model that emphasized regurgitation over inference.\nTaken together, these strands exposed a gap: while attribute inference is known and language signals are rich, there was no comprehensive assessment of modern pretrained LLMs\u2019 ability to infer multiple sensitive attributes directly from users\u2019 free-form text, nor of the practical threat posed by conversational agents that elicit such signals. The present work synthesizes these insights by operationalizing attribute inference with LLMs on real Reddit profiles, quantifying accuracy and human\u2013model cost/time trade-offs, and demonstrating privacy-invasive questioning strategies that leverage LLMs\u2019 inference strengths.",
  "target_paper": {
    "title": "Beyond Memorization: Violating Privacy via Inference with Large Language Models",
    "authors": "Robin Staab, Mark Vero, Mislav Balunovic, Martin Vechev",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Privacy, Large Language Models",
    "abstract": "Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models\u2019 inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals\u2019 privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to 85% top-1 and 95% top-3 accuracy at a fraction of the cost (100x) and time (240x) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions.",
    "openreview_id": "kmn0BhQk7p",
    "forum_id": "kmn0BhQk7p"
  },
  "analysis_timestamp": "2026-01-06T05:51:18.225456"
}
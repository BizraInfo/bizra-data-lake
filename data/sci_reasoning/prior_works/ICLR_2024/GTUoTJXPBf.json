{
  "prior_works": [
    {
      "title": "Tempered Overfitting in Neural Networks",
      "authors": "Mallinar et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "This paper empirically documented 'tempered overfitting' under label noise, and the current work directly answers it by giving the first rigorous, nuanced analysis (L1 tempered vs. L2 catastrophic) for minimum-\u21132-norm interpolants of two-layer ReLU networks."
    },
    {
      "title": "Benign Overfitting in Linear Regression",
      "authors": "Peter L. Bartlett et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "It formalized the benign-overfitting paradigm for minimum-norm interpolation in linear models, providing the conceptual and technical baseline that this work extends beyond linear predictors to shallow ReLU networks and contrasts with its tempered/catastrophic findings."
    },
    {
      "title": "Harmless (Benign) Interpolation of Noisy Data in Regression",
      "authors": "Adhitya Muthukumar et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "Their precise risk analysis for ridgeless (minimum-norm) linear regression serves as the main comparative template that this paper departs from, by showing qualitatively different behavior for minimum-\u21132-norm ReLU interpolants\u2014bounded L1 but divergent L2 risk."
    },
    {
      "title": "Consistency of Interpolation with Kernel Methods",
      "authors": "Alexander Rakhlin and Xiyu Zhai",
      "year": 2019,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "By establishing when minimum-norm kernel interpolants generalize under noise, this work frames the interpolation-learning question that the current paper resolves in the non-kernel, parameter-norm\u2013regularized ReLU setting with contrasting (tempered vs. catastrophic) outcomes."
    },
    {
      "title": "A Function Space View of Deep ReLU Networks",
      "authors": "John Ongie et al.",
      "year": 2019,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Their function-space and spline characterization of shallow ReLU networks\u2014especially in 1D as linear splines with data-aligned knots\u2014provides the structural lens the current paper exploits to analyze how the minimum-\u21132-norm interpolant propagates label noise between samples."
    },
    {
      "title": "Breaking the Curse of Dimensionality with Convex Neural Networks",
      "authors": "Francis Bach",
      "year": 2017,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work introduced a function-space regularization framework and representer-type insights for shallow ReLU networks, motivating the study of minimum-parameter-norm interpolants that the present paper adopts to derive rigorous overfitting behavior."
    }
  ],
  "synthesis_narrative": "Mallinar and colleagues observed that when neural networks interpolate noisy labels, test error often does not blow up but also does not reach Bayes error, dubbing the phenomenon \u201ctempered overfitting.\u201d In parallel, Bartlett et al. established the formal notion of benign overfitting for minimum-norm interpolants in linear regression, identifying when perfect fit can still yield near-optimal risk. Muthukumar et al. sharpened this in ridgeless linear regression by providing explicit risk characterizations for minimum-norm solutions, clarifying when interpolation remains harmless. For nonparametric interpolants, Rakhlin and Zhai proved consistency guarantees for minimum-norm kernel fits under noise, reinforcing that interpolation need not be pathological. On the representational side, Ongie et al. provided a function-space view of shallow ReLU networks, showing in one dimension they realize linear splines with knots aligned to data, a structure crucial for precise noise propagation analysis. Bach\u2019s convex neural networks perspective supplied representer-style intuition linking parameter norms to function-space regularizers, legitimizing a minimum-parameter-norm lens for shallow ReLU. Together, these threads highlighted a sharp gap: while minimum-norm interpolants can be analyzed rigorously in linear and kernel settings, the overfitting behavior of minimum-\u21132-norm shallow ReLU interpolants under noise was unresolved. Leveraging the 1D spline structure and the minimum-norm perspective, the current work fills this gap, proving that interpolation with shallow ReLUs yields bounded L1 risk yet catastrophic L2 risk, thereby formalizing and refining the tempered-overfitting observation.",
  "target_paper": {
    "title": "Noisy Interpolation Learning with Shallow Univariate ReLU Networks",
    "authors": "Nirmit Joshi, Gal Vardi, Nathan Srebro",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Interpolation Learning, Benign Overfitting, ReLU Networks",
    "abstract": "Understanding how overparameterized neural networks generalize despite perfect interpolation of noisy training data is a fundamental question. Mallinar et. al. (2022) noted that neural networks seem to often exhibit ``tempered overfitting'', wherein the population risk does not converge to the Bayes optimal error, but neither does it approach infinity, yielding non-trivial generalization. However, this has not been studied rigorously.  We provide the first rigorous analysis of the overfiting behaviour of regression with minimum norm ($\\ell_2$ of weights), focusing on univariate two-layer ReLU networks.  We show overfitting is tempered (with high probability) when measured with respect to the $L_1$ loss, but also show that the situation is more complex than suggested by Mallinar et. al., and overfitting is catastrophic with respect to the $L_2$ loss, or when taking an expectation over the training set.",
    "openreview_id": "GTUoTJXPBf",
    "forum_id": "GTUoTJXPBf"
  },
  "analysis_timestamp": "2026-01-06T23:39:57.427541"
}
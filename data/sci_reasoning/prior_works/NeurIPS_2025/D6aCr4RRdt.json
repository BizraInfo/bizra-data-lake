{
  "prior_works": [
    {
      "title": "Arbitrary-stepsize Gradient Descent Converges for Linearly Separable Logistic Regression",
      "authors": "Wu et al.",
      "year": 2024,
      "role": "Immediate predecessor establishing any-stepsize GD convergence in the separable logistic case",
      "relationship_sentence": "The present paper generalizes Wu et al.\u2019s COLT 2024 result\u2014proved via the logistic loss\u2019s self-bounding property\u2014by replacing that key assumption with a Fenchel\u2013Young-based descent argument to obtain any-stepsize convergence for a broad loss family."
    },
    {
      "title": "Learning with Fenchel\u2013Young Losses",
      "authors": "Mathieu Blondel, Andr\u00e9 F. T. Martins, Vlad Niculae",
      "year": 2019,
      "role": "Loss-class framework and key properties used to define the target family",
      "relationship_sentence": "This work formalizes Fenchel\u2013Young losses and their convex-analytic structure, supplying the class and toolkit (Fenchel conjugates, gradients, calibration) that the new paper leverages to state and prove any-stepsize GD convergence beyond logistic loss."
    },
    {
      "title": "The Implicit Bias of Gradient Descent on Separable Data",
      "authors": "Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, Nathan Srebro",
      "year": 2018,
      "role": "Foundational analysis of GD dynamics in the separable regime",
      "relationship_sentence": "By characterizing GD\u2019s behavior on separable data (e.g., max-margin direction under logistic loss), this paper provides the separability setting and asymptotic lens that the new work maintains while altering the stepsize regime and loss class."
    },
    {
      "title": "Risk and Parameter Convergence of Logistic Regression",
      "authors": "Ziwei Ji, Matus Telgarsky",
      "year": 2019,
      "role": "Technical refinements for separable logistic dynamics and rates under step-size constraints",
      "relationship_sentence": "The new paper extends beyond these small-stepsize analyses by proving convergence for arbitrary stepsizes, while inheriting techniques for tracking loss/gradient decay in the separable regime."
    },
    {
      "title": "Convergence of Gradient Descent on Separable Data for General Losses",
      "authors": "Mor Shpigel Nacson, Nathan Srebro, Daniel Soudry",
      "year": 2019,
      "role": "Generalization beyond logistic via loss-tail assumptions (typically requiring stable stepsizes)",
      "relationship_sentence": "This line shows how loss-shape assumptions yield separable GD convergence but within stable stepsizes; the new work removes those stepsize restrictions and replaces tail/self-boundedness with Fenchel\u2013Young structure."
    },
    {
      "title": "Relatively Smooth Convex Optimization by First-Order Methods",
      "authors": "Huan N. Lu, Robert M. Freund, Yurii Nesterov",
      "year": 2018,
      "role": "Technical tool: modified descent lemmas via relative smoothness/Bregman geometry",
      "relationship_sentence": "The paper\u2019s \u2018modified descent\u2019 reasoning echoes relative-smoothness ideas, and the authors effectively exploit convex-analytic structure (akin to Bregman/Fenchel tools) to obtain descent inequalities that hold for any stepsize."
    },
    {
      "title": "Gradient Descent Finds the Gradient Norm Minimizer on the Edge of Stability",
      "authors": "Jeremy Cohen, John Duchi, Andre Wibisono",
      "year": 2021,
      "role": "Conceptual lens on large-stepsize dynamics and edge-of-stability behavior",
      "relationship_sentence": "This work motivates studying GD beyond the stable regime; the new paper contributes by pinpointing loss properties (via Fenchel\u2013Young) that still guarantee convergence at arbitrary stepsizes in separable problems."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014showing gradient descent converges with arbitrary stepsizes on linearly separable data for a broad class of Fenchel\u2013Young losses\u2014builds directly on two pillars: recent any-stepsize analysis for logistic regression and the convex-analytic framework of Fenchel\u2013Young losses. Wu et al. (COLT 2024) established the first any-stepsize convergence result in the separable logistic case using the logistic loss\u2019s self-bounding property to craft a modified descent lemma. The new paper both generalizes and reframes this by replacing self-boundedness with properties inherent to Fenchel\u2013Young losses, enabling a descent inequality that holds for arbitrary stepsizes across this family.\n\nThis generalization is enabled by Blondel\u2013Martins\u2013Niculae\u2019s Fenchel\u2013Young framework, which supplies the precise loss construction and conjugate tools the authors exploit. Foundational analyses of separable GD, especially Soudry et al. (2018) and Ji\u2013Telgarsky (2019), provide the separable-data dynamics, asymptotics, and tracking techniques that the present work adapts to the any-stepsize regime. Prior generalizations beyond logistic (e.g., Nacson\u2013Srebro\u2013Soudry, 2019) identified loss-shape conditions ensuring convergence but typically required stepsizes within the stable range; the new result removes this restriction by tying the argument to Fenchel\u2013Young structure rather than tail/self-bounding assumptions. Finally, insights from the edge-of-stability literature (Cohen\u2013Duchi\u2013Wibisono, 2021) frame why arbitrary stepsizes are both relevant and challenging, while ideas akin to relative smoothness (Lu\u2013Freund\u2013Nesterov, 2018) inform the modified descent reasoning that underpins the proof.",
  "analysis_timestamp": "2026-01-07T00:21:32.302105"
}
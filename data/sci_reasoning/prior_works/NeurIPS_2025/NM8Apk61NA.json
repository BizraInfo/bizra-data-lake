{
  "prior_works": [
    {
      "title": "Poincar\u00e9 Embeddings for Learning Hierarchical Representations",
      "authors": "Maximilian Nickel, Douwe Kiela",
      "year": 2017,
      "role": "Foundational theory for using hyperbolic geometry to encode hierarchies",
      "relationship_sentence": "Established that distances and radii in hyperbolic space naturally encode hierarchical depth, directly motivating HyperET\u2019s use of radius as a controllable handle for multi-granularity alignment."
    },
    {
      "title": "Hyperbolic Neural Networks",
      "authors": "Octavian-Eugen Ganea, Gary B\u00e9cigneul, Thomas Hofmann",
      "year": 2018,
      "role": "Methodological toolkit for deep learning in hyperbolic space",
      "relationship_sentence": "Provided the exp/log maps, gyrovector operations, and Riemannian training machinery HyperET relies on to learn visual-text representations natively in hyperbolic manifolds."
    },
    {
      "title": "Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry",
      "authors": "Maximilian Nickel, Douwe Kiela",
      "year": 2018,
      "role": "Stable modeling of hyperbolic embeddings and radius-depth interpretability",
      "relationship_sentence": "Showed the stability and interpretability of Lorentz-model embeddings where norm/radius correlates with hierarchical levels, underpinning HyperET\u2019s dynamic hyperbolic radius adjustment."
    },
    {
      "title": "Riemannian Adaptive Optimization Methods",
      "authors": "Gary B\u00e9cigneul, Octavian-Eugen Ganea",
      "year": 2019,
      "role": "Optimization for efficient training on manifolds",
      "relationship_sentence": "Introduced Riemannian Adam-style optimizers that make large-scale, compute-efficient hyperbolic training feasible\u2014crucial for HyperET\u2019s efficiency claims."
    },
    {
      "title": "Poincar\u00e9 GloVe: Unsupervised Word Embeddings in Hyperbolic Space",
      "authors": "Adina Tifrea, Gary B\u00e9cigneul, Octavian-Eugen Ganea",
      "year": 2019,
      "role": "Evidence that linguistic hierarchies are well captured in hyperbolic space",
      "relationship_sentence": "Demonstrated that textual semantics are hierarchically organized and better modeled in hyperbolic geometry, motivating HyperET to align visual features to text in a shared hyperbolic space."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford, Jong Wook Kim, et al.",
      "year": 2021,
      "role": "Dominant vision-language pretraining baseline with global alignment",
      "relationship_sentence": "Serves as the primary vision encoder baseline whose lack of multi-granularity alignment motivates HyperET\u2019s hierarchical, hyperbolic alignment mechanism."
    },
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": "Junnan Li, Dongxu Li, Silvio Savarese, Steven C. H. Hoi",
      "year": 2023,
      "role": "MLLM training paradigm focused on efficiency via frozen encoders",
      "relationship_sentence": "Highlights the compute bottlenecks and alignment limitations in MLLMs; HyperET offers an alternative by learning in hyperbolic space with dynamic radius to achieve granular alignment more efficiently."
    }
  ],
  "synthesis_narrative": "HyperET\u2019s core idea\u2014achieving efficient multi-granularity alignment by training in hyperbolic space with dynamically adjusted radii\u2014sits at the intersection of hyperbolic representation theory and multimodal training practice. Nickel and Kiela\u2019s Poincar\u00e9 Embeddings established that hyperbolic geometry intrinsically encodes hierarchies, where distance from the origin reflects depth; their Lorentz-model work further stabilized training and clarified radius\u2013hierarchy semantics. Building on this geometric foundation, Ganea et al.\u2019s Hyperbolic Neural Networks supplied the practical apparatus (exp/log maps, gyrovector arithmetic) to operate neural computations on manifolds, while B\u00e9cigneul and Ganea\u2019s Riemannian adaptive optimizers made such training computationally feasible at scale\u2014key for HyperET\u2019s efficiency target. On the language side, Tifrea et al. showed that lexical semantics are naturally hierarchical and better captured in hyperbolic space, directly motivating HyperET to align visual representations to text within a shared hyperbolic geometry.\nIn parallel, CLIP popularized global image\u2013text alignment but exposed limitations in capturing fine- to coarse-grained correspondences central to MLLMs. BLIP-2 emphasized efficiency by freezing encoders, yet still inherited limited granularity alignment. HyperET synthesizes these threads: it replaces Euclidean/global alignment with a hyperbolic formulation where dynamic radius controls granularity, enabling the visual encoder to align to text across arbitrary levels while leveraging Riemannian optimization for compute-efficient training.",
  "analysis_timestamp": "2026-01-07T00:05:12.550433"
}
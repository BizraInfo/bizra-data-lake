{
  "prior_works": [
    {
      "title": "Identifying natural images from human brain activity",
      "authors": "Kendrick N. Kay; Thomas Naselaris; Ryan J. Prenger; Jack L. Gallant",
      "year": 2008,
      "role": "Foundational voxel-wise encoding model for fMRI",
      "relationship_sentence": "Established linear regression from stimulus features to BOLD as the canonical encoding baseline, which JNE explicitly generalizes beyond by quantifying departures from linearity in the feature-to-BOLD mapping."
    },
    {
      "title": "Encoding and decoding in fMRI",
      "authors": "Thomas Naselaris; Kendrick N. Kay; Shinji Nishimoto; Jack L. Gallant",
      "year": 2011,
      "role": "Conceptual framework for model-based fMRI",
      "relationship_sentence": "Provided the formal framework for building and evaluating encoding models, motivating the need for principled metrics\u2014like JNE\u2014to characterize specific model properties such as nonlinearity."
    },
    {
      "title": "Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream",
      "authors": "Umut G\u00fc\u00e7l\u00fc; Marcel A. J. van Gerven",
      "year": 2015,
      "role": "DNN-to-fMRI alignment via linear readouts",
      "relationship_sentence": "Showed that deep nonlinear features align with fMRI using linear voxel-wise readouts, highlighting that residual nonlinearities may remain in the feature-to-BOLD mapping that JNE seeks to quantify."
    },
    {
      "title": "The feature-weighted receptive field: an interpretable encoding model for complex feature spaces",
      "authors": "Gael M. St-Yves; Thomas Naselaris",
      "year": 2018,
      "role": "Interpretable, largely linear encoding architecture",
      "relationship_sentence": "Demonstrated interpretable, mostly linear mappings from complex feature spaces to BOLD, providing a reference point against which JNE can detect when nonlinear heads add value beyond linear models."
    },
    {
      "title": "Neural Encoding and Decoding with Deep Learning for Dynamic Natural Vision",
      "authors": "Haiguang Wen; Zhenhao Shi; Yong Zhang; Zhongming Liu",
      "year": 2018,
      "role": "Nonlinear deep learning for fMRI encoding/decoding",
      "relationship_sentence": "Showed benefits of nonlinear deep networks for predicting brain activity, directly motivating a rigorous metric like JNE to characterize and compare the degree of nonlinearity in such encoding models."
    },
    {
      "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
      "authors": "Karen Simonyan; Andrea Vedaldi; Andrew Zisserman",
      "year": 2013,
      "role": "Gradient/Jacobian-based local linear explanations",
      "relationship_sentence": "Popularized using gradients (local Jacobians) as local linear approximations for interpretability; JNE extends this idea to representation-to-BOLD mappings and aggregates their dispersion to quantify nonlinearity."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot; Franck Gabriel; Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Theoretical grounding for Jacobian-based local linearization",
      "relationship_sentence": "Formalized neural networks as locally linear via Jacobians (NTK), directly underpinning JNE\u2019s use of local Jacobians and the notion that variability across them reflects function nonlinearity."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014JNE, a Jacobian-based metric that quantifies nonlinearity in neural encoding models by measuring dispersion of local linear mappings\u2014emerges at the intersection of fMRI encoding traditions and Jacobian-centric views of neural networks. Foundational voxel-wise encoding work (Kay et al., 2008) and the broader framework for encoding/decoding (Naselaris et al., 2011) established linear readouts from feature spaces to BOLD as the default, setting the baseline that JNE evaluates and generalizes beyond. As deep neural network features became standard for modeling brain responses (G\u00fc\u00e7l\u00fc & van Gerven, 2015), linear readouts remained prevalent, leaving the extent and locus of nonlinearity between ANN representations and BOLD under-characterized. Interpretable but largely linear architectures like fwRF (St-Yves & Naselaris, 2018) reinforced the need to know when nonlinear heads are justified.\nOn the methodological side, gradient-based interpretability (Simonyan et al., 2013) framed Jacobians as local linear explanations, while the neural tangent kernel (Jacot et al., 2018) formalized network behavior as a collection of local linearizations governed by Jacobians. Nonlinear deep encoding/decoding for naturalistic stimuli (Wen et al., 2018) further underscored the practical need to measure nonlinearity in end-to-end models. JNE synthesizes these strands: it adopts the Jacobian-as-local-linear-map perspective to the representation-to-BOLD function and operationalizes nonlinearity as the statistical dispersion of these Jacobians across stimuli, providing a principled, model-agnostic metric that complements accuracy and interpretability analyses in modern fMRI encoding.",
  "analysis_timestamp": "2026-01-07T00:29:42.070321"
}
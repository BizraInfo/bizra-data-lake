{
  "prior_works": [
    {
      "title": "Finite-time Analysis of the Multiarmed Bandit Problem",
      "authors": "Peter Auer et al.",
      "year": 2002,
      "arxiv_id": "unknown",
      "role": "Baseline",
      "relationship_sentence": "UCB-S is a direct modification of the UCB principle from Auer et al., repurposing confidence-index selection to both learn post-click rewards and embed incentive signals that shape strategic arms\u2019 best responses."
    },
    {
      "title": "Cascading Bandits: Learning to Rank in the Cascade Model",
      "authors": "Branislav Kveton et al.",
      "year": 2015,
      "arxiv_id": "unknown",
      "role": "Foundation",
      "relationship_sentence": "The strategic click-bandit adopts the cascade-style separation between a click probability and a post-click utility, using this decomposition to model arms choosing click-rates while the learner estimates post-click rewards."
    },
    {
      "title": "Multi-armed Bandit Auctions",
      "authors": "Moshe Babaioff et al.",
      "year": 2010,
      "arxiv_id": "unknown",
      "role": "Inspiration",
      "relationship_sentence": "This work pioneered mechanism-design-driven bandit allocation rules that remain incentive compatible while learning unknown CTRs, directly inspiring the idea of embedding incentives into a bandit algorithm that must also learn unknown outcome parameters."
    },
    {
      "title": "Implementing the Wisdom of the Crowd",
      "authors": "Ilia Kremer et al.",
      "year": 2014,
      "arxiv_id": "unknown",
      "role": "Related Problem",
      "relationship_sentence": "Their approach to designing exploration policies that are incentive compatible for myopic agents under uncertainty motivates the incentive-aware learning principle UCB-S uses to align strategic arms\u2019 choices with welfare despite unknown rewards."
    },
    {
      "title": "Strategic Classification",
      "authors": "Moritz Hardt et al.",
      "year": 2016,
      "arxiv_id": "unknown",
      "role": "Related Problem",
      "relationship_sentence": "By formalizing learning under agent gaming and analyzing equilibria induced by decision rules, this paper motivates modeling click-rates as strategic actions and anticipating equilibrium behavior when designing selection policies."
    },
    {
      "title": "Online Learning in Stackelberg Games",
      "authors": "Tanner Fiez et al.",
      "year": 2020,
      "arxiv_id": "unknown",
      "role": "Foundation",
      "relationship_sentence": "Their leader\u2013follower learning framework under best-response dynamics informs the analysis of arms\u2019 Nash equilibria against a fixed selection policy and guides how to learn payoffs while accounting for strategic responses."
    }
  ],
  "synthesis_narrative": "UCB\u2019s index-based selection (Auer et al., 2002) established a template for balancing exploration and exploitation via confidence intervals, enabling algorithms that learn unknown rewards with finite-time guarantees. Cascade-style learning-to-rank (Kveton et al., 2015) made explicit the separation between a user\u2019s click probability and post-click utility, yielding a two-stage feedback structure that distinguishes attraction (CTR) from satisfaction. Mechanism-design-infused bandit mechanisms (Babaioff et al., 2010) showed that allocation rules can be crafted to be incentive compatible while simultaneously learning unknown click parameters, inaugurating a paradigm where incentive constraints are embedded into bandit learning. Work on incentivizing exploration with myopic agents (Kremer et al., 2014) demonstrated how to design recommendation policies that remain incentive compatible despite uncertainty. Strategic classification (Hardt et al., 2016) formalized learning with strategic agents who game selection rules, emphasizing equilibrium-aware design. Finally, online learning in Stackelberg games (Fiez et al., 2020) provided technical tools for learning when followers best respond to a leader\u2019s policy, bringing equilibrium analysis into learning dynamics. Together, these works revealed a gap: existing bandit-mechanism designs focus on strategic bidders or users, not strategic content providers who manipulate click propensity, despite the click/utility separation being well-modeled by cascade feedback. The natural next step is to redesign UCB-style indices as mechanism-like allocation rules that anticipate best responses, aligning strategic click choices with post-click welfare and enabling equilibrium characterization while learning unknown post-click rewards\u2014precisely the synthesis that yields an incentive-aware bandit like UCB-S.",
  "target_paper": {
    "title": "Bandits Meet Mechanism Design to Combat Clickbait in Online Recommendation",
    "authors": "Thomas Kleine Buening, Aadirupa Saha, Christos Dimitrakakis, Haifeng Xu",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "bandits, mechanism design, incentive-aware learning, nash equilibrium",
    "abstract": "We study a strategic variant of the multi-armed bandit problem, which we coin the strategic click-bandit. This model is motivated by applications in online recommendation where the choice of recommended items depends on both the click-through rates and the post-click rewards. Like in classical bandits, rewards follow a fixed unknown distribution. However, we assume that the click-rate of each arm is chosen  strategically by the arm (e.g., a host on Airbnb)  in order to maximize  the number of times it gets clicked. The algorithm designer does not know the post-click rewards nor the arms' actions (i.e., strategically chosen click-rates) in advance, and must learn both values over time. To solve this problem, we design an incentive-aware learning algorithm, UCB-S, which achieves two goals simultaneously: (a) incentivizing desirable arm behavior under uncertainty; (b) minimizing regret by learning unknown parameters.  We approximately characterize all Nash equilibria of the arms under UCB",
    "openreview_id": "lsxeNvYqCj",
    "forum_id": "lsxeNvYqCj"
  },
  "analysis_timestamp": "2026-01-06T10:37:28.976792"
}
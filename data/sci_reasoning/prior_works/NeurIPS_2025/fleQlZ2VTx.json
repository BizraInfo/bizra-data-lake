{
  "prior_works": [
    {
      "title": "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection (INLP)",
      "authors": "Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, Yoav Goldberg",
      "year": 2020,
      "role": "Methodological precursor for linear concept erasure",
      "relationship_sentence": "The paper\u2019s inference-time ablation of language-specific representations directly builds on INLP\u2019s idea of projecting hidden states onto a nullspace to remove a targeted attribute."
    },
    {
      "title": "Amnesic Probing: Behavioral Explanation with Amnesic Probing",
      "authors": "Yanai Elazar, Shauli Ravfogel, Alon Jacovi, Yoav Goldberg",
      "year": 2021,
      "role": "Causal diagnostic via representation removal",
      "relationship_sentence": "Their use of causal intervention to test whether removing specific information changes behavior informs the paper\u2019s causal framing and layer-wise analyses of language vs. reasoning features."
    },
    {
      "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation",
      "authors": "Siddharth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, Rosanne Liu",
      "year": 2020,
      "role": "Inference-time control via activation steering",
      "relationship_sentence": "Demonstrates that modifying internal activations at inference can steer model behavior without fine-tuning, a key operational precedent for the paper\u2019s ablation-at-inference intervention."
    },
    {
      "title": "How Multilingual is Multilingual BERT?",
      "authors": "Telmo Pires, Eva Schlinger, Dan Garrette",
      "year": 2019,
      "role": "Empirical foundation on multilingual representation sharing",
      "relationship_sentence": "Provides evidence that multilingual transformers learn cross-lingual abstractions alongside language-specific signals, motivating the hypothesis that language and reasoning can be disentangled."
    },
    {
      "title": "BERT Rediscovers the Classical NLP Pipeline",
      "authors": "Ian Tenney, Dipanjan Das, Ellie Pavlick",
      "year": 2019,
      "role": "Layer-wise feature localization",
      "relationship_sentence": "Establishes that different linguistic competencies localize at different layers, underpinning the paper\u2019s layer-wise disentanglement analyses and the finding that top-layer language features should be preserved."
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc V. Le, Denny Zhou, et al.",
      "year": 2022,
      "role": "Elicitation of reasoning processes",
      "relationship_sentence": "Shows that reasoning can be surfaced as an internal process separable from final answers, supporting the paper\u2019s premise that reasoning representations can be teased apart from language form."
    },
    {
      "title": "Language and thought are not the same thing: Evidence from neuroimaging and neurological patients",
      "authors": "Evelina Fedorenko, Rosemary Varley",
      "year": 2016,
      "role": "Conceptual inspiration from cognitive neuroscience",
      "relationship_sentence": "Provides the core cognitive hypothesis that reasoning can operate independently of language, which the paper tests in LLMs through causal representation ablation."
    }
  ],
  "synthesis_narrative": "This paper\u2019s core contribution\u2014showing that ablating language-specific representations at inference improves multilingual reasoning and reveals a disentanglement between language and reasoning\u2014sits at the nexus of conceptual, empirical, and methodological priors. Conceptually, Fedorenko and Varley (2016) argued that human reasoning need not rely on language, priming the hypothesis that analogous separability might exist in LLMs. Empirically, Pires et al. (2019) demonstrated that multilingual transformers learn both shared, language-agnostic features and language-specific ones, suggesting the feasibility of isolating a language subspace. Tenney et al. (2019) established that different competencies localize by layer, motivating the paper\u2019s layer-wise analyses and its finding that preserving top-layer language features is beneficial.\n\nMethodologically, the work leverages a lineage of representation interventions. INLP (Ravfogel et al., 2020) introduced nullspace projection to remove targeted attributes from hidden states, and Amnesic Probing (Elazar et al., 2021) framed such removals as causal tests of a representation\u2019s functional role\u2014both directly informing the paper\u2019s causal intervention design. PPLM (Dathathri et al., 2020) showed that modifying activations at inference can reliably steer generation without fine-tuning, validating the practicality of the paper\u2019s inference-time ablation approach. Finally, Chain-of-Thought prompting (Wei et al., 2022) made explicit that reasoning processes can be elicited and analyzed separately from surface form, aligning with the paper\u2019s central claim that reasoning and language are separable within LLMs. Together, these works culminate in a principled, causal ablation strategy that boosts multilingual reasoning by suppressing language-specific features while preserving essential high-level language competence.",
  "analysis_timestamp": "2026-01-07T00:02:04.954972"
}
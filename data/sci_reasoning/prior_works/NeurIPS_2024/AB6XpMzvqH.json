{
  "prior_works": [
    {
      "title": "Language Models are Few-Shot Learners",
      "authors": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, et al.",
      "year": 2020,
      "role": "Foundational ICL formulation",
      "relationship_sentence": "Established few-shot in-context learning as a capability of large LMs, providing the core paradigm that the paper extends from few-shot to the many-shot regime."
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, et al.",
      "year": 2022,
      "role": "Methodological basis for rationale-augmented prompts",
      "relationship_sentence": "Introduced chain-of-thought prompting, directly motivating the paper\u2019s use of rationales within many-shot prompts and its exploration of replacing human rationales."
    },
    {
      "title": "Large Language Models are Zero-Shot Reasoners",
      "authors": "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa",
      "year": 2022,
      "role": "Eliciting self-generated reasoning",
      "relationship_sentence": "Showed that LMs can generate their own step-by-step rationales via simple instructions, underpinning the paper\u2019s Reinforced ICL idea of using model-generated CoT in place of human-written rationales."
    },
    {
      "title": "STaR: Bootstrapping Reasoning With Reasoning",
      "authors": "Eric Zelikman, Yuhuai Wu, Jesse M. Han, Noah D. Goodman",
      "year": 2022,
      "role": "Bootstrapping with model-generated rationales",
      "relationship_sentence": "Demonstrated that self-generated rationales can improve reasoning performance by bootstrapping, inspiring the paper\u2019s use of model-produced CoT to populate many-shot exemplars without human annotations."
    },
    {
      "title": "Rethinking the Role of Demonstrations: Are Labels Necessary for In-Context Learning?",
      "authors": "Sewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi",
      "year": 2022,
      "role": "Evidence for label/rationale-light demonstrations",
      "relationship_sentence": "Found that labels and detailed explanations are not always necessary for ICL, directly motivating the paper\u2019s Unsupervised ICL setting that omits rationales and uses only domain-specific inputs."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, et al.",
      "year": 2022,
      "role": "Improving quality of model-generated rationales",
      "relationship_sentence": "Showed that aggregating multiple model-generated reasoning paths improves accuracy, informing the paper\u2019s use and trust in model-generated CoT at scale in many-shot prompts."
    },
    {
      "title": "Lost in the Middle: How Language Models Use Long Context",
      "authors": "Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Percy Liang, Christopher D. Manning",
      "year": 2023,
      "role": "Long-context behavior and prompt design",
      "relationship_sentence": "Characterized limitations of LMs over long contexts, directly informing the many-shot regime\u2019s prompt construction and expectations for performance as examples scale into hundreds or thousands."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014scaling in-context learning into the many-shot regime and replacing scarce human rationales with model-generated or rationale-free inputs\u2014builds on several direct intellectual threads. First, Brown et al. established few-shot in-context learning as a powerful inference-time paradigm, providing the foundation that this work scales to hundreds or thousands of exemplars. Wei et al.\u2019s chain-of-thought prompting introduced rationale-augmented exemplars as a key lever for eliciting reasoning, while Kojima et al. showed that models can generate their own step-by-step rationales from simple instructions. Together, these works directly motivate Reinforced ICL: populating many-shot prompts with model-produced CoT when human explanations are scarce.\nSTaR further demonstrated that self-generated rationales can bootstrap better reasoning, strengthening the premise that high-quality, model-produced CoT can substitute for human-written explanations in large quantities. Complementing this, Min et al. provided evidence that full labels or explanations are not always necessary for effective ICL, directly inspiring the Unsupervised ICL setting that uses only domain-specific inputs in the many-shot context. As many-shot prompting depends critically on long sequences, Liu et al.\u2019s analysis of long-context usage (Lost in the Middle) informs prompt design choices and expectations about where exemplars are placed and how benefits scale with length. Finally, Wang et al.\u2019s self-consistency result supports strategies to enhance reliability of model-generated rationales at scale. Collectively, these works establish the feasibility and mechanics of many-shot ICL with self- or unlabeled exemplars and provide methodological guidance for exploiting expanded context windows.",
  "analysis_timestamp": "2026-01-06T23:39:42.970348"
}
{
  "prior_works": [
    {
      "title": "DeepSeek-R1: Incentivizing Reasoning in Large Language Models via Reinforcement Learning",
      "authors": "DeepSeek-AI Team",
      "year": 2024,
      "role": "Foundational slow-thinking RL system and training recipe",
      "relationship_sentence": "VL-Rethinker directly extends the DeepSeek-R1 recipe\u2014especially its preference-free RL for reasoning\u2014to the multimodal setting and targets R1\u2019s limitations by adding explicit mechanisms for self-reflection and verification."
    },
    {
      "title": "Group Relative Policy Optimization (GRPO)",
      "authors": "DeepSeek-AI Team",
      "year": 2024,
      "role": "Core RL algorithm adapted",
      "relationship_sentence": "The paper\u2019s primary optimization backbone is an adaptation of GRPO; VL-Rethinker modifies it and introduces Selective Sample Replay to counter the vanishing-advantage issue that arises during GRPO-style reasoning RL."
    },
    {
      "title": "OpenAI o1 (System Card): Process-Supervised, Slow-Thinking Reasoning",
      "authors": "OpenAI",
      "year": 2024,
      "role": "Motivation and evidence for explicit reflection and slow-thinking",
      "relationship_sentence": "o1 demonstrates that incentivizing multi-step reflection and verification improves hard reasoning, motivating VL-Rethinker to bring similar slow-thinking gains to vision-language models via RL rather than distillation."
    },
    {
      "title": "Let\u2019s Verify Step by Step: Process Reward Models for Math Reasoning",
      "authors": "OpenAI",
      "year": 2023,
      "role": "Process supervision and verifier-based reward shaping",
      "relationship_sentence": "VL-Rethinker\u2019s emphasis on encouraging self-verification is grounded in PRM-style process supervision, using verifiers/step-level signals to reward reflective reasoning rather than only final answers."
    },
    {
      "title": "Training Verifiers to Solve Math Word Problems",
      "authors": "Karl Cobbe et al.",
      "year": 2021,
      "role": "Verifier paradigm for mathematical reasoning",
      "relationship_sentence": "The use of automated verifiers to assess solution correctness and intermediate steps directly informs VL-Rethinker\u2019s reward design for incentivizing self-checking in multimodal math."
    },
    {
      "title": "Prioritized Experience Replay",
      "authors": "Tom Schaul, John Quan, Ioannis Antonoglou, David Silver",
      "year": 2015,
      "role": "Experience replay principle for focusing learning on informative samples",
      "relationship_sentence": "Selective Sample Replay in VL-Rethinker is conceptually aligned with prioritized replay\u2014reusing and upweighting informative trajectories to avoid vanishing advantages and maintain strong learning signals."
    },
    {
      "title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
      "authors": "Noah Shinn, Joseph Ho, et al.",
      "year": 2023,
      "role": "Explicit self-reflection loop to improve performance",
      "relationship_sentence": "Reflexion\u2019s core idea\u2014prompted self-critique and correction\u2014motivates VL-Rethinker\u2019s explicit incentives for self-reflection/self-verification during training rather than only at inference."
    }
  ],
  "synthesis_narrative": "VL-Rethinker\u2019s central contribution\u2014making vision-language models practice slow-thinking through reinforcement learning without distillation\u2014stands on a lineage that merges reasoning-centric RL, verifier-guided supervision, and explicit self-reflection. DeepSeek-R1 provided the most immediate precedent: it showed that preference-free RL can substantially improve reasoning and popularized GRPO, a stable, value-free policy optimization method tailored to reasoning traces. VL-Rethinker adopts and adapts GRPO as its optimization backbone, but identifies a practical failure mode\u2014vanishing advantages\u2014as training progresses. To address this, the paper introduces Selective Sample Replay, drawing directly on the principle behind Prioritized Experience Replay to retain and upweight informative trajectories, preserving meaningful gradients for reasoning.\n\nOn the supervision side, the work is guided by process-based verification. Early verifier efforts for math (Cobbe et al.) and later process reward models (OpenAI\u2019s Let\u2019s Verify Step by Step) established that stepwise checking improves reasoning and that verifiers can serve as reliable training signals beyond final-answer accuracy. VL-Rethinker blends these insights with the slow-thinking ethos of OpenAI o1, which showed the value of explicit reflection, and with Reflexion\u2019s idea of prompting models to critique and refine their own outputs. The result is a multimodal RL framework that not only optimizes for correctness but explicitly rewards self-reflection and self-verification, effectively transporting the recent successes of text-only slow-thinking systems to the visual-language domain while adding algorithmic innovations to stabilize training.",
  "analysis_timestamp": "2026-01-07T00:21:33.154058"
}
{
  "prior_works": [
    {
      "title": "Bandit based Monte-Carlo Planning",
      "authors": "Levente Kocsis, Csaba Szepesv\u00e1ri",
      "year": 2006,
      "role": "Foundational algorithm for MCTS (UCT) combining tree search with UCB exploration.",
      "relationship_sentence": "The paper builds directly on UCT\u2019s selection rule and MCTS framework, introducing a modified UCT score augmented with feedback-derived bonuses."
    },
    {
      "title": "Progressive strategies for Monte-Carlo Tree Search",
      "authors": "Guillaume M. J.-B. Chaslot, Mark H. M. Winands, H. Jaap van den Herik, Jos W. H. M. Uiterwijk, Bruno Bouzy",
      "year": 2008,
      "role": "Technique to bias UCT with heuristic priors (progressive bias/widening) for faster, guided search.",
      "relationship_sentence": "The proposed cluster-specific bonus is conceptually aligned with progressive bias\u2014injecting prior guidance into UCT\u2014except it learns priors from aggregated past trajectories within semantically similar problem clusters."
    },
    {
      "title": "Mastering the game of Go without human knowledge",
      "authors": "David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, et al.",
      "year": 2017,
      "role": "Popularized PUCT, blending policy priors with UCT to guide exploration/exploitation in MCTS.",
      "relationship_sentence": "Their PUCT-style augmentation motivates this work\u2019s UCT modification, where the \u2018prior\u2019 term is instantiated as a cluster-conditioned feedback bonus derived from historical success patterns."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan",
      "year": 2023,
      "role": "Introduced inference-time tree search over LLM-generated reasoning steps.",
      "relationship_sentence": "This work extends ToT\u2019s idea of structured exploration of LLM-generated options by using an MCTS objective tailored to information gain and augmenting selection with learned feedback signals."
    },
    {
      "title": "Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information",
      "authors": "Sudha Rao, Hal Daum\u00e9 III",
      "year": 2018,
      "role": "Formulated clarifying question selection via EVPI, linking question choice to uncertainty reduction.",
      "relationship_sentence": "The paper\u2019s information-gain/EVPI framing directly underpins the objective for selecting questions in MCTS rollouts and evaluations."
    },
    {
      "title": "Asking Clarifying Questions in Open-Domain Information-Seeking Conversations",
      "authors": "Mehdi Aliannejadi, Hamed Zamani, Fabio Crestani, W. Bruce Croft",
      "year": 2019,
      "role": "Established the task and evaluation of clarifying question generation for conversational search.",
      "relationship_sentence": "This work motivates the goal-oriented, uncertainty-minimizing question-asking setup that the proposed framework optimizes with LLM generation and MCTS selection."
    },
    {
      "title": "A Contextual-Bandit Approach to Personalized News Article Recommendation",
      "authors": "Lihong Li, Wei Chu, John Langford, Robert E. Schapire",
      "year": 2010,
      "role": "Introduced LinUCB, showing how context conditions UCB-style exploration via learned priors.",
      "relationship_sentence": "The cluster-specific bonus parallels contextual UCB ideas by modulating exploration with context (problem-cluster) information learned from past interactions."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014feedback-aware MCTS for question selection\u2014sits at the confluence of three lines of work: Monte Carlo tree search with principled exploration, inference-time reasoning with LLMs, and information-seeking dialogue optimized for uncertainty reduction. UCT (Kocsis & Szepesv\u00e1ri, 2006) provides the backbone for search-time decision-making, while enhancements like progressive bias (Chaslot et al., 2008) and PUCT (Silver et al., 2017) establish that UCT can be safely and effectively augmented with prior-guidance terms. These precedents directly inform the authors\u2019 modification: a cluster-conditioned bonus integrated into UCT to prioritize historically effective trajectories.\n\nOn the LLM side, Tree of Thoughts (Yao et al., 2023) demonstrates that tree-structured, inference-time planning over model-generated steps can outperform single-pass decoding. The present work adapts that paradigm to goal-oriented information seeking, using LLMs to propose candidate questions while MCTS selects among them to maximize information gain. The objective itself is grounded in information-seeking research: Rao & Daum\u00e9 III (2018) formalize clarifying question selection via EVPI, and Aliannejadi et al. (2019) establish evaluation and methodological baselines for clarifying questions in conversational search. Finally, contextual bandit theory (Li et al., 2010) underpins the idea of conditioning exploration bonuses on context; here, the context is instantiated as semantic clusters of problems, and aggregated feedback from past interactions acts as a learned prior.\n\nTogether, these strands yield a principled, data-driven UCT variant that leverages past interaction patterns at the cluster level, enabling MCTS to more efficiently explore LLM-generated question trajectories that are most likely to reduce uncertainty for similar tasks.",
  "analysis_timestamp": "2026-01-07T00:02:04.923648"
}
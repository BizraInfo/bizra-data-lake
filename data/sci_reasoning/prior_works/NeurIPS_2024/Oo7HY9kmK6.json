{
  "prior_works": [
    {
      "title": "Breaking the Curse of Dimensionality with Convex Neural Networks",
      "authors": "Francis Bach",
      "year": 2017,
      "role": "Motivating application and convex signed-measure formulation",
      "relationship_sentence": "This work formalized risk minimization for infinite-width two-layer networks as convex optimization over signed measures with a total-variation norm, providing the primary application and the bilevel TV\u2013mass parametrization that the present paper leverages to reduce signed to probability measures."
    },
    {
      "title": "Exact Support Recovery for Sparse Spikes Deconvolution",
      "authors": "Vincent Duval, Gabriel Peyr\u00e9",
      "year": 2015,
      "role": "Motivating application in sparse deconvolution (BLASSO) over signed measures",
      "relationship_sentence": "By casting super-resolution/sparse deconvolution as convex optimization over signed measures with TV regularization (the Beurling LASSO), this paper supplies a canonical problem class motivating the need to extend mean-field Langevin methods beyond probability measures."
    },
    {
      "title": "Variational Formulation of the Fokker\u2013Planck Equation",
      "authors": "Richard Jordan, David Kinderlehrer, Felix Otto",
      "year": 1998,
      "role": "Foundational link between Langevin/Fokker\u2013Planck dynamics and variational minimization in Wasserstein space",
      "relationship_sentence": "The JKO scheme underpins the interpretation of Langevin dynamics as a Wasserstein gradient flow of energy plus entropy, which the present paper exploits to design and analyze mean-field Langevin dynamics in the (bilevel-reduced) probability-measure setting and to justify annealing via vanishing entropy."
    },
    {
      "title": "Gradient Flows: In Metric Spaces and in the Space of Probability Measures",
      "authors": "Luigi Ambrosio, Nicola Gigli, Giuseppe Savar\u00e9",
      "year": 2008,
      "role": "General theory of Wasserstein gradient flows and convergence under (displacement) convexity",
      "relationship_sentence": "This monograph provides the technical framework\u2014existence, stability, and convergence rates of Wasserstein gradient flows under convexity\u2014that the paper leverages to obtain guarantees for mean-field Langevin dynamics after reducing signed to probability measures."
    },
    {
      "title": "A Convexity Principle for Interacting Gases",
      "authors": "Robert J. McCann",
      "year": 1997,
      "role": "Displacement convexity foundation",
      "relationship_sentence": "McCann\u2019s displacement convexity is crucial for establishing convexity of functionals along Wasserstein geodesics, a key assumption enabling the paper\u2019s convergence-rate analysis of MLFD in the bilevel reduction."
    },
    {
      "title": "A Mean Field View of the Landscape of Two-Layer Neural Networks",
      "authors": "Song Mei, Andrea Montanari, Phan-Minh Nguyen",
      "year": 2018,
      "role": "Mean-field dynamics for neural networks as optimization over measures",
      "relationship_sentence": "This work formalizes parameter-distribution dynamics (McKean\u2013Vlasov limits) for two-layer networks, directly inspiring the present paper\u2019s use of measure-valued dynamics and connecting MLFD with infinite-width network risk minimization."
    },
    {
      "title": "Trainability and Accuracy of Neural Networks using Interacting Particle Systems",
      "authors": "Grant Rotskoff, Eric Vanden-Eijnden",
      "year": 2018,
      "role": "Interacting-particle/mean-field methodology and analysis",
      "relationship_sentence": "By framing network training as an interacting particle system converging to mean-field flows, this paper provides methodological precedent for the present work\u2019s particle-based MLFD and its mean-field analysis."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014extending mean-field Langevin dynamics (MLFD) from probability to signed measures via a bilevel reduction with provable rates\u2014rests on three converging threads. First, convex formulations of learning and inverse problems over signed measures motivate the need for such an extension: Bach (2017) cast infinite-width two-layer networks as convex risk minimization over signed measures with total-variation (TV) regularization, while Duval and Peyr\u00e9 (2015) established sparse spikes deconvolution (BLASSO) as a TV-regularized signed-measure program. These works also implicitly furnish the bilevel parameterization that separates mass (TV norm) from a normalized probability distribution, a reduction the present paper selects and analyzes.\nSecond, the design and analysis of MLFD fundamentally rely on Wasserstein gradient-flow theory. The JKO scheme (Jordan\u2013Kinderlehrer\u2013Otto, 1998) connects Langevin/Fokker\u2013Planck dynamics to variational minimization of energy plus entropy, enabling an annealing perspective via vanishing noise. Ambrosio\u2013Gigli\u2013Savar\u00e9 (2008) provide the rigorous gradient-flow framework and convergence under (displacement) convexity, while McCann (1997) supplies the displacement convexity principle used to certify convexity of the objective along Wasserstein geodesics after reduction to probabilities.\nThird, mean-field neural network dynamics (Mei\u2013Montanari\u2013Nguyen, 2018; Rotskoff\u2013Vanden-Eijnden, 2018) establish interacting-particle and McKean\u2013Vlasov limits for parameter distributions, directly informing the particle implementation and analysis of MLFD adopted here. Synthesizing these strands, the paper shows that the bilevel reduction yields stronger guarantees and faster rates in the low-noise regime, at the expense of higher per-iteration complexity.",
  "analysis_timestamp": "2026-01-07T00:02:04.737728"
}
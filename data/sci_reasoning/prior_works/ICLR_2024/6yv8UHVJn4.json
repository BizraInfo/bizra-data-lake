{
  "prior_works": [
    {
      "title": "Adversarial Linear MDPs with Bandit Feedback: Improved Regret via Variance-Aware Analysis",
      "authors": "Kong et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "Their simulator-free but computationally inefficient algorithm achieved ~O(K^{4/5} + 1/\u03bb_min) regret in adversarial linear MDPs with bandit feedback, and this paper directly addresses both the suboptimal K-exponent and the undesired dependence on \u03bb_min by attaining the first ~O(\u221aK) rate without a simulator."
    },
    {
      "title": "Efficient Adversarial Reinforcement Learning in Linear MDPs with Bandit Feedback",
      "authors": "Sherman et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "Their computationally efficient policy-optimization framework achieved ~O(K^{6/7}) regret, and this paper builds on and tightens that approach with a refined loss estimator and exploration schedule to obtain ~O(K^{3/4}) while preserving efficiency."
    },
    {
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation",
      "authors": "Jin et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "The linear MDP factorization and occupancy-measure viewpoint introduced here provide the structural model and analytical tools that this paper assumes and operates within when deriving adversarial bandit-regret guarantees."
    },
    {
      "title": "Online Markov Decision Processes",
      "authors": "Even-Dar et al.",
      "year": 2009,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work formalized the online/adversarial MDP setting and regret against the best policy, establishing the problem template that the current paper tackles under linear function approximation and bandit feedback."
    },
    {
      "title": "Online Learning in Markov Decision Processes with Bandit Feedback",
      "authors": "Neu et al.",
      "year": 2010,
      "arxiv_id": "",
      "role": "Extension",
      "relationship_sentence": "The paper\u2019s bandit occupancy-measure loss estimator and OMD-style policy updates are adapted and refined here to the linear-MDP setting, with variance control that is crucial for the improved K-dependence achieved by this work."
    },
    {
      "title": "Online Learning in Episodic Markov Decision Processes by Mixing Past Policies",
      "authors": "Zimin et al.",
      "year": 2013,
      "arxiv_id": "",
      "role": "Related Problem",
      "relationship_sentence": "Their policy-mixing strategy for adversarial episodic MDPs informs the stability and bias-variance tradeoffs of the policy optimization schemes that the present paper tailors to the linear-function-approximation and bandit-feedback regime."
    }
  ],
  "synthesis_narrative": "Online Markov Decision Processes established the adversarial RL template, defining regret against the best policy and clarifying the episodic structure of nonstochastic losses. Online Learning in Markov Decision Processes with Bandit Feedback extended this to bandit feedback, introducing importance-weighted occupancy-measure estimators and mirror-descent style updates that balance exploration with adversarial loss estimation under high variance. Online Learning in Episodic Markov Decision Processes by Mixing Past Policies further developed policy-mixing techniques that stabilize updates in adversarial episodic control, highlighting how to temper estimator bias and variance over trajectories. Provably Efficient Reinforcement Learning with Linear Function Approximation introduced the linear MDP model, factorizing transition dynamics and enabling low-dimensional occupancy representations and analyses that became the standard structural assumption for function approximation in RL.\nBuilding on these foundations, Efficient Adversarial Reinforcement Learning in Linear MDPs with Bandit Feedback provided the first computationally efficient framework for adversarial linear MDPs with bandit feedback but achieved only ~O(K^{6/7}) regret, revealing remaining variance and exploration inefficiencies. Adversarial Linear MDPs with Bandit Feedback: Improved Regret via Variance-Aware Analysis obtained ~O(K^{4/5}) with an inefficient procedure and incurred an undesirable 1/\u03bb_min dependence, underscoring both statistical and structural bottlenecks. The current paper synthesizes these insights: it adapts bandit occupancy-measure estimation to the linear MDP structure with sharper variance control, yielding a simulator-free, rate-optimal ~O(\u221aK) (albeit inefficient) algorithm, and, by tightening the efficient policy-optimization pipeline of Sherman et al., achieves a computationally efficient ~O(K^{3/4}) regret bound\u2014closing key gaps highlighted by prior work.",
  "target_paper": {
    "title": "Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback",
    "authors": "Haolin Liu, Chen-Yu Wei, Julian Zimmert",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "adversarial MDPs, policy optimization, bandit feedback",
    "abstract": "We study online reinforcement learning in linear Markov decision processes with adversarial losses and bandit feedback. We introduce two algorithms that achieve improved regret performance compared to existing approaches. The first algorithm, although computationally inefficient, achieves a regret of $\\widetilde{O}(\\sqrt{K})$ without relying on simulators, where $K$ is the number of episodes. This is the first rate-optimal result in the considered setting. The second algorithm is computationally efficient and achieves a regret of  $\\widetilde{O}(K^{\\frac{3}{4}})$ . These results significantly improve over the prior state-of-the-art: a computationally inefficient algorithm by Kong et al. (2023) with $\\widetilde{O}(K^{\\frac{4}{5}}+1/\\lambda_{\\min})$ regret, and a computationally efficient algorithm by Sherman et al. (2023b) with $\\widetilde{O}(K^{\\frac{6}{7}})$ regret.",
    "openreview_id": "6yv8UHVJn4",
    "forum_id": "6yv8UHVJn4"
  },
  "analysis_timestamp": "2026-01-06T08:09:17.614147"
}
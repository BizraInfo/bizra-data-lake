{
  "prior_works": [
    {
      "title": "Maximum Likelihood Estimation of Observer Error-Rates Using the EM Algorithm",
      "authors": "Dawid et al.",
      "year": 1979,
      "role": "Foundation",
      "relationship_sentence": "TLLC\u2019s worker modeling inherits the Dawid\u2013Skene confusion-matrix formulation for annotator-specific reliabilities and adapts it by learning transferable representations that enable estimating worker behavior from very few annotations."
    },
    {
      "title": "Whose Vote Should Count More: Optimal Integration of Labels from Labelers of Unknown Expertise",
      "authors": "Whitehill et al.",
      "year": 2009,
      "role": "Foundation",
      "relationship_sentence": "GLAD\u2019s modeling of worker ability and item difficulty underpins TLLC\u2019s goal of capturing worker-specific behavior; TLLC addresses GLAD\u2019s data-sparsity weakness by transferring knowledge from high-confidence instances before per-worker adaptation."
    },
    {
      "title": "Learning From Crowds",
      "authors": "Raykar et al.",
      "year": 2010,
      "role": "Gap Identification",
      "relationship_sentence": "Raykar et al. showed joint learner\u2013annotator modeling via EM but rely on sufficient labels per worker; TLLC explicitly tackles this limitation by pretraining on a high-confidence source domain and transferring to worker-specific sparse targets."
    },
    {
      "title": "A Survey on Transfer Learning",
      "authors": "Pan et al.",
      "year": 2010,
      "role": "Foundation",
      "relationship_sentence": "TLLC instantiates Pan and Yang\u2019s source\u2013target transfer framework by treating agreement-filtered high-confidence items as the source domain and each worker\u2019s sparse annotations as the target domain for adaptation."
    },
    {
      "title": "Learning a similarity metric discriminatively, with application to face verification",
      "authors": "Chopra et al.",
      "year": 2005,
      "role": "Inspiration",
      "relationship_sentence": "TLLC borrows the Siamese metric-learning paradigm to pretrain a representation on reliable (high-confidence) instance pairs, which is then transferred to stabilize worker-specific modeling under sparsity."
    },
    {
      "title": "Deep Learning from Crowds",
      "authors": "Rodrigues et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Crowd Layer integrates annotator-specific parameters into neural networks; TLLC improves on this neural worker-modeling line by adding a transfer step from high-confidence data, making worker modeling viable when each annotator labels few instances."
    },
    {
      "title": "Learning From Noisy Labels with Deep Neural Networks Using Model Bootstrapped EM",
      "authors": "Khetan et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "MBEM\u2019s use of model-driven high-confidence labels to bootstrap annotator/label estimation directly inspires TLLC\u2019s idea to identify high-confidence instances as a source domain for pretraining before worker-specific transfer."
    }
  ],
  "synthesis_narrative": "TLLC stands at the intersection of annotator modeling and transfer learning, addressing a core pain point: each worker often labels only a handful of items, making per-worker modeling unreliable. The classical Dawid\u2013Skene framework and GLAD established the foundational view that workers possess individual reliabilities (and tasks have varying difficulties), but both implicitly require enough observations per annotator to estimate these parameters robustly. Raykar et al.\u2019s joint learner\u2013annotator EM elevated this idea by tying representation learning to annotator modeling, yet it too suffers when per-worker data are scarce. TLLC reframes the setting through Pan and Yang\u2019s source\u2013target transfer lens: agreement-filtered, high-confidence items become a source domain from which transferable knowledge is learned, and each worker\u2019s sparse annotations constitute the target domain. To make that transfer effective, TLLC adopts the Siamese metric-learning paradigm (Chopra et al.), pretraining a Siamese network on abundant, confident pairs so that the representation encodes annotator-relevant structure before per-worker adaptation. This directly extends neural annotator modeling \u00e0 la Deep Learning from Crowds (Rodrigues and Pereira), which embeds confusion behavior in neural architectures but struggles under extreme sparsity. Finally, MBEM\u2019s insight\u2014leveraging a high-confidence subset to bootstrap estimation\u2014motivates TLLC\u2019s explicit construction of a reliable source set for pretraining. Collectively, these works define the problem, expose the sparsity gap, and supply the transfer and Siamese mechanisms that TLLC integrates into a label completion pipeline.",
  "analysis_timestamp": "2026-01-06T23:07:19.613229"
}
{
  "prior_works": [
    {
      "title": "AudioLDM: Text-to-Audio Generation with Latent Diffusion Models",
      "authors": "Haohe Liu et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "SpatialSonic adopts AudioLDM\u2019s latent diffusion pipeline for text-to-audio and directly augments it with spatial-aware encoders plus explicit azimuth-state conditioning to overcome AudioLDM\u2019s mono-only, spatially-indistinct outputs."
    },
    {
      "title": "AudioGen: Textually Guided Audio Generation",
      "authors": "Felix Kreuk et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "As a leading text-to-audio system producing mono waveforms without controllable spatial placement, AudioGen exemplifies the gap this work targets\u2014accurate, language-driven stereo spatialization."
    },
    {
      "title": "WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset and Benchmark for Audio-Language Learning",
      "authors": "Xinhao Mei et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "BEWO-1M explicitly builds on WavCaps\u2019 GPT-assisted curation paradigm, extending it to generate rich spatial and motion-aware descriptions that supervise stereo/spatial audio generation."
    },
    {
      "title": "SoundSpaces 2.0: A Simulation Platform for Audio-Visual Navigation in Complex 3D Environments",
      "authors": "Changan Chen et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "The BEWO-1M simulation pipeline draws on SoundSpaces 2.0\u2019s principles of controllable, physics-based spatial rendering with moving and multiple sources, which we repurpose at scale for stereo training data."
    },
    {
      "title": "Learning to Binauralize in the Wild",
      "authors": "Ruohan Gao et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Mono2Binaural established learning-based binauralization and highlighted the centrality of interaural cues and azimuth; SpatialSonic internalizes these cues via spatial-aware encoders and azimuth-state guidance within a generative diffusion model."
    },
    {
      "title": "Self-Supervised Generation of Spatial Audio for 360\u00b0 Video",
      "authors": "Pedro Morgado et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "This work formulated data-driven spatial audio generation (ambisonics/binaural) but relied on video; we generalize the problem to language (and optional images) and scale to multi-source, moving soundscapes."
    },
    {
      "title": "LAION-CLAP: Open Large-Scale Contrastive Language-Audio Pretraining",
      "authors": "Yusong Wu et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "We start from CLAP-style audio\u2013text contrastive encoders for semantic conditioning and extend them with spatial-aware representations so that conditioning preserves azimuth/interaural cues that vanilla CLAP discards."
    }
  ],
  "synthesis_narrative": "Both Ears Wide Open\u2019s core innovation\u2014language-driven, controllable stereo spatial audio generation\u2014emerges from fusing latent diffusion with explicit spatial conditioning and a scalable, simulation-based, GPT-assisted corpus. AudioLDM provides the immediate generative backbone and conditioning paradigm for text-to-audio diffusion, but its mono outputs and lack of spatial control define a clear gap that the authors directly address. AudioGen further crystallizes this limitation among strong baselines: mono waveforms without controllable spatial placement. To make spatial control feasible at scale, the dataset contribution BEWO-1M follows WavCaps\u2019 ChatGPT-assisted curation strategy, extending it to encode spatial attributes (azimuth, motion, multiplicity) critical for supervising spatial generation. On the rendering side, SoundSpaces 2.0 offers the conceptual and practical foundation for large-scale, controllable simulation with moving and multiple sources, which BEWO adapts to mass-produce stereo training data. Methodologically, classic spatialization works\u2014Mono2Binaural and Morgado et al.\u2019s 360\u00b0 spatial audio generation\u2014demonstrate how interaural cues and azimuth/ambisonics structure enable learning-based spatial audio; the present work transposes those insights into a generative diffusion setting and removes the reliance on video inputs. Finally, CLAP-style audio\u2013text encoders enable semantic conditioning but are spatially agnostic; SpatialSonic explicitly extends them with spatial-aware encoders and azimuth-state modeling, yielding precise spatial guidance that turns previously random, indistinct stereo outputs into controlled, language-aligned spatial soundscapes.",
  "analysis_timestamp": "2026-01-06T23:08:23.934220"
}
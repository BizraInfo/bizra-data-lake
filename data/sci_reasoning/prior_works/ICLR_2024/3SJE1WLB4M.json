{
  "prior_works": [
    {
      "title": "Generalization error of random features and kernel methods",
      "authors": "Song Mei et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Provided asymptotically exact generalization error formulas for kernel ridge regression under high-dimensional Gaussian design, which this paper generalizes from KRR to arbitrary spectral learning profiles h(\u03bb) (including gradient descent) and to an additional low-dimensional translation-invariant model."
    },
    {
      "title": "Just Interpolate: Kernel 'Ridgeless' Regression Can Generalize",
      "authors": "Tengyuan Liang et al.",
      "year": 2020,
      "arxiv_id": "arXiv:1808.00387",
      "role": "Gap Identification",
      "relationship_sentence": "Established benign overfitting and risk characterizations for ridgeless KRR under spectral/source conditions, highlighting the limitation to KRR that this work explicitly overcomes by treating a broader family of spectral algorithms and GD within a unified h(\u03bb) framework."
    },
    {
      "title": "Spectral bias and task-model alignment explain generalization in kernel regression",
      "authors": "Murat Canatar et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Introduced an eigenmode-wise decomposition of kernel regression error and analyzed power-law task/kernel spectra, which this paper leverages to derive closed-form asymptotics for general spectral filters and to prove loss localization on specific spectral scales."
    },
    {
      "title": "On regularization algorithms in learning theory",
      "authors": "Franz Josef Bauer et al.",
      "year": 2007,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Formalized spectral-filter regularization (including Tikhonov/KRR and Landweber/gradient descent) and the qualification/saturation concepts; this paper adopts that spectral-filter view via the learning profile h(\u03bb) and provides asymptotically precise generalization characterizations within it."
    },
    {
      "title": "Early stopping and nonparametric regression: an optimal data-dependent stopping rule",
      "authors": "Garvesh Raskutti et al.",
      "year": 2014,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Analyzed early-stopped gradient descent in RKHS as a spectral regularizer and obtained optimal-rate bounds, which this work extends by giving exact asymptotics for GD through its general h(\u03bb)-based spectral algorithm framework."
    },
    {
      "title": "Optimal rates for the regularized least-squares algorithm",
      "authors": "Antonio Caponnetto et al.",
      "year": 2007,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Established power-law source/capacity conditions and optimal learning rates for KRR, providing the power-law spectral assumptions on kernels and targets that this paper uses to compute precise asymptotic errors for general spectral algorithms."
    }
  ],
  "synthesis_narrative": "Asymptotically precise learning-curve analyses for kernel methods were first obtained for kernel ridge regression under high-dimensional Gaussian models, with Mei et al. deriving exact generalization formulas that depended on the kernel eigenvalue distribution. Liang and Rakhlin characterized benign overfitting and risk of ridgeless KRR via spectral and source conditions, clarifying when interpolation can generalize but restricting attention to the KRR filter. In parallel, Canatar, Bordelon, and Pehlevan showed that kernel learning error decomposes additively across eigenmodes and that power-law structure in target and kernel spectra governs the dominant scales, providing a mode-wise lens and empirical-theoretic evidence of spectral scale dominance. The classical learning-theory and inverse-problems literature, notably Bauer, Pereverzev, and Rosasco, formalized spectral-filter algorithms\u2014encompassing Tikhonov (KRR) and Landweber (gradient descent)\u2014and introduced qualification and saturation, explaining why some filters saturate while others can exploit higher smoothness. Raskutti, Wainwright, and Yu analyzed early-stopped gradient descent in RKHS as a spectral regularizer and proved optimal-rate guarantees, linking iterative methods to spectral filtering. Caponnetto and De Vito established the power-law source/capacity conditions under which rates manifest, setting the spectral assumptions used widely in kernel theory. Together, these works exposed a gap: exact-asymptotic generalization was known primarily for KRR, while the broader class of spectral filters\u2014including gradient descent\u2014lacked precise characterizations. By unifying the spectral-filter viewpoint with eigenmode decompositions and power-law assumptions, the present work derives exact generalization as a functional of a learning profile h(\u03bb), covers both high-dimensional Gaussian and low-dimensional translation-invariant settings, and clarifies how loss localizes on spectral scales, offering a refined account of KRR saturation and its avoidance by alternative spectral algorithms.",
  "target_paper": {
    "title": "Generalization error of spectral algorithms",
    "authors": "Maksim Velikanov, Maxim Panov, Dmitry Yarotsky",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "gradient descent, kernel ridge regression, optimal algorithm, generalization, asymptotic error rates, power-laws",
    "abstract": "The asymptotically precise estimation of the generalization of kernel methods has recently received attention due to the parallels between neural networks and their associated kernels. However, prior works derive such estimates for training by kernel ridge regression (KRR), whereas neural networks are typically trained with gradient descent (GD). In the present work, we consider the training of kernels with a family of \\emph{spectral algorithms} specified by profile $h(\\lambda)$, and including KRR and GD as special cases. Then, we derive the generalization error as a functional of learning profile $h(\\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional translation-invariant model. \nUnder power-law assumptions on the spectrum of the kernel and target, we use our framework to (i) give full loss asymptotics for both noisy and noiseless observations (ii) show that the loss localizes on certain spectral scales, giving a new perspective on the KRR saturation phenomeno",
    "openreview_id": "3SJE1WLB4M",
    "forum_id": "3SJE1WLB4M"
  },
  "analysis_timestamp": "2026-01-06T06:03:36.560453"
}
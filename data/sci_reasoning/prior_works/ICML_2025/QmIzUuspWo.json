{
  "prior_works": [
    {
      "title": "The DC (difference of convex functions) programming and DCA: an overview",
      "authors": "Le Thi Hoai An et al.",
      "year": 2005,
      "role": "Foundation",
      "relationship_sentence": "This work formalized DC programming, DC critical points, and the convex\u2013concave decomposition that the new algorithm exploits with distinct sampling rates for the convex and concave components."
    },
    {
      "title": "Variational Analysis",
      "authors": "R. T. Rockafellar et al.",
      "year": 1998,
      "role": "Foundation",
      "relationship_sentence": "The paper\u2019s key O(sqrt(p/n)) result for sample-average approximation of subdifferential mappings builds directly on variational analysis tools (graphical convergence of subdifferentials, measurability, and interchange rules) developed in this monograph."
    },
    {
      "title": "Lectures on Stochastic Programming: Modeling and Theory (2nd ed.)",
      "authors": "A. Shapiro et al.",
      "year": 2014,
      "role": "Foundation",
      "relationship_sentence": "The SAA framework and finite-sample convergence theory for expectations provided here are directly extended in the paper from objective values to pointwise convergence of subdifferential set-valued maps."
    },
    {
      "title": "On the rate of convergence of the sample average approximation method",
      "authors": "A. Shapiro et al.",
      "year": 2000,
      "role": "Extension",
      "relationship_sentence": "Classical SAA rate results (O(1/sqrt{n})) from this paper are sharpened and specialized to nonsmooth settings by deriving a pointwise O(sqrt(p/n)) bound for empirical subdifferentials, which underpins the adaptive sampling schedule."
    },
    {
      "title": "Stochastic subgradient method converges on tame functions",
      "authors": "D. Davis et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "This work\u2019s convergence under static distributions relies on measurable subgradient selectors; the new paper removes that requirement and matches their sample-size guarantees while handling time-varying distributions."
    },
    {
      "title": "Adaptive sampling strategies for stochastic optimization",
      "authors": "R. Bollapragada et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "The idea of iteration-dependent, accuracy-driven batch sizing directly inspires the algorithm\u2019s online adaptive sampling, which is extended here to use distinct sampling rates for the convex and concave DC components."
    },
    {
      "title": "Non-stationary stochastic optimization",
      "authors": "O. Besbes et al.",
      "year": 2015,
      "role": "Related Problem",
      "relationship_sentence": "This paper motivates optimization under drifting (time-varying) distributions; the new work adopts an online, current-distribution-only sampling paradigm and imposes distribution-convergence conditions aligned with such nonstationarity models."
    }
  ],
  "synthesis_narrative": "The core of the paper rests on the DC paradigm of Le Thi and Pham Dinh Tao, which defines DC criticality and the convex\u2013concave decomposition that the algorithm leverages with asymmetric sampling across the two components. To analyze this nonsmooth, stochastic regime, the authors build on Rockafellar and Wets\u2019s variational analysis for subdifferential calculus, measurability, and graphical convergence, and on the stochastic programming canon of Shapiro, Dentcheva, and Ruszczy\u0144ski. Classical SAA rate theory, particularly Shapiro and Homem-de-Mello\u2019s bounds, is directly extended from function values to subdifferential set-valued mappings, yielding the paper\u2019s key O(\u221a(p/n)) pointwise convergence rate that dictates how many samples are needed per iteration. On the algorithmic side, the notion of adaptive, accuracy-driven batch sizing from Bollapragada, Byrd, and Nocedal inspires the online adaptive sampling mechanism; the present work advances it by coupling the variance\u2013bias control to DC structure via distinct sampling rates for convex and concave parts. Relative to nonsmooth stochastic methods like Davis and Drusvyatskiy\u2019s subgradient scheme, which require measurable subgradient selectors under static distributions, the new analysis removes the selector requirement and proves matching sample-size guarantees while allowing distributions to evolve over time. Finally, the nonstationarity perspective of Besbes, Gur, and Zeevi motivates the current-distribution-only data usage and convergence assumptions, anchoring the paper\u2019s online setting and guarantees.",
  "analysis_timestamp": "2026-01-06T23:07:19.598360"
}
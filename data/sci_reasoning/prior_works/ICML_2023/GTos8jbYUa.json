{
  "prior_works": [
    {
      "title": "A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning of Structured State Space Models (KVAE)",
      "authors": "M. Fraccaro et al.",
      "year": 2017,
      "role": "Extension",
      "relationship_sentence": "NCDSSM generalizes KVAE\u2019s core idea of introducing auxiliary variables to decouple recognition from linear-Gaussian dynamics\u2014and using Kalman-style marginalization of dynamic states\u2014to the continuous-time, irregularly observed (continuous\u2013discrete) setting."
    },
    {
      "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data (DVBF)",
      "authors": "M. Karl et al.",
      "year": 2016,
      "role": "Inspiration",
      "relationship_sentence": "DVBF introduced auxiliary process variables and amortized inference only for these auxiliaries to maintain tractable filtering; NCDSSM adopts this disentanglement principle and adapts it to continuous-time dynamics with exact Bayesian filtering of the dynamic states."
    },
    {
      "title": "Deep Kalman Filters",
      "authors": "R. G. Krishnan et al.",
      "year": 2015,
      "role": "Foundation",
      "relationship_sentence": "Deep Kalman Filters established deep generative state-space modeling with variational inference, and NCDSSM builds on this framework while addressing its limitation of amortizing inference over all dynamic states by filtering/marginalizing them and amortizing only auxiliary variables."
    },
    {
      "title": "Latent ODEs for Irregularly-Sampled Time Series",
      "authors": "Y. Rubanova et al.",
      "year": 2019,
      "role": "Baseline",
      "relationship_sentence": "Latent ODEs formulated continuous-time latent variable models for irregular sampling but rely on trajectory-level amortized encoders; NCDSSM directly targets this gap by replacing amortized state inference with continuous\u2013discrete Bayesian filtering and marginalization of dynamic states."
    },
    {
      "title": "Stochastic Processes and Filtering Theory",
      "authors": "A. H. Jazwinski",
      "year": 1970,
      "role": "Foundation",
      "relationship_sentence": "NCDSSM\u2019s exact inference over continuous-time latent dynamics with discrete observations is enabled by continuous\u2013discrete filtering theory (Kalman\u2013Bucy and related results) laid out in Jazwinski\u2019s classical framework."
    },
    {
      "title": "Neural Controlled Differential Equations for Irregular Time Series",
      "authors": "P. Kidger et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "Neural CDEs crystallized the continuous-time formulation for irregularly sampled data; NCDSSM adopts the continuous-time viewpoint but contributes a generative, Bayesian state-space treatment with filtering-based inference rather than purely supervised controlled dynamics."
    },
    {
      "title": "GP-VAE: Deep Probabilistic Time Series Imputation using Gaussian Process Priors",
      "authors": "V. Fortuin et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "GP-VAE addresses irregular sampling via GP priors but is constrained by GP expressivity and scaling; NCDSSM targets this limitation by learning flexible neural continuous\u2013discrete dynamics while retaining principled Bayesian inference through filtering."
    }
  ],
  "synthesis_narrative": "NCDSSM sits at the intersection of deep state-space modeling and continuous-time learning for irregularly sampled series. Its core move\u2014amortizing inference only for auxiliary variables while performing exact Bayesian updates for the dynamic states\u2014directly builds on the disentanglement principle pioneered by DVBF and made operational in KVAE, where a linear\u2013Gaussian substructure enables Kalman-style marginalization of states. NCDSSM extends this blueprint from discrete-time to continuous\u2013discrete systems, invoking Jazwinski\u2019s continuous-time filtering theory to compute accurate posteriors for continuous dynamics observed at discrete times. This addresses a key limitation of the Latent ODE line of work: although Latent ODEs established the modern continuous-time latent generative formulation for irregular sampling, they rely on trajectory-level amortized encoders and do not exploit Bayesian filtering structure, which NCDSSM provides. Neural CDEs further motivated the continuous-time perspective for irregular data, but are predominantly supervised; NCDSSM contributes a fully generative, Bayesian alternative with closed-form filtering updates. Finally, GP-VAE highlighted the benefits of continuous-time priors for imputation under irregular sampling but exposed expressivity and scalability constraints inherent to GP priors; NCDSSM replaces these with flexible neural continuous\u2013discrete dynamics while still enjoying exact (filtering-based) inference for the dynamic states. Together, these works directly shaped NCDSSM\u2019s key innovation: disentangled, auxiliary-variable recognition coupled with continuous\u2013discrete Bayesian filtering and state marginalization.",
  "analysis_timestamp": "2026-01-06T23:09:26.573179"
}
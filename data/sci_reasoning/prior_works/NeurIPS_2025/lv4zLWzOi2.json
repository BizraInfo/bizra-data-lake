{
  "prior_works": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever",
      "year": 2021,
      "role": "Foundational VLM and baseline",
      "relationship_sentence": "The paper builds on CLIP\u2019s zero-shot vision-language alignment and its known cross-domain generalization, framing the need to selectively forget domain-specific cues (e.g., illustrated vs. real) while preserving CLIP\u2019s utility."
    },
    {
      "title": "Domain-Adversarial Training of Neural Networks (DANN)",
      "authors": "Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, Victor Lempitsky",
      "year": 2016,
      "role": "Mechanism for removing domain information",
      "relationship_sentence": "DANN\u2019s adversarial objective to make features domain-invariant directly inspires treating \u2018domain\u2019 as the forgetting target\u2014suppressing domain-identifying evidence while maintaining task-relevant recognition."
    },
    {
      "title": "Towards Making Systems Forget with Machine Unlearning",
      "authors": "Yinzhi Cao, Junfeng Yang",
      "year": 2015,
      "role": "Foundational machine unlearning formulation",
      "relationship_sentence": "This work motivates selective removal without full retraining; the present paper adopts the unlearning goal but relaxes it to an approximate, efficient setting suitable for large VLMs."
    },
    {
      "title": "Making AI Forget: Data Deletion in Machine Learning",
      "authors": "Antonio Ginart, Melody Y. Guan, Gregory Valiant, James Y. Zou",
      "year": 2019,
      "role": "Approximate unlearning and efficiency",
      "relationship_sentence": "Introduces practical, approximate deletion with utility guarantees, informing the paper\u2019s focus on efficient unlearning procedures and evaluation that balance forgetting with performance retention."
    },
    {
      "title": "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection (INLP)",
      "authors": "Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, Yoav Goldberg",
      "year": 2020,
      "role": "Attribute removal while preserving task performance",
      "relationship_sentence": "INLP demonstrates targeted removal of specific attributes from representations without degrading core task accuracy, a principle the paper extends to removing \u2018domain\u2019 signals in joint vision-language embeddings."
    },
    {
      "title": "Surgical Concept Erasure for Text-to-Image Diffusion Models",
      "authors": "Kartik Gandikota, et al.",
      "year": 2023,
      "role": "Targeted concept forgetting with retention constraints",
      "relationship_sentence": "Shows how to erase high-level concepts while retaining general capabilities, directly informing the paper\u2019s approximate, selective forgetting objective and retention regularization for VLMs."
    },
    {
      "title": "Learning to Prompt for Vision-Language Models (CoOp)",
      "authors": "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu",
      "year": 2022,
      "role": "Domain-aware adaptation of VLMs via prompts",
      "relationship_sentence": "CoOp reveals CLIP\u2019s domain sensitivity and offers a mechanism to specify domains via prompts, which the paper leverages to define, supervise, and evaluate domain-targeted forgetting in VLMs."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014approximate domain unlearning in vision-language models\u2014sits at the intersection of VLM foundations, domain signal suppression, and selective unlearning. CLIP established the joint image\u2013text embedding space and strong cross-domain generalization that make VLMs powerful yet prone to retaining domain cues irrelevant or risky for downstream tasks. To forget domain-specific information while preserving recognition ability, the paper draws on DANN\u2019s adversarial principle: treat domain as a nuisance attribute and explicitly suppress its recoverability, aligning with the unlearning objective when the forgetting target is \u2018domain.\u2019 From the machine unlearning literature, the work inherits both motivation and practicality. Cao and Yang formalized the goal of removing information without full retraining, while Ginart et al. advocated approximate, efficient procedures with utility considerations\u2014both directly motivating scalable, retention-aware strategies for large VLMs.\n\nOperationally, removing attributes without collapsing task utility echoes INLP, which iteratively projects out attribute directions while preserving performance; this guides the paper\u2019s representation-level treatment of domain signals in a multimodal space. Complementing this, surgical concept erasure in diffusion models provides concrete tactics and metrics for targeted forgetting under retention constraints, a blueprint adapted here from generative models to VLMs. Finally, CoOp\u2019s prompt-based control of CLIP highlights how domains can be specified and evaluated via textual prompts, offering supervision and benchmarks for domain-targeted forgetting. Together, these works crystallize an approach that selectively erases domain evidence while maintaining VLM competence.",
  "analysis_timestamp": "2026-01-07T00:05:12.560663"
}
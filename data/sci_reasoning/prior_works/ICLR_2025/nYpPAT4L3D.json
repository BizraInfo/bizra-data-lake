{
  "prior_works": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": "Radford et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "fVLM adopts the CLIP-style contrastive image\u2013text pretraining paradigm and extends it from global image\u2013report alignment to explicit anatomy\u2013sentence alignment for CT."
    },
    {
      "title": "ConVIRT: Contrastive Learning of Medical Visual Representations from Paired Images and Text",
      "authors": "Zhang et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "ConVIRT established using radiology reports as supervision for medical images via global contrastive learning, which fVLM directly builds upon while addressing ConVIRT\u2019s limitation of ignoring local region\u2013sentence associations."
    },
    {
      "title": "GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition",
      "authors": "Huang et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "GLoRIA\u2019s word\u2013patch global-local alignment for chest X-rays inspired fVLM\u2019s fine-grained alignment, which fVLM generalizes to 3D CT by explicitly aligning anatomy regions with sentence-level report descriptions."
    },
    {
      "title": "BioViL: Self-supervised Vision\u2013Language Pretraining for Biomedical Tasks",
      "authors": "Boecking et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "BioViL demonstrates weak/attention-based grounding with global report supervision on chest X-rays, and fVLM explicitly addresses this gap by enforcing anatomy\u2013sentence contrastive matching with explicit anatomical regions in CT."
    },
    {
      "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports",
      "authors": "Jain et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "fVLM relies on structured extraction of anatomical entities and findings from reports, a capability operationalized by RadGraph, to map sentences to specific anatomies for supervision."
    },
    {
      "title": "TotalSegmentator: Robust Segmentation of 104 Anatomical Structures in CT Images",
      "authors": "Wasserthal et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "The availability of accurate, automatic multi-organ CT segmentation from TotalSegmentator enables fVLM to define anatomy-level regions that are explicitly aligned to corresponding report sentences during fine-grained contrastive pretraining."
    }
  ],
  "synthesis_narrative": "The core innovation of fVLM is moving from coarse, study-level image\u2013report contrast to explicit anatomy\u2013sentence alignment for CT, and its lineage is a direct progression across vision\u2013language pretraining and radiology-specific advances. CLIP laid the foundational contrastive image\u2013text formulation that fVLM retains but applies at multiple granularities. ConVIRT translated this paradigm to radiology by supervising medical images with paired reports, yet it remained global; fVLM targets this precise shortcoming by aligning localized image regions with the textual units that describe them. GLoRIA demonstrated that local (token\u2013patch) alignment improves medical representation learning on chest X-rays; fVLM generalizes this idea to the CT domain and elevates the granularity from generic patches to anatomically meaningful 3D regions paired with sentence-level descriptions. BioViL further highlighted the promise and limitations of global supervision with weak attention-based grounding, motivating fVLM\u2019s explicit region\u2013sentence contrast as a remedy for interpretability and performance. Two enabling components make anatomy-level alignment practical at scale: RadGraph\u2019s structured extraction of anatomical entities and relations from radiology text, which supports sentence-to-anatomy attribution, and TotalSegmentator\u2019s robust multi-organ CT segmentation, which supplies anatomy masks/regions for visual grounding. Together, these works directly informed and enabled fVLM\u2019s fine-grained, anatomy-aware vision\u2013language pretraining for enhanced CT understanding.",
  "analysis_timestamp": "2026-01-06T23:09:26.637702"
}
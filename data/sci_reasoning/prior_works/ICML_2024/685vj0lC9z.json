{
  "prior_works": [
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang, Jeff Wu, Xu Jiang, et al.",
      "year": 2022,
      "role": "Foundational method for alignment via RLHF",
      "relationship_sentence": "This paper provides the core RLHF procedure whose effects on honesty and helpfulness the ICML 2024 study explicitly measures, showing RLHF can jointly improve both properties."
    },
    {
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
      "authors": "Yuntao Bai, Andy Jones, Kamal Ndousse, et al.",
      "year": 2022,
      "role": "Alignment target defining \u201cHHH\u201d conversational values",
      "relationship_sentence": "By operationalizing helpfulness, harmlessness, and honesty as optimization targets, this work sets the value framework the new paper probes to see which values LLMs internalize and how they trade off."
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei, Xuezhi Wang, Dale Schuurmans, et al.",
      "year": 2022,
      "role": "Inference-time reasoning technique",
      "relationship_sentence": "The ICML paper tests chain-of-thought prompting as introduced here and finds it skews models toward helpfulness over honesty, directly evaluating CoT\u2019s behavioral side effects."
    },
    {
      "title": "Pragmatic language interpretation as probabilistic inference",
      "authors": "Noah D. Goodman, Michael C. Frank",
      "year": 2016,
      "role": "Psychological framework (Rational Speech Act) for cooperative communication",
      "relationship_sentence": "The study adopts RSA-style pragmatic reasoning to structure tasks where speakers trade off truthfulness and informativeness, enabling human\u2013LLM comparisons grounded in cognitive theory."
    },
    {
      "title": "Reasoning About Pragmatics with Neural Listeners and Speakers",
      "authors": "Jacob Andreas, Dan Klein",
      "year": 2016,
      "role": "Bridge between pragmatic theory and machine language models",
      "relationship_sentence": "This work demonstrates how pragmatic speaker\u2013listener models can be instantiated with neural systems, foreshadowing the ICML paper\u2019s application of psychological paradigms to LLM behavior."
    },
    {
      "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
      "authors": "Stephanie Lin, Jacob Hilton, Owain Evans",
      "year": 2021,
      "role": "Benchmark and methodology for evaluating truthfulness",
      "relationship_sentence": "By formalizing measurements of honesty in LMs, TruthfulQA motivates the ICML paper\u2019s focus on honesty metrics and complements its analysis of honesty\u2013helpfulness trade-offs."
    },
    {
      "title": "Logic and Conversation",
      "authors": "H. P. Grice",
      "year": 1975,
      "role": "Foundational theory of conversational maxims (Quality vs. Quantity/Relation)",
      "relationship_sentence": "Grice\u2019s maxims provide the conceptual backbone for the paper\u2019s central question about when a cooperative speaker prioritizes strict truth (Quality) versus informativeness/helpfulness (Quantity/Relation)."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014using psychological models of pragmatic communication to dissect how LLMs navigate honesty\u2013helpfulness trade-offs, and testing how RLHF and inference-time reasoning shape these values\u2014emerges at the intersection of alignment, prompting, and cognitive pragmatics. Grice\u2019s conversational maxims crystallize the tension between strict truthfulness (Quality) and cooperative informativeness (Quantity/Relation), a tension that the Rational Speech Act framework formalizes as probabilistic reasoning about speaker and listener goals. Building on this theory, neural pragmatics work showed how speaker\u2013listener modeling could be integrated with machine learning, paving the way for applying human experimental paradigms to modern LLMs.\n\nOn the alignment side, InstructGPT established RLHF as a practical method for optimizing models for human preferences, while Anthropic\u2019s helpful\u2013harmless (and honest) alignment agenda defined concrete conversational values. These works directly motivate the paper\u2019s empirical tests of whether RLHF internalizes honesty and helpfulness and how those values trade off. Inference-time reasoning via chain-of-thought prompting, introduced by Wei et al., provided a controllable mechanism the authors could manipulate; their finding that CoT shifts behavior toward helpfulness over honesty isolates a specific behavioral effect of test-time reasoning. Finally, TruthfulQA\u2019s methodology for measuring model truthfulness informs the paper\u2019s honesty assessments and contextualizes improvements due to RLHF. Together, these lines of work enable the paper\u2019s central contribution: a psychologically grounded, model-agnostic framework showing that modern LLMs exhibit human-like pragmatics and that their abstract conversational values can be predictably steered by alignment training and zero-shot prompting.",
  "analysis_timestamp": "2026-01-07T00:02:04.889811"
}
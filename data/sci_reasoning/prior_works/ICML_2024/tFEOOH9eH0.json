{
  "prior_works": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "The paper aligns a tactile encoder to a vision\u2013language embedding for open\u2011vocabulary recognition; CLIP provides the core contrastive formulation and target embedding space that this work explicitly trains the touch modality to join."
    },
    {
      "title": "ImageBind: One Embedding Space To Bind Them All",
      "authors": "Rohit Girdhar et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "ImageBind demonstrated aligning multiple non-text modalities into a CLIP\u2011anchored space via image as a hub; this work directly extends that paradigm to the tactile modality, which ImageBind did not cover, to realize tri\u2011modal touch\u2013vision\u2013language alignment."
    },
    {
      "title": "AudioCLIP: Extending CLIP to Image, Text and Audio",
      "authors": "Andrey A. Guzhov et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "AudioCLIP\u2019s strategy of distilling a new sensory encoder into the CLIP image\u2013text space directly informs this paper\u2019s method of training a tactile encoder to be compatible with vision\u2013language representations."
    },
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": "Junnan Li et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "The TVL model follows BLIP\u20112\u2019s recipe of coupling a frozen modality encoder to a frozen LLM via a small adaptor for text generation, extending the approach by swapping in a trained tactile encoder to enable touch\u2011conditioned generation."
    },
    {
      "title": "Visual Instruction Tuning",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "LLaVA showed that GPT\u20114 can be used to synthesize large\u2011scale vision\u2013language supervision; this work adopts the same principle with GPT\u20114V to pseudo\u2011label 90% of vision\u2013touch pairs, enabling scalable language alignment for tactile data."
    },
    {
      "title": "GPT-4V(ision): Multimodal capabilities and system card",
      "authors": "OpenAI et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "GPT\u20114V is directly used to generate pseudo\u2011labels that provide the language supervision necessary to align tactile and visual observations with text at scale."
    },
    {
      "title": "PaLM-E: An Embodied Multimodal Language Model",
      "authors": "Martin Driess et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "PaLM\u2011E established multimodal LLMs for embodied settings without integrating touch; this omission motivates the present work\u2019s dataset and modeling to incorporate tactile sensing into multimodal alignment and generation."
    }
  ],
  "synthesis_narrative": "The core idea of this paper is to bind tactile sensing into the established vision\u2013language ecosystem for open\u2011vocabulary understanding and text generation. CLIP is the foundational scaffold: its contrastive image\u2013text space enables open\u2011vocabulary semantics, and the paper\u2019s tactile encoder is explicitly trained to inhabit that space. ImageBind then provides the direct blueprint for unifying additional modalities by using images as the hub to align disparate sensors to the CLIP space; the present work extends this paradigm to a modality ImageBind did not include\u2014tactile\u2014closing a key gap. AudioCLIP further crystallizes the technique for bringing a new sensory encoder into CLIP via distillation, directly inspiring the training of a touch encoder compatible with vision\u2013language embeddings. For generative capabilities, BLIP\u20112\u2019s modular coupling of frozen encoders to frozen LLMs via lightweight adapters underpins the TVL model\u2019s architecture, with the tactile encoder substituted to enable touch\u2011conditioned generation. Scaling language supervision is made possible by LLaVA\u2019s insight to use GPT\u20114 for synthetic vision\u2013language data, realized here with GPT\u20114V to pseudo\u2011label 90% of the dataset at scale. Finally, PaLM\u2011E highlights the broader limitation that state\u2011of\u2011the\u2011art embodied multimodal LLMs omit touch, directly motivating a tri\u2011modal dataset and alignment method that demonstrably improves tactile\u2013vision\u2013language grounding.",
  "analysis_timestamp": "2026-01-06T23:09:26.460707"
}
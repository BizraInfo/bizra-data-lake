{
  "prior_works": [
    {
      "title": "Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks",
      "authors": "Buesing et al.",
      "year": 2011,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Established that recurrent stochastic neural dynamics can implement MCMC/Langevin sampling, which this work adopts as the neural mechanism for drawing samples from priors and posteriors."
    },
    {
      "title": "Cortical-like Dynamics in Recurrent Circuits Performing Sampling-Based Probabilistic Inference",
      "authors": "Echeveste et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Provided a recurrent-network implementation of sampling-based inference driven by inputs, which this paper advances by endowing the circuit with a learned, complex prior via dendritic denoising and an oscillation-controlled noise schedule."
    },
    {
      "title": "What Regularized Auto-Encoders Learn from the Data-Generating Distribution",
      "authors": "Alain and Bengio",
      "year": 2014,
      "arxiv_id": "1211.4246",
      "role": "Foundation",
      "relationship_sentence": "Showed that denoising mappings estimate the score (gradient of log-density), directly motivating the paper\u2019s use of dendritic nonlinearities trained for denoising to represent the prior\u2019s score."
    },
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Ho et al.",
      "year": 2020,
      "arxiv_id": "2006.11239",
      "role": "Inspiration",
      "relationship_sentence": "Demonstrated that iterative denoising across noise scales learns powerful priors, inspiring the circuit\u2019s denoising-based prior representation and the idea of controlling noise levels during sampling."
    },
    {
      "title": "Generative Modeling by Estimating Gradients of the Data Distribution",
      "authors": "Song and Ermon",
      "year": 2019,
      "arxiv_id": "1907.05600",
      "role": "Extension",
      "relationship_sentence": "Introduced annealed Langevin dynamics with learned score functions, which this work neurally instantiates by modulating somatic noise via a global oscillator to implement a noise schedule."
    },
    {
      "title": "Regularization by Denoising: Clarifications and New Interpretations",
      "authors": "Romano et al.",
      "year": 2017,
      "arxiv_id": "1611.02862",
      "role": "Related Problem",
      "relationship_sentence": "Framed denoisers as implicit priors combined with data-fidelity terms, paralleling this paper\u2019s use of denoising dendrites (prior) with sensory/context inputs (likelihood) to shape posterior sampling."
    },
    {
      "title": "Diffusion Posterior Sampling for General Noisy Linear Inverse Problems",
      "authors": "Chung et al.",
      "year": 2022,
      "arxiv_id": "2209.14687",
      "role": "Gap Identification",
      "relationship_sentence": "Showed how score-based priors can be combined with likelihoods to sample posteriors, highlighting the absence of a neurally plausible mechanism that this paper provides via recurrent sampling with oscillatory noise control."
    }
  ],
  "synthesis_narrative": "Recurrent stochastic neural dynamics were shown to implement sampling-based computation, with early work establishing that network trajectories can realize MCMC/Langevin sampling from an energy landscape (Buesing et al.). Later, recurrent circuits with cortical-like dynamics demonstrated sampling-based probabilistic inference driven by inputs, validating a biologically grounded route to posterior sampling in neural networks (Echeveste et al.). In parallel, denoising autoencoders were proven to estimate the score\u2014the gradient of the log data density\u2014linking denoising objectives to probabilistic priors (Alain and Bengio). Building on this, denoising diffusion models revealed that training denoisers across noise scales yields expressive priors usable through iterative denoising (Ho et al.), while annealed Langevin dynamics with learned scores formalized sampling with a noise schedule that traverses scales (Song and Ermon). In inverse problems, denoisers were cast as implicit priors combined with data fidelity (Romano et al.), and diffusion posterior sampling explicitly combined score-based priors with likelihood gradients to sample posteriors (Chung et al.). Together, these works suggested that a neurally plausible circuit could implement flexible posterior sampling if it could (i) represent complex priors via a denoising-derived score and (ii) control a noise schedule during dynamics. The present synthesis takes the natural next step: dendritic nonlinearities trained for denoising instantiate the prior\u2019s score, while a global oscillation modulates somatic noise to realize annealed Langevin-like dynamics; recurrent interactions provide the sampler, and sensory or contextual inputs supply likelihood terms, collectively yielding a circuit that samples from complex priors and flexibly encodes task-specific posteriors.",
  "target_paper": {
    "title": "Complex priors and flexible inference in recurrent circuits with dendritic nonlinearities",
    "authors": "Benjamin S. H. Lyo, Cristina Savin",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "computational neuroscience, probabilistic coding, neural sampling, priors",
    "abstract": "Despite many successful examples in which probabilistic inference can account for perception, we have little understanding of how the brain represents and uses structured priors that capture the complexity of natural input statistics. Here we construct a recurrent circuit model that can implicitly represent priors over latent variables, and combine them with sensory and contextual sources of information to encode task-specific posteriors. Inspired by the recent success of diffusion models as means of learning and using priors over images, our model uses dendritic nonlinearities optimized for denoising, and stochastic somatic integration with the degree of noise modulated by an oscillating global signal. Combining these elements into a recurrent network yields a stochastic dynamical system that samples from the prior at a rate prescribed by the period of the global oscillator. Additional inputs reflecting sensory or top-down contextual information alter these dynamics to generate sample",
    "openreview_id": "S5aUhpuyap",
    "forum_id": "S5aUhpuyap"
  },
  "analysis_timestamp": "2026-01-06T16:53:03.456381"
}
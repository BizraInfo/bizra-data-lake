{
  "prior_works": [
    {
      "title": "SpikeProp: Backpropagation for networks of spiking neurons",
      "authors": "J.M. Bohte, J.N. Kok, H. La Poutre",
      "year": 2002,
      "role": "Foundational time-based supervised learning for SNNs",
      "relationship_sentence": "Introduced gradient-based learning through spike times, providing the core calculus that enables mapping objective functions into the time domain\u2014an idea this paper leverages when translating rate-based losses to time-based counterparts."
    },
    {
      "title": "Supervised learning based on temporal coding in spiking neural networks",
      "authors": "Hesham Mostafa",
      "year": 2017,
      "role": "Temporal-coding objective (TTFS) and time-based losses",
      "relationship_sentence": "Showed how class information can be encoded in precise spike times (e.g., time-to-first-spike) with differentiable training, directly informing the paper\u2019s emphasis on designing and analyzing time-based loss formulations."
    },
    {
      "title": "SLAYER: Spike Layer Error Reassignment in Time",
      "authors": "Sumit Bam Shrestha, Garrick Orchard",
      "year": 2018,
      "role": "Temporal credit assignment and spike-train losses via surrogate gradients",
      "relationship_sentence": "Provided a principled temporal loss and surrogate gradient framework for errors defined over spike trains, a direct precursor to the paper\u2019s argument that rate-style objectives can be re-expressed and optimized in the time domain."
    },
    {
      "title": "SuperSpike: Supervised learning in multilayer spiking neural networks",
      "authors": "Friedemann Zenke, Surya Ganguli",
      "year": 2018,
      "role": "Surrogate gradient methodology and gradient flow analysis",
      "relationship_sentence": "Established practical surrogate gradients for non-differentiable spikes and analyzed gradient flow, underpinning this paper\u2019s theoretical focus on ensuring sufficiently positive overall gradients for effective time-based training."
    },
    {
      "title": "Spatio-Temporal Backpropagation for Training High-Performance Spiking Neural Networks (STBP)",
      "authors": "Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, Luping Shi",
      "year": 2018,
      "role": "Direct training with rate-coded objectives and spike-count losses",
      "relationship_sentence": "Popularized direct SNN training with surrogate gradients and mean-square spike-count objectives\u2014the very baseline (mean square count) the present paper critiques and replaces with an enhanced counting loss."
    },
    {
      "title": "Conversion of continuous-valued deep networks to efficient event-driven spiking neural networks",
      "authors": "Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, Shih-Chii Liu",
      "year": 2017,
      "role": "Rate coding formalization and spike-count as proxy for activations",
      "relationship_sentence": "Formalized the correspondence between analog activations and spike rates, motivating the prevalence of rate-coded losses and providing the conceptual bridge the paper exploits to map rate-based objectives into time-based forms."
    },
    {
      "title": "Temporal Efficient Training of Spiking Neural Networks (TET)",
      "authors": "Deng et al.",
      "year": 2022,
      "role": "Time-based training with per-timestep losses dominated by rate coding",
      "relationship_sentence": "Demonstrated effective time-dimension training yet with rate-dominated objectives, directly motivating the paper\u2019s analysis of gradient positivity and its proposal of an enhanced counting loss better aligned with time-based learning."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014systematically mapping rate-based losses to time-based counterparts and proposing an enhanced counting loss (ECL) guided by a positive overall gradient principle\u2014rests on two converging threads of prior work. First, time-based learning foundations (Bohte et al., Mostafa, SLAYER) established how to formulate and optimize objectives directly over spike times or spike trains. SpikeProp introduced backpropagation through spike timing, Mostafa\u2019s TTFS formulation demonstrated supervised learning with precise spike timing, and SLAYER operationalized temporal credit assignment and spike-train losses with surrogate gradients. These works make it natural to reinterpret conventional objectives in the time domain.\nSecond, the modern surrogate-gradient training lineage (SuperSpike; STBP) enabled scalable learning but predominantly with rate-coded targets and spike-count losses. STBP, in particular, popularized the mean-square spike-count objective that this paper diagnoses as suboptimal for time-based training. Complementing this, Rueckauer et al. formalized the rate\u2013activation correspondence, explaining why rate-style losses became default choices, while TET exemplified recent time-oriented training that still leans on rate-dominated objectives.\nBy synthesizing these lines, the authors justify why rate-based losses can be validly mapped to time-based forms, then use gradient-flow insights in the surrogate-gradient framework to argue for losses that ensure adequate positive overall gradients. This directly motivates and yields their enhanced counting loss, a drop-in replacement for mean-square count that better exploits temporal information in time-based SNN training.",
  "analysis_timestamp": "2026-01-07T00:02:04.864637"
}
{
  "prior_works": [
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": "Junnan Li et al.",
      "year": 2023,
      "arxiv_id": "2301.12597",
      "role": "Foundation",
      "relationship_sentence": "BLIP-2 established the paradigm of converting a text-only LLM into a multimodal system by keeping the LLM mostly frozen and training a lightweight bridge, which this work reframes as domain adaptation and replaces with tuning only attention LayerNorms."
    },
    {
      "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "authors": "Jean-Baptiste Alayrac et al.",
      "year": 2022,
      "arxiv_id": "2204.14198",
      "role": "Foundation",
      "relationship_sentence": "Flamingo showed that inserting small, trainable cross-attention modules into a largely frozen LLM can yield strong multimodal capabilities, directly motivating the present focus on the attention block as the key locus of adaptation where only LayerNorms are tuned."
    },
    {
      "title": "Visual Instruction Tuning",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "arxiv_id": "2304.08485",
      "role": "Foundation",
      "relationship_sentence": "LLaVA introduced multimodal instruction tuning with conversational data to turn an LLM into an MLLM, a recipe this work streamlines by showing that selective conversational tuning combined with attention LayerNorm tuning is sufficient and more efficient."
    },
    {
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": "Edward J. Hu et al.",
      "year": 2022,
      "arxiv_id": "2106.09685",
      "role": "Baseline",
      "relationship_sentence": "LoRA is the primary parameter-efficient finetuning baseline that this paper explicitly targets, with the proposed attention-LayerNorm-only tuning demonstrating higher multimodal performance while using fewer trainable parameters and memory."
    },
    {
      "title": "LLaMA-Adapter: Efficient Fine-tuning of LLaMA",
      "authors": "Zhang et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "LLaMA-Adapter injects lightweight adapters at attention layers to align external inputs with a frozen LLM, informing the insight that the attention stack is the critical interface which this work further simplifies by tuning only its LayerNorms."
    },
    {
      "title": "Adaptive Batch Normalization for Practical Domain Adaptation",
      "authors": "Li et al.",
      "year": 2016,
      "arxiv_id": "1603.04779",
      "role": "Inspiration",
      "relationship_sentence": "AdaBN demonstrated that adapting only normalization parameters can bridge domain shift, directly inspiring the idea to treat modality shift as domain adaptation and adjust only LayerNorms in Transformer attention blocks."
    },
    {
      "title": "BitFit: Simple Parameter-Efficient Fine-Tuning for Transformers",
      "authors": "Ben Zaken et al.",
      "year": 2022,
      "arxiv_id": "2106.10199",
      "role": "Inspiration",
      "relationship_sentence": "BitFit showed that tuning a tiny subset of parameters (bias terms) can suffice for effective transfer, motivating the extreme minimalism of updating only LayerNorm parameters to achieve multimodal adaptation."
    }
  ],
  "synthesis_narrative": "BLIP-2 introduced a pragmatic route to multimodality by keeping a powerful LLM frozen and learning a small bridge (Q-Former) from vision to language, proving that most capacity can remain intact while a compact interface handles modality alignment. Flamingo reinforced this idea by inserting lightweight cross-attention modules into a largely frozen LLM, pinpointing the attention stack as the key locus where multimodal information should be integrated. LLaVA then showed that visual instruction tuning with conversational data is sufficient to endow an LLM with strong multimodal conversational abilities, suggesting that the adaptation signal can be distilled from dialogue-style supervision. LoRA established a dominant parameter-efficient finetuning baseline, enabling LLM adaptation via low-rank updates but still incurring nontrivial parameter and memory overhead. LLaMA-Adapter extended the \u201cadapt at attention\u201d principle with zero-init adapters, further underscoring attention layers as the crucial interface for modality alignment. AdaBN revealed a powerful domain adaptation insight: modifying only normalization statistics can correct distribution shift, highlighting normalization layers as high-leverage adaptation knobs. BitFit demonstrated that updating only a tiny subset of parameters can be surprisingly effective, encouraging minimalist finetuning strategies.\nTogether these works exposed attention layers as the natural integration point and normalization as a potent mechanism for distribution shift, while showing conversational instruction tuning provides sufficient supervision. The next step was to unify these insights: treat text-to-multimodal transfer as domain adaptation and adapt the attention interface with the smallest possible change\u2014tuning only LayerNorm parameters\u2014thereby surpassing LoRA-level efficiency while preserving or improving multimodal performance.",
  "target_paper": {
    "title": "Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning",
    "authors": "Bingchen Zhao, Haoqin Tu, Chen Wei, Jieru Mei, Cihang Xie",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "multi-modality; large language models; generation; model efficiency;",
    "abstract": "This paper introduces an efficient strategy to transform Large Language Models (LLMs) into Multi-Modal Large Language Models. \nBy conceptualizing this transformation as a domain adaptation process, \\ie, transitioning from text understanding to embracing multiple modalities, we intriguingly note that, within each attention block, tuning LayerNorm suffices to yield strong performance. \nMoreover, when benchmarked against other tuning approaches like full parameter finetuning or LoRA, its benefits on efficiency are substantial.\nFor example, when compared to LoRA on a 13B model scale, performance can be enhanced by an average of over 20\\% across five multi-modal tasks, and meanwhile, \nresults in a significant reduction of trainable parameters by 41.9\\% and a decrease in GPU memory usage by 17.6\\%. On top of this LayerNorm strategy, we showcase that selectively tuning only with conversational data can improve efficiency further. \nBeyond these empirical outcomes, we provide a comprehensive an",
    "openreview_id": "YR3ETaElNK",
    "forum_id": "YR3ETaElNK"
  },
  "analysis_timestamp": "2026-01-06T20:06:17.214637"
}
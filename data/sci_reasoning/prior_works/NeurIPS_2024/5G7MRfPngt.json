{
  "prior_works": [
    {
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan)",
      "authors": "Ahn et al.",
      "year": 2022,
      "role": "LLM-grounded embodied planning with affordance/value signals and subgoal decomposition",
      "relationship_sentence": "ICAL\u2019s abstraction into feasible subgoals and its use of execution-time feedback builds on SayCan\u2019s core idea of grounding language plans in environment affordances to select and refine action sequences."
    },
    {
      "title": "Code as Policies: Language Model Programs for Embodied Control",
      "authors": "Liang et al.",
      "year": 2023,
      "role": "LLM-generated executable programs that call perception/control modules for robot policies",
      "relationship_sentence": "ICAL\u2019s \u2018embodied programs of thought\u2019 mirror CaP\u2019s programmatic control formulation; ICAL extends this by distilling such programs from noisy demonstrations and iteratively correcting them with feedback."
    },
    {
      "title": "Voyager: An Open-Ended Embodied Agent in Minecraft",
      "authors": "Wang et al.",
      "year": 2023,
      "role": "Autonomous skill discovery with a growing library of reusable code skills and lifelong memory",
      "relationship_sentence": "ICAL\u2019s memory of generalized, reusable procedures echoes Voyager\u2019s skills library distilled from experience, but targets multimodal, causally annotated programs extracted from demonstrations."
    },
    {
      "title": "Reflexion: Language Agents with Verbal Self-Reflection",
      "authors": "Shinn et al.",
      "year": 2023,
      "role": "Feedback-driven self-critique and memory to iteratively improve agent performance",
      "relationship_sentence": "ICAL\u2019s loop of executing, receiving human/environment feedback, and revising abstractions is directly inspired by Reflexion\u2019s self-reflective updates that store lessons for future tasks."
    },
    {
      "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
      "authors": "Wang et al.",
      "year": 2022,
      "role": "LMs bootstrap their own high-quality instruction data from weak seeds",
      "relationship_sentence": "ICAL asks VLMs to generate higher-quality exemplars from sub-optimal demos, closely paralleling Self-Instruct\u2019s principle of self-generating and curating better in-context training data."
    },
    {
      "title": "DreamCoder: Growing Generalizable, Interpretable Knowledge via Library Learning",
      "authors": "Ellis et al.",
      "year": 2021,
      "role": "Iterative program synthesis with compression into reusable library abstractions",
      "relationship_sentence": "ICAL\u2019s distillation of trajectories into compositional programs with causal/temporal structure aligns with DreamCoder\u2019s abstraction-by-compression, turning experiences into reusable program primitives."
    },
    {
      "title": "D-REX: Deep Reward Extrapolation for Learning from Suboptimal Demonstrations",
      "authors": "Brown et al.",
      "year": 2019,
      "role": "Learning from imperfect demonstrations via preference-based extrapolation",
      "relationship_sentence": "ICAL\u2019s premise that sub-optimal demonstrations can be corrected and made useful through feedback follows D-REX\u2019s insight that informative structure can be extracted from imperfect demos."
    }
  ],
  "synthesis_narrative": "ICAL\u2019s core contribution\u2014distilling sub-optimal multimodal demonstrations into reusable, executable \u201cembodied programs of thought\u201d and iteratively refining them with feedback\u2014sits at the intersection of programmatic control, grounded planning, self-improvement, and self-generated data. SayCan established that language plans must be grounded in environmental affordances to yield feasible subgoals; ICAL leverages this principle when abstracting trajectories into actionably grounded steps. Code as Policies showed that representing plans as executable code that calls perception and control modules yields interpretable and compositional behavior; ICAL adopts this programmatic representation but learns it from noisy trajectories rather than prompting it directly. Voyager demonstrated that agents can accumulate a skill library from experience and retrieve it for new tasks; ICAL similarly builds a memory of generalized procedures, but focuses on causal/temporal annotations derived from demonstrations. Reflexion contributed the feedback-driven loop of self-critique and memory updates; ICAL uses human/environment feedback to correct inefficiencies and mistakes, refining abstractions over repeated executions. Self-Instruct provided a blueprint for LMs generating their own higher-quality exemplars, a key idea behind ICAL\u2019s transformation of sub-optimal demos into superior in-context examples. Finally, DreamCoder\u2019s library learning motivates ICAL\u2019s conversion of experiences into reusable abstractions, while D-REX underpins the premise that imperfect demonstrations remain valuable when paired with preference-like feedback. Together, these works directly scaffold ICAL\u2019s method for creating, refining, and reusing programmatic memories from imperfect multimodal experience.",
  "analysis_timestamp": "2026-01-06T23:33:36.288733"
}
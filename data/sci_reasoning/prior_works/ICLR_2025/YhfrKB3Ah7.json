{
  "prior_works": [
    {
      "title": "Preferential Bayesian Optimization",
      "authors": "Javier Gonz\u00e1lez et al.",
      "year": 2017,
      "role": "Baseline",
      "relationship_sentence": "This paper formalized PBO with pairwise-preference likelihoods and PES-style acquisitions, and PABBO directly replaces its per-iteration GP inference and acquisition computation with a fully amortized, meta-learned surrogate and policy."
    },
    {
      "title": "Preference Learning with Gaussian Processes",
      "authors": "Wei Chu et al.",
      "year": 2005,
      "role": "Foundation",
      "relationship_sentence": "GP-based preference learning (GPPL) introduced the probit-link pairwise likelihood that underpins PBO; PABBO targets the resulting non-conjugate inference bottleneck by amortizing posterior inference with a neural-process surrogate."
    },
    {
      "title": "Predictive Entropy Search for Efficient Global Optimization of Black-box Functions",
      "authors": "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato et al.",
      "year": 2014,
      "role": "Gap Identification",
      "relationship_sentence": "Information-theoretic acquisitions like PES (adapted in PBO) are computationally heavy; PABBO explicitly addresses this limitation by learning an amortized acquisition policy via reinforcement learning to avoid repeated costly acquisition computations."
    },
    {
      "title": "Conditional Neural Processes",
      "authors": "Marta Garnelo et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "CNP introduced amortized, task-distribution-level function inference from context sets; PABBO adopts this neural-process paradigm to amortize inference over latent utility functions from preference data across tasks."
    },
    {
      "title": "Attentive Neural Processes",
      "authors": "Hyunjik Kim et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "ANP showed that attention improves neural-process surrogates; PABBO extends this line with a transformer neural-process architecture tailored to encode sets of comparisons for preference BO."
    },
    {
      "title": "Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization",
      "authors": "Matthias Volpp et al.",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "This work demonstrated that acquisition strategies can be meta-learned across tasks (e.g., with policy-gradient training); PABBO directly builds on this idea to meta-learn the acquisition policy in the preferential setting while jointly training the surrogate."
    },
    {
      "title": "Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks",
      "authors": "Juho Lee et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "Set Transformer established transformer-based, permutation-invariant encoders for set-structured inputs; PABBO leverages this architectural principle in its transformer neural process to handle unordered context/query sets of comparisons."
    }
  ],
  "synthesis_narrative": "PABBO\u2019s core innovation\u2014fully amortizing preferential Bayesian optimization by jointly meta-learning the surrogate and acquisition\u2014arises at the intersection of preference-based BO and amortized meta-inference. Preferential Bayesian Optimization (Gonz\u00e1lez et al., 2017) defined the problem setting of querying pairs and modeling latent utilities, typically with a GP preference model from Chu and Ghahramani (2005). However, the non-conjugate preference likelihood and information-theoretic acquisitions (e.g., Predictive Entropy Search; Hern\u00e1ndez-Lobato et al., 2014) make each PBO step computationally intensive, creating a practical bottleneck for interactive human-in-the-loop use. Neural Processes (Garnelo et al., 2018) offered a blueprint for amortized inference across a task distribution, directly enabling PABBO to replace per-iteration GP inference with a learned, distribution-conditioned surrogate. Attentive Neural Processes (Kim et al., 2019) and Set Transformer (Lee et al., 2019) informed PABBO\u2019s transformer neural process architecture, providing attention-based, permutation-invariant encoders tailored to set-structured context and comparison data. Complementing the surrogate, meta-learning acquisition policies across tasks had been shown feasible by Volpp et al. (2020), motivating PABBO\u2019s reinforcement-learning-based acquisition that removes repeated expensive acquisition optimization. Together, these works shape PABBO\u2019s direct lineage: from the PBO formulation and its computational pain points (GPPL and PES), through amortized meta-inference (NP/ANP) and set-attention architectures, to meta-learned acquisition policies\u2014culminating in a practical, fully amortized framework for preference-driven black-box optimization.",
  "analysis_timestamp": "2026-01-06T23:09:26.627436"
}
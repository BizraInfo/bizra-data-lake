{
  "prior_works": [
    {
      "title": "3D Gaussian Splatting for Real-Time Radiance Field Rendering",
      "authors": "Bernhard Kerbl; Georgios Kopanas; Thomas Leimk\u00fchler; George Drettakis",
      "year": 2023,
      "role": "Representation blueprint (Gaussian primitives and splatting)",
      "relationship_sentence": "GaussianFusion adopts learnable Gaussian primitives as compact, interpretable carriers and iteratively optimizes their parameters\u2014an idea directly inspired by 3DGS, adapted from 3D radiance fields to 2D scene-wide BEV Gaussians for multi-sensor fusion."
    },
    {
      "title": "Lift, Splat, Shoot: Encoding Images Into a Bird\u2019s-Eye View Grid for Autonomous Driving",
      "authors": "Jonathon Philion; Sanja Fidler",
      "year": 2020,
      "role": "Baseline BEV geometric transformation paradigm",
      "relationship_sentence": "By replacing LSS\u2019s dense lifting-and-splatting to a discrete BEV grid with continuous, compact Gaussian carriers, GaussianFusion directly addresses LSS\u2019s computational overhead while retaining an interpretable world-centric representation."
    },
    {
      "title": "BEVFormer: Learning Bird\u2019s-Eye-View Representation from Multi-Camera Images via Transformers",
      "authors": "Zhiqi Li et al.",
      "year": 2022,
      "role": "Transformer-based BEV feature learning baseline",
      "relationship_sentence": "GaussianFusion substitutes BEVFormer\u2019s query/grid BEV tokens with physically grounded Gaussian tokens, enabling progressive multi-modal refinement with improved efficiency and interpretability compared to transformer-heavy BEV reasoning."
    },
    {
      "title": "BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird\u2019s-Eye View Representation",
      "authors": "Yihan Liu et al.",
      "year": 2022,
      "role": "State-of-the-art BEV multi-sensor fusion baseline",
      "relationship_sentence": "GaussianFusion targets the same unified BEV fusion goal as BEVFusion but replaces dense BEV tensors with compact Gaussian carriers, reducing compute while maintaining a unified space for cross-sensor aggregation."
    },
    {
      "title": "TransFuser: End-to-End Autonomous Driving with Multi-Modal Transformers",
      "authors": "Aditya Prakash; Kashyap Chitta; Andreas Geiger (and collaborators)",
      "year": 2021,
      "role": "Attention-based flatten fusion for E2E driving",
      "relationship_sentence": "Positioned as an alternative to flatten attention fusion, GaussianFusion uses spatially localized Gaussian tokens to fuse camera/LiDAR features, mitigating the scalability and interpretability issues observed in TransFuser-style attention."
    },
    {
      "title": "SplatNet: Sparse Lattice Networks for Point Cloud Processing",
      "authors": "Hang Su; Varun Jampani; Deqing Sun; Orazio Gallo; Erik Learned-Miller; Jan Kautz",
      "year": 2018,
      "role": "Cross-domain feature splatting foundation",
      "relationship_sentence": "GaussianFusion\u2019s multi-modal aggregation via splatting onto continuous carriers is grounded in SplatNet\u2019s idea of projecting and aggregating features across domains using Gaussian/bilateral splats on a latent lattice."
    },
    {
      "title": "UniAD: Unified Perception and Planning for Autonomous Driving",
      "authors": "W. Qin et al.",
      "year": 2023,
      "role": "End-to-end joint perception\u2013prediction\u2013planning framework",
      "relationship_sentence": "GaussianFusion leverages UniAD-style end-to-end training and supervision targets, integrating its Gaussian intermediate representation into a unified pipeline that jointly benefits perception, prediction, and planning."
    }
  ],
  "synthesis_narrative": "GaussianFusion\u2019s core idea\u2014using compact, learnable Gaussian primitives as intermediate carriers for multi-sensor fusion in a unified driving scene\u2014sits at the intersection of three influential threads. First, 3D Gaussian Splatting demonstrated that explicit Gaussian primitives with optimizable means, covariances, and features can be an efficient, interpretable, and differentiable scene representation. GaussianFusion transposes this to a 2D world-centric canvas, using Gaussians as tokens that accumulate and refine multi-modal evidence over time. Second, prior fusion paradigms either relied on dense geometric lifting to BEV (Lift-Splat-Shoot; BEVFormer; BEVFusion) or attention-based flatten fusion (TransFuser). These methods established the value of a unified BEV space and cross-modal attention, but also highlighted limitations: heavy computation, memory, and limited interpretability of latent tokens. GaussianFusion addresses these by replacing discrete grids and opaque attention tokens with physically parameterized Gaussians that enable localized, continuous splatting and progressive refinement of explicit (semantic/spatial) and implicit features. Third, end-to-end autonomous driving frameworks like UniAD shaped the training and evaluation setting\u2014showing how joint perception\u2013prediction\u2013planning can be optimized holistically. GaussianFusion integrates within such pipelines, letting Gaussian carriers serve as the shared intermediate that feeds multiple heads. The mathematical precedent for cross-domain splatting from SplatNet further underpins the paper\u2019s multi-sensor aggregation mechanism. Together, these works directly scaffold GaussianFusion\u2019s interpretable, efficient, and end-to-end trainable fusion design.",
  "analysis_timestamp": "2026-01-07T00:21:32.306283"
}
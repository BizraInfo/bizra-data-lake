{
  "prior_works": [
    {
      "title": "Fairness in Learning: Classic and Contextual Bandits",
      "authors": "Matthew Joseph, Michael Kearns, Jamie Morgenstern, Aaron Roth",
      "year": 2016,
      "role": "Foundational work on imposing fairness constraints in bandit learning",
      "relationship_sentence": "Established how to enforce fairness while learning unknown rewards in bandits and showed intrinsic T^{2/3}-type rates under fairness, directly informing the paper\u2019s fairness-aware exploration design and regret benchmark."
    },
    {
      "title": "Online Learning with an Unknown Fairness Metric",
      "authors": "Stephen Gillen, Christopher Jung, Michael Kearns, Seth Neel, Aaron Roth",
      "year": 2018,
      "role": "Fairness-constrained online learning with distributional decisions",
      "relationship_sentence": "Demonstrated how to maintain fairness constraints during learning by acting over probability distributions and using confidence bounds, a template mirrored here when choosing distributions over arms while preserving fairness in expectation."
    },
    {
      "title": "Bandits with Knapsacks",
      "authors": "Ashwinkumar Badanidiyuru, Robert Kleinberg, Aleksandrs Slivkins",
      "year": 2013,
      "role": "Core modeling of bandits under linear resource/feasibility constraints",
      "relationship_sentence": "Provided the key reduction and perspective that one can select distributions over arms to satisfy linear constraints in expectation, which this paper adapts by treating envy-freeness and proportionality as linear fair-division constraints."
    },
    {
      "title": "Linear Contextual Bandits with Knapsacks",
      "authors": "Shipra Agrawal, Nikhil R. Devanur",
      "year": 2016,
      "role": "Algorithmic techniques for constrained bandits via mixed actions",
      "relationship_sentence": "Reinforced the practice of operating over action distributions and leveraging primal-dual style reasoning for linear constraints, a methodological backbone for allocating items via distributions that respect fairness while learning values."
    },
    {
      "title": "The Efficient Allocation of Individuals to Positions",
      "authors": "A. Hylland, R. Zeckhauser",
      "year": 1979,
      "role": "Foundational ex-ante (in-expectation) fairness via randomized assignments",
      "relationship_sentence": "Introduced lottery-based allocation ensuring ex-ante envy-freeness/proportionality, motivating this paper\u2019s formulation of fairness in expectation as linear constraints over allocation probabilities."
    },
    {
      "title": "A New Solution to the Random Assignment Problem",
      "authors": "A. Bogomolnaia, H. Moulin",
      "year": 2001,
      "role": "Randomized allocation achieving ex-ante envy-freeness",
      "relationship_sentence": "Showed how probabilistic serial attains ex-ante fairness for indivisible goods, underpinning the paper\u2019s choice of envy-freeness and proportionality in expectation as the fairness notions enforced during learning."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014casting online fair division with unknown values as a constrained stochastic bandit problem and designing an explore-then-commit algorithm with \\tilde{O}(T^{2/3}) regret under ex-ante envy-freeness or proportionality\u2014sits at the intersection of two threads. From fair division and random assignment, Hylland\u2013Zeckhauser (1979) and Bogomolnaia\u2013Moulin (2001) crystallized ex-ante fairness: fairness in expectation achieved by lotteries over indivisible allocations. This perspective makes fairness constraints linear in allocation probabilities, a structural property the present work exploits to maintain fairness at every step while learning. From online learning, Bandits with Knapsacks (Badanidiyuru\u2013Kleinberg\u2013Slivkins, 2013) and its linear contextual extension (Agrawal\u2013Devanur, 2016) established that one can act over distributions of arms to satisfy linear constraints in expectation, providing the modeling bridge that turns fair division with unknown utilities into a constrained bandit. The fairness-in-bandits line (Joseph\u2013Kearns\u2013Morgenstern\u2013Roth, 2016; Gillen\u2013Jung\u2013Kearns\u2013Neel\u2013Roth, 2018) demonstrated both methodology and performance limits for learning under fairness constraints, notably the emergence of T^{2/3}-type rates and confidence-based exploration that preserves fairness. Synthesizing these ideas, the paper leverages the random-assignment linearity of envy-freeness/proportionality to define tractable constraints over arm distributions and adopts a fairness-aware explore-then-commit schedule tailored to those constraints, achieving fast no-regret while guaranteeing ex-ante fairness throughout.",
  "analysis_timestamp": "2026-01-06T23:33:35.575360"
}
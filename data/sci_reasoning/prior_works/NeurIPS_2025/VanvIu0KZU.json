{
  "prior_works": [
    {
      "title": "Diffusion Maps",
      "authors": "R. R. Coifman et al.",
      "year": 2006,
      "role": "Foundation",
      "relationship_sentence": "Diffusion geometry provides the affinity-based manifold framework our method uses to construct data-informed trees whose structures guide the Tree-Wasserstein computations across modes."
    },
    {
      "title": "Treelets: An adaptive multiscale basis for sparse unordered data",
      "authors": "Ann B. Lee et al.",
      "year": 2008,
      "role": "Inspiration",
      "relationship_sentence": "Treelets introduced hierarchical Haar-like representations on learned trees, directly inspiring our use of tree-based multiscale structure and Haar-style edge weights in the TWD-driven hierarchy."
    },
    {
      "title": "The phylogenetic Kantorovich\u2013Rubinstein metric for environmental sequence samples",
      "authors": "Frederick A. Matsen et al.",
      "year": 2012,
      "role": "Foundation",
      "relationship_sentence": "This work establishes the closed-form W1 (KR) formula on trees as a sum of weighted edge mass imbalances, which is the exact computational primitive our alternating sample\u2013feature TWD relies on."
    },
    {
      "title": "Fast image retrieval via embeddings of Earth Mover\u2019s Distance into normed spaces",
      "authors": "Piotr Indyk et al.",
      "year": 2003,
      "role": "Foundation",
      "relationship_sentence": "By showing EMD can be efficiently approximated via tree metrics, this paper laid the algorithmic groundwork that motivates employing tree metrics for scalable transport, a premise we adopt with exact TWD on learned trees."
    },
    {
      "title": "Tree-Sliced Wasserstein Distances",
      "authors": "Le et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Tree-sliced Wasserstein demonstrated computational benefits of tree-based OT but relied on random or uninformed trees, a limitation our method addresses by jointly learning data-informed trees for both samples and features with convergence guarantees."
    },
    {
      "title": "Co-clustering documents and words using bipartite spectral graph partitioning",
      "authors": "Inderjit S. Dhillon",
      "year": 2001,
      "role": "Foundation",
      "relationship_sentence": "This seminal co-clustering formulation defined the two-mode organization problem that we generalize to hierarchical representations by alternating tree construction and TWD across samples and features."
    },
    {
      "title": "Hyperbolic Graph Convolutional Networks",
      "authors": "Ines Chami et al.",
      "year": 2019,
      "role": "Baseline",
      "relationship_sentence": "HGCN is the primary downstream model we enhance; our learned hierarchical TWD pre-processing improves its link prediction and node classification, making it the key baseline our method advances."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core idea\u2014jointly and iteratively learning hierarchical representations of samples and features using Tree-Wasserstein Distance\u2014sits at the intersection of diffusion geometry, tree-based multiscale representations, and tree-metric optimal transport. Diffusion Maps (Coifman et al., 2006) provides the manifold-geometry foundation for constructing data-driven affinities from which meaningful hierarchical trees can be built. Treelets (Lee et al., 2008) contributes the notion of hierarchical Haar-like representations on learned trees, informing how edge-based multiscale structure can parameterize distances and representations. On the optimal transport side, Indyk and Thaper (2003) established the value of tree metrics for efficient EMD, while Matsen et al. (2012) gave the exact closed-form for W1 on trees as weighted edge mass differences\u2014precisely the computational primitive exploited by the proposed alternating TWD scheme. Tree-Sliced Wasserstein (Le et al., 2019) popularized tree-based OT for scalability but used random or uninformed trees; the present work directly addresses this gap by learning data-informed trees and coupling them across modes with a provable convergent alternation. Conceptually, the problem follows the two-mode organization set by spectral co-clustering (Dhillon, 2001), now lifted to hierarchical, transport-aware geometry. Finally, integrating the learned hierarchies as pre-processing for Hyperbolic GCNs (Chami et al., 2019) demonstrates practical gains, positioning HGCN as the primary baseline the method improves.",
  "analysis_timestamp": "2026-01-06T23:08:23.962440"
}
{
  "prior_works": [
    {
      "title": "Cognitive Mapping and Planning for Visual Navigation",
      "authors": "Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, Jitendra Malik",
      "year": 2017,
      "role": "Foundational method linking egocentric perception to allocentric maps for downstream decision-making",
      "relationship_sentence": "EgoEnv\u2019s core idea\u2014learning a representation that maps short egocentric observations to a local, bird\u2019s-eye spatial description of the surroundings\u2014builds directly on CMP\u2019s paradigm of constructing an explicit top-down map from first-person inputs."
    },
    {
      "title": "Neural Map: Structured Memory for Deep Reinforcement Learning",
      "authors": "Emilio Parisotto, Ruslan Salakhutdinov",
      "year": 2018,
      "role": "Structured spatial memory representation",
      "relationship_sentence": "By encoding environment knowledge into a spatially indexed memory, Neural Map inspired EgoEnv\u2019s use of environment-aware features that persist beyond the current view and support predictions about unseen local space."
    },
    {
      "title": "Semi-Parametric Topological Memory for Navigation",
      "authors": "Nikolay Savinov, Alexey Dosovitskiy, Vladlen Koltun",
      "year": 2018,
      "role": "Egocentric video to persistent environment memory",
      "relationship_sentence": "EgoEnv\u2019s goal of tethering egocentric video to a persistent representation of the surrounding environment echoes the SPTM idea of building long-term, queryable memories from first-person experience."
    },
    {
      "title": "Habitat: A Platform for Embodied AI Research",
      "authors": "Manolis Savva et al.",
      "year": 2019,
      "role": "Simulation platform enabling large-scale photorealistic egocentric training with full environment observability",
      "relationship_sentence": "EgoEnv\u2019s training on simulated agents with complete environment access is made feasible by Habitat\u2019s high-throughput, realistic simulators, which provide the supervision necessary to learn local-surroundings predictions."
    },
    {
      "title": "Replica: A Dataset of Photorealistic 3D Scenes",
      "authors": "Julian Straub et al.",
      "year": 2019,
      "role": "Photorealistic 3D indoor environments for supervision and sim-to-real transfer",
      "relationship_sentence": "EgoEnv relies on richly scanned 3D interiors to supervise ground-truth local environment layout during simulation training, a capability enabled by Replica-style reconstructions."
    },
    {
      "title": "Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D",
      "authors": "Julien Philion, Sanja Fidler",
      "year": 2020,
      "role": "Cross-view transformation to bird\u2019s-eye-view semantic/occupancy maps",
      "relationship_sentence": "EgoEnv\u2019s conversion of egocentric visual evidence into a local, top-down environment description is conceptually aligned with LSS\u2019s learned image-to-BEV mapping for scene understanding beyond the immediate field of view."
    },
    {
      "title": "Ego4D: Around the World in 3,000 Hours of Egocentric Video",
      "authors": "Kristen Grauman et al.",
      "year": 2022,
      "role": "Large-scale egocentric benchmark highlighting human-centric tasks and the limits of short-clip features",
      "relationship_sentence": "Ego4D\u2019s tasks and scale motivate EgoEnv\u2019s environment-aware representations, and provide real-world evaluation to demonstrate the benefits over traditional short-clip features."
    }
  ],
  "synthesis_narrative": "EgoEnv\u2019s key contribution is to bridge egocentric video with a persistent, human-centric understanding of the surrounding environment by learning features predictive of a wearer\u2019s (even unseen) local surroundings, trained entirely in simulation and transferred to real videos. This idea stands on three converging lines of prior work. First, cognitive mapping and spatial memory methods from embodied AI\u2014exemplified by Cognitive Mapping and Planning (Gupta et al.) and Neural Map (Parisotto & Salakhutdinov), as well as Semi-Parametric Topological Memory (Savinov et al.)\u2014established that first-person observations can be transformed into allocentric, persistent map-like representations that support reasoning beyond the current view. EgoEnv adopts this principle but tailors it to human-centric video understanding by learning representations explicitly predictive of local surroundings, not only for navigation but to enrich egocentric video features.\nSecond, simulation ecosystems such as Habitat, combined with photorealistic 3D scene datasets like Replica, make it possible to supervise these representations with full environment observability, enabling learning to predict out-of-view spatial context from egocentric inputs. This simulation-first training is crucial to amass the diverse supervision required for environment-aware features and underpins EgoEnv\u2019s sim-to-real transfer.\nThird, cross-view scene understanding methods like Lift, Splat, Shoot demonstrate how to transform perspective images into bird\u2019s-eye spatial maps, conceptually paralleling EgoEnv\u2019s egocentric-to-local-map prediction. Finally, large-scale egocentric datasets such as Ego4D crystallize the need: short-clip features often ignore persistent environment structure, and EgoEnv\u2019s representations directly address this gap, improving human-centric video tasks in the wild.",
  "analysis_timestamp": "2026-01-07T00:02:04.810520"
}
{
  "prior_works": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "role": "Foundation vision-language model for aligning images and text and enabling zero-shot transfer.",
      "relationship_sentence": "InstructHOI builds on CLIP-style multi-modal alignment to represent HOI categories and to ground instruction-driven reasoning in a shared image\u2013text space."
    },
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": "Junnan Li, Dongxu Li, Silvio Savarese, Steven C. H. Hoi",
      "year": 2023,
      "role": "Architecture that connects vision encoders to LLMs for general-purpose multimodal reasoning with minimal task-specific tuning.",
      "relationship_sentence": "InstructHOI\u2019s multi-modal reasoning pipeline and HOI-domain fine-tuning align with the BLIP-2 paradigm of coupling a frozen visual backbone with a language model to elicit reasoning over images."
    },
    {
      "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
      "authors": "Wenliang Dai, Junnan Li, Dongxu Li, Silvio Savarese, Steven C. H. Hoi",
      "year": 2023,
      "role": "Introduced visual instruction tuning to teach VLMs to follow task-specific instructions and reason over images.",
      "relationship_sentence": "InstructHOI directly adopts the instruction-tuning principle, constructing 140K interaction\u2013reasoning pairs to endow a VLM with HOI-specific reasoning skills."
    },
    {
      "title": "LLaVA: Visual Instruction Tuning",
      "authors": "Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee",
      "year": 2023,
      "role": "Pioneered generating large-scale image\u2013instruction datasets with LLMs and fine-tuning VLMs to follow context-rich instructions.",
      "relationship_sentence": "InstructHOI\u2019s Context-aware Instruction Generator and synthetic HOI reasoning corpus follow LLaVA\u2019s recipe of LLM-driven instruction creation tailored to the target domain."
    },
    {
      "title": "Learning to Prompt for Vision-Language Models (CoOp)",
      "authors": "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu",
      "year": 2022,
      "role": "Showed that learnable prompts can adapt CLIP to downstream tasks, popularizing prompt-based transfer for vision-language models.",
      "relationship_sentence": "InstructHOI departs from CoOp-style static prompt transfer by replacing fixed prompts with instructions that elicit step-by-step reasoning for ambiguous HOIs."
    },
    {
      "title": "Conditional Prompt Learning for Vision-Language Models (CoCoOp)",
      "authors": "Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu",
      "year": 2022,
      "role": "Extended prompt learning to be image-conditional, improving generalization to domain shifts.",
      "relationship_sentence": "InstructHOI generalizes CoCoOp\u2019s context sensitivity from conditional prompts to context-aware instructions that guide multi-modal reasoning conditioned on scene and object cues."
    },
    {
      "title": "GLIP: Grounded Language-Image Pre-training",
      "authors": "Li et al.",
      "year": 2022,
      "role": "Unified detection and grounding via language, enabling open-vocabulary localization conditioned on text.",
      "relationship_sentence": "InstructHOI leverages GLIP-style grounding to connect instructions about humans, objects, and verbs to localized regions, supporting open-world HOI recognition under instruction guidance."
    }
  ],
  "synthesis_narrative": "InstructHOI shifts HOI detection from prompt-driven feature transfer to instruction-guided multi-modal reasoning. This pivot is anchored in two complementary lines of prior art. First, CLIP established a robust image\u2013text alignment space that underlies modern HOI methods; CoOp and CoCoOp then demonstrated that adapting CLIP with learnable (and context-conditional) prompts can significantly improve downstream performance, a strategy widely adopted by prompt-based HOI detectors. InstructHOI explicitly moves beyond these discriminative prompts by using instructions to elicit reasoning, addressing the ambiguity and open-world nature of interactions.\nSecond, the visual instruction tuning literature\u2014BLIP-2, InstructBLIP, and LLaVA\u2014showed how to connect strong image encoders to language models and to fine-tune them with synthetic, LLM-authored instruction data so models can follow task directives and produce context-aware rationales. InstructHOI directly instantiates this recipe for the HOI domain: it constructs a large-scale (140K) interaction\u2013reasoning corpus, fine-tunes a VLM to bridge the HOI knowledge gap, and introduces a Context-aware Instruction Generator to tailor instructions to detected humans/objects and scene context.\nFinally, GLIP\u2019s grounded pretraining provides the open-vocabulary localization and language grounding capabilities needed to tie instructions about verbs and objects to specific regions. Together, these works enable InstructHOI\u2019s core innovation: context-aware instruction generation and instruction-tuned multi-modal reasoning that materially improves ambiguous and open-world HOI detection.",
  "analysis_timestamp": "2026-01-07T00:02:04.960352"
}
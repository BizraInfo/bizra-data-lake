{
  "prior_works": [
    {
      "title": "Robust Adversarial Reinforcement Learning",
      "authors": "Lerrel Pinto et al.",
      "year": 2017,
      "arxiv_id": "1703.02702",
      "role": "Baseline",
      "relationship_sentence": "This minimax adversarial-training approach is the primary worst-case robust RL baseline whose over-conservatism the paper addresses by adaptively selecting among a non-dominated set instead of committing to a single worst-case policy."
    },
    {
      "title": "Action Robust Reinforcement Learning",
      "authors": "Chen Tessler et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By showing that PR-MDP/NR-MDP minimax formulations against action disturbances yield conservative policies, this work crystallizes the key limitation of worst-case robustification that the paper tackles under state-adversarial attacks via a set of non-dominated policies."
    },
    {
      "title": "Adversarial Attacks on Neural Network Policies",
      "authors": "Sandy Huang et al.",
      "year": 2017,
      "arxiv_id": "1702.02284",
      "role": "Foundation",
      "relationship_sentence": "This paper introduced the now-standard state-adversarial, Lp-bounded test-time perturbation model that the paper adopts to define performance across varying attack strengths."
    },
    {
      "title": "Theoretically Principled Trade-off Between Robustness and Accuracy",
      "authors": "Hongyang Zhang et al.",
      "year": 2019,
      "arxiv_id": "1901.08573",
      "role": "Gap Identification",
      "relationship_sentence": "By formalizing the inherent robustness\u2013accuracy trade-off in adversarial training, this work motivates the paper\u2019s beyond-worst-case stance to cover multiple robustness levels and preserve clean performance via adaptive selection."
    },
    {
      "title": "The Nonstochastic Multiarmed Bandit Problem",
      "authors": "Peter Auer et al.",
      "year": 2002,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The adversarial bandit regret framework (e.g., EXP3) underpins the paper\u2019s test-time regret minimization and its guarantee of sublinear regret when selecting among a finite compact set of baseline policies."
    },
    {
      "title": "A Survey of Multi-Objective Sequential Decision-Making",
      "authors": "Diederik M. Roijers et al.",
      "year": 2013,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "This survey formalizes non-dominated (Pareto) policy sets and coverage concepts, which the paper repurposes to construct a compact set of non-dominated policies spanning attack strengths for adaptive defense."
    }
  ],
  "synthesis_narrative": "Minimax robust RL methods such as Robust Adversarial Reinforcement Learning train policies against a worst-case adversary, yielding robustness but often at the cost of overly conservative behavior. Action Robust Reinforcement Learning sharpened this point in the PR-MDP/NR-MDP setting, showing that worst-case formulations aimed at action disturbances can significantly degrade nominal performance. Concurrently, the adversarial test-time threat model for reinforcement learning was crystallized by Adversarial Attacks on Neural Network Policies, which established the now-standard Lp-bounded state-perturbation framework used to evaluate and design defenses. Beyond the RL domain, Theoretically Principled Trade-off Between Robustness and Accuracy formalized the inherent trade-off between clean and robust performance, introducing the idea of tuning across robustness levels rather than optimizing only for the worst case. For online decision-making under adversarial feedback, The Nonstochastic Multiarmed Bandit Problem provided the core regret-minimization machinery (e.g., EXP3) to adaptively choose among a finite set of experts with sublinear regret. Finally, the multi-objective decision-making literature, summarized in A Survey of Multi-Objective Sequential Decision-Making, introduced non-dominated (Pareto) policy sets to cover trade-offs across competing objectives. Taken together, these works reveal a gap: worst-case robust RL secures robustness but sacrifices clean performance, even though the attack strength at test time can vary. The natural next step is to precompute policies spanning the robustness\u2013performance trade-off (leveraging Pareto/non-dominated ideas) and then use adversarial bandit-style selection to adapt online to the realized attack strength under the standard state-attack model, thereby going beyond worst-case defenses with principled regret guarantees.",
  "target_paper": {
    "title": "Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies",
    "authors": "Xiangyu Liu, Chenghao Deng, Yanchao Sun, Yongyuan Liang, Furong Huang",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "robust reinforcement learning; beyond worse-case",
    "abstract": "In light of the burgeoning success of reinforcement learning (RL) in diverse real-world applications, considerable focus has been directed towards ensuring RL policies are robust to adversarial attacks during test time. Current approaches largely revolve around solving a minimax problem to prepare for potential worst-case scenarios. While effective against strong attacks, these methods often compromise performance in the absence of attacks or the presence of only weak attacks. To address this, we study policy robustness under the well-accepted state-adversarial attack model, extending our focus beyond merely worst-case attacks. We first formalize this task at test time as a regret minimization problem and establish its intrinsic difficulty in achieving sublinear regret when the baseline policy is from a general continuous policy class, $\\Pi$. This finding prompts us to \\textit{refine} the baseline policy class $\\Pi$ prior to test time, aiming for efficient adaptation within a compact, ",
    "openreview_id": "DFTHW0MyiW",
    "forum_id": "DFTHW0MyiW"
  },
  "analysis_timestamp": "2026-01-06T09:19:08.926499"
}
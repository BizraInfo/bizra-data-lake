{
  "prior_works": [
    {
      "title": "DoReMi: Optimizing Data Mixtures Makes Language Models Better",
      "authors": "Anonymous et al.",
      "year": 2023,
      "arxiv_id": "unknown",
      "role": "Baseline",
      "relationship_sentence": "RegMix directly targets the same problem as DoReMi\u2014automatically finding effective pretraining data mixtures\u2014but replaces DoReMi\u2019s online mixture-weight optimization with a surrogate regression trained on many small-scale mixture runs to predict the best unseen mixture."
    },
    {
      "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
      "authors": "Leo Gao et al.",
      "year": 2021,
      "arxiv_id": "2101.00027",
      "role": "Foundation",
      "relationship_sentence": "By formalizing LM pretraining as a mixture over many heterogeneous sources, The Pile established the multi-source mixture setting that RegMix treats as a regression over mixture weights."
    },
    {
      "title": "LLaMA: Open and Efficient Foundation Language Models",
      "authors": "Hugo Touvron et al.",
      "year": 2023,
      "arxiv_id": "2302.13971",
      "role": "Gap Identification",
      "relationship_sentence": "LLaMA highlighted that heuristic, human-chosen data mixture weights critically affect performance, motivating RegMix\u2019s aim to automate mixture selection and outperform manual recipes across scales."
    },
    {
      "title": "Tensor Programs V: Tuning Large Neural Networks via \u03bc-Transfer",
      "authors": "Greg Yang et al.",
      "year": 2022,
      "arxiv_id": "2203.03466",
      "role": "Inspiration",
      "relationship_sentence": "\u03bc-Transfer\u2019s core insight\u2014that hyperparameters tuned on small models can reliably transfer to large models\u2014directly inspires RegMix\u2019s strategy of training many small proxy models to learn a predictor that selects mixtures for much larger-scale training."
    },
    {
      "title": "Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets (FABOLAS)",
      "authors": "Aaron Klein et al.",
      "year": 2017,
      "arxiv_id": "1605.07079",
      "role": "Related Problem",
      "relationship_sentence": "FABOLAS shows that low-fidelity evaluations (e.g., smaller datasets) can train a surrogate to predict high-fidelity performance, a principle RegMix adapts by fitting a regression surrogate from small-scale mixture runs to forecast performance of unseen mixtures at large scale."
    },
    {
      "title": "Training Compute-Optimal Large Language Models",
      "authors": "Jordan Hoffmann et al.",
      "year": 2022,
      "arxiv_id": "2203.15556",
      "role": "Foundation",
      "relationship_sentence": "Chinchilla\u2019s compute-optimal scaling results justify RegMix\u2019s design of using small, cheap runs to guide choices for longer, larger-scale training, assuming systematic cross-scale regularities that a regression model can exploit."
    }
  ],
  "synthesis_narrative": "A body of work established that large language model pretraining typically draws from heterogeneous corpora, with The Pile explicitly framing pretraining as a mixture over diverse sources and thereby defining mixture weights as a key design variable. LLaMA, while relying on heuristic composition across web, code, books, and scholarly text, underscored that the precise mixture critically affects final performance, revealing the fragility and manual nature of existing data recipes. Beyond data, scaling studies such as Chinchilla showed stable cross-scale regularities that permit principled decisions about compute and data, suggesting that choices made at small scale can guide large-scale outcomes. In parallel, \u03bc-Transfer demonstrated that hyperparameters tuned on small proxy models can transfer faithfully to much larger models, providing a concrete mechanism for leveraging small-scale experiments to inform large-scale training. From an optimization standpoint, FABOLAS established that low-fidelity evaluations can train a surrogate predictor for high-fidelity performance, legitimizing surrogate-based search under resource constraints. Finally, DoReMi directly tackled mixture optimization for LM pretraining via online weight updates, empirically validating that automated mixture search can beat human selection.\nTogether these works expose a clear opportunity: automate mixture selection using many inexpensive, small-scale trials while ensuring decisions transfer to large-scale training. The current paper synthesizes these insights by casting data mixture selection as supervised regression, fitting a surrogate on numerous small-model runs to predict performance for unseen mixtures, and then scaling the best predicted mixture to large compute\u2014achieving automation that addresses LLaMA\u2019s heuristic gap and rivals DoReMi with a simpler, more scalable surrogate approach.",
  "target_paper": {
    "title": "RegMix: Data Mixture as Regression for Language Model Pre-training",
    "authors": "Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, Min Lin",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "language model pre-training, data mixture, regression",
    "abstract": "The data mixture for large language model pre-training significantly impacts performance, yet how to determine an effective mixture remains unclear. We propose RegMix to automatically identify a high-performing data mixture by formulating it as a regression task. RegMix trains many small models on diverse data mixtures, uses regression to predict performance of unseen mixtures, and applies the best predicted mixture to train a large-scale model with orders of magnitude more compute. To empirically validate RegMix, we train 512 models with 1M parameters for 1B tokens to fit the regression model and predict the best data mixture. Using this mixture we train a 1B parameter model for 25B tokens (i.e. 1000\u00d7 larger and 25\u00d7 longer) which we find performs best among 64 candidate 1B parameter models with other mixtures. Furthermore, RegMix consistently outperforms human selection in experiments involving models up to 7B models trained on 100B tokens, while matching or exceeding DoReMi using jus",
    "openreview_id": "5BjQOUXq7i",
    "forum_id": "5BjQOUXq7i"
  },
  "analysis_timestamp": "2026-01-06T16:43:37.636278"
}
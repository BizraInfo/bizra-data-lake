{
  "prior_works": [
    {
      "title": "Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators",
      "authors": "Lu Lu et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized operator learning as data-driven approximation of maps between infinite-dimensional function spaces, providing the problem formulation that our work analyzes under active data collection."
    },
    {
      "title": "Fourier Neural Operator for Parametric Partial Differential Equations",
      "authors": "Zongyi Li et al.",
      "year": 2021,
      "role": "Baseline",
      "relationship_sentence": "FNO exemplifies the prevailing operator-learning practice of training on i.i.d. (passively collected) function\u2013output pairs, which our theory contrasts against by proving strictly better rates under active data collection."
    },
    {
      "title": "Methodology and convergence rates for functional linear regression",
      "authors": "Peter Hall et al.",
      "year": 2007,
      "role": "Gap Identification",
      "relationship_sentence": "Hall and Horowitz linked estimation error for linear operators with functional inputs to the eigenvalue decay of the input covariance under random design; our results identify and overcome the implied rate limitations by switching to active designs."
    },
    {
      "title": "Optimal rates for the regularized least-squares algorithm",
      "authors": "Andrea Caponnetto et al.",
      "year": 2007,
      "role": "Foundation",
      "relationship_sentence": "Their spectral/effective-dimension analysis for kernel regression under i.i.d. sampling underpins our use of covariance-kernel eigenvalue decay to characterize passive-sampling error and to frame the gains achievable via active selection."
    },
    {
      "title": "A-optimal design for infinite-dimensional Bayesian linear inverse problems",
      "authors": "Alen Alexanderian et al.",
      "year": 2016,
      "role": "Inspiration",
      "relationship_sentence": "This work shows that, for linear inverse problems with Gaussian priors, actively chosen experiments aligned with prior covariance eigenfunctions minimize posterior variance; we adapt this idea to operator learning to obtain arbitrarily fast convergence when eigenvalues decay rapidly."
    },
    {
      "title": "Optimum designs in regression problems",
      "authors": "Jack Kiefer et al.",
      "year": 1959,
      "role": "Foundation",
      "relationship_sentence": "Classical optimal design theory (e.g., A/D-optimality) provides the variance-minimization framework that our active data collection strategy leverages in the functional (operator-learning) setting."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014establishing a sharp separation between active and passive data collection for operator learning with rates governed by the covariance-kernel spectrum\u2014builds on two pillars: the operator-learning formulation and spectral-statistical analysis of linear problems. DeepONet introduced operator learning as learning maps between function spaces, and FNO instantiated the dominant training paradigm based on passive i.i.d. sampling; these works together set the baseline practice our theory scrutinizes. On the statistical side, Hall and Horowitz showed in functional linear regression that estimation error under random design is fundamentally tied to the eigenvalue decay of the predictor\u2019s covariance operator. Caponnetto and De Vito further developed a spectral/effective-dimension framework for learning with kernels that makes this dependence precise in terms of eigenvalue decay, providing the mathematical machinery we repurpose to derive passive-sampling lower bounds in operator learning. The key spark for our active approach comes from optimal experimental design in infinite-dimensional linear inverse problems: Alexanderian, Petra, Stadler, and Ghattas proved that designs aligned with prior covariance eigenfunctions (A-optimal) minimize posterior variance, suggesting that querying along leading eigen-directions can accelerate learning. Our work transposes this OED insight to the operator-learning context, showing that active selection can achieve arbitrarily fast convergence when covariance eigenvalues decay rapidly, while passive strategies remain bottlenecked\u2014thereby theoretically justifying a move from passive to actively designed data collection in operator learning.",
  "analysis_timestamp": "2026-01-06T23:07:19.607272"
}
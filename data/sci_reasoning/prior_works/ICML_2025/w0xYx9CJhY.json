{
  "prior_works": [
    {
      "title": "Object Hallucination in Image Captioning",
      "authors": "Anna Rohrbach, Lisa Anne Hendricks, Trevor Darrell, Bernt Schiele",
      "year": 2018,
      "role": "Problem formulation and metrics",
      "relationship_sentence": "Established object-level hallucination as a core failure mode and introduced CHAIR-style evaluations that MARINE targets explicitly when reducing non-existent object mentions."
    },
    {
      "title": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
      "authors": "Peter Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, L. Zhang",
      "year": 2018,
      "role": "Object-centric grounding paradigm",
      "relationship_sentence": "Demonstrated that object detector\u2013derived region features improve grounded language generation, inspiring MARINE\u2019s use of object-level signals to constrain LVLM decoding."
    },
    {
      "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
      "authors": "Pengchuan Zhang et al.",
      "year": 2021,
      "role": "Evidence that stronger detectors reduce hallucination",
      "relationship_sentence": "Showed that improving detection-driven visual representations yields more faithful captions, motivating MARINE\u2019s choice to plug in high-quality detectors as guidance sources."
    },
    {
      "title": "OWL-ViT: Open-Vocabulary Object Detection via Vision Transformers",
      "authors": "Matthias Minderer et al.",
      "year": 2022,
      "role": "Enabling open-vocabulary object extraction",
      "relationship_sentence": "Provides an open-source detector that can enumerate objects without category-locked training, enabling MARINE\u2019s API-free, object-level guidance across broad vocabularies."
    },
    {
      "title": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",
      "authors": "Shilong Liu et al.",
      "year": 2023,
      "role": "Phrase grounding and detection backbone",
      "relationship_sentence": "Supplies robust, open-set phrase grounding that MARINE can query to align generated tokens with concrete visual evidence during inference."
    },
    {
      "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
      "authors": "Andy Zeng et al.",
      "year": 2022,
      "role": "Training-free model composition",
      "relationship_sentence": "Introduced a paradigm for composing frozen vision and language models via lightweight interfaces, directly informing MARINE\u2019s training-free, plug-and-play integration of external vision modules."
    },
    {
      "title": "Plug-and-Play Language Models: A Simple Approach to Controlled Text Generation",
      "authors": "Sharan Narang, Siddharth Dathathri, Andrea Madotto, et al.",
      "year": 2020,
      "role": "Inference-time guidance without fine-tuning",
      "relationship_sentence": "Pioneered guiding generation at inference via external signals without model retraining, a principle MARINE adapts to LVLMs by injecting image-grounded object signals to steer decoding."
    }
  ],
  "synthesis_narrative": "MARINE addresses object hallucination in LVLMs by injecting image-grounded, object-level guidance during inference\u2014training-free and API-free. Rohrbach et al. (2018) crystallized the hallucination problem and introduced object-level evaluation (e.g., CHAIR), defining the precise failure mode MARINE targets. The object-centric lineage from Anderson et al. (2018) showed that using detector-derived region features yields more grounded text, establishing the utility of explicit object signals for language generation. VinVL (2021) further evidenced that stronger detection improves caption faithfulness, reinforcing the idea that high-quality object cues can curb hallucinations.\nCrucially, modern open-source detectors make such cues broadly accessible. OWL-ViT (2022) provides open-vocabulary detection, while Grounding DINO (2023) enables robust phrase grounding\u2014together they furnish the flexible, object-level evidence MARINE aggregates to validate or veto candidate tokens. On the algorithmic side, MARINE\u2019s training-free integration philosophy is influenced by Socratic Models (2022), which compose frozen vision and language systems via lightweight messaging, and by Plug-and-Play Language Models (2020), which pioneered inference-time guidance without fine-tuning. MARINE adapts these ideas to the multimodal setting: instead of gradient or classifier signals, it leverages detector-derived object lists and grounding scores to steer LVLM decoding, ensuring generated content remains consistent with the image. By uniting object-centric vision backbones with plug-and-play, inference-time control, MARINE delivers a practical, model-agnostic approach to suppress object hallucinations.",
  "analysis_timestamp": "2026-01-07T00:21:32.368790"
}
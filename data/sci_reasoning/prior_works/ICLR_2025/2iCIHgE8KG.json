{
  "prior_works": [
    {
      "title": "Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity",
      "authors": "Byron M. Yu et al.",
      "year": 2009,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The proposed model directly extends GPFA\u2019s formulation of GP-driven latent trajectories and linear loadings by replacing its fixed, always-active factor set with a nonparametric, time-selective feature allocation."
    },
    {
      "title": "Nonparametric Bayesian Sparse Factor Models with the Indian Buffet Process",
      "authors": "David M. Knowles et al.",
      "year": 2011,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "This work\u2019s use of an IBP prior to infer the number of factors and induce sparsity in factor loadings is explicitly adapted to endow GPFA with an unbounded, data-driven set of latent factors."
    },
    {
      "title": "The Indian Buffet Process: An Introduction and Review",
      "authors": "Thomas L. Griffiths et al.",
      "year": 2011,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The IBP provides the core infinite latent feature prior that enables the paper\u2019s \u2018infinite GPFA\u2019 construction and automatic determination of how many factors are needed."
    },
    {
      "title": "Dependent Indian Buffet Processes",
      "authors": "Sinead A. Williamson et al.",
      "year": 2010,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "The idea of introducing dependencies (e.g., via GPs) into IBP feature usage directly informs the paper\u2019s temporally varying on/off gating of factors to achieve compositional activity over time."
    },
    {
      "title": "Recurrent Switching Linear Dynamical Systems",
      "authors": "Scott W. Linderman et al.",
      "year": 2017,
      "arxiv_id": "1610.08466",
      "role": "Gap Identification",
      "relationship_sentence": "This switching LDS framework highlights the utility of discrete switching in neural dynamics but is limited to one regime at a time, motivating the paper\u2019s factor-wise, compositional switching within a GPFA manifold."
    },
    {
      "title": "The Beta Process Autoregressive HMM",
      "authors": "Emily B. Fox et al.",
      "year": 2011,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "By using a beta-process/IBP construction for an unbounded set of dynamical regimes, this work motivates the paper\u2019s nonparametric feature allocation for time series, which is extended here to continuous GP latent factors with concurrent (compositional) activation."
    }
  ],
  "synthesis_narrative": "Gaussian-process factor analysis (GPFA) established a framework in which low-dimensional neural trajectories are modeled as smooth Gaussian processes combined through a linear loading matrix, but with a fixed number of factors that contribute at all times. Nonparametric Bayesian sparse factor models with the Indian Buffet Process (IBP) showed how an IBP prior can infer the number of factors and induce sparse loadings in factor analysis, offering a principled route to remove manual factor selection. The Indian Buffet Process itself provided the infinite latent feature prior enabling unbounded factor spaces. Dependent IBP models introduced covariate-dependent feature usage\u2014often instantiated via Gaussian processes\u2014demonstrating how feature activations can vary smoothly with time, furnishing a mechanism for temporally structured on/off gating. Recurrent switching linear dynamical systems (rSLDS) demonstrated the value of discrete switching for neural dynamics but restricted switching to single regimes rather than additive, compositional latent contributions. The beta process autoregressive HMM similarly used beta/IBP constructions to allow an unbounded set of dynamical atoms, but activations were regime-based rather than factor-wise and concurrent. Together, these works revealed an opportunity to merge GPFA\u2019s smooth latent trajectories with IBP-driven, temporally dependent feature allocation, yielding a model that learns both how many factors are needed and which subset is active at each moment. The present paper synthesizes these insights by placing an (dependent) IBP prior over the factor loading process in GPFA, enabling temporally compositional switching of an unbounded factor set and addressing the key limitations of fixed dimensionality and always-on factors while retaining GPFA\u2019s neural-manifold interpretability.",
  "target_paper": {
    "title": "Discovering Temporally Compositional Neural Manifolds with Switching Infinite GPFA",
    "authors": "Changmin Yu, Maneesh Sahani, M\u00e1t\u00e9 Lengyel",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Computational neuroscience, neural data analysis, Bayesian nonparametrics, latent variable modelling;",
    "abstract": "Gaussian Process Factor Analysis (GPFA) is a powerful latent variable model for extracting low-dimensional manifolds underlying population neural activities. However, one limitation of standard GPFA models is that the number of latent factors needs to be pre-specified or selected through heuristic-based processes, and that all factors contribute at all times. We propose the infinite GPFA model, a fully Bayesian non-parametric extension of the classical GPFA by incorporating an Indian Buffet Process (IBP) prior over the factor loading process, such that it is possible to infer a potentially infinite set of latent factors, and the identity of those factors that contribute to neural firings in a compositional manner at \\textit{each} time point. Learning and inference in the infinite GPFA model is performed through variational expectation-maximisation, and we additionally propose scalable extensions based on sparse variational Gaussian Process methods. We empirically demonstrate that the i",
    "openreview_id": "2iCIHgE8KG",
    "forum_id": "2iCIHgE8KG"
  },
  "analysis_timestamp": "2026-01-06T19:23:49.097690"
}
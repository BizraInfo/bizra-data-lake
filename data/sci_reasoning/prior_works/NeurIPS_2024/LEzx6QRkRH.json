{
  "prior_works": [
    {
      "title": "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning (Options)",
      "authors": "Sutton, Precup, and Singh",
      "year": 1999,
      "role": "Hierarchical RL foundation",
      "relationship_sentence": "Established the manager\u2013worker (temporal abstraction) paradigm that RL-GPT adopts as its slow\u2013fast agent decomposition, delegating high-level decisions to one controller and low-level execution to another."
    },
    {
      "title": "HIRO: Hierarchical Reinforcement Learning with Off-Policy Correction",
      "authors": "Ofir Nachum et al.",
      "year": 2018,
      "role": "Method for efficient hierarchical control",
      "relationship_sentence": "Demonstrated stable training and data efficiency when a high-level policy sets subgoals for a low-level RL policy, directly informing RL-GPT\u2019s design where a slow agent guides a fast, RL-refined executor."
    },
    {
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan)",
      "authors": "Ahn et al.",
      "year": 2022,
      "role": "LLM planning grounded by value functions",
      "relationship_sentence": "Showed how LLM high-level planning can be coupled with learned value models/controllers, paralleling RL-GPT\u2019s split where language/code handles high-level structure and RL specializes low-level control."
    },
    {
      "title": "Code as Policies: Language Models as Zero-Shot Planners for Embodied Control",
      "authors": "Google Research Robotics team",
      "year": 2023,
      "role": "Code-as-policy paradigm",
      "relationship_sentence": "Introduced emitting executable code from LLMs for embodied control, a core precedent that RL-GPT extends by adding an RL-driven fast agent and a hierarchical gate deciding which actions should be coded."
    },
    {
      "title": "ChatGPT for Robotics: Design Principles and Model Abilities",
      "authors": "Vemprala et al.",
      "year": 2023,
      "role": "LLM-to-code for real robot/tool control",
      "relationship_sentence": "Showed practical pipelines where LLMs generate and execute control code, directly motivating RL-GPT\u2019s fast agent that executes code and its integration with environment feedback loops."
    },
    {
      "title": "Voyager: An Open-Ended Embodied Agent with LLMs",
      "authors": "Wang et al.",
      "year": 2023,
      "role": "LLM code-generation for Minecraft skills",
      "relationship_sentence": "Demonstrated automatic curriculum and skill libraries via code in Minecraft, which RL-GPT builds on by adding hierarchical slow/fast agents and RL-based refinement to achieve faster diamond acquisition and SOTA MineDojo results."
    },
    {
      "title": "MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge",
      "authors": "Fan et al.",
      "year": 2022,
      "role": "Benchmark and interface for code-driven agents",
      "relationship_sentence": "Provided the task suite, APIs, and evaluation setting where code-executing agents thrive; RL-GPT leverages MineDojo\u2019s interfaces to validate its hierarchical code+RL pipeline."
    }
  ],
  "synthesis_narrative": "RL-GPT\u2019s core contribution is a two-level agent that fuses code-as-policy with reinforcement learning: a slow agent decides which actions are amenable to coding and a fast agent executes the code, while low-level control is refined by RL. This design stands on the classic principles of hierarchical RL, where Options (Sutton, Precup, Singh) introduced temporal abstraction and HIRO showed how a high-level controller can efficiently guide a low-level learner with robust off-policy training. In parallel, recent embodied AI demonstrated that language models can produce executable control code. Code as Policies crystallized the idea of emitting code to control embodied systems, and ChatGPT for Robotics operationalized LLM-to-code pipelines with real tool and robot execution. SayCan then provided a blueprint for combining LLM high-level intent with value-grounded controllers, foreshadowing RL-GPT\u2019s division of labor: symbolic/code planning on top and learned policies underneath. In the Minecraft domain, Voyager proved that LLM-generated code can accumulate reusable skills and drive open-ended progress, directly motivating RL-GPT\u2019s use of code for high-level structure while relying on RL to master precise, task-specific low-level behaviors. MineDojo supplies the enabling environment and benchmarks that reward code execution and learned control, letting RL-GPT demonstrate its efficiency (rapid diamond acquisition) and breadth (SOTA across tasks) through this principled integration of hierarchical RL and code-as-policy.",
  "analysis_timestamp": "2026-01-06T23:42:49.037827"
}
{
  "prior_works": [
    {
      "title": "Extracting Training Data from Large Language Models",
      "authors": "Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Song, \u00dalfar Erlingsson, Alina Oprea, Colin Raffel",
      "year": 2021,
      "role": "Foundational data-extraction attack on LMs",
      "relationship_sentence": "ProPILE operationalizes the extraction paradigm from this work into a user-centric probing tool by crafting PII-conditioned prompts to gauge whether an LLM will regurgitate a data subject\u2019s information."
    },
    {
      "title": "The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks",
      "authors": "Nicholas Carlini, Chang Liu, \u00dalfar Erlingsson, Jernej Kos, Dawn Song",
      "year": 2019,
      "role": "Memorization measurement (canaries, exposure metric)",
      "relationship_sentence": "ProPILE\u2019s notion of probing for specific strings and quantifying leakage risk builds directly on the memorization framing and exposure-style reasoning introduced by Secret Sharer."
    },
    {
      "title": "Membership Inference Attacks Against Machine Learning Models",
      "authors": "Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov",
      "year": 2017,
      "role": "Conceptual foundation for individual-centric privacy auditing",
      "relationship_sentence": "ProPILE adapts the membership inference perspective to generative LLMs, enabling data subjects to test whether their personal records likely influenced the model."
    },
    {
      "title": "LOGAN: Membership Inference Attacks Against Generative Models",
      "authors": "Jamie Hayes, Luca Melis, George Danezis, Emiliano De Cristofaro",
      "year": 2019,
      "role": "Membership inference for generative models",
      "relationship_sentence": "By showing that generative models can leak who is in the training set, this work motivates ProPILE\u2019s generative probing of LLMs for PII exposure."
    },
    {
      "title": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
      "authors": "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, et al.",
      "year": 2021,
      "role": "Training corpus used as ground truth source",
      "relationship_sentence": "ProPILE\u2019s evaluation hinges on auditing potential PII originating from The Pile; this dataset choice shapes the probing targets and the validation of leakage findings."
    },
    {
      "title": "OPT: Open Pre-trained Transformer Language Models",
      "authors": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, et al.",
      "year": 2022,
      "role": "Open LLM testbed enabling reproducible probing",
      "relationship_sentence": "OPT-1.3B provides the accessible, reproducible model ProPILE probes, directly enabling their methodology to be demonstrated and validated."
    }
  ],
  "synthesis_narrative": "ProPILE\u2019s core contribution\u2014a user-centric probing tool for assessing PII leakage in large language models\u2014builds on two converging lines of prior work: unintended memorization measurement and practical data extraction from LMs. The Secret Sharer introduced the idea that models memorize specific strings and proposed exposure-based metrics for auditing memorization, establishing that targeted strings can be probed for leakage. Carlini et al.\u2019s later work on extracting training data from large language models translated this into concrete adversarial prompting strategies that elicit verbatim memorized content, particularly for rare or duplicated strings such as PII. ProPILE generalizes these insights from a model- or researcher-centric extraction attack into a data-subject-centric audit: individuals supply their own PII to generate targeted prompts and assess the likelihood of regurgitation.\n\nThis framing is grounded in the broader conceptual foundation of membership inference (Shokri et al.), which formalized individual-level privacy risk, and its adaptation to generative models (LOGAN), demonstrating that generative systems can leak training membership through their outputs. These works directly motivate ProPILE\u2019s goal of letting data subjects test whether their personal records may have influenced the model. Crucially, ProPILE\u2019s empirical setup relies on open infrastructure: the OPT family supplies an accessible LLM whose behavior can be reproducibly probed, while The Pile provides a plausible source of PII within web-scale corpora, enabling validation of leakage assessments. Together, these prior contributions shape ProPILE\u2019s methodology: exposure-informed, prompt-based probing that operationalizes membership-style auditing for real-world PII in open LLMs.",
  "analysis_timestamp": "2026-01-07T00:02:04.841184"
}
{
  "prior_works": [
    {
      "title": "Eligibility Traces for Off-Policy Policy Evaluation",
      "authors": "Doina Precup, Richard S. Sutton, Satinder Singh",
      "year": 2000,
      "role": "Foundational OPE via importance sampling (and per-decision IS) showing variance grows rapidly with horizon.",
      "relationship_sentence": "The paper\u2019s critique of sequential importance sampling in POMDPs and its horizon-induced variance directly builds on the IS framework established by Precup et al., motivating a value-based alternative that avoids horizon blow-up."
    },
    {
      "title": "Doubly Robust Off-policy Evaluation for Reinforcement Learning",
      "authors": "Nan Jiang, Lihong Li",
      "year": 2016,
      "role": "Introduced DR estimators and a bridge-function viewpoint linking model-based and weighting-based OPE via conditional moment restrictions.",
      "relationship_sentence": "The new work extends the DR/bridge-function perspective to partially observable settings by defining future-dependent value functions and deriving off-policy Bellman conditional moments with instruments, generalizing DR beyond fully observed MDPs."
    },
    {
      "title": "Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Evaluation",
      "authors": "Yao Liu, Akshay Krishnamurthy, Nan Jiang",
      "year": 2018,
      "role": "Proposed marginalized importance sampling to mitigate horizon dependence in fully observed MDPs.",
      "relationship_sentence": "While MIS reduces horizon variance in MDPs, this paper addresses the analogous challenge in POMDPs by leveraging future-dependent value functions and IV-style Bellman equations to curb horizon blow-up without requiring full observability."
    },
    {
      "title": "Fitted Q Evaluation (FQE): A Simple and Scalable Off-Policy Evaluation Method",
      "authors": "Hoang M. Le, Cameron Voloshin, Yisong Yue",
      "year": 2019,
      "role": "Established value-function-based OPE via fitted Q-learning style regression (FQE).",
      "relationship_sentence": "The paper explicitly contrasts with FQE, showing that na\u00efvely regressing Q on observed histories in POMDPs fails, and instead proposes future-dependent value functions with an off-policy Bellman equation to restore identifiability and consistency."
    },
    {
      "title": "Predictive State Representations",
      "authors": "Michael L. Littman, Richard S. Sutton, Satinder Singh",
      "year": 2001,
      "role": "Introduced representing latent state via predictions of future observables (futures) rather than latent variables.",
      "relationship_sentence": "The core idea of conditioning value functions on future proxies echoes the PSR principle that futures can summarize latent states; the paper operationalizes this in OPE by learning future-dependent value functions."
    },
    {
      "title": "Identifying Causal Effects with Proxy Variables: A Proximal Causal Inference Framework",
      "authors": "Wei Miao, Zhi Geng, Eric Tchetgen Tchetgen",
      "year": 2018,
      "role": "Developed identification via proxy variables and instrumental-style conditional moment equations using negative-control proxies.",
      "relationship_sentence": "The paper\u2019s off-policy Bellman equation uses history proxies as instruments and future proxies as outcomes, directly adapting proximal/negative-control identification ideas to the sequential decision setting in POMDPs."
    },
    {
      "title": "Minimax Weight and Q-Function Learning for Off-Policy Evaluation",
      "authors": "Nathan Kallus, Masatoshi Uehara",
      "year": 2020,
      "role": "Cast OPE as solving Bellman-based conditional moment equations via a minimax (GMM-like) objective with function approximation.",
      "relationship_sentence": "The proposed minimax learning of future-dependent value functions follows the same conditional-moment/minimax template, extending it to instrumented Bellman equations tailored for partial observability."
    }
  ],
  "synthesis_narrative": "This paper\u2019s key contribution\u2014future-dependent value-based OPE for POMDPs using an instrumented off-policy Bellman equation\u2014sits at the intersection of four lines of work. First, classical importance sampling for OPE (Precup et al., 2000) and doubly robust estimation (Jiang & Li, 2016) established the core estimators and the conditional-moment viewpoint underpinning modern OPE, while highlighting the curse of horizon. Second, attempts to tame horizon dependence in fully observed MDPs, notably marginalized importance sampling (Liu et al., 2018), showed that careful reweighting can reduce variance but do not address partial observability; and value-based regression approaches like FQE (Le, Voloshin, Yue, 2019) lack identifiability in POMDPs. Third, the representational insight from Predictive State Representations (Littman, Sutton, Singh, 2001) demonstrated that predictive futures can serve as sufficient statistics of latent state, directly inspiring the paper\u2019s future-dependent value functions that condition on future proxies. Fourth, the proximal causal inference framework with proxy variables (Miao, Geng, Tchetgen Tchetgen, 2018) and minimax/GMM formulations for Bellman equations (Kallus & Uehara, 2020) provided the identification and estimation machinery: use history proxies as instruments and learn via a saddle-point objective enforcing conditional moment restrictions. By combining PSR-style sufficiency (futures/histories) with proximal identification and minimax learning of instrumented Bellman equations, the paper delivers a model-free OPE method in POMDPs that avoids horizon blow-up and admits PAC guarantees under completeness/sufficiency conditions.",
  "analysis_timestamp": "2026-01-07T00:02:04.779307"
}
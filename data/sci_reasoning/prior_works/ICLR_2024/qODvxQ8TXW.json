{
  "prior_works": [
    {
      "title": "Comparing Rewinding and Fine-Tuning in Neural Network Pruning",
      "authors": "Alex Renda et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work introduced Learning Rate Rewinding (resetting only the learning-rate schedule when retraining pruned networks) and showed it outperforms weight rewinding, providing the exact pruning variant whose superior behavior this paper explains via sign dynamics and mask/parameter disentanglement."
    },
    {
      "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks",
      "authors": "Jonathan Frankle et al.",
      "year": 2019,
      "arxiv_id": "1803.03635",
      "role": "Foundation",
      "relationship_sentence": "It established the IMP framework and the lottery ticket problem formulation\u2014coupling mask discovery with parameter training\u2014that this paper directly probes to separate mask learning from parameter optimization."
    },
    {
      "title": "Stabilizing the Lottery Ticket Hypothesis",
      "authors": "Jonathan Frankle et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "By showing that weight rewinding to an early checkpoint strengthens IMP, it serves as the principal alternative baseline to LRR and motivates this paper\u2019s analysis of why LRR outperforms weight rewinding in both mask identification and sparse optimization."
    },
    {
      "title": "Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask",
      "authors": "Hattie Zhou et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "It revealed the central role of weight signs and masks (supermasks), directly inspiring this paper\u2019s focus on early sign flips and robustness to sign perturbations as the mechanism behind LRR\u2019s advantage."
    },
    {
      "title": "What\u2019s Hidden in a Randomly Weighted Neural Network?",
      "authors": "Vijay Ramanujan et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "By demonstrating that learning only a mask over fixed random weights can yield high accuracy, it motivates this paper\u2019s experimental disentanglement of mask learning from parameter optimization and tests on optimizing diverse (including random) masks."
    },
    {
      "title": "Rethinking the Value of Network Pruning",
      "authors": "Zhuang Liu et al.",
      "year": 2019,
      "arxiv_id": "1810.05270",
      "role": "Gap Identification",
      "relationship_sentence": "Its finding that training pruned architectures from scratch can match fine-tuning raised the open question of how much performance stems from the mask versus parameters\u2014a gap this paper addresses by isolating and analyzing these two components under LRR."
    }
  ],
  "synthesis_narrative": "Learning Rate Rewinding (LRR) was introduced as a retraining strategy that resets only the learning-rate schedule after pruning and was empirically shown to outperform weight rewinding, elevating it as a strong variant of iterative magnitude pruning. The Lottery Ticket Hypothesis defined the iterative magnitude pruning (IMP) setting by coupling mask discovery with parameter training, establishing the canonical procedure for uncovering sparse, trainable subnetworks. Stabilizing the Lottery Ticket Hypothesis further refined IMP by rewinding weights to an early checkpoint, creating the key alternative to LRR and framing the question of why different rewinding strategies succeed. Deconstructing Lottery Tickets highlighted that weight signs and binary masks can be sufficient to support learning, revealing that sign patterns are a critical representational degree of freedom. Complementarily, What\u2019s Hidden in a Randomly Weighted Neural Network showed that masks alone can induce high accuracy over fixed random weights, underscoring that mask learning can be meaningfully separated from parameter optimization. Rethinking the Value of Network Pruning reported that training pruned architectures from scratch can match fine-tuning, sharpening the inquiry into the relative contributions of mask structure and parameter states.\nTogether, these works exposed a gap: why LRR so reliably excels at both identifying masks and optimizing sparse networks. Building on the sign-centric insights and mask-only results, it is natural to hypothesize that LRR\u2019s advantage stems from enabling early sign flips and robustness to sign perturbations. This synthesis motivated disentangling mask learning from parameter optimization empirically and proving, in simplified settings, that LRR escapes problematic initial sign configurations more often than IMP.",
  "target_paper": {
    "title": "Masks, Signs, And Learning Rate Rewinding",
    "authors": "Advait Harshal Gadhikar, Rebekka Burkholz",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "sparsity, pruning, lottery tickets, learning rate rewinding, iterative magnitude pruning",
    "abstract": "Learning Rate Rewinding (LRR) has been established as a strong variant of Iterative Magnitude Pruning (IMP) to find lottery tickets in deep overparameterized neural networks. While both iterative pruning schemes couple structure and parameter learning, understanding how LRR excels in both aspects can bring us closer to the design of more flexible deep learning algorithms that can optimize diverse sets of sparse architectures. To this end, we conduct experiments that disentangle the effect of mask learning and parameter optimization and how both benefit from overparameterization. The ability of LRR to flip parameter signs early and stay robust to sign perturbations seems to make it not only more effective in mask identification but also in optimizing  diverse sets of masks, including random ones. In support of this hypothesis, we prove in a simplified single hidden neuron setting that LRR succeeds in more cases than IMP, as it can escape initially problematic sign configurations.",
    "openreview_id": "qODvxQ8TXW",
    "forum_id": "qODvxQ8TXW"
  },
  "analysis_timestamp": "2026-01-06T11:36:30.151343"
}
{
  "prior_works": [
    {
      "title": "Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning",
      "authors": "Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Shin Ishii",
      "year": 2018,
      "role": "Introduced label-free virtual adversarial perturbations and a divergence-based robustness objective",
      "relationship_sentence": "RCS borrows VAT\u2019s idea of generating label-agnostic adversarial variants and minimizing a clean\u2013adversarial divergence, adapting it to representation space to drive robustness-aware subset selection."
    },
    {
      "title": "Theoretically Principled Trade-off between Robustness and Accuracy (TRADES)",
      "authors": "Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, Michael I. Jordan",
      "year": 2019,
      "role": "Formalized robustness via minimizing divergence between clean and adversarial examples",
      "relationship_sentence": "TRADES motivates the paper\u2019s robustness-aware objective by framing robustness as minimizing a clean\u2013adversarial divergence, which RCS instantiates in representation space for coreset construction."
    },
    {
      "title": "An analysis of approximations for maximizing submodular set functions",
      "authors": "G. L. Nemhauser, L. A. Wolsey, M. L. Fisher",
      "year": 1978,
      "role": "Greedy optimality guarantee for submodular maximization",
      "relationship_sentence": "The paper\u2019s transformation of RCS into a submodular maximization surrogate relies on Nemhauser et al.\u2019s 1\u22121/e guarantee to justify an efficient greedy selection with performance bounds."
    },
    {
      "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach",
      "authors": "Ozan Sener, Silvio Savarese",
      "year": 2018,
      "role": "Coreset selection for deep learning to reduce training cost",
      "relationship_sentence": "RCS builds on the core idea that a small, well-chosen subset can approximate full-dataset training, extending coreset selection to an unsupervised, robustness-aware criterion tailored to adversarial contrastive learning."
    },
    {
      "title": "A Class of Submodular Functions for Document Summarization",
      "authors": "Hui Lin, Jeff Bilmes",
      "year": 2011,
      "role": "Submodular coverage/diversity functions and greedy selection for summarization",
      "relationship_sentence": "The design of a submodular surrogate that captures representational coverage and diversity in RCS is informed by classical facility-location/coverage submodular formulations and their efficient greedy optimization."
    },
    {
      "title": "A Simple Framework for Contrastive Learning of Visual Representations (SimCLR)",
      "authors": "Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton",
      "year": 2020,
      "role": "Foundational contrastive self-supervised learning framework",
      "relationship_sentence": "RCS targets adversarial contrastive pipelines derived from SimCLR-style objectives, making its subset selection directly applicable to speeding up contrastive pretraining without labels."
    },
    {
      "title": "Fast Is Better Than Free: Revisiting Adversarial Training",
      "authors": "Eric Wong, Leslie Rice, J. Zico Kolter",
      "year": 2020,
      "role": "Efficiency techniques for adversarial training via single-step/FGSM-style updates",
      "relationship_sentence": "By highlighting the computational bottleneck of generating adversarial examples, this work motivates complementary efficiency strategies, and RCS addresses the same bottleneck via data subset selection rather than attack simplification."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014an efficient, label-free robustness-aware coreset selection (RCS) for adversarial contrastive learning\u2014sits at the intersection of unsupervised adversarial robustness, data subset selection, and submodular optimization. VAT provides the essential mechanism and philosophy for generating label-agnostic adversarial variants and minimizing a clean\u2013adversarial divergence, which this work adapts from output distributions to representation space to guide selection without labels. TRADES reinforces the principle of robustness via divergence minimization, further legitimizing the paper\u2019s objective of aligning clean and adversarial representations. To make the intractable subset search practical with guarantees, the authors draw on classic submodular maximization theory\u2014specifically Nemhauser et al.\u2019s 1\u22121/e guarantee\u2014and on submodular coverage/diversity constructions from Lin and Bilmes, enabling a principled surrogate objective that supports greedy selection with provable performance. From the data efficiency side, Sener and Savarese\u2019s core-set approach demonstrates that carefully chosen subsets can approximate full training, a blueprint that RCS extends to the robustness-aware, unsupervised contrastive setting. SimCLR supplies the base contrastive framework into which adversarial variants are integrated, making RCS directly useful for accelerating widely adopted SSL pipelines. Finally, efficiency-focused adversarial training like Wong et al. underscores the computational burden of adversarial example generation, to which RCS offers an orthogonal remedy via subset selection rather than attack simplification, collectively forming the foundation for the paper\u2019s scalable ACL solution.",
  "analysis_timestamp": "2026-01-07T00:02:04.863237"
}
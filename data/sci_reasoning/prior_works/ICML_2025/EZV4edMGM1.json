{
  "prior_works": [
    {
      "title": "Efficient noise-tolerant learning from statistical queries",
      "authors": "Michael Kearns",
      "year": 1998,
      "role": "Foundational framework (SQ model) and link to classification noise",
      "relationship_sentence": "The paper\u2019s super-polynomial lower bounds are proved in the Statistical Query (SQ) model introduced by Kearns, and his noise-tolerance simulation ensures that SQ hardness translates to impossibility under Random Classification Noise (RCN), directly enabling the core hardness result."
    },
    {
      "title": "A General Characterization of the Statistical Query Complexity",
      "authors": "Vitaly Feldman",
      "year": 2017,
      "role": "Lower-bound technique (SQ dimension/average correlation)",
      "relationship_sentence": "The lower-bound machinery (statistical dimension/average correlation bounds) from Feldman provides the technical template for constructing families of distributions with small pairwise correlations, which this work adapts to multiclass linear classification under RCN to obtain super-polynomial SQ complexity."
    },
    {
      "title": "Learning with Noisy Labels",
      "authors": "Nagarajan Natarajan, Inderjit S. Dhillon, Pradeep K. Ravikumar, Ambuj Tewari",
      "year": 2013,
      "role": "Modeling of multiclass class-conditional noise and loss correction",
      "relationship_sentence": "This work formalized multiclass class-conditional (RCN) noise with a known confusion matrix H and developed loss-correction methods under separation/invertibility conditions; the present paper adopts this RCN model (including diagonal dominance via \u03c3) as its setting and shows hardness despite these standard assumptions."
    },
    {
      "title": "Learning from Noisy Examples",
      "authors": "Dana Angluin, Philip Laird",
      "year": 1988,
      "role": "Foundational noise model in PAC learning",
      "relationship_sentence": "Angluin\u2013Laird\u2019s classification noise model is the canonical PAC formulation that RCN generalizes; the current paper\u2019s distribution-free PAC and noise setup builds directly on this lineage to position its multiclass hardness result."
    },
    {
      "title": "Agnostic Learning of Halfspaces under Massart Noise: Efficient Algorithms and Optimal Bounds",
      "authors": "Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart",
      "year": 2019,
      "role": "Algorithmic benchmark for k=2 (binary) case",
      "relationship_sentence": "State-of-the-art polynomial-time algorithms achieving optimal error for binary halfspaces under benign label noise (including RCN as a special case of Massart) provide the contrast point; the new paper\u2019s key contribution is proving a sharp complexity jump to hardness when moving to \u22653 labels."
    },
    {
      "title": "Statistical Query Lower Bounds for Robust Estimation of High-Dimensional Distributions",
      "authors": "Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, Alistair Stewart",
      "year": 2016,
      "role": "SQ lower-bound toolkit (moment matching/correlation constructions)",
      "relationship_sentence": "The construction techniques for SQ lower bounds via moment-matching and pairwise-correlation control from this work inform the present paper\u2019s hard-instance design for multiclass linear classification under RCN."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central contribution\u2014showing a super-polynomial Statistical Query (SQ) hardness for multiclass (k\u22653) linear classification under Random Classification Noise (RCN)\u2014rests on two intertwined threads: the SQ framework for proving distribution-free lower bounds, and the modern formulation of multiclass label noise. Kearns\u2019 SQ model established the paradigm for noise-tolerant learning and, crucially, that SQ algorithms can be simulated under classification noise, so SQ lower bounds imply hardness in the RCN setting. Feldman\u2019s characterization of SQ complexity provides the technical vehicle: by designing families of distributions with small pairwise correlations (statistical dimension), one obtains quantitative lower bounds on the number/precision of SQs required, which this work tailors to multiclass linear separators with a known noise matrix.\nOn the modeling side, Natarajan et al. introduced multiclass class-conditional noise with a known confusion matrix H and separation/invertibility conditions, giving loss-correction schemes widely used for learning with noisy labels; the present paper adopts precisely this RCN structure (including the non-negative separation \u03c3) and shows that, despite these favorable assumptions, the SQ complexity becomes super-polynomial for k\u22653. This stands in stark contrast to the binary case: recent algorithms for learning halfspaces under benign noise (e.g., Massart/RCN) achieve optimal error in polynomial time, establishing tractability when k=2. Methodologically, the lower-bound constructions borrow from SQ techniques developed for robust estimation\u2014moment-matching and correlation control\u2014to instantiate hard multiclass distributions aligned with linear decision boundaries but obfuscated by RCN. Together, these strands yield a sharp, conceptually clean complexity separation between binary and multiclass linear classification under random label noise.",
  "analysis_timestamp": "2026-01-07T00:21:32.376669"
}
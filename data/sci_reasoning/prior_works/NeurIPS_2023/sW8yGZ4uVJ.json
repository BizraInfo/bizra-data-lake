{
  "prior_works": [
    {
      "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation",
      "authors": [
        "Richard S. Sutton",
        "David McAllester",
        "Satinder Singh",
        "Yishay Mansour"
      ],
      "year": 2000,
      "role": "Foundational policy gradient theorem and update formulas",
      "relationship_sentence": "The paper\u2019s ordering-based analysis of Softmax PG and NPG builds directly on the policy gradient theorem, which formalizes how gradients of expected return depend on the policy parametrization under function approximation."
    },
    {
      "title": "A Natural Policy Gradient",
      "authors": [
        "Sham M. Kakade"
      ],
      "year": 2002,
      "role": "Introduced natural policy gradient and Fisher-geometry view",
      "relationship_sentence": "Mei et al. analyze global convergence of NPG and show representation conditions differ from vanilla PG, a distinction that hinges on Kakade\u2019s Fisher information geometry underlying natural gradients."
    },
    {
      "title": "Conservative Policy Iteration",
      "authors": [
        "Sham M. Kakade",
        "John Langford"
      ],
      "year": 2002,
      "role": "Monotonic improvement via KL/trust-region style updates",
      "relationship_sentence": "Their monotonic improvement framework motivates examining how policy-update geometry (e.g., KL constraints) interacts with representation\u2014an interaction Mei et al. formalize via ordering conditions that separate PG from NPG."
    },
    {
      "title": "Trust Region Policy Optimization",
      "authors": [
        "John Schulman",
        "Sergey Levine",
        "Philipp Moritz",
        "Michael I. Jordan",
        "Pieter Abbeel"
      ],
      "year": 2015,
      "role": "Practical KL-constrained policy optimization closely related to NPG",
      "relationship_sentence": "By connecting KL-constrained updates to natural gradients, TRPO underpins the paper\u2019s claim that algorithm geometry changes the needed representation conditions, leading to distinct ordering requirements for Softmax PG vs. NPG."
    },
    {
      "title": "On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift",
      "authors": [
        "Alekh Agarwal",
        "Sham M. Kakade",
        "Jason D. Lee",
        "Gaurav Mahajan"
      ],
      "year": 2021,
      "role": "Modern convergence theory for PG/NPG (tabular and with function approximation)",
      "relationship_sentence": "Mei et al. refine and challenge the prevailing view from this line of work by proving global convergence in linear bandits without policy/reward realizability and showing approximation error is not the key quantity\u2014replacing it with ordering-based conditions."
    },
    {
      "title": "Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator",
      "authors": [
        "Maryam Fazel",
        "Rong Ge",
        "Sham M. Kakade",
        "Mehran Mesbahi"
      ],
      "year": 2018,
      "role": "Global convergence of PG in a structured, nonconvex control problem",
      "relationship_sentence": "This result demonstrates that special structural properties can yield global convergence of PG; Mei et al. extend this perspective by identifying ordering structures in linear bandits that guarantee global convergence without realizability."
    },
    {
      "title": "A Theory of Regularized Markov Decision Processes",
      "authors": [
        "Matthieu Geist",
        "Bruno Scherrer",
        "Olivier Pietquin"
      ],
      "year": 2019,
      "role": "Unifies policy optimization as mirror descent with Bregman geometries",
      "relationship_sentence": "The mirror-descent/regularization lens clarifies why different update geometries (entropy/KL for NPG vs. Euclidean for PG) imply different structural requirements; Mei et al. leverage this to formulate algorithm-specific ordering conditions."
    }
  ],
  "synthesis_narrative": "Mei et al. investigate when policy gradient methods achieve global convergence under linear function approximation in finite-arm bandits, ultimately arguing that the decisive factor is an ordering relation between the representation and the update geometry rather than approximation error or realizability. This perspective is anchored in the policy gradient theorem of Sutton et al., which formalizes how parameterized policies induce gradient directions, and in Kakade\u2019s natural policy gradient, which defines a Fisher-geometry where updates respect the information structure of the policy. Building on monotonic improvement ideas from Conservative Policy Iteration and their KL-constrained realization in TRPO, the paper examines how trust-region-like geometries change convergence behavior. Recent theory on policy gradients by Agarwal, Kakade, Lee, and Mahajan provides baselines for global convergence in tabular and function-approximation settings that often quantify suboptimality via approximation error; Mei et al. depart from this by proving global convergence without policy or reward realizability and by identifying ordering-based representation conditions instead. The LQR global convergence result of Fazel et al. motivates the search for structural properties that make nonconvex policy optimization globally well-behaved; here, the authors pinpoint order preservation between features and action returns as the crucial structure. Finally, the regularized-MDP/mirror-descent view of Geist et al. clarifies why Softmax PG and NPG require different representation conditions: the underlying optimization geometries (Euclidean vs. KL/Fisher) induce distinct order-preservation requirements, leading to the paper\u2019s algorithm-specific ordering conditions for global convergence.",
  "analysis_timestamp": "2026-01-06T23:33:35.589223"
}
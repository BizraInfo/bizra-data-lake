{
  "prior_works": [
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho, Ajay Jain, Pieter Abbeel",
      "year": 2020,
      "role": "Foundational objective and training paradigm for diffusion models",
      "relationship_sentence": "The paper\u2019s analytical framework explicitly studies the gradient-flow training dynamics of the denoising objective introduced by DDPM, and derives how the learned (generated) distribution evolves under this training."
    },
    {
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "authors": "Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole",
      "year": 2021,
      "role": "Score-based view linking denoising training to generative sampling and distributions",
      "relationship_sentence": "By leveraging the score-based formulation, the present work connects denoiser training to the evolution of the learned distribution and provides closed-form expressions (including KL) along training, extending the score-based perspective to exact, mode-wise dynamics."
    },
    {
      "title": "A Connection Between Score Matching and Denoising Autoencoders",
      "authors": "Pascal Vincent",
      "year": 2011,
      "role": "Theoretical link between denoising objectives and score estimation",
      "relationship_sentence": "The analysis relies on the denoising\u2013score matching equivalence to characterize what the denoiser learns at each noise level, enabling the closed-form characterization of the generated distribution during diffusion model training."
    },
    {
      "title": "Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks",
      "authors": "Andrew M. Saxe, James L. McClelland, Surya Ganguli",
      "year": 2014,
      "role": "Analytical gradient-flow solutions and eigenmode decoupling in linear nets",
      "relationship_sentence": "The paper extends the Saxe et al. eigenmode analysis to denoising objectives, showing that linear denoisers converge along principal components with mode-dependent time scales, which underpins the derived spectral bias."
    },
    {
      "title": "Spectral Bias and Task-Model Alignment in Kernel Regression",
      "authors": "Benjamin Bordelon, Abigail Canatar, Cengiz Pehlevan",
      "year": 2021,
      "role": "Eigen-spectrum\u2013dependent learning rates and spectral bias theory",
      "relationship_sentence": "Building on this kernel regression theory, the paper demonstrates an analogous eigenmode-dependent convergence for diffusion denoisers and quantifies a power-law dependence of convergence time on mode variance."
    },
    {
      "title": "The Singular Values of Convolutional Layers",
      "authors": "Meysam Sedghi, Vineet Gupta, Philip M. Long",
      "year": 2019,
      "role": "Fourier-domain diagonalization of convolutional operators",
      "relationship_sentence": "The paper exploits the Fourier diagonalization of linear convolution to obtain exact, decoupled mode-wise dynamics for convolutional denoisers, showing convergence along Fourier modes with variance-dependent time scales."
    },
    {
      "title": "On the Spectral Bias of Neural Networks: Towards Understanding the Frequency Dependence of Learning",
      "authors": "Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred A. Hamprecht, Yoshua Bengio, Aaron Courville",
      "year": 2019,
      "role": "Empirical and conceptual precursor on spectral/frequency bias",
      "relationship_sentence": "This work motivated the frequency-first learning phenomenon; the present paper provides a rigorous, closed-form account of spectral bias in diffusion training and identifies an inverse power-law dependence on mode variance."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014an exact, analytical account of spectral bias in the training dynamics of diffusion denoisers and the resulting evolution of the generated distribution\u2014rests on three pillars: the denoising diffusion/score-matching training objective, eigenmode-resolved learning dynamics in linear architectures, and Fourier diagonalization of convolution. The DDPM objective (Ho et al.) and the score-based formulation (Song et al.) establish the denoising loss and its link to generative modeling, while the denoising\u2013score matching equivalence (Vincent) supplies the theoretical bridge needed to map denoiser training at each noise level to the score of a perturbed distribution. Building on the Saxe et al. framework, the authors solve gradient-flow dynamics exactly for linear denoisers, revealing decoupled evolution along principal components with mode-dependent time scales. The kernel regression theory of spectral bias and eigen-spectrum\u2013dependent learning rates (Bordelon, Canatar, Pehlevan) provides a mathematically aligned precedent for inverse-eigenvalue time scales; this paper translates that intuition to diffusion denoisers and to the generated distribution itself, deriving closed-form KL trajectories. For convolutional denoisers, the Fourier-domain diagonalization of convolutional operators (Sedghi, Gupta, Long) enables an analogous mode-wise analysis, now in the frequency domain. Finally, prior observations of spectral/frequency bias in neural networks (Rahaman et al.) are placed on firm analytical footing: the paper proves a pronounced, power-law spectral bias\u2014convergence time inversely scaling with mode variance\u2014robust across linear and convolutional settings and persisting empirically in deeper architectures.",
  "analysis_timestamp": "2026-01-07T00:02:04.941657"
}
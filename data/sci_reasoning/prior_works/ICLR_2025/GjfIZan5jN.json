{
  "prior_works": [
    {
      "title": "Network Dissection: Quantifying Interpretability of Deep Visual Representations",
      "authors": "David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba",
      "year": 2017,
      "role": "Established a rigorous unit-level metric by aligning neurons with human-interpretable concepts, popularizing the view that a representation can be decomposed into interpretable and uninterpretable components.",
      "relationship_sentence": "IIS reframes Network Dissection\u2019s idea\u2014from counting interpretable units\u2014to an information-loss view that quantifies the proportion of interpretable semantics in the whole representation."
    },
    {
      "title": "Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks",
      "authors": "Ruth C. Fong, Andrea Vedaldi",
      "year": 2018,
      "role": "Showed that concept subspaces can be extracted via simple linear models over filters, offering a measurable bridge between features and human concepts.",
      "relationship_sentence": "IIS builds on Net2Vec\u2019s concept-subspace projection, generalizing it from per-concept alignment to a global score of how much of the representation is capturable by interpretations."
    },
    {
      "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
      "authors": "Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Vi\u00e9gas, Rory Sayres",
      "year": 2018,
      "role": "Introduced concept activation vectors to quantify model sensitivity to user-defined concepts, grounding interpretability in concept-aligned directions.",
      "relationship_sentence": "IIS inherits TCAV\u2019s concept-based lens, but moves from per-concept sensitivity to estimating the aggregate fraction of representation aligned with interpretable concept directions."
    },
    {
      "title": "On the (In)fidelity and Sensitivity of Explanations",
      "authors": "Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun Suggala, David I. Inouye, Pradeep Ravikumar",
      "year": 2019,
      "role": "Formalized faithfulness metrics that quantify how much information an explanation preserves about the model\u2019s behavior.",
      "relationship_sentence": "IIS adopts an information-centric formulation closely related to infidelity, explicitly measuring information loss between full representations and their interpretable components."
    },
    {
      "title": "A Benchmark for Interpretability Methods in Deep Neural Networks",
      "authors": "Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim",
      "year": 2019,
      "role": "Proposed ROAR (Remove And Retrain) to causally test whether explanations capture functionally important information.",
      "relationship_sentence": "IIS\u2019s operationalization of interpretability via information removal parallels ROAR\u2019s causal lens, inspiring its use of loss-of-information as a quantitative proxy."
    },
    {
      "title": "A Framework for the Quantitative Evaluation of Disentangled Representations",
      "authors": "Cian Eastwood, Christopher K. I. Williams",
      "year": 2018,
      "role": "Linked mutual information between factors and latents to disentanglement/interpretability through quantitative metrics (e.g., MIG, SAP).",
      "relationship_sentence": "IIS echoes disentanglement metrics by treating interpretability as the share of semantic factor information present in representations, extending the idea to pretrained visual features and interpretation capture."
    },
    {
      "title": "Understanding Intermediate Layers Using Linear Classifiers",
      "authors": "Guillaume Alain, Yoshua Bengio",
      "year": 2016,
      "role": "Introduced linear probes to quantify how linearly decodable class information is at each layer, standardizing the notion of classifiability.",
      "relationship_sentence": "The paper\u2019s linear-probe paradigm underpins this work\u2019s classifiability assessments and its empirical finding that enhancing linear separability can improve IIS."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014Inherent Interpretability Score (IIS) and the finding that boosting classifiability can improve interpretability\u2014sits at the intersection of concept-based interpretability, information-theoretic faithfulness, and linear separability evaluation. Network Dissection and Net2Vec first grounded the idea that internal representations can be decomposed into interpretable concept-aligned components and provided concrete procedures to project features onto concept subspaces. TCAV further crystallized the concept-based view, quantifying model sensitivity along human-defined concept directions. Building on this lineage, IIS shifts from per-unit or per-concept assessments to a holistic metric that estimates the ratio of interpretable semantics within a representation.\n\nTo make that shift rigorous, the work draws on formal faithfulness criteria: Yeh et al.\u2019s infidelity and ROAR\u2019s remove-and-retrain both operationalize explanation quality via information preserved or lost. IIS echoes this information-centric framing by quantifying the information gap between the full representation and what interpretations can capture, thereby defining interpretability as an information ratio rather than a count of interpretable units. In parallel, disentanglement metrics (Eastwood & Williams) motivate treating \u201csemantic factor content\u201d as a measurable quantity in representations, which IIS adapts to the supervised visual setting.\n\nFinally, the link to classifiability is anchored by linear probes (Alain & Bengio), providing a standardized measure of linear separability. Using this probe-based lens, the paper demonstrates that representation refinements that improve classifiability also increase IIS, unifying prior strands into a coherent, measurable classifiability\u2013interpretability connection.",
  "analysis_timestamp": "2026-01-06T23:42:48.094196"
}
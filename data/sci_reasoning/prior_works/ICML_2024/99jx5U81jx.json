{
  "prior_works": [
    {
      "title": "Towards A Rigorous Science of Interpretable Machine Learning",
      "authors": "Finale Doshi-Velez, Been Kim",
      "year": 2017,
      "role": "Conceptual foundation for simulatability and human-grounded evaluation",
      "relationship_sentence": "Introduced simulatability as a human-grounded criterion and advocated predict-the-model tasks, directly motivating this paper\u2019s central idea of evaluating explanations by how well they let humans infer model behavior."
    },
    {
      "title": "Do explanations make VQA models more predictable to a human?",
      "authors": "Arjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Dhruv Batra, Devi Parikh",
      "year": 2018,
      "role": "Precedent for human simulatability evaluation via predict-the-model",
      "relationship_sentence": "Established the methodology of measuring whether explanations help humans predict model outputs; the present work generalizes this paradigm to LLMs and extends it to counterfactual simulatability with formal metrics."
    },
    {
      "title": "Evaluating Explainable AI: Which Explanations Help You Predict Model Behavior?",
      "authors": "Peter Hase, Mohit Bansal",
      "year": 2020,
      "role": "Human-grounded simulatability methodology in NLP",
      "relationship_sentence": "Directly inspired assessing explanations by their utility for predicting model decisions; this paper advances that idea by testing predictions across diverse counterfactual variants of inputs."
    },
    {
      "title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models",
      "authors": "Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, Byron C. Wallace",
      "year": 2020,
      "role": "Metrics linking explanations to model behavior (sufficiency/comprehensiveness)",
      "relationship_sentence": "Influenced the emphasis on behavior-grounded faithfulness metrics; the paper\u2019s precision and generality echo ERASER\u2019s principle that explanations should tightly predict model outputs."
    },
    {
      "title": "CheckList: A Behavioral Testing Framework for NLP",
      "authors": "Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin",
      "year": 2020,
      "role": "Behavioral testing and templated counterfactuals",
      "relationship_sentence": "Provided the coverage-driven, contrastive testing mindset and template-based perturbations that underpin the paper\u2019s large-scale construction of diverse counterfactual inputs."
    },
    {
      "title": "Polyjuice: Automated, General-Purpose Counterfactual Generation",
      "authors": "Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, Daniel S. Weld",
      "year": 2021,
      "role": "Automatic counterfactual generation techniques",
      "relationship_sentence": "Supplied methods for producing diverse, minimally edited counterfactuals; the paper leverages this paradigm to probe explanation generality across systematic input variations."
    },
    {
      "title": "e-SNLI: Natural Language Inference with Natural Language Explanations",
      "authors": "Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, Phil Blunsom",
      "year": 2018,
      "role": "Foundational dataset and paradigm for natural language explanations",
      "relationship_sentence": "Established NLEs as a training and evaluation target; the present work interrogates whether such explanations actually enable humans to simulate model behavior, especially under counterfactual changes."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014evaluating counterfactual simulatability of natural language explanations with precision and generality metrics\u2014emerges by integrating three strands of prior work. First, Doshi-Velez and Kim articulated simulatability as a central, human-grounded goal for interpretability, and Chandrasekaran et al. as well as Hase and Bansal operationalized this through predict-the-model experiments showing whether explanations help people anticipate model outputs. The present paper advances this line by moving beyond single inputs to systematic families of counterfactuals, asking whether an explanation constrains a model\u2019s behavior in a way humans can reliably extrapolate.\nSecond, behavior-grounded metrics for explanations informed the measurement design. ERASER\u2019s sufficiency and comprehensiveness emphasized tying explanations to actual model decisions rather than plausibility alone; analogously, the proposed precision and generality quantify how tightly an explanation predicts model outputs across controlled input changes.\nThird, large, diverse counterfactual sets are crucial for stress-testing explanation generality. CheckList contributed the behavioral testing mindset and templated perturbations, while Polyjuice provided automated techniques to generate varied counterfactuals at scale. Finally, the tradition of natural language explanations popularized by e-SNLI situates the work\u2019s focus: explanations produced by models themselves. By unifying these threads\u2014human simulatability evaluation, behavior-grounded metrics, and systematic counterfactual generation\u2014the paper introduces a concrete, scalable protocol to test whether LLMs\u2019 natural language explanations genuinely help humans model LLM behavior under counterfactual changes.",
  "analysis_timestamp": "2026-01-06T23:42:48.074318"
}
{
  "prior_works": [
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Jonathan Ho, Ajay Jain, Pieter Abbeel",
      "year": 2020,
      "role": "Foundational diffusion framework",
      "relationship_sentence": "NOS builds on the DDPM denoising process and sampling dynamics, extending the diffusion paradigm to protein sequence design."
    },
    {
      "title": "Diffusion Models Beat GANs on Image Synthesis",
      "authors": "Prafulla Dhariwal, Alexander Nichol",
      "year": 2021,
      "role": "Introduced classifier guidance",
      "relationship_sentence": "The idea of steering diffusion sampling with gradients from a discriminative model directly motivates NOS\u2019s guidance, adapted to discrete sequence modeling."
    },
    {
      "title": "Classifier-Free Diffusion Guidance",
      "authors": "Jonathan Ho, Tim Salimans",
      "year": 2021,
      "role": "Guidance without external classifier",
      "relationship_sentence": "Concepts from classifier-free guidance inform NOS\u2019s approach to balancing fidelity and objective satisfaction during conditional sampling."
    },
    {
      "title": "Discrete Denoising Diffusion Probabilistic Models",
      "authors": "Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, Rianne van den Berg",
      "year": 2021,
      "role": "Diffusion for discrete variables",
      "relationship_sentence": "NOS relies on the D3PM formulation to operate in token (amino acid) space, addressing the challenges of discrete sequence diffusion."
    },
    {
      "title": "Plug and Play Language Models: A Simple Approach to Controlled Text Generation",
      "authors": "Sumanth Dathathri et al.",
      "year": 2020,
      "role": "Hidden-state gradient guidance for discrete generation",
      "relationship_sentence": "NOS\u2019s key technical idea\u2014optimizing gradients in model hidden states rather than discrete inputs\u2014parallels PPLM\u2019s hidden-state control for language models."
    },
    {
      "title": "RFdiffusion: Generative protein design using diffusion models of protein structure",
      "authors": "Joseph L. Watson et al.",
      "year": 2023,
      "role": "Structure-based diffusion for protein design",
      "relationship_sentence": "RFdiffusion established guided diffusion for protein structure and subsequent inverse folding, which NOS explicitly circumvents by operating directly in sequence space."
    },
    {
      "title": "LaMBO: Language Model Bayesian Optimization for Sequence Design",
      "authors": "Samuel D. Stanton et al.",
      "year": 2022,
      "role": "BO with generative language models for sequences",
      "relationship_sentence": "NOS generalizes LaMBO\u2019s framework for multi-objective, edit-constrained sequence optimization by replacing LM sampling with guided discrete diffusion."
    }
  ],
  "synthesis_narrative": "The core contribution of Protein Design with Guided Discrete Diffusion is NOS, a guidance method that enables conditional sampling for discrete diffusion in protein sequence space by following gradients in the denoiser\u2019s hidden states, and its integration into a LaMBO-style Bayesian optimization pipeline. This advances diffusion-based protein design beyond structure-centric methods and inverse folding.\nDDPM provides the denoising diffusion backbone, while Dhariwal and Nichol\u2019s classifier guidance formulates how discriminative gradients can steer sampling toward desired properties. Ho and Salimans\u2019 classifier-free guidance influences the calibration of guidance strength and trade-offs between realism and objective satisfaction. Austin et al.\u2019s D3PM makes diffusion directly applicable to discrete tokens, giving NOS the modeling substrate for amino acid sequences.\nCritically, NOS\u2019s insight to act in hidden states rather than on discrete inputs echoes PPLM\u2019s hidden-state gradient control for language models, resolving the non-differentiability of token spaces. In protein design, RFdiffusion demonstrated powerful guided diffusion but required structure modeling and inverse folding; NOS instead performs design directly in sequence space, addressing data scarcity and inverse-design brittleness. Finally, LaMBO established an effective BO framework for optimizing sequences with a generative prior and black-box objectives; NOS generalizes this to discrete diffusion, enabling multi-objective, edit-constrained design directly during the denoising trajectory. Together, these works directly shape NOS\u2019s algorithmic design and its application to protein sequence optimization.",
  "analysis_timestamp": "2026-01-07T00:02:04.870111"
}
{
  "prior_works": [
    {
      "title": "AudioCaps: Generating Captions for Audios in the Wild",
      "authors": "Kim et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "ADIFF derives one of its two source corpora from AudioCaps and reformulates its single-audio captions into pairwise difference explanations, making AudioCaps the foundational dataset the benchmark is built upon."
    },
    {
      "title": "Clotho: an Audio Captioning Dataset",
      "authors": "Konstantinos Drossos et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "ADIFF constructs the second half of its benchmark directly from Clotho captions, extending the audio captioning problem formulation to the new task of audio difference explanation."
    },
    {
      "title": "LAION-CLAP: Learning Audio-Text Embeddings at Scale",
      "authors": "Ben Elizalde et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "ADIFF\u2019s baseline uses pre-trained audio embeddings from audio\u2013language models like CLAP to condition a frozen LLM, directly relying on CLAP\u2019s audio\u2013text representation as the enabling backbone."
    },
    {
      "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
      "authors": "Xiang Lisa Li et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "ADIFF\u2019s primary baseline extends prefix-tuning by constructing continuous prompts from two audio embeddings to steer a frozen language model toward difference explanations."
    },
    {
      "title": "Multimodal Few-Shot Learning with Frozen Language Models",
      "authors": "Maria Tsimpoukelli et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "The strategy of keeping the language model frozen and injecting non-text features as soft prompts directly inspires ADIFF\u2019s design of mapping audio features into a frozen LLM for generation."
    },
    {
      "title": "WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset",
      "authors": "Jinzheng Mei et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "ADIFF\u2019s data construction pipeline that uses LLMs to synthesize and structure textual supervision for audio is motivated by WavCaps\u2019 demonstration that LLMs can reliably scale audio caption annotations."
    },
    {
      "title": "Change Captioning: Describing Differences Between Two Images",
      "authors": "Park et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Prior work on change/difference captioning in vision directly motivates ADIFF\u2019s problem formulation of generating natural-language explanations that focus on differences between paired inputs."
    }
  ],
  "synthesis_narrative": "ADIFF\u2019s core innovation\u2014framing and benchmarking natural\u2011language explanations of differences between two audio recordings\u2014stands on three pillars: problem formulation, data, and a lightweight multimodal generation pipeline. The problem formulation is inspired by change/difference captioning in vision, where paired inputs are described in terms of what differs, providing a direct conceptual template ADIFF adapts to audio. On the data side, the work repurposes established audio captioning corpora\u2014AudioCaps and Clotho\u2014into a paired setting, making these datasets the foundation for constructing the new benchmark. To populate supervision at multiple granularity levels, ADIFF follows the emerging paradigm of LLM\u2011assisted annotation exemplified by WavCaps, using large language models to synthesize structured difference explanations from existing captions. Methodologically, ADIFF\u2019s baseline follows the line of injecting non\u2011text modality features into a frozen language model to enable generative reasoning: the general idea is drawn from multimodal few\u2011shot learning with frozen LMs, while the concrete mechanism is an extension of prefix\u2011tuning that converts two audio embeddings into continuous prompts. This pipeline crucially depends on modern audio\u2013language representation learning, with CLAP providing text\u2011aligned audio embeddings that make the conditioning effective. Together, these works directly enable ADIFF\u2019s benchmark creation, task definition, and baseline model for audio difference explanation.",
  "analysis_timestamp": "2026-01-06T23:09:26.634895"
}
{
  "prior_works": [
    {
      "title": "Apprenticeship Learning via Inverse Reinforcement Learning",
      "authors": [
        "Pieter Abbeel",
        "Andrew Y. Ng"
      ],
      "year": 2004,
      "role": "Foundational IRL formulation highlighting reward inference as a surrogate for behavior alignment",
      "relationship_sentence": "This work established the IRL paradigm and exposed reward ambiguity, directly motivating the paper\u2019s choice to bypass reward design and align policies to trajectories instead."
    },
    {
      "title": "Maximum Entropy Inverse Reinforcement Learning",
      "authors": [
        "Brian D. Ziebart",
        "Andrew Maas",
        "J. Andrew Bagnell",
        "Anind K. Dey"
      ],
      "year": 2008,
      "role": "Trajectory-distribution perspective via maximum likelihood under entropy regularization",
      "relationship_sentence": "By framing expert behavior as a probability distribution over trajectories, MaxEnt IRL is the conceptual precursor to maximizing expert-trajectory likelihood, which this paper adopts while removing reward-modeling."
    },
    {
      "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning (DAgger)",
      "authors": [
        "St\u00e9phane Ross",
        "Geoffrey J. Gordon",
        "J. Andrew Bagnell"
      ],
      "year": 2011,
      "role": "Imitation learning addressing covariate shift with per-step supervision",
      "relationship_sentence": "DAgger\u2019s analysis of compounding error under per-state action matching exposes the limitations of stepwise alignment, directly motivating this paper\u2019s long-horizon trajectory alignment objective."
    },
    {
      "title": "Generative Adversarial Imitation Learning",
      "authors": [
        "Jonathan Ho",
        "Stefano Ermon"
      ],
      "year": 2016,
      "role": "Adversarial occupancy measure matching to imitate experts without explicit rewards",
      "relationship_sentence": "GAIL demonstrates reward-free imitation via distribution alignment, but primarily at the occupancy (marginal) level; the new paper extends this by aligning full expert trajectories to preserve long-horizon coherence."
    },
    {
      "title": "Deep Reinforcement Learning from Human Preferences",
      "authors": [
        "Paul F. Christiano",
        "Jan Leike",
        "Tom B. Brown",
        "Miljan Martic",
        "Shane Legg",
        "Dario Amodei"
      ],
      "year": 2017,
      "role": "Preference-based reward learning from trajectory comparisons",
      "relationship_sentence": "This work shows trajectory-level supervision via preferences but suffers from reward-model ambiguity and distribution mismatch, issues the present paper avoids by directly aligning policies to labeled trajectories."
    },
    {
      "title": "Trajectory-Ranked Reward Extrapolation (T-REX)",
      "authors": [
        "Daniel S. Brown",
        "Wonjoon Goo",
        "Scott Niekum"
      ],
      "year": 2019,
      "role": "Learning rewards from ranked trajectories to capture long-horizon behavior",
      "relationship_sentence": "T-REX leverages trajectory-level information to address long-horizon structure, providing the impetus for using sequence-level supervision, while the new paper removes the intermediate reward learning step."
    },
    {
      "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
      "authors": [
        "Rafael Rafailov",
        "et al."
      ],
      "year": 2023,
      "role": "Direct sequence-level alignment from preferences without explicit reward modeling",
      "relationship_sentence": "DPO\u2019s reward-free, sequence-level policy optimization informs this paper\u2019s philosophy of directly optimizing likelihood over labeled trajectories rather than fitting a reward proxy."
    }
  ],
  "synthesis_narrative": "The core innovation of Trajectory Graph Learning is to align policies directly with expert-labeled long trajectories\u2014maximizing the probability of generating those trajectories\u2014while avoiding reward design. This builds on a lineage that first framed expert behavior through reward inference. Abbeel and Ng (2004) introduced apprenticeship learning via IRL, revealing reward ambiguity that often undermines faithful behavior replication. Ziebart\u2019s Maximum Entropy IRL (2008) shifted the perspective to probability distributions over trajectories via maximum-likelihood principles, foreshadowing the paper\u2019s trajectory-level likelihood view, yet still tied to reward parameterization. In parallel, imitation learning advanced through DAgger (Ross et al., 2011), whose analysis of compounding errors under per-state supervision highlighted the need to preserve long-horizon coherence rather than just matching local actions. GAIL (Ho & Ermon, 2016) removed explicit reward design by matching occupancy measures adversarially, but primarily at marginal distributions, leaving room for methods that explicitly respect sequence structure. Preference-based approaches, notably Christiano et al. (2017) and T-REX (2019), injected trajectory-level supervision (comparisons or rankings) to capture long-horizon behavior, though they reintroduced reward modeling with its ambiguity and distribution mismatch. Recent sequence-level direct optimization like DPO (2023) demonstrated that one can align policies to preferences without a reward model, conceptually motivating this work\u2019s direct likelihood maximization on trajectories. The present paper synthesizes these threads, formalizing trajectory-level alignment (including its NP-completeness) and proposing a graph-based learning procedure that preserves long-horizon structure without surrogate rewards.",
  "analysis_timestamp": "2026-01-07T00:21:32.241159"
}
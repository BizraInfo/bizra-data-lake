{
  "prior_works": [
    {
      "title": "Distribution-Free, Risk-Controlling Prediction Sets",
      "authors": "Stephen Bates et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "Provides the standard multi-class conformal prediction procedures (e.g., APS/RAPS) and calibration protocols that the paper uses to construct marginal and group-calibrated prediction sets when evaluating fairness and disparate impact."
    },
    {
      "title": "Least Ambiguous Set-Valued Classifiers with Bounded Error",
      "authors": "Mauricio Sadinle and Jing Lei",
      "year": 2019,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Introduces the ambiguity notion (expected set size) as the key efficiency measure for set-valued classifiers, directly motivating the paper\u2019s proposal to equalize set sizes across protected groups as a fairness objective."
    },
    {
      "title": "Multicalibration: Calibration for the (Computationally-Identifiable) Masses",
      "authors": "Ilan Hebert-Johnson et al.",
      "year": 2018,
      "arxiv_id": "1807.00263",
      "role": "Foundation",
      "relationship_sentence": "Establishes subgroup-conditional calibration as a fairness principle, which underlies the equalized-coverage goal (group-conditional validity) that this paper scrutinizes in the context of prediction sets."
    },
    {
      "title": "Disparate Interactions: An Algorithm-in-the-Loop Experiment in Criminal Justice Risk Assessment",
      "authors": "Ben Green and Yiling Chen",
      "year": 2019,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Demonstrates that human decision-makers\u2019 use of model outputs can induce disparate impact, directly motivating the paper\u2019s human-subject experiments assessing how prediction sets affect downstream group outcomes."
    },
    {
      "title": "On Fairness and Calibration",
      "authors": "Geoff Pleiss et al.",
      "year": 2017,
      "arxiv_id": "1709.02012",
      "role": "Gap Identification",
      "relationship_sentence": "Shows inherent tensions between calibration-like constraints and error-based fairness, prompting the paper\u2019s central question of whether enforcing equalized coverage (a coverage-calibration analogue) actually improves downstream fairness."
    },
    {
      "title": "Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness",
      "authors": "Michael Kearns et al.",
      "year": 2018,
      "arxiv_id": "1711.05144",
      "role": "Related Problem",
      "relationship_sentence": "Frames fairness as guaranteeing comparable behavior across protected (and richer) subgroups, a perspective that informs both equalized coverage and the paper\u2019s shift toward parity in set sizes across groups."
    }
  ],
  "synthesis_narrative": "Distribution-free predictive set methods established concrete procedures for constructing and calibrating multi-class prediction sets, including mechanisms to attain marginal coverage and to adjust calibration to target specific groups. Complementing this, set-valued classification research formalized ambiguity\u2014the expected size of a prediction set\u2014as the primary efficiency notion, revealing that controlling set size is central to the usability of set outputs. Independent work on multicalibration articulated subgroup-conditional guarantees as a fairness objective, cultivating the idea that coverage should hold uniformly across protected groups. Human\u2013algorithm interaction studies then documented that people use model outputs in ways that can amplify disparities, underscoring that fairness must be judged on downstream decisions, not just statistical guarantees. Finally, results on fairness\u2013calibration incompatibilities highlighted that enforcing calibration-like constraints can trade off with other fairness goals, and subgroup-fairness frameworks advocated parity across groups or subgroups as the fairness target. Taken together, these threads created a natural tension: equalized coverage offers a principled fairness guarantee for prediction sets, while ambiguity-driven efficiency and human decision dynamics determine real outcomes. The current work synthesizes these insights by empirically testing how equalized coverage affects human decisions and by leveraging the ambiguity notion to propose equalizing set sizes across groups, showing this parity-of-ambiguity criterion better aligns statistical uncertainty reporting with downstream fairness.",
  "target_paper": {
    "title": "Conformal Prediction Sets Can Cause Disparate Impact",
    "authors": "Jesse C. Cresswell, Bhargava Kumar, Yi Sui, Mouloud Belbahri",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Conformal Prediction, Fairness, Uncertainty Quantification, Trustworthy ML, Human Subject Experiments",
    "abstract": "Conformal prediction is a statistically rigorous method for quantifying uncertainty in models by having them output sets of predictions, with larger sets indicating more uncertainty. However, prediction sets are not inherently actionable; many applications require a single output to act on, not several. To overcome this limitation, prediction sets can be provided to a human who then makes an informed decision. In any such system it is crucial to ensure the fairness of outcomes across protected groups, and researchers have proposed that Equalized Coverage be used as the standard for fairness. By conducting experiments with human participants, we demonstrate that providing prediction sets can lead to disparate impact in decisions. Disquietingly, we find that providing sets that satisfy Equalized Coverage actually increases disparate impact compared to marginal coverage. Instead of equalizing coverage, we propose to equalize set sizes across groups which empirically leads to lower dispara",
    "openreview_id": "fZK6AQXlUU",
    "forum_id": "fZK6AQXlUU"
  },
  "analysis_timestamp": "2026-01-06T10:20:43.748696"
}
{
  "prior_works": [
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Shunyu Yao et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "rStar-Math replaces ToT\u2019s heuristic breadth/depth search with Monte Carlo Tree Search guided by a learned process reward model, directly improving the core idea of test-time tree search over reasoning trajectories."
    },
    {
      "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm",
      "authors": "David Silver et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "rStar-Math adapts the AlphaZero blueprint\u2014policy plus value/reward signals optimized via MCTS\u2014by instantiating a policy SLM and a process reward model to guide search and self-evolution for language reasoning."
    },
    {
      "title": "Training Verifiers to Solve Math Word Problems",
      "authors": "Markus Cobbe et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "The paper\u2019s use of learned verifiers to evaluate mathematical solutions underpins rStar-Math\u2019s use of automated verification and a learned process reward signal to judge and guide intermediate reasoning steps."
    },
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": "Jason Wei et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "rStar-Math trains its policy SLM on code-augmented chain-of-thought trajectories, building directly on the CoT formulation for stepwise reasoning supervision."
    },
    {
      "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
      "authors": "Xuezhi Wang et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "rStar-Math generalizes self-consistency\u2019s multi-sample exploration and aggregation into a principled search procedure, using MCTS with a process reward to evaluate and select promising reasoning branches."
    },
    {
      "title": "STaR: Bootstrapping Reasoning With Reasoning",
      "authors": "Eric Zelikman et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "rStar-Math extends STaR\u2019s self-improvement via verified rationales by generating verified multi-step rollouts with MCTS and co-evolving both a policy model and a process preference model from scratch."
    },
    {
      "title": "PAL: Program-Aided Language Models",
      "authors": "Luyu Gao et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "rStar-Math\u2019s code-augmented CoT data synthesis and step verification directly leverage PAL\u2019s core idea of delegating computation to external code to validate and scaffold mathematical reasoning."
    }
  ],
  "synthesis_narrative": "rStar-Math sits at the confluence of three lines of work: structured search over thoughts, verification-driven supervision, and self-bootstrapping of reasoning skills. Chain-of-Thought established the stepwise reasoning format that rStar-Math adopts and extends with code-augmented trajectories, while Self-Consistency demonstrated that exploring multiple reasoning paths and aggregating them can markedly improve reliability. Tree of Thoughts transformed this idea into explicit search over intermediate thoughts; rStar-Math directly builds on this by replacing heuristic search with an AlphaZero-inspired MCTS that uses a learned policy (an SLM) and a process reward signal to guide expansion and pruning of the reasoning tree. The AlphaZero framework provides the foundational insight that coupling a search policy with learned evaluation yields strong performance, which rStar-Math translates from games to mathematical reasoning. On the supervision side, Training Verifiers to Solve Math Word Problems showed that verifiers can assess mathematical solutions; rStar-Math deepens this by training a process reward model to evaluate intermediate steps, enabling both search guidance and higher-quality supervision. Finally, STaR pioneered self-improvement via verified rationales; rStar-Math extends this into a full self-evolution loop that co-trains the policy SLM and the process preference model from scratch using millions of verified MCTS rollouts. PAL\u2019s use of executable code supports rStar-Math\u2019s code-augmented CoT synthesis and step verification, providing reliable signals that make the self-evolution and MCTS guidance effective.",
  "analysis_timestamp": "2026-01-06T23:07:19.582460"
}
{
  "prior_works": [
    {
      "title": "Variational Learning of Inducing Variables in Sparse Gaussian Processes",
      "authors": "Michalis Titsias et al.",
      "year": 2009,
      "role": "Baseline",
      "relationship_sentence": "This paper introduced the standard sparse variational GP formulation q(f,u)=p(f|u)q(u) and a collapsed ELBO; the new work explicitly relaxes that factorization by learning a richer q(f|u) and analytically collapsing its extra parameters to obtain a strictly tighter bound."
    },
    {
      "title": "Gaussian Processes for Big Data",
      "authors": "James Hensman et al.",
      "year": 2013,
      "role": "Extension",
      "relationship_sentence": "SVGP made the Titsias (2009) bound trainable with minibatches under the same q(f|u)=p(f|u) assumption; the new bound is designed to drop into this SVGP training pipeline with minimal code changes while delivering a tighter objective."
    },
    {
      "title": "A Unifying View of Sparse Approximate Gaussian Process Regression",
      "authors": "Joaquin Qui\u00f1onero-Candela et al.",
      "year": 2005,
      "role": "Foundation",
      "relationship_sentence": "This work formalized inducing-variable approximations built around the conditional prior p(f|u), precisely the structural assumption that the new paper generalizes in its variational family."
    },
    {
      "title": "Sparse Gaussian Processes using Pseudo-inputs",
      "authors": "Edward Snelson et al.",
      "year": 2006,
      "role": "Foundation",
      "relationship_sentence": "Pseudo-input inducing points provided the practical inducing-variable mechanism retained by the new method, which strengthens the objective not by changing inducing variables but by relaxing the posterior factorization."
    },
    {
      "title": "Understanding Probabilistic Sparse Gaussian Process Approximations",
      "authors": "Matthias Bauer et al.",
      "year": 2016,
      "role": "Gap Identification",
      "relationship_sentence": "By analyzing when VFE-based sparse approximations can be conservative, this paper highlighted the limitations of the standard variational restriction that the new work directly addresses with a richer q(f|u) and a tighter collapsed bound."
    },
    {
      "title": "Orthogonally Decoupled Variational Gaussian Processes",
      "authors": "Hugh Salimbeni et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "This paper enlarged the variational family to tighten ELBOs while preserving stochastic optimization; the new work pursues the same goal via a different mechanism\u2014introducing per-datapoint parameters in q(f|u) and collapsing them analytically."
    },
    {
      "title": "Rates of Convergence for Sparse Variational Gaussian Process Regression",
      "authors": "David R. Burt et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "By quantifying how the VFE/SVGP approximation error scales with the number of inducing points, this work underscored the variational gap that the new tighter bound seeks to reduce without increasing model complexity."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014relaxing the canonical variational factorization q(f,u)=p(f|u)q(u) and collapsing newly introduced per-datapoint parameters to yield a tighter, tractable ELBO\u2014sits squarely on the inducing-variable lineage. Qui\u00f1onero-Candela and Rasmussen (2005) established the inducing-variable framework and the central role of the conditional prior p(f|u), while Snelson and Ghahramani (2006) operationalized it via pseudo-inputs. Titsias (2009) then provided the decisive variational formulation: a VFE bound derived under q(f|u)=p(f|u) with an analytically optimized (collapsed) q(u), which became the de facto baseline that the present paper directly tightens by enlarging q(f|u).\nHensman et al. (2013) extended this approach to stochastic variational inference, enabling minibatch training that today underpins most scalable GP implementations; the new bound is crafted to integrate seamlessly into this SVGP pipeline. A series of analyses revealed the cost of the standard variational restriction: Bauer et al. (2016) identified cases where VFE is conservative, and Burt et al. (2019) quantified convergence rates, both motivating a strictly tighter objective. Parallel efforts to expand the variational family, notably Orthogonally Decoupled VGPs (Salimbeni et al., 2018), demonstrated that greater variational flexibility can materially tighten ELBOs while preserving scalability. Building on these insights, the present work proposes a principled, minimal-change relaxation\u2014introducing N extra parameters within q(f|u) that can be analytically collapsed in GP regression\u2014to surpass the Titsias (2009)/SVGP bound while keeping the same inducing-variable machinery and stochastic optimization workflow.",
  "analysis_timestamp": "2026-01-06T23:07:19.614738"
}
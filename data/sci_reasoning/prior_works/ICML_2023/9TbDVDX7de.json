{
  "prior_works": [
    {
      "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
      "authors": "Aleksander Madry et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Adversarial training from Madry et al. is the primary robustness baseline and the source of the well-documented phenomenon that robust models exhibit perceptually aligned input gradients, which this paper inverts by directly training for such alignment to test whether it implies robustness."
    },
    {
      "title": "Adversarial Examples Are Not Bugs, They Are Features",
      "authors": "Andrew Ilyas et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "Ilyas et al. established the robust vs. non-robust feature framework, providing the conceptual basis that robustness is tied to human-perceptible (robust) features\u2014directly motivating the hypothesis that enforcing perceptually aligned gradients could induce robustness."
    },
    {
      "title": "On the Connection Between Adversarial Robustness and Saliency Map Interpretability",
      "authors": "Tobias Etmann et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "Etmann et al. theoretically and empirically linked robustness to more interpretable, human-aligned gradient-based saliency, a specific linkage this work operationalizes by explicitly promoting gradient\u2013perception alignment during training."
    },
    {
      "title": "Image Synthesis with a Single (Robust) Classifier",
      "authors": "Surya Santurkar et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Santurkar et al. showed that robust classifiers have gradients aligned well enough with semantic content to synthesize realistic class images, directly inspiring the idea to treat perceptually aligned gradients as a target property rather than a byproduct."
    },
    {
      "title": "Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients",
      "authors": "Andrew Ross et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "Ross and Doshi-Velez demonstrated that constraining input gradients can simultaneously improve interpretability and robustness; this paper extends that paradigm by aligning gradients to a perceptual target (rather than merely shrinking or masking them) and showing the alignment itself yields robustness."
    },
    {
      "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric",
      "authors": "Richard Zhang et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "LPIPS from Zhang et al. provides the perceptual similarity metric that this paper relies on to define and quantify perceptual alignment of gradients and to construct a training objective that promotes such alignment."
    },
    {
      "title": "Theoretically Principled Trade-off between Robustness and Accuracy (TRADES)",
      "authors": "Hongyang Zhang et al.",
      "year": 2019,
      "role": "Baseline",
      "relationship_sentence": "TRADES is a core adversarial training baseline against which the proposed perceptual-gradient alignment objective is compared and with which it is combined to demonstrate robustness gains attributable to improved gradient alignment."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014directly training classifiers to have perceptually aligned input gradients and testing whether this property itself implies robustness\u2014stands on a lineage that first established a robust\u2013interpretability link and then hinted that manipulating input gradients can affect robustness. Madry et al. introduced adversarial training, the dominant robustness baseline and the empirical source of the observation that robust models exhibit perceptually aligned gradients. Ilyas et al.\u2019s robust vs. non-robust feature framework provided the conceptual foundation: robustness correlates with human-perceptible features, suggesting that encouraging gradients to point along perceptual directions may induce robustness. Etmann et al. formally connected adversarial robustness to more interpretable saliency, reinforcing the specific target\u2014human-aligned gradients\u2014that this work operationalizes.\n\nTwo lines of prior work then enable the method design. Santurkar et al. demonstrated that robust classifiers\u2019 gradients align so well with semantics that gradient ascent can synthesize class images, directly inspiring the idea to treat perceptually aligned gradients as a trainable goal. Ross and Doshi-Velez showed that regularizing input gradients can improve both interpretability and robustness, a technique this paper extends by aligning to a perceptual target rather than merely shrinking gradients. Finally, Zhang et al.\u2019s LPIPS supplies the perceptual metric needed to define and measure alignment, while TRADES (alongside Madry\u2019s PGD training) serves as a principal baseline and integration point, enabling controlled tests that reveal a bidirectional link: improving perceptual gradient alignment causally improves adversarial robustness.",
  "analysis_timestamp": "2026-01-06T23:09:26.538809"
}
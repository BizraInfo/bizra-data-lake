{
  "prior_works": [
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, et al.",
      "year": 2022,
      "role": "Principle-driven AI-feedback alignment",
      "relationship_sentence": "Directly inspired SELF-ALIGN\u2019s core idea of replacing extensive human labels with a small set of written principles and model self-critique to produce preference signals, which SELF-ALIGN generalizes beyond harmlessness and uses to train from scratch with minimal human supervision."
    },
    {
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
      "authors": "Yuntao Bai, Andy Jones, et al.",
      "year": 2022,
      "role": "HHH alignment framing and RLHF baseline",
      "relationship_sentence": "Established the Helpful\u2013Harmless\u2013Honest (HHH) alignment objectives and the RLHF pipeline that SELF-ALIGN explicitly seeks to reduce reliance on, while still targeting similar behavioral goals via principle-driven self-generated feedback."
    },
    {
      "title": "Training language models to follow instructions with human feedback (InstructGPT)",
      "authors": "Long Ouyang, Jeffrey Wu, Xu Jiang, et al.",
      "year": 2022,
      "role": "SFT+RLHF paradigm and motivation to cut human labels",
      "relationship_sentence": "Provided the dominant SFT+RLHF recipe and highlighted the cost and scalability limits of human preference data, motivating SELF-ALIGN\u2019s pursuit of instruction-following behavior using principles and AI-generated supervision instead."
    },
    {
      "title": "Deep reinforcement learning from human preferences",
      "authors": "Paul F. Christiano, Jan Leike, Tom B. Brown, et al.",
      "year": 2017,
      "role": "Foundational preference-learning framework",
      "relationship_sentence": "Introduced learning from pairwise human preferences over model outputs; SELF-ALIGN adopts the preference-learning setup but replaces human comparisons with principle-grounded, model-generated comparisons."
    },
    {
      "title": "Learning to summarize with human feedback",
      "authors": "Nisan Stiennon, Long Ouyang, Jeffrey Wu, et al.",
      "year": 2020,
      "role": "Applied preference models for alignment at scale",
      "relationship_sentence": "Demonstrated that preference data can effectively steer models toward desired behaviors on a concrete task; SELF-ALIGN mirrors this pipeline while synthesizing preference data via principle-driven critiques rather than human judgments."
    },
    {
      "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
      "authors": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, et al.",
      "year": 2022,
      "role": "Synthetic instruction generation and bootstrapping",
      "relationship_sentence": "Directly informed SELF-ALIGN\u2019s use of LLMs to create diverse prompts and expand coverage with minimal human input, which SELF-ALIGN augments with topic guidance and principle-based supervision."
    },
    {
      "title": "Alpaca: A Strong, Replicable Instruction-Following Model",
      "authors": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori Hashimoto",
      "year": 2023,
      "role": "Low-cost instruction tuning via synthetic data",
      "relationship_sentence": "Showed that small models can be aligned using inexpensive, synthetic instruction\u2013response pairs; SELF-ALIGN extends this idea by not relying on external model outputs and instead generating and judging data with principles."
    }
  ],
  "synthesis_narrative": "SELF-ALIGN\u2019s key contribution\u2014principle-driven self-alignment from scratch with minimal human supervision\u2014sits at the intersection of RLHF-based alignment and synthetic data bootstrapping. The RLHF lineage (Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022) established that preference data and reinforcement learning can steer large language models toward desired behavior, but also underscored the cost and scalability challenges of human annotation. Anthropic\u2019s work catalyzed a shift: the HHH framework (Bai et al., 2022) clarified alignment objectives, and Constitutional AI (Bai et al., 2022) demonstrated that a concise set of principles can replace human preference labels with AI feedback via self-critique and revision. SELF-ALIGN directly adopts and generalizes this principle-driven supervision, broadening beyond harmlessness to multi-principle guidance and leveraging it to construct preference data without heavy human involvement.\n\nIn parallel, Synthetic instruction generation methods (Self-Instruct) and low-cost instruction tuning (Alpaca) showed that LLMs can bootstrap their own training corpora. SELF-ALIGN integrates this idea through LLM-generated prompts and topic-guided augmentation to ensure diversity and coverage. The synthesis is a pipeline where LLMs generate prompts, produce candidate responses, and\u2014guided by a compact set of human-written principles\u2014produce preference signals to train models from scratch. Thus, SELF-ALIGN marries the constitutional, AI-feedback paradigm with self-instructional data creation, reducing dependence on human labels while preserving the alignment objectives codified by HHH and operationalized by RLHF.",
  "analysis_timestamp": "2026-01-07T00:02:04.812112"
}
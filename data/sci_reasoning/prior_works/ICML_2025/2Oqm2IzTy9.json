{
  "prior_works": [
    {
      "title": "Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization",
      "authors": "Martin Jaggi",
      "year": 2013,
      "role": "Foundational LMO/conditional-gradient method",
      "relationship_sentence": "Introduces the linear minimization oracle (LMO) over norm-balls and projection-free updates, which the present paper repurposes as the core mechanism to generate geometry-adaptive steps that also unify several optimizers."
    },
    {
      "title": "Projection-Free Online Learning",
      "authors": "Elad Hazan, Satyen Kale",
      "year": 2012,
      "role": "Stochastic/online conditional-gradient framework",
      "relationship_sentence": "Provides stochastic projection-free methodology with LMOs that directly motivates the paper\u2019s new family of stochastic LMO-based algorithms and their analysis in large-scale settings."
    },
    {
      "title": "Mirror Descent and Nonlinear Projected Subgradient Methods for Convex Optimization",
      "authors": "Amir Beck, Marc Teboulle",
      "year": 2003,
      "role": "Geometry via norms and dual norms (steepest descent viewpoint)",
      "relationship_sentence": "Establishes how choosing a norm induces an update geometry (via dual norms), underpinning the paper\u2019s view that an LMO over a norm-ball yields normalized, geometry-aware steps that can be applied even to unconstrained problems."
    },
    {
      "title": "Path-SGD: Path-Normalized Optimization of Deep Neural Networks",
      "authors": "Behnam Neyshabur, Ruslan Salakhutdinov, Nathan Srebro",
      "year": 2015,
      "role": "Explicit norm/geometry design for deep networks",
      "relationship_sentence": "Demonstrates the power of architecture-tailored norms for scale-invariant training, informing the paper\u2019s explicit norm choice for deep models and its effects on optimization behavior."
    },
    {
      "title": "signSGD with Majority Vote is Communication Efficient",
      "authors": "Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, Anima Anandkumar",
      "year": 2018,
      "role": "Normalized/sign updates as a special case of norm-induced geometry",
      "relationship_sentence": "Shows that sign-based updates are effective; the paper subsumes signSGD as the LMO direction for an \u2113\u221e-ball, supporting its unification claim across optimizers via norm choices."
    },
    {
      "title": "Large Batch Training of Neural Networks with Layer-wise Adaptive Rate Scaling",
      "authors": "Yang You, Igor Gitman, Boris Ginsburg",
      "year": 2017,
      "role": "Layer-wise norm-adaptive updates for deep nets (scaling/transfer)",
      "relationship_sentence": "Introduces norm-based, layer-wise scaling that inspires the paper\u2019s geometry-aware updates and connects norm choice to stable hyperparameter behavior across model scales."
    },
    {
      "title": "Tensor Programs V: Tuning Large Neural Networks via \u03bc-Parameterization (\u03bcP)",
      "authors": "Greg Yang et al.",
      "year": 2021,
      "role": "Hyperparameter transfer across model sizes",
      "relationship_sentence": "Establishes principles for transferring hyperparameters across widths; the paper\u2019s explicit norm choice similarly yields transferability of hyperparameters across model sizes."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core idea\u2014training via a stochastic family of updates generated by a linear minimization oracle (LMO) over norm-balls\u2014rests squarely on the conditional-gradient/Frank\u2013Wolfe paradigm introduced and popularized by Jaggi (2013), where an LMO replaces costly projections and returns extreme points of norm-balls. Hazan and Kale (2012) extend this paradigm to stochastic/online settings, directly enabling the paper\u2019s projection-free stochastic algorithms at deep-learning scale. Beck and Teboulle (2003) provide the geometric backbone: choosing a norm defines a dual-norm steepest-descent direction, which explains why an LMO over a norm-ball yields normalized, geometry-adaptive steps\u2014even for unconstrained problems\u2014and clarifies the unifying view that different norms recover different optimizers. This unification is concretized by Bernstein et al. (2018), where signSGD emerges as the \u2113\u221e-ball LMO direction, and by the broader family of normalized steps encompassed by the proposed framework. For deep architectures, Neyshabur et al. (2015) show the value of architecture-aware norms (e.g., path norms) to achieve scale-invariance, informing the paper\u2019s explicit norm design that stabilizes training across depths and widths. Complementarily, You et al. (2017) demonstrate layer-wise norm-guided scaling (LARS), reinforcing the link between norm geometry and practical robustness at large scale. Finally, Yang et al. (2021) provide theoretical and empirical grounding for hyperparameter transfer across model sizes, echoed here through a norm choice that preserves optimizer behavior across architectures.",
  "analysis_timestamp": "2026-01-07T00:04:09.136371"
}
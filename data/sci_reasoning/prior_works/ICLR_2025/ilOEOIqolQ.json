{
  "prior_works": [
    {
      "title": "Extracting Training Data from Large Language Models",
      "authors": "Nicholas Carlini et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "By showing that LLMs can verbatim reproduce rare training sequences, this work motivated the need for a systematic, scalable way to attribute generated text to web sources; CREATIVITY INDEX directly addresses this gap by quantifying verbatim and near-verbatim reuse via web attribution rather than model probing."
    },
    {
      "title": "Preventing Verbatim Memorization in Language Models",
      "authors": "Daphne Ippolito et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "This paper\u2019s focus on mitigating verbatim copying highlighted the lack of a holistic metric to measure reuse versus novelty; CREATIVITY INDEX operationalizes that need by measuring reconstructability of a text from web snippets, enabling comparison of human and LLM \u2018creativity.\u2019"
    },
    {
      "title": "On the resemblance and containment of documents",
      "authors": "Andrei Z. Broder",
      "year": 1997,
      "role": "Foundation",
      "relationship_sentence": "Broder\u2019s shingling/Jaccard framework established the core notion of near-duplicate detection that underlies matching text fragments to large corpora; DJ SEARCH builds on this foundation to handle mosaic, near-verbatim matches across open-web sources."
    },
    {
      "title": "Finding near-duplicate web pages: A large-scale evaluation of algorithms",
      "authors": "Monika R. Henzinger",
      "year": 2006,
      "role": "Related Problem",
      "relationship_sentence": "Henzinger\u2019s web-scale near-duplicate detection insights directly inform the paper\u2019s premise that open-web matching is feasible at scale; DJ SEARCH extends this line by performing fine-grained, multi-source alignment for attribution rather than page-level duplication."
    },
    {
      "title": "Identification of common molecular subsequences",
      "authors": "Temple F. Smith and Michael S. Waterman",
      "year": 1981,
      "role": "Inspiration",
      "relationship_sentence": "The local alignment paradigm pioneered here inspires DJ SEARCH\u2019s dynamic-programming-based alignment of candidate web snippets to target text, enabling robust detection of near-verbatim segments with insertions, deletions, and substitutions."
    },
    {
      "title": "An Evaluation Framework for Plagiarism Detection",
      "authors": "Martin Potthast et al.",
      "year": 2010,
      "role": "Foundation",
      "relationship_sentence": "The plagiarism-detection framework (PAN) formalized external source retrieval and mosaic plagiarism evaluation; CREATIVITY INDEX generalizes this idea from academic corpora to the open web to quantify how much of a text can be reconstructed from prior sources."
    },
    {
      "title": "Deduplicating Training Data Makes Language Models Better",
      "authors": "Katherine Lee et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "By linking near-duplicate training data to memorization and downstream behavior, this work provided empirical motivation to measure reuse; the present paper shifts from training-set dedup to output-level attribution and turns it into a creativity metric."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014CREATIVITY INDEX with DJ SEARCH\u2014emerges at the intersection of text reuse detection and memorization research. Foundationally, Broder\u2019s shingling work and Henzinger\u2019s web-scale near-duplicate studies established that large-scale detection of textual resemblance on the open web is both meaningful and feasible. In parallel, the PAN plagiarism detection framework (Potthast et al.) formalized external source retrieval and mosaic plagiarism evaluation, crystallizing the notion that a text can be reconstructed from multiple prior sources. Algorithmically, the local alignment paradigm of Smith\u2013Waterman provided the dynamic-programming template for robustly aligning segments under edits, a principle DJ SEARCH adapts to align and sequence multiple web snippets into an optimal reconstruction of a target document.\nOn the LLM side, Carlini et al. demonstrated that models can memorize and regurgitate training data, exposing a need to systematically attribute generated content to sources rather than rely on ad hoc prompts or rare strings. Ippolito et al. highlighted attempts to reduce verbatim copying but also underscored the absence of a general metric for reuse versus novelty. Complementing this, Lee et al. connected near-duplicate training data to memorization, motivating an output-level measure that reflects how much a text depends on prior web material. CREATIVITY INDEX directly operationalizes these insights: it generalizes plagiarism-style mosaic attribution to the open web and uses a DP-based search to capture verbatim and near-verbatim reuse, thereby quantifying linguistic creativity in both human and machine text.",
  "analysis_timestamp": "2026-01-06T23:09:26.618292"
}
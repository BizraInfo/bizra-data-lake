{
  "prior_works": [
    {
      "title": "Denoising Diffusion Probabilistic Models",
      "authors": "Ho et al.",
      "year": 2020,
      "role": "Foundational diffusion training objective and stochastic denoising loss",
      "relationship_sentence": "Establishes the stochastic training paradigm for diffusion models that this paper contrasts with, probing whether analogous stochasticity in flow-matching losses is necessary for generalization."
    },
    {
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "authors": "Song et al.",
      "year": 2021,
      "role": "Theoretical underpinning via probability flow ODEs (deterministic counterpart to SDEs)",
      "relationship_sentence": "Introduces the probability flow ODE showing that deterministic flows can share marginals with stochastic processes, supporting the paper\u2019s thesis that stochasticity is not essential for generalization."
    },
    {
      "title": "Flow Matching for Generative Modeling",
      "authors": "Lipman et al.",
      "year": 2023,
      "role": "Introduces the flow-matching objective and closed-form optimal vector fields for specific interpolants",
      "relationship_sentence": "Provides the core FM framework and the analytic (closed-form) targets that the present work leverages to compare against stochastic/Monte Carlo conditional targets."
    },
    {
      "title": "Conditional Flow Matching: Simulation-Free Training of Continuous-Time Diffusion Models",
      "authors": "Liu et al.",
      "year": 2023,
      "role": "Defines the stochastic conditional flow-matching (CFM) loss used in practice",
      "relationship_sentence": "Supplies the widely used stochastic CFM objective that the paper empirically and theoretically benchmarks against its closed-form counterpart."
    },
    {
      "title": "Stochastic Interpolants: A Framework for Deep Generative Modeling",
      "authors": "Albergo and Vanden-Eijnden",
      "year": 2023,
      "role": "Unifying theory linking diffusion, flow matching, and conditional expectations",
      "relationship_sentence": "Derives conditional expectation formulas and variance properties for interpolant-driven training, directly motivating the equivalence between stochastic and closed-form FM targets examined here."
    },
    {
      "title": "Flow Straight and Fast: Learning to Generate and Edit with Rectified Flow",
      "authors": "Liu et al.",
      "year": 2023,
      "role": "Practical closed-form flow training with linear paths and constant-velocity targets",
      "relationship_sentence": "Demonstrates strong empirical performance of a closed-form FM-style objective, providing a key baseline and evidence that deterministic targets can be competitive or superior."
    },
    {
      "title": "Neural Ordinary Differential Equations",
      "authors": "Chen et al.",
      "year": 2018,
      "role": "Foundational ODE framework for continuous-time generative models",
      "relationship_sentence": "Supplies the continuous-time ODE modeling and training machinery that FM/CFM instantiate, enabling the analysis of deterministic vector-field learning versus stochastic objectives."
    }
  ],
  "synthesis_narrative": "The paper interrogates whether the stochasticity of the conditional flow matching (CFM) loss is a primary driver of generalization in modern flow-based generative models. This inquiry stands on three pillars of prior work. First, diffusion and score-based models established the stochastic denoising paradigm (Ho et al., 2020) and, crucially, the probability flow ODE (Song et al., 2021), which showed that deterministic flows can reproduce the marginals of stochastic processes\u2014suggesting that noise in training is not intrinsically necessary. Second, Flow Matching (Lipman et al., 2023) formalized learning vector fields via regression and derived closed-form optimal targets under common interpolants, while Conditional Flow Matching (Liu et al., 2023) popularized a practical, simulation-free but stochastic training objective based on sampling conditional bridges. Third, Stochastic Interpolants (Albergo & Vanden-Eijnden, 2023) unified these views, providing conditional expectation identities and variance analyses that connect Monte Carlo targets to their analytic counterparts. Empirically, Rectified Flow (Liu et al., 2023) demonstrated that training with simple, closed-form velocities along linear paths can yield strong performance, reinforcing the plausibility that deterministic targets suffice. Grounded in the Neural ODE framework (Chen et al., 2018), the present work leverages these theoretical links and practical baselines to show that in high dimensions the stochastic and closed-form FM losses are nearly equivalent and that closed-form training can match or outperform CFM. Consequently, it rules out target stochasticity as the key source of generalization in flow matching.",
  "analysis_timestamp": "2026-01-06T23:42:48.129831"
}
{
  "prior_works": [
    {
      "title": "Associative Memory (Linear Associative Memory/Correlation Matrix Memory)",
      "authors": "Teuvo Kohonen",
      "year": 1972,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "The paper\u2019s memory matrix is the linear associative memory W = \u03a3 yx^T introduced by Kohonen, and the work derives precise scaling laws and estimator efficiency for this exact outer-product formulation."
    },
    {
      "title": "Neural networks and physical systems with emergent collective computational abilities",
      "authors": "John J. Hopfield",
      "year": 1982,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Hopfield\u2019s associative memory framework grounded capacity-vs-dimension tradeoffs via Hebbian outer products, which this work refines to continuous, high-dimensional embeddings with exact sample/parameter scaling predictions."
    },
    {
      "title": "Sparse Distributed Memory",
      "authors": "Pentti Kanerva",
      "year": 1988,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Kanerva\u2019s high-dimensional analysis of addressable memories and retrieval error informs the probabilistic framework this paper adapts to derive analytic scaling laws for key\u2013value outer-product memories."
    },
    {
      "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
      "authors": "Jacob Geva et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By showing transformer feed-forward layers implement key\u2013value memories via inner products, this work motivates modeling transformer internals as outer-product associative memories whose scaling this paper characterizes."
    },
    {
      "title": "Hopfield Networks is All You Need",
      "authors": "Johannes Ramsauer et al.",
      "year": 2021,
      "arxiv_id": "2008.02217",
      "role": "Related Problem",
      "relationship_sentence": "The equivalence between attention and modern Hopfield networks established here underpins the connection between associative memory dynamics and transformer mechanisms that this paper leverages to interpret its scaling results."
    },
    {
      "title": "Scaling Laws for Neural Language Models",
      "authors": "Jared Kaplan et al.",
      "year": 2020,
      "arxiv_id": "2001.08361",
      "role": "Gap Identification",
      "relationship_sentence": "This empirical scaling-law study motivates the need for principled, mechanistic scaling analyses, which the present work provides in a tractable associative-memory setting."
    },
    {
      "title": "Toy Models of Superposition in Neural Networks",
      "authors": "Nelson Elhage et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "Their analysis of feature superposition and interference as width-limited phenomena directly informs this paper\u2019s formalization of interference in associative memories and its closed-form error scaling with dimension and number of stored pairs."
    }
  ],
  "synthesis_narrative": "Kohonen\u2019s linear associative memory precisely formalized key\u2013value storage as a correlation matrix W formed by outer products \u03a3 yx^T, with retrieval by linear projection and improvements via pseudoinverse rules; this directly specifies the estimator family at the heart of associative memory. Hopfield\u2019s model established that Hebbian outer-product storage induces interference that limits capacity, highlighting how retrieval error scales with representational dimension. Kanerva extended this line of thought to high-dimensional addressable memories, providing probabilistic analyses of retrieval and noise that scale with dimension and sparsity. Geva and colleagues showed that transformer feed-forward layers function as key\u2013value memories queried by inner products, concretely linking outer-product memory mechanisms to inner transformer computations. Ramsauer and coauthors connected attention updates to modern Hopfield dynamics, reinforcing the equivalence between associative memory retrieval and transformer operations. Kaplan and collaborators documented robust empirical power-law scaling in language models, underscoring the need for principled theories of scaling. Elhage and collaborators revealed how superposition creates interference when features outnumber dimensions, clarifying the linear-algebraic mechanics governing capacity.\nTogether, these works exposed a tractable, mechanistic memory (outer-product key\u2013value storage) that closely mirrors transformer internals, identified interference as the core limitation, and called for analytic scaling characterizations akin to empirical LLM scaling laws. Building on Kohonen/Hopfield formulations with Kanerva\u2019s high-dimensional perspective and the transformer\u2013memory equivalence from Geva and Ramsauer, the paper derives precise sample- and parameter-size scaling laws and compares estimator efficiencies (Hebbian, pseudoinverse, and optimization-based), providing a principled explanation of how associative memory capacity and error scale in transformer-like settings.",
  "target_paper": {
    "title": "Scaling Laws for Associative Memories",
    "authors": "Vivien Cabannes, Elvis Dohmatob, Alberto Bietti",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "scaling law, associative memory, mechanistic interpretability, Hopfield network",
    "abstract": "Learning arguably involves the discovery and memorization of abstract rules. The aim of this paper is to study associative memory mechanisms. Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models. We derive precise scaling laws with respect to sample size and parameter size, and discuss the statistical efficiency of different estimators, including optimization-based algorithms. We provide extensive numerical experiments to validate and interpret theoretical results, including fine-grained visualizations of the stored memory associations.",
    "openreview_id": "Tzh6xAJSll",
    "forum_id": "Tzh6xAJSll"
  },
  "analysis_timestamp": "2026-01-06T12:25:04.874659"
}
{
  "prior_works": [
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Introduced the modern paradigm of AI oversight via AI-generated feedback at scale; this paper directly interrogates that paradigm by showing how growing model similarity can systematically bias AI-based supervision and evaluation."
    },
    {
      "title": "Chatbot Arena: Benchmarking LLMs with Crowdsourced Elo Ratings",
      "authors": "Lianmin Zheng et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "Established widely used LLM-as-a-judge and pairwise-comparison evaluation pipelines that the present work analyzes, demonstrating with CAPA that judges favor models similar to themselves."
    },
    {
      "title": "MT-Bench: Multi-Turn Benchmark for Evaluating Large Language Models",
      "authors": "Lianmin Zheng et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "Provides a canonical LLM-as-a-judge setting for multi-turn tasks; the paper shows MT-Bench-style judge scores are confounded by judge\u2013candidate similarity, quantified by the proposed CAPA metric."
    },
    {
      "title": "Weak-to-Strong Generalization",
      "authors": "Collin Burns et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "Formulated training stronger models from weaker model annotations; the present work directly extends this line by showing that weak-to-strong gains depend on complementary (non-overlapping) knowledge measured via mistake-overlap similarity."
    },
    {
      "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
      "authors": "Yizhong Wang et al.",
      "year": 2023,
      "role": "Related Problem",
      "relationship_sentence": "Demonstrated large-scale supervision from model-generated data; this paper scrutinizes that AI-supervision regime, showing how increasing model similarity can compound shared mistakes rather than correct them."
    },
    {
      "title": "Similarity of Neural Network Representations Revisited",
      "authors": "Simon Kornblith et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "CKA popularized representation-level similarity, but does not capture behavioral mistake overlap; CAPA is proposed to fill this gap by chance-adjusted, probabilistic agreement focused on shared errors."
    },
    {
      "title": "A Coefficient of Agreement for Nominal Scales",
      "authors": "Jacob Cohen",
      "year": 1960,
      "role": "Foundation",
      "relationship_sentence": "Introduced chance-adjusted agreement (Cohen\u2019s kappa); CAPA directly builds on this principle, extending chance-adjusted agreement to probabilistic LM predictions and focusing specifically on overlap in mistakes."
    }
  ],
  "synthesis_narrative": "This paper\u2019s core innovation is CAPA, a chance-adjusted, probabilistic mistake-overlap metric used to reveal how model similarity undermines AI oversight in both evaluation (LLM-as-a-judge) and training (weak-to-strong generalization). The intellectual roots trace to the AI-feedback paradigm of Constitutional AI, which normalized using models to supervise and evaluate other models at scale. That paradigm\u2019s practical instantiations\u2014MT-Bench and Chatbot Arena\u2014established LLM-as-a-judge pipelines that became de facto baselines; the present work shows these pipelines inherently reward systems similar to the judge, a bias made explicit and measurable via CAPA. On the training side, Weak-to-Strong Generalization formulated learning from weaker model annotations; this paper extends that framework by demonstrating that the benefit hinges on complementary knowledge between teacher and student, which CAPA quantifies through error overlap. The methodology also draws on classic chance-adjusted agreement from Cohen\u2019s kappa, generalizing it to probabilistic LM outputs and centering the analysis on shared mistakes rather than raw output agreement. Finally, representation-similarity tools like CKA motivated a gap: representation-level alignment fails to capture behaviorally relevant similarity. CAPA addresses this by focusing on mistakes\u2014the quantity that most directly matters for oversight\u2014yielding the paper\u2019s central finding that as models become more capable, their errors become more similar, thereby eroding the promise of AI-led oversight.",
  "analysis_timestamp": "2026-01-06T23:07:19.575971"
}
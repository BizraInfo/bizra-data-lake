{
  "prior_works": [
    {
      "title": "Cubic regularization of Newton method and its global performance",
      "authors": "Yurii Nesterov and Boris T. Polyak",
      "year": 2006,
      "role": "Foundation",
      "relationship_sentence": "The paper\u2019s global convergence to a second-order stationary point with cubic regularization is a direct extension of Nesterov\u2013Polyak\u2019s cubic-regularized Newton framework to the setting where the Hessian is deliberately not refreshed every iteration."
    },
    {
      "title": "Adaptive cubic regularisation methods for unconstrained optimization. Part II: worst-case global evaluation complexity",
      "authors": "Coralia Cartis et al.",
      "year": 2011,
      "role": "Baseline",
      "relationship_sentence": "ARC provides the baseline second-order method whose per-iteration Hessian updates this work makes lazy; the new analysis preserves ARC\u2019s SOSP guarantees while provably reducing total arithmetic complexity by updating curvature only once every d iterations."
    },
    {
      "title": "A modification of Newton\u2019s method",
      "authors": "N. V. Shamanskii",
      "year": 1967,
      "role": "Inspiration",
      "relationship_sentence": "Shamanskii introduced periodic Jacobian/Hessian updates\u2014reusing the same curvature across several steps\u2014which is the precise \u2018lazy Hessian\u2019 idea this paper formalizes and endows with modern nonconvex complexity guarantees via (cubic/quadratic) regularization."
    },
    {
      "title": "Inexact Newton methods",
      "authors": "R. S. Dembo et al.",
      "year": 1982,
      "role": "Foundation",
      "relationship_sentence": "The paper\u2019s local superlinear-rate justification for convex problems with quadratic regularization leverages the inexact-Newton viewpoint, interpreting stale Hessians as structured inexactness and verifying conditions under which superlinear convergence still holds."
    },
    {
      "title": "Newton Sketch: A near-linear-time optimization method",
      "authors": "Mert Pilanci and Martin J. Wainwright",
      "year": 2015,
      "role": "Related Problem",
      "relationship_sentence": "Newton Sketch reduces Hessian costs by sketching curvature each iteration; the present work tackles the same bottleneck by showing one can avoid any per-iteration recomputation, reusing an exact Hessian for multiple steps while retaining strong guarantees."
    },
    {
      "title": "Sub-sampled Newton methods I: Global convergence",
      "authors": "Farbod Roosta-Khorasani and Michael W. Mahoney",
      "year": 2016,
      "role": "Gap Identification",
      "relationship_sentence": "Subsampled Newton methods highlight the iteration-by-iteration curvature cost and propose stochastic approximations; this paper addresses that gap by proving that exact Hessians need not be recomputed each iteration\u2014optimally only every d steps\u2014yielding provable arithmetic savings."
    }
  ],
  "synthesis_narrative": "The core of \u201cSecond-Order Optimization with Lazy Hessians\u201d marries a classical but under-theorized idea\u2014periodically reusing curvature\u2014with the modern theory of cubic-regularized Newton methods. Nesterov and Polyak established cubic regularization as a robust globalization device ensuring convergence to second-order stationary points for nonconvex problems. Building on this foundation, Cartis, Gould, and Toint\u2019s ARC framework set the prevailing baseline and worst-case evaluation complexity for cubic-regularized second-order methods, but implicitly assumes new curvature each iteration. Shamanskii\u2019s modified Newton method supplied the key operational idea: compute a Jacobian/Hessian intermittently and reuse it across several steps. The present paper lifts Shamanskii\u2019s intuition into the nonconvex optimization era, showing that with cubic regularization one can rigorously guarantee SOSP convergence even when the Hessian is kept \u2018lazy\u2019 for many iterations, and it quantifies an optimal refresh rate of once every d steps. For convex problems, the local superlinear convergence of the proposed quadratic-regularized variant can be understood through the inexact Newton lens of Dembo, Eisenstat, and Steihaug by treating the stale Hessian as structured inexactness. Finally, the work positions itself against contemporary attempts to lower curvature cost\u2014such as Newton Sketch and subsampled Newton\u2014that approximate Hessians every iteration; instead, it demonstrates that exact curvature can be amortized across iterations with provable complexity gains, improving the total arithmetic cost by a factor \u221ad while preserving strong convergence guarantees.",
  "analysis_timestamp": "2026-01-06T23:09:26.533687"
}
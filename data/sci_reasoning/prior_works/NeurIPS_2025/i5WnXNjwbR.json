{
  "prior_works": [
    {
      "title": "ImageNet-trained CNNs are biased towards texture; increasing shape bias in deep networks improves accuracy and robustness",
      "authors": "Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, Wieland Brendel",
      "year": 2019,
      "role": "Empirical finding and evaluation protocol (cue-conflict, Stylized-ImageNet)",
      "relationship_sentence": "This paper\u2019s cue-conflict methodology and texture-bias claim are the central baseline that the NeurIPS 2025 work critiques and replaces with a controlled suppression framework to measure feature reliance without forced-choice confounds."
    },
    {
      "title": "Image Style Transfer Using Convolutional Neural Networks",
      "authors": "Leon A. Gatys, Alexander S. Ecker, Matthias Bethge",
      "year": 2016,
      "role": "Methodological tool enabling texture-shape decoupling",
      "relationship_sentence": "Neural style transfer underpins the texture-shape manipulations used by Geirhos et al., providing the technical foundation that highlighted limitations the new paper addresses via domain-agnostic cue suppression."
    },
    {
      "title": "Approximating CNNs with Bag-of-Local-Features models works surprisingly well on ImageNet",
      "authors": "Wieland Brendel, Matthias Bethge",
      "year": 2019,
      "role": "Empirical finding on local feature reliance (BagNets)",
      "relationship_sentence": "Evidence that high ImageNet accuracy is attainable from strictly local evidence directly motivates the new paper\u2019s finding that CNNs predominantly rely on local shape and its effort to disentangle local shape from texture."
    },
    {
      "title": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations (ImageNet-C)",
      "authors": "Dan Hendrycks, Thomas Dietterich",
      "year": 2019,
      "role": "Benchmark/design principle for controlled corruptions",
      "relationship_sentence": "The idea of systematically applying controlled, semantically interpretable perturbations informs the proposed domain-agnostic suppression protocol for selectively attenuating shape, texture, and color cues."
    },
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "authors": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, et al.",
      "year": 2021,
      "role": "Architecture with different inductive biases (global attention)",
      "relationship_sentence": "Vision Transformers are evaluated by the new paper as a contrasting architecture whose global receptive fields can mitigate CNNs\u2019 local-shape reliance, validating claims about architecture-dependent feature use."
    },
    {
      "title": "A ConvNet for the 2020s (ConvNeXt)",
      "authors": "Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie",
      "year": 2022,
      "role": "Modern ConvNet architecture/baseline",
      "relationship_sentence": "ConvNeXt provides a contemporary convolutional baseline the authors use to show that training/architecture choices can substantially reduce local-shape reliance identified by their suppression framework."
    },
    {
      "title": "Unmasking Clever Hans predictors and assessing what machines really learn",
      "authors": "Sebastian Lapuschkin, Stephan W\u00e4ldchen, Alexander Binder, Gr\u00e9goire Montavon, Wojciech Samek, Klaus-Robert M\u00fcller",
      "year": 2019,
      "role": "Evidence of spurious cue reliance across domains via explainability",
      "relationship_sentence": "By revealing models\u2019 dependence on dataset-specific, often local artifacts, this work motivates the paper\u2019s cross-domain analysis and its need for cue-level quantification beyond standard accuracy metrics."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014revisiting the texture-bias hypothesis by replacing forced-choice cue conflicts with a domain-agnostic, controlled suppression framework\u2014builds directly on and responds to several key threads in the literature. Geirhos et al. (2019) established the prevailing claim that ImageNet CNNs are texture-biased using cue-conflict stimuli constructed with neural style transfer (Gatys et al., 2016). While pivotal, that methodology entangles multiple cues and forces a discrete choice; the present paper addresses these limitations by isolating and systematically suppressing shape, texture, and color cues, inspired by the principled use of controlled corruptions popularized by ImageNet-C (Hendrycks & Dietterich, 2019).\n\nA second line of influence comes from evidence that CNNs can succeed using strictly local evidence, notably BagNets (Brendel & Bethge, 2019). This directly motivates the paper\u2019s key empirical finding: ImageNet CNNs are not inherently texture-biased but predominantly rely on local shape features. The authors further probe how architectural inductive biases and training strategies affect reliance, leveraging modern baselines such as Vision Transformers (Dosovitskiy et al., 2021), whose global attention can favor more holistic shape processing, and ConvNeXt (Liu et al., 2022), a strong contemporary ConvNet, to show such reliance can be mitigated.\n\nFinally, the decision to extend analyses across computer vision, medical imaging, and remote sensing reflects the broader insight that models often exploit whichever cues are most accessible, sometimes spurious\u2014highlighted by explainability-driven revelations of \"Clever Hans\" behavior (Lapuschkin et al., 2019). Together, these works shape a unified, cue-suppression methodology that reinterprets prior texture-bias claims, quantifies feature reliance rigorously, and demonstrates its dependence on architecture, training, and domain.",
  "analysis_timestamp": "2026-01-07T00:29:42.056639"
}
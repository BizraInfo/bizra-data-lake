{
  "prior_works": [
    {
      "title": "A Symbolic Approach to Explaining Bayesian Network Classifiers",
      "authors": "Andy Shih et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "This work formalized local explanations as sufficient reasons/prime implicants of a classifier\u2019s decision function, providing the logical machinery that the present paper generalizes to formalize and analyze local vs. global explanations and their uniqueness."
    },
    {
      "title": "Abduction-Based Explanations for Machine Learning Models",
      "authors": "Alexey Ignatiev et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "The abductive (hitting-set\u2013based) formulation of minimal explanations and its associated complexity/duality perspective directly underpins the paper\u2019s duality between local and global explanations and its complexity analyses across model classes."
    },
    {
      "title": "A Theory of Diagnosis from First Principles",
      "authors": "Raymond Reiter",
      "year": 1987,
      "role": "Foundation",
      "relationship_sentence": "Reiter\u2019s hitting-set duality between diagnoses and conflict sets is the classical theoretical basis that the paper adapts to establish a formal duality between local and global forms of ML explanations."
    },
    {
      "title": "Constructing optimal binary decision trees is NP-complete",
      "authors": "Laurent Hyafil et al.",
      "year": 1976,
      "role": "Foundation",
      "relationship_sentence": "The classic NP-completeness of optimizing decision trees serves as the foundational hardness used in this paper\u2019s complexity results for computing global explanations of tree-based models."
    },
    {
      "title": "Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks",
      "authors": "Guy Katz et al.",
      "year": 2017,
      "role": "Related Problem",
      "relationship_sentence": "Reluplex established formal verification hardness for ReLU networks; the present paper leverages these verification-complexity insights to derive hardness results for computing explanations in neural networks."
    },
    {
      "title": "Why Should I Trust You? Explaining Any Classifier",
      "authors": "Marco Tulio Ribeiro et al.",
      "year": 2016,
      "role": "Gap Identification",
      "relationship_sentence": "LIME popularized local, model-agnostic explanations but lacked rigorous guarantees, a shortcoming explicitly addressed here by providing a formal complexity-theoretic framework for local explanations."
    },
    {
      "title": "Anchors: High-Precision Model-Agnostic Explanations",
      "authors": "Marco Tulio Ribeiro et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Anchors framed local rule-based explanations empirically; the present work responds by formalizing such local/global rule explanations and proving complexity and uniqueness properties they lacked."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014casting local and global interpretability within a unified, complexity-theoretic framework built on a provable duality and uniqueness\u2014rests on a direct lineage from logical and abductive explanation formalisms. Shih, Choi, and Darwiche introduced a symbolic view of local explanations as sufficient reasons/prime implicants, giving a precise language for local interpretability that this paper generalizes and connects to global forms. Ignatiev, Narodytska, and Marques-Silva\u2019s abduction-based explanations supplied the crucial hitting-set machinery and complexity perspective for minimal explanations, which the authors extend to formalize a local\u2013global duality and to reason about inherent uniqueness of certain global forms. This duality itself is anchored in Reiter\u2019s classic diagnosis theory, whose hitting-set duality between diagnoses and conflicts provides the theoretical backbone adapted here to ML explanations. To characterize model-specific computational barriers, the paper leverages foundational hardness results: Hyafil and Rivest\u2019s NP-completeness for optimizing decision trees informs the complexity of computing global explanations for tree models, while Reluplex\u2019s verification results guide reductions establishing hardness for neural networks. Finally, widely used local explanation methods such as LIME and Anchors delineated the practical problem space but lacked formal guarantees; their limitations directly motivate the paper\u2019s rigorous complexity framework and the proofs of duality and uniqueness that clarify when and how explanations can be computed across linear models, decision trees, and neural networks.",
  "analysis_timestamp": "2026-01-06T23:09:26.399248"
}
{
  "prior_works": [
    {
      "title": "Mutual Information for the Multi-Layer Generalized Linear Model",
      "authors": "Andre Manoel et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Introduced the multi-layer generalized linear model (ML-GLM) teacher\u2013student framework and AMP/replica characterizations for deep compositions, which this paper adopts to model deep random Gaussian networks and to derive Bayes-optimal predictions."
    },
    {
      "title": "Optimal Errors and Phase Transitions in High-Dimensional Generalized Linear Models",
      "authors": "Jean Barbier et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Provided rigorous Bayes-optimal error formulas and phase-transition characterizations for GLMs that the present work specializes and extends to multi-layer random networks with extensive width to obtain closed-form test errors."
    },
    {
      "title": "Generalisation error in learning with random features and neural networks",
      "authors": "Federico Gerace et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "Derived learning-curve predictions for random features and two-layer neural networks in the high-dimensional teacher\u2013student setting; the current paper generalizes this line to deep random networks and compares Bayes-optimal, kernel, RF, and ridge errors in closed form."
    },
    {
      "title": "High-dimensional asymptotics of prediction: Ridge regression and classification",
      "authors": "Edgar Dobriban et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "Established closed-form risk asymptotics for ridge regression and classification with Gaussian design, which the present work leverages to show that optimally regularized ridge attains the Bayes-optimal error for deep random-network teachers in the proportional regime."
    },
    {
      "title": "A Random Matrix Perspective on Random Features: Beyond the Kernel Approximation",
      "authors": "Olivier Louart et al.",
      "year": 2017,
      "role": "Related Problem",
      "relationship_sentence": "Developed RMT tools for analyzing random feature Gram matrices and their generalization behavior, informing this paper\u2019s explicit test-error calculations for random features and kernels induced by deep random networks."
    },
    {
      "title": "A Modern Maximum-Likelihood Theory for High-Dimensional Logistic Regression",
      "authors": "Pragya Sur et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "Characterized high-dimensional test error of logistic regression with Gaussian covariates; the current work benchmarks against these asymptotics to show logistic loss achieves near-Bayes-optimal classification in the deep random-teacher setting."
    },
    {
      "title": "On the Power of Over-parameterization in Neural Networks Beyond Lazy Training",
      "authors": "Zeyuan Allen-Zhu et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Identified regimes and tasks where kernel/NTK methods are provably suboptimal while feature-learning neural networks succeed, motivating the paper\u2019s investigation of sample-rich regimes where ridge/kernel become suboptimal but neural networks achieve near-zero error."
    }
  ],
  "synthesis_narrative": "The core advance of this paper\u2014closed-form Bayes-optimal test errors for learning deep, extensive-width random Gaussian networks, together with exact learning curves for ridge, kernel, and random-features methods\u2014rests on the teacher\u2013student ML-GLM framework and its Bayes-optimal characterization. Manoel et al. introduced the multi-layer GLM model and AMP/replica methodology that precisely match the present paper\u2019s deep compositional random network setting, while Barbier et al. provided rigorous Bayes-optimal error formulas and phase-transition results for GLMs that are here specialized to the deep, extensive-width regime and distilled into closed-form expressions. Building on the two-layer random-features and neural-network analyses of Gerace et al., this work extends the scope to arbitrarily deep random networks and compares multiple learners within a unified asymptotic framework. The ridge and kernel baselines are anchored by Dobriban and Wager\u2019s high-dimensional risk formulas and Louart et al.\u2019s random-matrix perspective on random features, which together enable explicit error characterizations for ridge, kernel, and random-features regression and reveal when optimally regularized ridge/kernel achieve Bayes-optimality. For classification, Sur and Cand\u00e8s\u2019 sharp asymptotics for logistic regression inform the result that logistic loss is near-optimal under the deep random teacher. Finally, theoretical separations between kernel/lazy training and feature-learning networks, as highlighted by Allen-Zhu and Li, motivate and contextualize the paper\u2019s finding that in sample-rich regimes ridge/kernel become suboptimal while neural networks attain vanishing error.",
  "analysis_timestamp": "2026-01-06T23:09:26.541769"
}
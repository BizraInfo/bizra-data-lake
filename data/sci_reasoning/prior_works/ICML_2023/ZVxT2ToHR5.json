{
  "prior_works": [
    {
      "title": "Deep Learning with Differential Privacy",
      "authors": "Abadi et al.",
      "year": 2016,
      "role": "Baseline",
      "relationship_sentence": "The paper directly improves upon DP-SGD by replacing per-step i.i.d. Gaussian noise with a carefully correlated, matrix-factorization-based noise design that explicitly accounts for multi-epoch (multiple participation) training."
    },
    {
      "title": "Private and Continual Release of Statistics",
      "authors": "Chan et al.",
      "year": 2010,
      "role": "Foundation",
      "relationship_sentence": "The tree-aggregation mechanism for continual observation is a canonical matrix-factorization view of releasing running sums; the present work generalizes this continual-release paradigm to adaptive streams with multiple participations, subsuming tree aggregation as a special case."
    },
    {
      "title": "Pan-Private Streaming Algorithms",
      "authors": "Dwork et al.",
      "year": 2010,
      "role": "Foundation",
      "relationship_sentence": "This paper formalized privacy under continual observation, providing the foundational problem setting that the current work extends to adaptive streams where each record may participate many times across epochs."
    },
    {
      "title": "The Matrix Mechanism: Optimizing Linear Query Workloads under Differential Privacy",
      "authors": "Li et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "The core idea of selecting an analysis (strategy) matrix and adding correlated noise to minimize error over a linear workload is adopted and extended here to online, adaptive gradient queries with multi-participation, including new sensitivity analyses and optimal matrix computation."
    },
    {
      "title": "Privacy Amplification by Iteration",
      "authors": "Feldman et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "By showing how iterative algorithms can yield improved privacy, this work highlighted limits of naively composing per-step noise over many epochs; the present paper addresses that gap by designing mechanisms whose temporal noise correlation exploits multi-epoch structure."
    },
    {
      "title": "Differential Privacy via Wavelet Transforms",
      "authors": "Xiao et al.",
      "year": 2010,
      "role": "Inspiration",
      "relationship_sentence": "This transform-based approach showed that working in a spectral domain can yield efficient DP mechanisms for structured workloads; the current paper\u2019s Fourier-transform-based mechanism follows this insight to implement a fast, near-optimal mechanism for very long training horizons."
    }
  ],
  "synthesis_narrative": "The core innovation of Multi-Epoch Matrix Factorization Mechanisms for Private Machine Learning is to design DP mechanisms that explicitly exploit multi-epoch participation in adaptive, gradient-based training by shaping temporally correlated noise via an optimal matrix-factorization viewpoint. This lineage begins with continual observation: Dwork et al. (2010) and Chan et al. (2010) established how to privately release long sequences (e.g., prefix sums) using structured mechanisms such as tree aggregation\u2014an implicit factorization of a workload matrix. Li et al. (2016) then formalized the matrix mechanism, showing how to choose an analysis matrix and correlated noise to minimize error for linear workloads. The present work extends these ideas non-trivially to adaptive streams where each record may participate in many steps, deriving new sensitivity bounds and optimal matrices tailored to multi-epoch training. On the application side, Abadi et al. (2016) DP-SGD is the dominant baseline; its per-step i.i.d. noise and standard composition are suboptimal for long horizons with repeated participation. Feldman et al. (2018) identified that iteration itself can amplify privacy, underscoring the opportunity to do better than naive composition and motivating mechanisms that exploit iterative structure. Finally, inspired by transform-based DP methods such as Xiao et al. (2010), the paper introduces a Fourier-transform-based implementation that approximates the optimal matrix mechanism with dramatically improved computational efficiency for tens of thousands of steps, achieving superior privacy-utility-computation tradeoffs in both example-level and user-level DP training.",
  "analysis_timestamp": "2026-01-06T23:09:26.576531"
}
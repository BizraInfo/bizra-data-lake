{
  "prior_works": [
    {
      "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Brian Ichter",
        "Quoc V. Le",
        "Ed H. Chi",
        "Denny Zhou"
      ],
      "year": 2022,
      "role": "Empirical foundation that explicit language reasoning improves problem solving",
      "relationship_sentence": "Thought Cloning builds on the core insight of Chain-of-Thought that verbalized intermediate steps enhance performance, turning this from an inference-time prompt into a learning signal by training agents to produce and use such thoughts during action."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": [
        "Shunyu Yao",
        "Jeffrey Zhao",
        "Dian Yu",
        "Nan Du",
        "Izhak Shafran",
        "Karthik Narasimhan",
        "Yiming Yang"
      ],
      "year": 2022,
      "role": "Framework combining natural language reasoning traces with actions in interactive environments",
      "relationship_sentence": "ReAct showed that interleaving thoughts and actions improves decision-making; Thought Cloning advances this by imitating human-provided thought-action trajectories so the agent learns the reasoning policy itself, not just generates it on the fly."
    },
    {
      "title": "e-SNLI: Natural Language Inference with Natural Language Explanations",
      "authors": [
        "Oana-Maria Camburu",
        "Tim Rockt\u00e4schel",
        "Thomas Lukasiewicz",
        "Phil Blunsom"
      ],
      "year": 2018,
      "role": "Demonstrated supervision with human explanations alongside labels",
      "relationship_sentence": "Thought Cloning generalizes the e-SNLI idea of training on human explanations by using human (or synthetic) thinking tokens as auxiliary supervision tied to actions in sequential decision-making."
    },
    {
      "title": "Explain Yourself! Leveraging Language to Teach Commonsense Reasoning",
      "authors": [
        "Nazneen Fatema Rajani",
        "Bryan McCann",
        "Caiming Xiong",
        "Richard Socher"
      ],
      "year": 2019,
      "role": "Showed that explanation-augmented training improves reasoning and generalization",
      "relationship_sentence": "The paper\u2019s finding that models learn better when trained to generate explanations directly motivates Thought Cloning\u2019s choice to clone human thoughts in addition to behaviors."
    },
    {
      "title": "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances (SayCan)",
      "authors": [
        "Michael Ahn",
        "Anthony Brohan",
        "Noah Brown",
        "Yevgen Chebotar",
        "Omar Cortes",
        "Ben Dias",
        "Chelsea Finn",
        "Keerthana Gopalakrishnan",
        "Karol Hausman",
        "Alex Herzog",
        "Jasmine Hsu",
        "Julian Ibarz",
        "Brian Ichter",
        "Alex Irpan",
        "Eric Jang",
        "Rosalia Messina",
        "Ofir Nachum",
        "Chris Paxton",
        "Avi Singh",
        "Michal Tjomek",
        "Vincent Vanhoucke",
        "Fei Xia",
        "Peng Xu",
        "Sergey Levine"
      ],
      "year": 2022,
      "role": "Used language models to plan and select actions via grounded affordances",
      "relationship_sentence": "SayCan\u2019s success in leveraging language as a planner for embodied control supports Thought Cloning\u2019s premise that linguistic \u201cthoughts\u201d can beneficially structure action policies."
    },
    {
      "title": "Modular Multitask Reinforcement Learning with Policy Sketches",
      "authors": [
        "Jacob Andreas",
        "Dan Klein",
        "Sergey Levine"
      ],
      "year": 2017,
      "role": "Textual high-level sketches as intermediate supervision for policy learning",
      "relationship_sentence": "Thought Cloning extends the idea of supervising policies with textual intermediate structures by cloning fine-grained human thinking steps aligned with actions rather than only high-level sketches."
    },
    {
      "title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos",
      "authors": [
        "Bowen Baker",
        "Ingmar Kanitscheider",
        "Jan Materzy\u0144ska",
        "Jeffrey Wu",
        "Michael Dennis",
        "Rapha\u00ebl Gontijo Lopes",
        "R\u00f3bert Csord\u00e1s",
        "Pavel Sountsov",
        "Sergio G\u00f3mez Colmenarejo"
      ],
      "year": 2022,
      "role": "Internet-scale imitation learning from videos",
      "relationship_sentence": "VPT demonstrated learning policies from large-scale internet data, directly informing Thought Cloning\u2019s proposed scaling path of pairing online videos with transcripts to learn both actions and thoughts."
    }
  ],
  "synthesis_narrative": "Thought Cloning\u2019s core innovation is to treat human thinking as a first-class training signal: instead of imitating only actions, the agent also learns to produce and use the natural-language thoughts that precede those actions. This idea rests on two converging lines of prior work. First, research in language modeling showed that explicit verbal reasoning improves performance: Chain-of-Thought prompting revealed that articulating intermediate steps boosts problem solving, while ReAct demonstrated that interleaving such reasoning with actions markedly improves interactive decision-making. Thought Cloning converts these inference-time practices into a training-time objective by imitating thought-action trajectories.\nSecond, work on learning from explanations established that human-provided rationales can supervise and improve models. e-SNLI and Rajani et al. collected and used human explanations to train models to explain-then-predict, yielding better generalization. Thought Cloning transfers this principle from static prediction to sequential control, aligning explanations temporally with actions so the agent internalizes how to think while acting.\nFinally, prior efforts that harness language to guide control and accelerate RL/IL demonstrate feasibility and scaling. SayCan showed language can structure planning for embodied agents, and policy sketches provided textual intermediate supervision to speed policy learning. VPT proved that imitation learning can scale to internet videos, motivating Thought Cloning\u2019s proposed path of coupling large video corpora with transcripts to supervise both thoughts and actions. Together, these works directly underpin the conceptual and practical design of Thought Cloning.",
  "analysis_timestamp": "2026-01-06T23:42:49.079891"
}
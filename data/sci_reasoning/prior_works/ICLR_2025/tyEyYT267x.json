{
  "prior_works": [
    {
      "title": "Denoising Diffusion Probabilistic Models for Discrete Data (D3PM)",
      "authors": "Jacob Austin et al.",
      "year": 2021,
      "arxiv_id": "2107.03006",
      "role": "Foundation",
      "relationship_sentence": "Provides the core discrete diffusion formulation and ELBO/training objective for categorical sequences that Block Diffusion generalizes to block-level transitions while retaining the discrete denoising likelihood machinery."
    },
    {
      "title": "Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions with Normalizing Flows",
      "authors": "Emiel Hoogeboom et al.",
      "year": 2021,
      "arxiv_id": "2102.05379",
      "role": "Extension",
      "relationship_sentence": "Introduces multinomial/categorical forward processes and loss weightings for discrete diffusion that Block Diffusion adapts to design blockwise transition kernels and informs its data-driven noise-schedule/variance-reduction recipe."
    },
    {
      "title": "Diffusion-LM Improves Controllable Text Generation",
      "authors": "Xiang Lisa Li et al.",
      "year": 2022,
      "arxiv_id": "2205.14217",
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrates the controllability and parallelism of diffusion language models but highlights fixed-length generation and weaker likelihood modeling, limitations that Block Diffusion explicitly overcomes with flexible-length generative blocks and improved likelihood."
    },
    {
      "title": "Mask-Predict: Parallel Decoding for Non-Autoregressive Neural Machine Translation",
      "authors": "Marjan Ghazvininejad et al.",
      "year": 2019,
      "arxiv_id": "1904.09324",
      "role": "Inspiration",
      "relationship_sentence": "Presents iterative mask-and-refine decoding with explicit length modeling, directly inspiring Block Diffusion\u2019s parallel token sampling and its strategy for handling variable-length sequences within a diffusion-style objective."
    },
    {
      "title": "Blockwise Parallel Decoding for Deep Autoregressive Models",
      "authors": "Mitchell Stern et al.",
      "year": 2018,
      "arxiv_id": "1811.03107",
      "role": "Inspiration",
      "relationship_sentence": "Introduces predicting multiple tokens per step with verification and amortized computation, a blockwise idea that Block Diffusion repurposes to bridge diffusion and autoregression while leveraging KV caching."
    },
    {
      "title": "BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model",
      "authors": "Alex Wang et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "Shows that masked LMs can be used for generation via iterative Gibbs-style token resampling, informing Block Diffusion\u2019s blockwise denoising updates that resample subsets of positions in parallel."
    },
    {
      "title": "Improved Denoising Diffusion Probabilistic Models",
      "authors": "Ben Poole Nichol et al.",
      "year": 2021,
      "arxiv_id": "2102.09672",
      "role": "Extension",
      "relationship_sentence": "Proposes schedule design and loss reweighting to reduce diffusion training variance, which Block Diffusion extends by estimating gradient variance in its block setting and learning data-driven noise schedules."
    }
  ],
  "synthesis_narrative": "Discrete diffusion for categorical sequences was formalized by Austin et al. (D3PM), who defined forward transition kernels and an ELBO for training denoisers over token spaces. Hoogeboom et al. introduced multinomial diffusion, detailing categorical noising processes and practical loss weightings that affect gradient variance and sampling behavior. Diffusion-LM showed that diffusion-based language models can yield strong controllability and parallel token updates, but also surfaced two key shortcomings in practice: fixed-length generation and weaker likelihood/perplexity compared to autoregressive models. In parallel, non-autoregressive work such as Mask-Predict demonstrated iterative mask-and-refine decoding with explicit length prediction, establishing a practical template for parallel token sampling and length handling. Stern et al. proposed blockwise parallel decoding for autoregressive models, revealing that multi-token steps with verification can amortize computation and benefit from KV caching. Finally, Wang and Cho reframed masked LMs as generative Markov random fields, validating iterative, parallel resampling of subsets of positions as a viable decoding paradigm.\nTogether these threads implied an opportunity: marry the likelihood and caching benefits of autoregression with the parallelism and controllability of diffusion, while removing the fixed-length constraint and stabilizing training via principled schedule design. Block Diffusion synthesizes D3PM/multinomial discrete denoising with Mask-Predict-style parallel updates and Stern et al.\u2019s blockwise efficiency, yielding a block-level diffusion process that interpolates between diffusion and AR factorizations. Building on schedule and weighting insights from multinomial diffusion and improved DDPM, it introduces variance estimators and data-driven noise schedules tailored to block transitions, enabling flexible-length generation, better likelihoods, and efficient KV-cached inference.",
  "target_paper": {
    "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models",
    "authors": "Marianne Arriola, Aaron Gokaslan, Justin T Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, Volodymyr Kuleshov",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Diffusion Models, Text Diffusion, Generative Models",
    "abstract": "Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with KV caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the ",
    "openreview_id": "tyEyYT267x",
    "forum_id": "tyEyYT267x"
  },
  "analysis_timestamp": "2026-01-06T07:20:52.131875"
}
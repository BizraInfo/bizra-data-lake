{
  "prior_works": [
    {
      "title": "Constrained Policy Optimization",
      "authors": "Joshua Achiam et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "The CMDP formulation and CPO baseline provide the primary safety-RL comparator that Phy-DRL improves upon by replacing expected-cost constraints with physics-regulated invariants that yield provable safety and actor\u2013critic compliance."
    },
    {
      "title": "Lyapunov-Based Safe Policy Optimization for Continuous Control",
      "authors": "Yinlam Chow et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "Its use of Lyapunov functions to guarantee constraint satisfaction highlights the need for practical mechanisms to enforce safety structure inside actor\u2013critic networks, which Phy-DRL answers via physics-guided neural network editing."
    },
    {
      "title": "Safe Exploration in Continuous Action Spaces",
      "authors": "Gal Dalal et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "The safety layer\u2019s model-based, minimal action correction directly motivates Phy-DRL\u2019s residual action policy that composes a physics-model action with a learned residual, but extends it with provable invariance and critic alignment."
    },
    {
      "title": "Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations",
      "authors": "M. Raissi et al.",
      "year": 2019,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "PINNs\u2019 core idea of embedding physics into learning guides Phy-DRL\u2019s shift from loss-based penalties to physics-model\u2013guided link and activation editing to enforce invariants within actor and critic."
    },
    {
      "title": "Control Barrier Function Based Quadratic Programs for Safety Critical Systems",
      "authors": "Aaron D. Ames et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "CBFs provide the formal safety certificates that Phy-DRL operationalizes as invariant constraints used to regulate the residual policy and to edit network structure/activations for guaranteed safe policies."
    },
    {
      "title": "Policy Invariance under Reward Transformations: Theory and Application to Reward Shaping",
      "authors": "Andrew Y. Ng et al.",
      "year": 1999,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Potential-based reward shaping underpins Phy-DRL\u2019s automatically constructed safety-embedded reward so that safety potentials from physics can be added without altering the optimal policy."
    },
    {
      "title": "Safe Model-based Reinforcement Learning with Stability Guarantees",
      "authors": "Felix Berkenkamp et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "Its demonstration that model knowledge and Lyapunov analysis can certify safe exploration informs Phy-DRL\u2019s use of a physics model to regulate both policy residuals and critic structure for scalable, provable safety."
    }
  ],
  "synthesis_narrative": "Constrained Policy Optimization formalized safety in reinforcement learning through constrained Markov decision processes, optimizing performance while bounding expected costs. Lyapunov-based safe policy optimization sharpened this by using Lyapunov functions to certify constraint satisfaction, revealing how stability structure can underwrite safe learning. The safety layer for continuous actions showed how a differentiable model could minimally correct a learned policy\u2019s output to satisfy constraints at each step, providing a practical template for composing model-based safety with learned control. Physics-Informed Neural Networks established that physics priors can be embedded into neural training so solutions respect differential constraints, pointing beyond data-only fitting toward physics-regularized learning. Control Barrier Function QPs supplied the formal certificates and computational mechanism to transform safety sets into enforceable constraints. Policy invariance under potential-based reward shaping proved that adding carefully constructed potential terms preserves optimality, legitimizing the addition of physics-derived safety potentials to rewards. Safe model-based RL with stability guarantees demonstrated that explicit models and Lyapunov analysis can certify safe exploration, though often with scalability limitations.\nTaken together, these works illuminated a path: use physics models to both shape objectives and constrain actions, but move from external penalties or post-hoc corrections to internalizing safety structure within actor\u2013critic networks. The resulting synthesis pairs a residual policy that composes physics-model control with a learned correction, automatically builds safety-embedded rewards via potential shaping and barrier functions, and enforces invariants through targeted neural link and activation editing\u2014yielding scalable, provably safe learning with critic and actor that comply with known physics.",
  "target_paper": {
    "title": "Physics-Regulated Deep Reinforcement Learning: Invariant Embeddings",
    "authors": "Hongpeng Cao, Yanbing Mao, Lui Sha, Marco Caccamo",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Physics-informed deep reinforcement learning, Safety-critical autonomous systems",
    "abstract": "This paper proposes the Phy-DRL: a physics-regulated deep reinforcement learning (DRL) framework for safety-critical autonomous systems. The Phy-DRL has three distinguished invariant-embedding designs: i) residual action policy (i.e., integrating data-driven-DRL action policy and physics-model-based action policy), ii) automatically constructed safety-embedded reward, and iii) physics-model-guided neural network (NN) editing, including link editing and activation editing. Theoretically, the Phy-DRL exhibits 1) a mathematically provable safety guarantee and 2) strict compliance of critic and actor networks with physics knowledge about the action-value function and action policy. Finally, we evaluate the Phy-DRL on a cart-pole system and a quadruped robot. The experiments validate our theoretical results and demonstrate that Phy-DRL features guaranteed safety compared to purely data-driven DRL and solely model-based design while offering remarkably fewer learning parameters and fast trai",
    "openreview_id": "5Dwqu5urzs",
    "forum_id": "5Dwqu5urzs"
  },
  "analysis_timestamp": "2026-01-06T19:16:56.233383"
}
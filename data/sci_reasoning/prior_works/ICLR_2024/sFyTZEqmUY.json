{
  "prior_works": [
    {
      "title": "Visual Foresight: Model-Based Deep Reinforcement Learning for Vision-Based Robotic Control",
      "authors": "Frederik Ebert et al.",
      "year": 2018,
      "arxiv_id": "1812.00568",
      "role": "Foundation",
      "relationship_sentence": "UniSim adopts the predict-and-plan paradigm of action\u2011conditioned video prediction with model\u2011predictive control introduced by Visual Foresight, scaling it from single\u2011robot settings to a universal simulator trained across heterogeneous real\u2011world datasets."
    },
    {
      "title": "Learning Latent Dynamics for Planning from Pixels (PlaNet)",
      "authors": "Danijar Hafner et al.",
      "year": 2019,
      "arxiv_id": "1811.04551",
      "role": "Foundation",
      "relationship_sentence": "UniSim builds on PlaNet\u2019s core idea of using a learned world model as a simulator for planning, replacing compact latent dynamics with a high\u2011fidelity generative simulator that supports visual, language, and action conditioning across domains."
    },
    {
      "title": "A Generalist Agent",
      "authors": "Scott Reed et al.",
      "year": 2022,
      "arxiv_id": "2205.06175",
      "role": "Inspiration",
      "relationship_sentence": "UniSim draws from Gato\u2019s demonstration that a single sequence model can unify diverse observation\u2013action interfaces, using that insight to orchestrate heterogeneous datasets under a shared control/conditioning interface in one simulator."
    },
    {
      "title": "GAIA-1: A Generative World Model for Autonomous Driving",
      "authors": "DeepMind/Wayve et al.",
      "year": 2023,
      "arxiv_id": "unknown",
      "role": "Gap Identification",
      "relationship_sentence": "By showing that action\u2011conditioned video diffusion can serve as a realistic simulator for a single domain (driving), GAIA\u20111 exposes the gap that UniSim addresses: extending such simulators beyond domain\u2011specific settings to a universal, multi\u2011domain model that supports planning."
    },
    {
      "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
      "authors": "Anthony Brohan et al.",
      "year": 2023,
      "arxiv_id": "2307.15818",
      "role": "Related Problem",
      "relationship_sentence": "RT\u20112\u2019s grounding of web\u2011scale vision\u2013language pretraining in robot actions motivates UniSim\u2019s use of diverse internet and embodiment data, but UniSim departs by learning an action\u2011conditioned simulator that enables planning rather than a direct policy."
    },
    {
      "title": "Pathdreamer: A World Model for Indoor Navigation",
      "authors": "Ajay Jain et al.",
      "year": 2021,
      "arxiv_id": "unknown",
      "role": "Related Problem",
      "relationship_sentence": "Pathdreamer\u2019s egocentric view synthesis conditioned on agent motion motivates UniSim\u2019s action\u2011conditioned generative simulation, while UniSim generalizes the idea beyond indoor navigation to robotics and diverse real\u2011world interactions."
    }
  ],
  "synthesis_narrative": "Action-conditioned generative prediction has long been used for decision making: Visual Foresight established the predict\u2011and\u2011plan recipe by learning video dynamics from robot experience and planning actions with model\u2011predictive control, while PlaNet formalized planning through a learned world model from pixels via latent dynamics. Both lines showed that a learned simulator can guide control, though each was confined to narrow robotic or benchmark domains. In parallel, unification across tasks and embodiments emerged: Gato demonstrated that a single sequence model can ingest diverse observations and emit actions across many agents by normalizing interfaces into a shared token space. Domain\u2011specific world simulators advanced fidelity, too\u2014GAIA\u20111 showed that action\u2011conditioned video diffusion could simulate realistic driving rollouts for planning\u2014yet remained siloed to one domain. For navigation, Pathdreamer generated egocentric views conditioned on agent motion, hinting at interactive, action\u2011aware visual simulators in embodied environments. Finally, RT\u20112 revealed that web\u2011scale vision\u2013language pretraining can be grounded into robot actions, suggesting that diverse internet data can supply semantics for control. Taken together, these works suggest that high\u2011fidelity action\u2011conditioned simulation enables planning, that unified interfaces can span heterogeneous datasets, and that web/embodied data can be synergistic. The remaining opportunity was to orchestrate diverse real\u2011world datasets\u2014robotics, navigation, and internet media\u2014under a single action\u2011aware generative model to produce an interactive simulator usable for planning across domains. Building on predict\u2011and\u2011plan world models, adopting unified conditioning from generalist agents, and generalizing domain\u2011specific simulators, the paper synthesizes these ideas into a universal real\u2011world simulator that supports action\u2011conditioned generation and planning end\u2011to\u2011end.",
  "target_paper": {
    "title": "Learning Interactive Real-World Simulators",
    "authors": "Sherry Yang, Yilun Du, Seyed Kamyar Seyed Ghasemipour, Jonathan Tompson, Leslie Pack Kaelbling, Dale Schuurmans, Pieter Abbeel",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "Generative simulator, simulating real-world interactions, planning, reinforcement learning, vision language models, video generation",
    "abstract": "Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how",
    "openreview_id": "sFyTZEqmUY",
    "forum_id": "sFyTZEqmUY"
  },
  "analysis_timestamp": "2026-01-06T17:25:52.175084"
}
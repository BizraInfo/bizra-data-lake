{
  "prior_works": [
    {
      "title": "Bayesian Persuasion",
      "authors": "Kamenica and Gentzkow",
      "year": 2011,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "This work defines the one-shot principal\u2013agent (persuasion) model with a committing principal and a best-responding agent and the sender-optimal value U*, which the present paper targets by reducing the repeated learning setting to an approximate best-response instance."
    },
    {
      "title": "Algorithmic Bayesian Persuasion",
      "authors": "Dughmi and Xu",
      "year": 2016,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "It formalizes the computational one-shot persuasion/GPA optimization under best responses, whose structure the present reduction preserves while replacing exact with approximate best response to analyze learning agents."
    },
    {
      "title": "A Simple Adaptive Procedure Leading to Correlated Equilibrium",
      "authors": "Hart and Mas-Colell",
      "year": 2000,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper establishes that external no-regret learning guarantees approximate best responses to empirical play, directly enabling the reduction from repeated interaction with a learning agent to a one-shot problem with an approximately best-responding agent."
    },
    {
      "title": "From External to Internal Regret",
      "authors": "Blum and Mansour",
      "year": 2007,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "By introducing swap/internal regret and its stronger per-action substitution (swap) constraints, this work provides the learning-theoretic notion used to show the negative result when the agent minimizes no-swap-regret."
    },
    {
      "title": "Bayes Correlated Equilibrium and the Comparison of Information Structures",
      "authors": "Bergemann and Morris",
      "year": 2016,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "BCE characterizes outcomes consistent with per-signal obedience constraints, supplying the benchmark that underpins the impossibility bound the paper derives when the agent employs no-swap-regret learning."
    },
    {
      "title": "Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits",
      "authors": "Agarwal et al.",
      "year": 2014,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work provides contextual no-regret algorithms and regret bounds Reg(T) for rich policy classes, which the paper plugs into its analysis to yield the U* \u2212 \u0398(sqrt(Reg(T)/T)) guarantee in contextual settings."
    },
    {
      "title": "Reputation and Equilibrium Selection in Games with a Patient Player",
      "authors": "Fudenberg and Levine",
      "year": 1989,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "It shows how a long-lived player without commitment can approach Stackelberg/commitment outcomes via dynamics, motivating the pursuit of commitment-level payoffs through learning-driven behavior rather than explicit commitment."
    }
  ],
  "synthesis_narrative": "Bayesian Persuasion established the canonical principal\u2013agent setting in which a principal commits to an information policy so that a best-responding agent takes actions favorable to the principal, defining the sender-optimal value U*. Algorithmic Bayesian Persuasion then gave a computational formulation of this one-shot optimization under best responses, clarifying the structural constraints that bind the principal when the agent is obedient. From a learning perspective, Hart and Mas-Colell showed that external no-regret dynamics ensure play is approximately a best response to the empirical distribution of the opponent\u2019s actions, a lens that converts repeated interaction into approximate best-response behavior. Blum and Mansour introduced swap (internal) regret, revealing a stronger guarantee that enforces per-action substitution constraints and thereby tighter obedience-style conditions. Bergemann and Morris formalized Bayes correlated equilibrium as the set of outcome distributions consistent with such obedience constraints, providing a precise benchmark for what can be sustained without full commitment. Finally, contextual bandit advances such as Agarwal et al. supplied concrete no-regret learning algorithms and regret rates for rich policy classes, grounding finite-time guarantees, while reputation results like Fudenberg and Levine showed that commitment-level payoffs can emerge from dynamics even absent commitment. Together these strands expose a gap: the classic one-shot design assumes exact best responses under commitment, yet repeated play with a learning agent offers only approximate obedience whose strength depends on the regret notion. By mapping external no-regret to approximate best response, one can recover near-U* performance with quantitative rates; but with swap-regret, outcomes are constrained by BCE, limiting the principal\u2019s attainable utility. This synthesis naturally yields the paper\u2019s reduction and contrasting guarantees.",
  "target_paper": {
    "title": "Generalized Principal-Agent Problem with a Learning Agent",
    "authors": "Tao Lin, Yiling Chen",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "principal-agent problems, Bayesian persuasion, no-regret learning, no-swap-regret",
    "abstract": "Generalized principal-agent problems, including Stackelberg games, contract design, and Bayesian persuasion, are a class of economic problems where an agent best responds to a principal's committed strategy. \nWe study repeated generalized principal-agent problems under the assumption that the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal. We reduce this problem to a one-shot generalized principal-agent problem where the agent approximately best responds. Using this reduction, we show that: (1) if the agent uses contextual no-regret learning algorithms with regret $\\mathrm{Reg}(T)$, then the principal can guarantee utility at least $U^* - \\Theta\\big(\\sqrt{\\tfrac{\\mathrm{Reg}(T)}{T}}\\big)$, where $U^*$ is the principal's optimal utility in the classic model with a best-responding agent.\n(2) If the agent uses contextual no-swap-regret learning algorithms with swap-regret $\\mathrm{SReg}(T)$, then the principal cannot obtain util",
    "openreview_id": "LqTz13JS2P",
    "forum_id": "LqTz13JS2P"
  },
  "analysis_timestamp": "2026-01-06T14:55:41.539348"
}
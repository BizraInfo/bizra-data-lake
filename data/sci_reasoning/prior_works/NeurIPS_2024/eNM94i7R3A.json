{
  "prior_works": [
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": "Andrew M. Saxe, James L. McClelland, Surya Ganguli",
      "year": 2014,
      "role": "Backbone for exact gradient-flow analyses in deep linear networks",
      "relationship_sentence": "This paper provides the exact ODE solutions and singular-mode decomposition that Kunin et al. extend to analyze how layer-wise initialization variances and learning rates shape feature learning versus lazy behavior."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Canonical theory of the lazy training regime",
      "relationship_sentence": "By formalizing the lazy/NTK regime where features do not evolve, Jacot et al. supply the baseline that the present work quantitatively departs from and bridges to via controlled unbalancing of layer scales."
    },
    {
      "title": "On Lazy Training in Differentiable Programming",
      "authors": "L\u00e9na\u00efc Chizat, Edouard Oyallon, Francis Bach",
      "year": 2019,
      "role": "Scaling laws linking initialization/learning-rate parameterization to lazy vs. feature-learning regimes",
      "relationship_sentence": "Chizat et al. showed that training regimes depend on parameterization and scaling, which Kunin et al. refine by giving exact solutions that expose how layer-specific variances and learning rates conspire through conserved quantities."
    },
    {
      "title": "Tensor Programs V: Tuning large neural networks via zero-shot hyperparameter transfer (\u03bcP)",
      "authors": "Greg Yang, Edward J. Hu",
      "year": 2021,
      "role": "Layer-wise scaling rules preserving feature learning at infinite width",
      "relationship_sentence": "\u03bcP highlighted the crucial role of layer-specific learning-rate and initialization scaling in enabling feature learning; the new paper provides an exact finite-width theory showing how such layer-wise scalings control the lazy\u2013rich transition."
    },
    {
      "title": "Gradient descent aligns the layers of deep linear networks",
      "authors": "Ziwei Ji, Matus Telgarsky",
      "year": 2019,
      "role": "Balancedness and invariants in deep linear gradient flow",
      "relationship_sentence": "Results on layer-wise alignment/balancedness and conserved structures under gradient flow motivate Kunin et al.\u2019s identification of conserved quantities that constrain learning trajectories and govern feature learning speed."
    },
    {
      "title": "A mean field view of the landscape of two-layer neural networks",
      "authors": "Song Mei, Andrea Montanari, Phan-Minh Nguyen",
      "year": 2018,
      "role": "Mean-field feature-learning regime beyond NTK",
      "relationship_sentence": "Mean-field analyses established a contrasting rich-regime paradigm where features evolve, which the present work connects to via a minimal solvable model that interpolates between mean-field\u2013like feature learning and NTK-like laziness."
    },
    {
      "title": "On-line learning in soft committee machines",
      "authors": "David Saad, Sara A. Solla",
      "year": 1995,
      "role": "Early exact ODE analyses of feature-learning dynamics in shallow networks",
      "relationship_sentence": "The tradition of deriving exact teacher\u2013student learning dynamics in shallow (piecewise-linear) networks informs Kunin et al.\u2019s extension of their linear analysis to shallow nonlinear architectures."
    }
  ],
  "synthesis_narrative": "Kunin et al. build directly on the exact-gradient-dynamics tradition inaugurated by Saxe, McClelland, and Ganguli, who solved learning trajectories in deep linear nets via singular-mode dynamics. Against this backdrop, Jacot et al.\u2019s NTK formalized the lazy regime in which features remain fixed, while Chizat, Oyallon, and Bach clarified that whether training is lazy or rich depends critically on parameterization\u2014specifically how initialization scales with width and how learning rates are set. The present paper sharpens this dependence by deriving exact solutions for a minimal model where layer-specific initialization variances and learning rates jointly determine the regime, revealing conserved quantities that reshape trajectories in parameter and function space.\n\nTwo complementary lines further shaped this advance. First, mean-field analyses (Mei, Montanari, Nguyen) established a rich regime at infinite width, demonstrating genuine feature learning beyond NTK; Kunin et al. analytically interpolate between these extremes and pinpoint when rapid feature learning occurs. Second, results on balancedness and invariants in deep linear networks (Ji and Telgarsky) suggested that conservation laws constrain gradient flow; this paper identifies the precise conserved quantities that encode the effect of unbalanced layer scales. Finally, practical scaling insights from \u03bcP (Yang and Hu) underscored the importance of layer-wise learning-rate and initialization choices for feature evolution; the new exact solutions explain, mechanistically, how such unbalanced choices accelerate feature learning. The lineage thus integrates exact linear dynamics, kernel vs mean-field regimes, and layer-wise scaling theory into a unified, solvable account of the lazy\u2013rich transition.",
  "analysis_timestamp": "2026-01-06T23:33:35.543672"
}
{
  "prior_works": [
    {
      "title": "Linearity of Relation Decoding in Large Language Models",
      "authors": "Hernandez et al.",
      "year": 2023,
      "role": "Direct methodological precursor",
      "relationship_sentence": "Introduced the linear operators that decode specific relational facts from subject representations; the present paper generalizes from single-relation decoders to organized collections and studies their structure."
    },
    {
      "title": "A Three-Way Model for Collective Learning on Multi-Relational Data",
      "authors": "Maximilian Nickel, Volker Tresp, Hans-Peter Kriegel",
      "year": 2011,
      "role": "Modeling precedent for tensor factorization across relations",
      "relationship_sentence": "RESCAL\u2019s order-3 tensor factorization of relation-specific linear maps directly motivates compressing a bank of relation decoders via simple tensor networks, as done in this paper."
    },
    {
      "title": "Complex Embeddings for Simple Link Prediction",
      "authors": "Th\u00e9o Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, Guillaume Bouchard",
      "year": 2016,
      "role": "Bilinear/tensor formulation of relational operators",
      "relationship_sentence": "ComplEx shows that a set of relations can be captured by low-parameter bilinear operators tied across entities, informing the paper\u2019s finding that many relation decoders share low-rank, compressible structure."
    },
    {
      "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
      "authors": "Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy",
      "year": 2021,
      "role": "Mechanistic basis for linear factual decoding",
      "relationship_sentence": "Demonstrates that factual associations are stored as linearly accessible key\u2013value memories in MLP layers, supporting the assumption that linear maps can decode relations from subject representations."
    },
    {
      "title": "Toy Models of Superposition",
      "authors": "Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, et al.",
      "year": 2022,
      "role": "Theoretical explanation for redundancy/compressibility",
      "relationship_sentence": "Provides a framework for features being superposed in shared subspaces, directly explaining why multiple relation decoders collapse onto coarse-grained property directions and are highly compressible."
    },
    {
      "title": "Linguistic Regularities in Continuous Space Word Representations",
      "authors": "Tomas Mikolov, Wen-tau Yih, Geoffrey Zweig",
      "year": 2013,
      "role": "Evidence for linear structure of relations",
      "relationship_sentence": "Shows that semantic relations manifest as linear transformations (vector offsets), foreshadowing the paper\u2019s cross-evaluation result that decoders capture shared property directions across relations."
    },
    {
      "title": "Language Models as Knowledge Bases?",
      "authors": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller",
      "year": 2019,
      "role": "Task framing and evaluation of factual relations",
      "relationship_sentence": "Established the factual-relation evaluation paradigm (e.g., T-REx/LAMA), providing the relational benchmarks and mindset that underlie training and assessing relation decoders."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014organizing many relation-decoding linear operators, showing they are compressible by simple order-3 tensor networks, and revealing that they capture coarse-grained properties\u2014builds on three intertwined threads. First, Hernandez et al. (2023) established that specific factual relations can be decoded with a single linear operator from subject representations; the present work scales this paradigm to sets of relations and probes their mutual structure via cross-evaluation. Second, the knowledge-graph embedding literature (Nickel et al., 2011; Trouillon et al., 2016) demonstrated that multi-relational data can be modeled with low-parameter order-3 tensor/bilinear forms that tie entity representations across relations. This directly informs the paper\u2019s finding that a bank of relation decoders admits strong compression with simple tensor networks, mirroring RESCAL/ComplEx-style factorization. Third, mechanistic and representation-learning results explain why such compression works and what these operators capture: Geva et al. (2021) showed that transformers store factual associations in linearly accessible key\u2013value memories, while Elhage et al. (2022) provided a superposition account whereby many semantic features cohabit shared subspaces. Together with classic evidence that relations behave linearly (Mikolov et al., 2013), these works predict the paper\u2019s property-centric structure and cross-relation generalization. Finally, the LAMA framework (Petroni et al., 2019) underpins the relational evaluation setting used to measure decoding accuracy and generalization across semantically related relations.",
  "analysis_timestamp": "2026-01-07T00:02:04.914718"
}
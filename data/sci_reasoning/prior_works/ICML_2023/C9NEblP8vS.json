{
  "prior_works": [
    {
      "title": "Blockwise Parallel Decoding for Autoregressive Models",
      "authors": "Mitchell Stern et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "Introduced the propose-then-verify paradigm (predict multiple future tokens in parallel, then verify with the base model), but required architectural changes/retraining and did not preserve the exact base model distribution\u2014limitations this paper removes by using an external draft model and an acceptance rule that yields identical outputs."
    },
    {
      "title": "Non-Autoregressive Neural Machine Translation",
      "authors": "Jiatao Gu et al.",
      "year": 2017,
      "role": "Related Problem",
      "relationship_sentence": "Established the goal of parallelizing sequence generation for speed, but demonstrated the quality gap that arises when abandoning strict autoregressive sampling\u2014motivating this paper\u2019s pursuit of parallelism while exactly preserving the target model\u2019s distribution."
    },
    {
      "title": "Mask-Predict: Parallel Decoding of Conditional Masked Language Models",
      "authors": "Marjan Ghazvininejad et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "Showed iterative, partially parallel decoding via masked LM refinement, yet required specialized training and altered the sampling distribution; the present work achieves similar parallel gains without retraining and keeps the exact target distribution."
    },
    {
      "title": "Distilling the Knowledge in a Neural Network",
      "authors": "Geoffrey Hinton et al.",
      "year": 2015,
      "role": "Extension",
      "relationship_sentence": "Provides the teacher\u2013student framework used to train a small draft model to closely approximate the large target model, directly boosting speculative decoding\u2019s acceptance rate while leaving the final samples exact."
    },
    {
      "title": "Monte Carlo sampling methods using Markov chains and their applications",
      "authors": "W. K. Hastings",
      "year": 1970,
      "role": "Foundation",
      "relationship_sentence": "Supplies the acceptance\u2013rejection principle underlying the paper\u2019s correctness: by comparing target and proposal probabilities, the verification step guarantees samples are distributed exactly as the large model, despite using a cheaper proposal."
    },
    {
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)",
      "authors": "Colin Raffel et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "Serves as the primary large autoregressive model (T5-XXL) whose standard token-by-token decoding is accelerated; the paper demonstrates drop-in speedups with identical outputs relative to the baseline T5 decoding."
    }
  ],
  "synthesis_narrative": "The core innovation of speculative decoding\u2014proposing multiple future tokens with a small model and verifying them in parallel with a large model while preserving the exact sampling distribution\u2014emerges from two converging lines of prior work. First, blockwise parallel decoding (Stern et al., 2018) established the propose-then-verify schedule that parallelizes multiple steps of autoregressive generation. However, it required specialized training and altered the model\u2019s behavior, highlighting a key gap: can we gain parallel speedups without retraining and with identical outputs? In parallel, non-autoregressive and iterative-parallel approaches (Gu et al., 2017; Ghazvininejad et al., 2019) demonstrated substantial speedups by relaxing strict left-to-right decoding, but at the cost of deviating from the base model\u2019s distribution and requiring bespoke training objectives. Speculative decoding directly addresses these limitations by decoupling proposal and verification via a separately trained draft model and by grounding correctness in classical acceptance\u2013rejection principles from Monte Carlo (Hastings, 1970), ensuring exact samples from the target model. Knowledge distillation (Hinton et al., 2015) provides a practical mechanism to train the draft model to closely mimic the target, raising acceptance rates and thus realized speedups. Finally, T5 (Raffel et al., 2020) supplies the concrete baseline system and task setting, allowing the authors to demonstrate drop-in accelerations of existing autoregressive decoders with identical outputs, fulfilling the longstanding goal of parallel speed without distributional compromise.",
  "analysis_timestamp": "2026-01-06T23:09:26.569258"
}
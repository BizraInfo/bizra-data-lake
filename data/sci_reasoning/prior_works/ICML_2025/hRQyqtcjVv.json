{
  "prior_works": [
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Established RLHF-based guardrails and refusal behavior that define what a \u201cjailbreak\u201d circumvents; the paper\u2019s methodology of aligning models to refuse selected topics directly builds on this alignment paradigm."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Introduced policy-driven harmlessness tuning that creates the modern refusal patterns jailbreaks target; the current work leverages this notion of safety-tuned behavior when re-aligning models to refuse benign, ground-truthable topics."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "authors": "Andy Zou et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "Provides the canonical automated adversarial-suffix (GCG) jailbreak and harmful-prompt suite that the present paper evaluates for post-jailbreak utility, revealing the \u2018jailbreak tax\u2019 beyond mere refusal bypass rates."
    },
    {
      "title": "AutoDAN: Automatic and Interpretable Adversarial Attacks on Aligned Language Models",
      "authors": "Zhu et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "Supplies an iterative, natural-language jailbreak baseline that the paper subjects to the new utility-centric evaluation, directly testing whether AutoDAN\u2019s bypasses still yield high-quality answers."
    },
    {
      "title": "HarmBench: A Standardized Evaluation of Harmfulness Mitigations in LLMs",
      "authors": "Mazeika et al.",
      "year": 2024,
      "role": "Gap Identification",
      "relationship_sentence": "Benchmarks jailbreak/safety via refusal and LLM-judge assessments on harmful tasks but cannot ground utility in objective correctness; the current work explicitly addresses this limitation by constructing benign, truth-evaluable proxy tasks."
    },
    {
      "title": "JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models",
      "authors": "Liu et al.",
      "year": 2024,
      "role": "Gap Identification",
      "relationship_sentence": "Focuses on jailbreak success rates and refusal circumvention across models; the present paper extends this line by measuring the downstream utility of jailbroken outputs, not just whether the guardrail is bypassed."
    }
  ],
  "synthesis_narrative": "The Jailbreak Tax builds on the modern alignment paradigm established by InstructGPT (Ouyang et al.) and Constitutional AI (Bai et al.), which created the refusal-centric guardrails that define what it means to \u201cjailbreak\u201d an LLM. With these foundations, a wave of jailbreak methods\u2014most prominently Zou et al.\u2019s GCG and the AutoDAN family\u2014demonstrated that aligned systems can be coerced into producing unsafe content. However, mainstream benchmarks such as HarmBench and JailbreakBench largely quantify bypass success (e.g., refusal rates, LLM-judge scores) on inherently hard-to-evaluate harmful tasks, leaving a critical question unanswered: are the resulting jailbroken outputs actually useful? This paper identifies that gap and proposes a direct remedy: re-align models to refuse benign, ground-truthable domains (e.g., math, biology), thereby preserving objective correctness metrics while simulating the guardrail-bypass setting. Using this framework, the authors systematically re-evaluate representative jailbreaks (e.g., GCG, AutoDAN) and uncover a consistent utility degradation\u2014the jailbreak tax\u2014showing that many attacks trade off competence for circumvention. Thus, the work\u2019s core innovation directly extends the alignment foundations (to create controlled refusals), subjects leading jailbreak baselines to this new lens, and explicitly overcomes the evaluation limitations highlighted by prior jailbreak/safety benchmarks.",
  "analysis_timestamp": "2026-01-06T23:07:19.564789"
}
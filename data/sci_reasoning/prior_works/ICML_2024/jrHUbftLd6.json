{
  "prior_works": [
    {
      "title": "HyperNetworks",
      "authors": "David Ha, Andrew M. Dai, Quoc V. Le",
      "year": 2016,
      "role": "Foundational method",
      "relationship_sentence": "FedMBridge\u2019s bridge module is a topology-aware hypernetwork that conceptually builds on HyperNetworks\u2019 idea of generating model parameters from a conditioning input, extending it to generate client- and architecture-aware multimodal parameters."
    },
    {
      "title": "Personalized Federated Learning with Hypernetworks (pFedHN)",
      "authors": "Avihu Shamsian et al.",
      "year": 2021,
      "role": "Personalization via hypernetworks in FL",
      "relationship_sentence": "pFedHN demonstrated that a single hypernetwork can produce personalized client models in FL, directly inspiring FedMBridge\u2019s use of a hypernetwork to handle client-specific statistical heterogeneity while also accommodating architectural and modality differences."
    },
    {
      "title": "Federated Learning with Personalization Layers (FedPer)",
      "authors": "Manoj Kumar Arivazhagan et al.",
      "year": 2019,
      "role": "Blockwise model aggregation and personalization",
      "relationship_sentence": "FedPer\u2019s blockwise sharing/personalization paradigm exemplifies the restrictive compositional designs used by prior MFL, which FedMBridge overcomes by removing block-alignment constraints through a hypernetwork that can bridge arbitrary multimodal architectures."
    },
    {
      "title": "Federated Model Aggregation via Matching (FedMA)",
      "authors": "Xiaoxiao Wang et al.",
      "year": 2020,
      "role": "Architecture-heterogeneous aggregation",
      "relationship_sentence": "FedMA addressed architectural heterogeneity via neuron matching across layers, and FedMBridge advances this line by eliminating the need for structural alignment, using a hypernetwork to translate across disparate multimodal model topologies."
    },
    {
      "title": "HeteroFL: Computation and Communication Efficient Federated Learning for Heterogeneous Clients",
      "authors": "Min Diao et al.",
      "year": 2021,
      "role": "Model heterogeneity with nested architectures",
      "relationship_sentence": "HeteroFL supports different client capacities using a supernet\u2013subnet design, highlighting the limitations of nested/compositional architectures that FedMBridge sidesteps with a bridgeable hypernetwork that supports arbitrary multimodal interaction strategies."
    },
    {
      "title": "Federated Learning on Non-IID Data Silos with Attentive Message Passing (FedAMP)",
      "authors": "Yiqiang Huang et al.",
      "year": 2020,
      "role": "Topology/relationship-aware personalization",
      "relationship_sentence": "FedAMP\u2019s client relation modeling via attention motivates FedMBridge\u2019s topology-aware conditioning, enabling the hypernetwork to weigh and digest information from statistically and architecturally similar clients."
    },
    {
      "title": "Clustered Federated Learning",
      "authors": "Felix Sattler et al.",
      "year": 2020,
      "role": "Client clustering under statistical heterogeneity",
      "relationship_sentence": "CFL showed that grouping clients by similarity improves personalization under non-IID data, informing FedMBridge\u2019s use of a topology-aware bridge that generalizes clustering to a continuous, multimodal-aware relation space."
    }
  ],
  "synthesis_narrative": "FedMBridge\u2019s core contribution\u2014bridging statistical and architectural heterogeneity in multimodal federated learning via a topology-aware hypernetwork\u2014sits at the intersection of three influential threads. First, the methodology of generating model parameters from learned conditioners traces back to HyperNetworks, with pFedHN translating this idea to FL and showing that a single hypernetwork can yield personalized client models. FedMBridge generalizes this hypernetwork approach to simultaneously capture client statistical traits, multimodal interaction strategies, and architectural idiosyncrasies.\nSecond, prior attempts to support model heterogeneity in FL\u2014such as FedPer\u2019s blockwise sharing, FedMA\u2019s neuron matching, and HeteroFL\u2019s nested supernet design\u2014rely on restrictive compositional or alignment assumptions. These works highlight the fragility of block- or layer-level correspondence across clients, motivating FedMBridge\u2019s architecture-agnostic bridge that learns to translate parameters and representations across disparate multimodal topologies without explicit matching.\nThird, personalization via client relationships (FedAMP) and clustering under non-IID distributions (Clustered FL) demonstrate the value of topology-aware information sharing. FedMBridge internalizes this principle by conditioning its hypernetwork on a learned client topology, enabling selective knowledge transfer from statistically and architecturally proximate clients. By fusing hypernetwork-based generation with topology-aware conditioning, FedMBridge unifies personalization, architecture heterogeneity handling, and multimodal sharing into a single bridgeable mechanism that removes compositional design constraints prevalent in earlier MFL systems.",
  "analysis_timestamp": "2026-01-07T00:02:04.889387"
}
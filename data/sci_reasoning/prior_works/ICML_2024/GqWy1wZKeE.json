{
  "prior_works": [
    {
      "title": "Combining Labeled and Unlabeled Data with Co-Training",
      "authors": "Avrim Blum; Tom Mitchell",
      "year": 1998,
      "role": "Foundational co-training framework",
      "relationship_sentence": "Introduced the two-view co-training paradigm and conditional-independence style assumptions that define the problem the paper tackles and generalizes via a more efficient, stream-based reduction."
    },
    {
      "title": "Co-Training and the Expansion Assumption",
      "authors": "Maria-Florina (Nina) Balcan; Avrim Blum; Ke Yang",
      "year": 2005,
      "role": "Assumptions and theory for co-training under weak dependence",
      "relationship_sentence": "Provided theoretical guarantees for co-training under weaker dependence/expansion assumptions, which the paper explicitly adopts and operationalizes in an efficient reduction to online learning."
    },
    {
      "title": "Improving Generalization with Active Learning",
      "authors": "David A. Cohn; Les E. Atlas; Richard E. Ladner",
      "year": 1994,
      "role": "Stream-based active learning model",
      "relationship_sentence": "Established the stream-based selective sampling framework that the paper uses to design and analyze its co-training algorithms with label-efficient querying."
    },
    {
      "title": "Learning Quickly When Irrelevant Attributes Are Absent: A New Linear-Threshold Algorithm",
      "authors": "Nick Littlestone",
      "year": 1988,
      "role": "Mistake-bound online learning foundation",
      "relationship_sentence": "Defined the mistake-bound online learning model and Littlestone dimension, enabling the paper\u2019s core reduction that transfers co-training under weak dependence to any concept class with an efficient mistake-bound online learner."
    },
    {
      "title": "Importance Weighted Active Learning",
      "authors": "Alina Beygelzimer; Sanjoy Dasgupta; John Langford",
      "year": 2009,
      "role": "Reduction methodology in active learning",
      "relationship_sentence": "Pioneered reduction-style active learning in the stream-based setting, influencing the paper\u2019s reduction approach that preserves computational efficiency while controlling label complexity."
    },
    {
      "title": "Worst-Case Analysis of Selective Sampling for Linear Classification",
      "authors": "Nicol\u00f2 Cesa-Bianchi; Claudio Gentile; Luca Zaniboni",
      "year": 2006,
      "role": "Linking selective sampling to online mistakes",
      "relationship_sentence": "Analyzed selective sampling algorithms whose label requests track online mistakes, a key idea the paper leverages to obtain error-independent label complexity from mistake-bound online learners."
    },
    {
      "title": "A Bound on the Label Complexity of Agnostic Active Learning",
      "authors": "Steve Hanneke",
      "year": 2007,
      "role": "Active learning label-complexity theory",
      "relationship_sentence": "Developed label-complexity analyses via disagreement-based ideas, informing the paper\u2019s framing of label efficiency in stream-based querying within co-training."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014an efficient reduction of co-training under weak dependence in the stream-based active learning model to online classification\u2014sits at the intersection of three strands of prior work. First, Blum and Mitchell\u2019s original co-training framework defined the two-view setting and independence-style assumptions, while Balcan, Blum, and Yang relaxed these to weak dependence/expansion conditions, supplying the assumptions this work explicitly adopts and operationalizes. Second, the stream-based active learning model of Cohn, Atlas, and Ladner provides the operational setting: examples arrive sequentially and the learner selectively queries labels. Within this model, the paper follows a reduction ethos exemplified by Beygelzimer, Dasgupta, and Langford\u2019s IWAL, ensuring computational efficiency while controlling label complexity via principled sampling. Third, the reduction targets the mistake-bound online learning paradigm introduced by Littlestone. By coupling selective sampling with mistake-bound predictors, as in the analyses of Cesa-Bianchi, Gentile, and Zaniboni, the paper shows that label requests can be tied to online mistakes, yielding error-independent label complexity for any concept class efficiently learnable with a mistake-bound online algorithm. Hanneke\u2019s label-complexity perspective further contextualizes the guarantees, clarifying how disagreement/uncertainty drives query efficiency. Together, these works enable the paper\u2019s main result: a fast, general co-training framework under weak dependence that inherits both computational and label efficiency from online learners in the stream-based active setting.",
  "analysis_timestamp": "2026-01-06T23:42:48.076802"
}
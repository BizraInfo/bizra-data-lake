{
  "prior_works": [
    {
      "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
      "authors": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer",
      "year": 2022,
      "role": "Foundational model and latent space",
      "relationship_sentence": "The paper\u2019s severity encoding is computed in the autoencoder latent space of a pretrained latent diffusion model, directly leveraging LDM\u2019s encoder/decoder geometry to obtain a compact, semantically aligned representation where corruption severity correlates with distance."
    },
    {
      "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
      "authors": "Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole",
      "year": 2021,
      "role": "Mathematical framework for diffusion-based posterior sampling",
      "relationship_sentence": "The work adopts the score-based SDE framework to perform measurement-aware sampling, with the proposed severity encoding modulating the diffusion trajectory and data-fidelity interactions derived from this formalism."
    },
    {
      "title": "Diffusion Posterior Sampling for General Noisy Inverse Problems",
      "authors": "Hyungjin Chung, Byeongsu Sim, Jong Chul Ye",
      "year": 2022,
      "role": "Algorithmic template for zero-shot inverse problems",
      "relationship_sentence": "Severity encoding augments DPS-style reconstruction by adapting guidance strength and sampling effort on a per-sample basis, addressing DPS\u2019s uniform compute and fixed hyperparameters across heterogeneous corruption levels."
    },
    {
      "title": "Plug-and-Play Priors for Model Based Reconstruction",
      "authors": "S. Venkatakrishnan, Charles A. Bouman, Brendt Wohlberg",
      "year": 2013,
      "role": "Conceptual blueprint for decoupling data-fidelity and learned priors",
      "relationship_sentence": "The proposed method inherits PnP\u2019s philosophy of balancing data consistency and learned priors, turning that balance into a data-dependent mechanism via severity encoding that tunes this trade-off per instance."
    },
    {
      "title": "FFDNet: Toward a Fast and Flexible Solution for CNN-based Image Denoising",
      "authors": "Kai Zhang, Wangmeng Zuo, Lei Zhang",
      "year": 2018,
      "role": "Precedent for explicit degradation-level control",
      "relationship_sentence": "FFDNet\u2019s use of a noise-level map to control denoising strength directly inspires the paper\u2019s idea of estimating a per-sample degradation descriptor\u2014generalized here to a latent severity code\u2014to adapt the restoration process."
    },
    {
      "title": "Adaptive Computation Time for Recurrent Neural Networks",
      "authors": "Alex Graves",
      "year": 2016,
      "role": "Principle of instance-adaptive compute allocation",
      "relationship_sentence": "The paper extends the ACT principle to diffusion-based inverse problems by allocating more (or fewer) sampling steps and stronger (or weaker) guidance based on estimated per-sample difficulty via severity encoding."
    },
    {
      "title": "Compressed Sensing using Generative Models",
      "authors": "Ashish Bora, Ajil Jalal, Eric Price, Alexandros G. Dimakis",
      "year": 2017,
      "role": "Generative priors for inverse problems",
      "relationship_sentence": "Building on the insight that generative models serve as powerful priors for underdetermined recovery, the paper replaces optimization over generator latents with diffusion in LDM latent space and makes this process sample-adaptive via severity estimation."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014severity encoding for sample-adaptive reconstruction within latent diffusion models\u2014arises at the intersection of generative priors for inverse problems, diffusion-based posterior sampling, and instance-adaptive computation. Latent Diffusion Models (Rombach et al., 2022) supply the autoencoder latent space in which the authors measure degradation severity; this space is low-dimensional yet semantically aligned, enabling a robust scalar/vector descriptor that tracks corruption level. The sampling backbone follows the score-based SDE framework (Song et al., 2021), which underpins modern diffusion posterior methods; Diffusion Posterior Sampling (Chung et al., 2022) provides a practical, zero-shot template the authors augment by making guidance strength, step count, and trajectories contingent on the estimated severity. Conceptually, the approach echoes Plug-and-Play priors (Venkatakrishnan et al., 2013) by balancing data fidelity and learned priors, but it operationalizes this balance in a data-dependent manner rather than through fixed global hyperparameters. The idea of explicit degradation descriptors aligns with FFDNet (Zhang et al., 2018), where a noise-level map controls denoising strength; here, the notion is generalized to a latent severity code applicable across diverse degradations. Finally, Adaptive Computation Time (Graves, 2016) motivates allocating computation proportional to instance difficulty, a principle the paper instantiates by dynamically adjusting diffusion sampling effort. Earlier evidence that generative models are powerful priors (Bora et al., 2017) justifies the overall strategy, while severity encoding delivers the missing mechanism to tailor reconstruction strength and compute to each sample\u2019s true corruption level.",
  "analysis_timestamp": "2026-01-07T00:02:04.898715"
}
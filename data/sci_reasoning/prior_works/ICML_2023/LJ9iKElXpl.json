{
  "prior_works": [
    {
      "title": "Counterfactual Risk Minimization: Learning from Logged Bandit Feedback",
      "authors": "Adith Swaminathan et al.",
      "year": 2015,
      "role": "Baseline",
      "relationship_sentence": "Introduces the CRM principle of minimizing IPS-based empirical risk for off-policy learning; the present work directly replaces CRM\u2019s clipping-based variance control with exponential smoothing and provides PAC-Bayes certificates for this objective."
    },
    {
      "title": "The Self-Normalized Estimator for Counterfactual Learning",
      "authors": "Adith Swaminathan et al.",
      "year": 2015,
      "role": "Gap Identification",
      "relationship_sentence": "Proposes self-normalized IPS and propensity clipping as practical stabilizations; the new paper targets these heuristics\u2019 bias/variance limitations by deriving a tractable two-sided PAC-Bayes bound and a smooth IPS regularizer, and it explains when such regularization is or is not necessary."
    },
    {
      "title": "Unbiased Offline Evaluation of Contextual-bandit-based News Article Recommendation",
      "authors": "Lihong Li et al.",
      "year": 2011,
      "role": "Foundation",
      "relationship_sentence": "Establishes IPS (and SNIPS) for off-policy evaluation in contextual bandits and motivates learning by minimizing IPS risk; the current paper operates on this same IPS formulation and augments it with exponential smoothing and generalization guarantees."
    },
    {
      "title": "Learning Bounds for Importance Weighting",
      "authors": "Corinna Cortes et al.",
      "year": 2010,
      "role": "Gap Identification",
      "relationship_sentence": "Provides generalization bounds for importance-weighted ERM under boundedness/variance assumptions; the new two-sided PAC-Bayes bound is designed to remain valid without assuming bounded importance weights, directly addressing this limitation."
    },
    {
      "title": "Challenging the empirical mean and empirical variance: a deviation study",
      "authors": "Olivier Catoni",
      "year": 2012,
      "role": "Inspiration",
      "relationship_sentence": "Introduces robust estimation via exponential tilting/smooth influence functions that yield sharp concentration under heavy tails; this idea directly inspires the paper\u2019s exponential smoothing of IPS to tame unbounded importance weights."
    },
    {
      "title": "PAC-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning",
      "authors": "Olivier Catoni",
      "year": 2007,
      "role": "Foundation",
      "relationship_sentence": "Provides the PAC-Bayesian framework and exponential weighting techniques that underpin the paper\u2019s tractable two-sided PAC-Bayes generalization bound and learning certificates."
    },
    {
      "title": "Doubly Robust Policy Evaluation and Learning",
      "authors": "Miroslav Dud\u00edk et al.",
      "year": 2011,
      "role": "Related Problem",
      "relationship_sentence": "Identifies IPS variance as a central challenge and proposes doubly robust solutions; this motivates stabilized IPS objectives, which the present work tackles via exponential smoothing while also clarifying when such regularization is unnecessary."
    }
  ],
  "synthesis_narrative": "The core innovation of Exponential Smoothing for Off-Policy Learning is to replace heuristic stabilizations of inverse propensity scoring (IPS)\u2014notably clipping and self-normalization\u2014with an exponentially smoothed IPS objective that admits a tractable, two-sided PAC-Bayes generalization bound valid even with unbounded importance weights. This line begins with Li et al. (2011), which established IPS/SNIPS as the backbone of off-policy evaluation in contextual bandits and implicitly motivated learning by IPS risk minimization. Swaminathan and Joachims (2015) formalized this in Counterfactual Risk Minimization (CRM), introducing clipping and self-normalization as practical variance controls; these methods became the dominant baseline the present paper directly improves upon. However, both CRM and earlier theory, such as Cortes, Mansour, and Mohri (2010), relied on boundedness or moment assumptions that are ill-suited to heavy-tailed importance weights, leaving a theoretical and practical gap.\nCatoni\u2019s PAC-Bayesian program (2007) and his robust mean estimation via exponential tilting (2012) provide the key conceptual tools to close this gap: smooth, exponentially weighted objectives yield sharp high-probability control under heavy tails. The present paper adapts these ideas to IPS, deriving a two-sided PAC-Bayes bound that remains valid without bounded weights and yields learning certificates. Finally, Dud\u00edk et al. (2011) highlighted IPS variance and introduced doubly robust alternatives, contextualizing why stabilized IPS objectives matter; the new analysis further identifies when regularization may be unnecessary, challenging prevailing practices around clipping.",
  "analysis_timestamp": "2026-01-06T23:09:26.531226"
}
{
  "prior_works": [
    {
      "title": "Dense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation",
      "authors": "Peter R. Florence et al.",
      "year": 2018,
      "arxiv_id": "1806.08756",
      "role": "Foundation",
      "relationship_sentence": "This work established using self-supervised dense pixel correspondences as an object- and view-invariant representation for manipulation, directly motivating our use of dense correspondences as the action-carrying representation in lieu of explicit action labels."
    },
    {
      "title": "Transporter Networks: Rearranging the Visual World for Robotic Manipulation",
      "authors": "Andy Zeng et al.",
      "year": 2021,
      "arxiv_id": "2010.14406",
      "role": "Extension",
      "relationship_sentence": "We extend the core Transporter idea of converting dense image correspondences into spatial action targets by generalizing beyond pick-and-place and deriving closed-form controls from frame-to-frame correspondences without any action supervision."
    },
    {
      "title": "CLIPort: What and Where Pathways for Robotic Manipulation",
      "authors": "Arjun Singh et al.",
      "year": 2021,
      "arxiv_id": "",
      "role": "Baseline",
      "relationship_sentence": "CLIPort demonstrated language-conditioned dense transport for manipulation, and our method retains the language goal-specification while eliminating its need for action-labeled data via correspondence-driven, closed-form action inference from videos."
    },
    {
      "title": "Behavioral Cloning from Observation",
      "authors": "Faraz Torabi et al.",
      "year": 2018,
      "arxiv_id": "1805.01954",
      "role": "Gap Identification",
      "relationship_sentence": "BCO formalized imitation from observation by inferring actions via learned inverse dynamics, and we explicitly address its limitation of robot- and domain-specific inverse models by replacing action inference with dense correspondence\u2013based closed-form control."
    },
    {
      "title": "Visual Servo Control. I. Basic Approaches",
      "authors": "Fran\u00e7ois Chaumette et al.",
      "year": 2006,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "Classical image-based visual servoing provides the closed-form mapping from image feature motion to control commands, which we instantiate over dense correspondences between video frames to compute actions without action annotations."
    },
    {
      "title": "TAPIR: Tracking Any Point with Iterative Refinement",
      "authors": "Carl Doersch et al.",
      "year": 2023,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "We rely on the capability of modern dense point tracking to obtain robust frame-to-frame correspondences, an essential enabler for turning actionless videos into actionable control via our correspondence-based formulation."
    }
  ],
  "synthesis_narrative": "Self-supervised dense correspondence has been shown to provide manipulation-relevant invariances: Dense Object Nets learn pixel-aligned descriptors that remain consistent across views and object poses, enabling point-specific manipulation without fiducials. Building on this, Transporter Networks convert dense correlations between current and target images into spatial transport maps that directly specify pick-and-place actions through pixel-to-robot mappings, demonstrating that dense matching can substitute for explicit action parameterization. CLIPort augments the transporter paradigm with language, using a semantic \u201cwhat\u201d pathway to select entities and a spatial \u201cwhere\u201d pathway to localize precise placements, establishing text as a flexible goal interface aligned with dense, pixel-level control. Classical image-based visual servoing formalizes a closed-form transformation from image-feature motion to control commands through interaction matrices, revealing that visual correspondences alone can drive control without action annotation. Recent advances in dense point tracking such as TAPIR make per-pixel correspondences between frames reliable and robust in real-world scenes, closing a critical perception gap for correspondence-driven control. Meanwhile, Behavioral Cloning from Observation formalized learning from actionless demonstrations but hinges on learned inverse dynamics tied to particular robots and domains, highlighting a generalization bottleneck.\nCollectively, these works suggest a path: use text for goal specification and dense visual correspondence as the action substrate, but avoid action-label supervision and robot-specific inverse models. By hallucinating robot-executed video snippets and instantiating an image-based visual servoing control law over dense correspondences between frames, the present work synthesizes these insights into a video-only training pipeline that produces transferable, closed-form visuomotor control across embodiments.",
  "target_paper": {
    "title": "Learning to Act from Actionless Videos through Dense Correspondences",
    "authors": "Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, Joshua B. Tenenbaum",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Video-Based Policy, Video Dense Correspondence",
    "abstract": "In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that \"hallucinate\" robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling th",
    "openreview_id": "Mhb5fpA1T0",
    "forum_id": "Mhb5fpA1T0"
  },
  "analysis_timestamp": "2026-01-06T19:50:57.986479"
}
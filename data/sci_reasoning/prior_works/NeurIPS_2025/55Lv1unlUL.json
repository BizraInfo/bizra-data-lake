{
  "prior_works": [
    {
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": "Edward J. Hu et al.",
      "year": 2022,
      "role": "Foundational parameter-efficient fine-tuning (PEFT) method",
      "relationship_sentence": "StelLA directly extends LoRA\u2019s low-rank adapter by replacing its AB factorization with an SVD-style U S V^T and adds geometric constraints, addressing LoRA\u2019s lack of subspace geometry."
    },
    {
      "title": "Optimization Algorithms on Matrix Manifolds",
      "authors": "P.-A. Absil, R. Mahony, R. Sepulchre",
      "year": 2008,
      "role": "Foundational theory of Riemannian optimization on Stiefel/Grassmann",
      "relationship_sentence": "The book provides the formal machinery (Riemannian gradients, retractions, vector transports) that StelLA uses to constrain and optimize U and V on the Stiefel manifold."
    },
    {
      "title": "The Geometry of Algorithms with Orthogonality Constraints",
      "authors": "Alan Edelman, Tom\u00e1s A. Arias, Steven T. Smith",
      "year": 1998,
      "role": "Geometric characterization and algorithms for Stiefel constraints",
      "relationship_sentence": "StelLA\u2019s orthonormal subspace learning for U and V builds on this work\u2019s geometric analysis and practical update schemes for matrices with orthogonality constraints."
    },
    {
      "title": "A Feasible Method for Optimization with Orthogonality Constraints",
      "authors": "Zaiwen Wen, Wotao Yin",
      "year": 2013,
      "role": "Practical Stiefel retractions (e.g., Cayley/QR) for maintaining orthogonality",
      "relationship_sentence": "StelLA\u2019s training keeps U and V on the Stiefel manifold using feasible updates inspired by retraction-based methods developed in this paper."
    },
    {
      "title": "Stochastic Gradient Descent on Riemannian Manifolds",
      "authors": "Silv\u00e8re Bonnabel",
      "year": 2013,
      "role": "General recipe for SGD on manifolds via retraction and transport",
      "relationship_sentence": "StelLA\u2019s conversion of standard SGD-style updates into manifold-aware ones follows Bonnabel\u2019s framework of Riemannian SGD for constrained geometry."
    },
    {
      "title": "Riemannian Adaptive Optimization Methods",
      "authors": "Nicolas B\u00e9cigneul, Octavian-Eugen Ganea",
      "year": 2019,
      "role": "Adaptive optimizers (Adam/Adagrad) generalized to Riemannian settings",
      "relationship_sentence": "The claim that StelLA can \"convert any Euclidean optimizer to a Riemannian one\" is operationalized by recipes like Riemannian Adam from this work."
    },
    {
      "title": "Trivializations for Gradient-Based Optimization on Manifolds",
      "authors": "Mario Lezcano-Casado",
      "year": 2019,
      "role": "Modular, deep-learning-friendly manifold optimization design",
      "relationship_sentence": "StelLA\u2019s flexible, plug-and-play geometric optimizer design parallels this paper\u2019s practical approach to implementing manifold optimization within standard DL pipelines."
    }
  ],
  "synthesis_narrative": "StelLA\u2019s core idea\u2014treating LoRA adapters as explicit subspace objects and optimizing them with orthonormality on the Stiefel manifold\u2014sits at the intersection of parameter-efficient fine-tuning and Riemannian optimization. The LoRA framework (Hu et al., 2022) supplied the foundational low-rank adapter abstraction and motivated improving performance without full fine-tuning. StelLA\u2019s SVD-like U S V^T factorization makes the adapter\u2019s input/output subspaces explicit, a move grounded in classical matrix manifold geometry. Absil, Mahony, and Sepulchre (2008) and Edelman, Arias, and Smith (1998) provided the essential geometric toolkit\u2014Riemannian gradients, retractions, and the structure of the Stiefel manifold\u2014that enables principled optimization of orthonormal subspaces. Translating this theory into practical training dynamics relies on feasible orthogonality-preserving updates, as developed by Wen and Yin (2013) via Cayley/QR-based retractions.\nBeyond feasibility, StelLA emphasizes a modular training pipeline that \u201cwraps\u201d standard optimizers into their Riemannian counterparts. Bonnabel (2013) outlined the general SGD-on-manifolds scheme via retraction and vector transport, while B\u00e9cigneul and Ganea (2019) extended adaptive optimizers like Adam to manifold settings, directly supporting StelLA\u2019s claim of converting Euclidean optimizers. Finally, Lezcano-Casado (2019) demonstrated how to implement manifold optimization in modern deep learning frameworks with minimal disruption, informing StelLA\u2019s plug-and-play design. Together, these works directly underwrite StelLA\u2019s key contributions: explicit subspace factorization, Stiefel-constrained learning of U and V, and a general, optimizer-agnostic Riemannian training wrapper compatible with existing PEFT pipelines.",
  "analysis_timestamp": "2026-01-07T00:05:12.556465"
}
{
  "prior_works": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Introduces the NTK formalism and kernel-gradient-flow equivalence that this paper uses to define the population NTK and its critical rate \u03b5_n, enabling transfer of early-stopping risk guarantees from kernel methods to over-parameterized two-layer networks."
    },
    {
      "title": "On Lazy Training in Differentiable Programming",
      "authors": "L\u00e9na\u00efc Chizat et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Provides the lazy-training/small-parameter-movement regime ensuring GD stays in a linearized (NTK) neighborhood; the present analysis leverages this to justify treating early-stopped GD on the network as kernel GD with the NTK."
    },
    {
      "title": "Early Stopping and Regularization Paths for Learning",
      "authors": "Yiming Yao et al.",
      "year": 2007,
      "role": "Baseline",
      "relationship_sentence": "Establishes early stopping as iterative regularization for least-squares in RKHS and derives sharp nonparametric rates, forming the classical kernel-GD-with-early-stopping baseline that this paper matches with NTK-driven neural GD."
    },
    {
      "title": "Optimal rates for the regularized least-squares algorithm",
      "authors": "Andrea Caponnetto et al.",
      "year": 2007,
      "role": "Foundation",
      "relationship_sentence": "Gives distribution-free kernel risk bounds via eigenvalue-based quantities (effective dimension), which this paper adapts to the NTK operator to express its sharp O(\u03b5_n^2) rate without assuming a specific covariate distribution."
    },
    {
      "title": "Local Rademacher Complexities",
      "authors": "Peter L. Bartlett et al.",
      "year": 2005,
      "role": "Foundation",
      "relationship_sentence": "Introduces the localized complexity and fixed-point (critical radius) framework; the paper\u2019s O(\u03b5_n^2) excess-risk bound is exactly in this critical-radius form specialized to the NTK."
    },
    {
      "title": "Statistical optimality of stochastic gradient descent on least squares",
      "authors": "Victor Pillaud-Vivien et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "Shows that early-stopped gradient methods in RKHS achieve minimax-optimal rates comparable to kernel ridge; the present work mirrors this result for full-batch GD by transporting it to the NTK of an over-parameterized network."
    },
    {
      "title": "On the Inductive Bias of Neural Tangent Kernels",
      "authors": "Alessandro Bietti et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Analyzes NTK spectra on the sphere under the uniform distribution assumption; the current paper explicitly removes this distributional assumption, providing distribution-free guarantees for any distribution supported on the unit sphere."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014showing that gradient descent on an over-parameterized two-layer network with early stopping achieves a sharp O(\u03b5_n^2) nonparametric risk, distribution-free over spherical covariates\u2014rests on three converging lines of work. First, Jacot et al. established the Neural Tangent Kernel and its kernel-gradient-flow equivalence, while Chizat et al. formalized the lazy training regime that guarantees neural gradient descent stays close to its NTK linearization. Together, these works enable treating early-stopped neural training as kernel gradient descent with the network\u2019s NTK. Second, classical RKHS theory by Bartlett et al. introduced localized Rademacher complexities and the critical-radius fixed point, and Caponnetto\u2013De Vito provided distribution-free kernel risk characterizations via spectral quantities. Yao\u2013Rosasco\u2013Caponnetto and subsequent iterative-regularization analyses (e.g., Pillaud-Vivien\u2013Rudi\u2013Bach) showed that early stopping in (stochastic or batch) gradient methods attains minimax rates comparable to kernel ridge, furnishing the baseline this paper seeks to match in the NTK setting. Third, prior NTK generalization and spectral analyses on the sphere (e.g., Bietti\u2013Mairal) typically assume a specific covariate distribution (uniform on the sphere), a limitation directly addressed here: the present work formulates bounds entirely through the NTK\u2019s population critical rate \u03b5_n, delivering the same sharp O(\u03b5_n^2) risk without any distributional assumption beyond support on the unit sphere. This synthesis yields a distribution-free, minimax-sharp generalization guarantee for over-parameterized neural networks trained by early-stopped gradient descent.",
  "analysis_timestamp": "2026-01-06T23:07:19.638837"
}
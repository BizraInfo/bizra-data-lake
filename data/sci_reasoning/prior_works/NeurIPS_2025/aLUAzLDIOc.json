{
  "prior_works": [
    {
      "title": "Multitask Learning",
      "authors": "Rich Caruana",
      "year": 1997,
      "role": "Foundational theory of transfer via shared representations",
      "relationship_sentence": "The paper operationalizes Caruana\u2019s principle that related tasks can share inductive biases by showing that a model\u2019s length generalization learned on an auxiliary task can transfer to a different target task through joint training."
    },
    {
      "title": "Learning to Execute",
      "authors": "Wojciech Zaremba, Ilya Sutskever",
      "year": 2014,
      "role": "Early demonstration of algorithmic reasoning and length generalization/curriculum effects",
      "relationship_sentence": "By building on the tradition of program-like algorithmic benchmarks introduced here, the paper studies length extrapolation on arithmetic and string tasks while adding the new angle of cross-task transfer of this ability."
    },
    {
      "title": "Neural GPUs Learn Algorithms",
      "authors": "\u0141ukasz Kaiser, Ilya Sutskever",
      "year": 2015,
      "role": "Architectural approach achieving strong length generalization on algorithmic tasks",
      "relationship_sentence": "Where Neural GPUs pursued architectural bias to achieve length generalization, this paper shows that transformers can instead acquire and transfer this capability across related tasks via joint training."
    },
    {
      "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)",
      "authors": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",
      "year": 2020,
      "role": "Scaled multi-task training demonstrating broad cross-task transfer in transformers",
      "relationship_sentence": "Inspired by T5\u2019s success with multi-task parameter sharing, the paper frames length generalization as a capability that can be jointly learned and then transferred between related tasks."
    },
    {
      "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation (ALiBi)",
      "authors": "Ofir Press, Noah A. Smith, Mike Lewis",
      "year": 2021,
      "role": "Method for length extrapolation via positional biasing",
      "relationship_sentence": "Contrasting architectural/positional solutions like ALiBi, the paper advances a complementary mechanism\u2014task-transfer\u2014that enables length generalization without relying solely on specialized positional encodings."
    },
    {
      "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding",
      "authors": "Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, Yunfeng Liu",
      "year": 2021,
      "role": "Positional encoding with better extrapolation properties widely used in pretrained LMs",
      "relationship_sentence": "Because many pretrained models use RoPE, the paper\u2019s observation that pretrained LMs display transferable length generalization connects directly to this line of work on extrapolative position encodings."
    },
    {
      "title": "In-context Learning and Induction Heads",
      "authors": "Catherine Olsson, Nelson Elhage, Neel Nanda, et al.",
      "year": 2022,
      "role": "Mechanistic account of length generalization via induction heads in transformers",
      "relationship_sentence": "The paper\u2019s finding that length generalization can be inherited across tasks aligns with the idea that reusable circuits (e.g., induction heads) learned on one task can be redeployed to another through joint training."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014recasting length generalization as a capability that can transfer across related tasks\u2014sits at the intersection of three strands of prior work. First, the concept of parameter sharing for cross-task inductive bias traces to Caruana\u2019s Multitask Learning and later large-scale realizations such as T5, which established that jointly training on related tasks can endow models with broadly transferable competencies. Second, algorithmic reasoning studies like Learning to Execute and Neural GPUs defined the experimental paradigm of training on short sequences and evaluating on longer ones, demonstrating that neural sequence models can (sometimes) extrapolate in arithmetic and string manipulation. Third, recent work on positional schemes (ALiBi, RoPE) and mechanistic analyses of induction heads clarified when and how transformers extrapolate to longer contexts, highlighting both architectural/positional enablers and reusable attention circuits.\nThis paper synthesizes these threads by showing that length generalization need not be tied exclusively to architecture or positional encodings; instead, it can be acquired on one task and transferred to another through joint training, across diverse algorithmic domains (arithmetic, string transforms, maze navigation). The observation that similar transfer emerges in pretrained language models connects the transfer lens to mainstream pretraining practice, where RoPE-like encodings and reusable induction circuits are common. Thus, the work extends multitask transfer from task accuracy to a more subtle property\u2014length extrapolation\u2014bridging algorithmic benchmarks, positional-extrapolation methods, and mechanistic insights into reusable attention circuitry.",
  "analysis_timestamp": "2026-01-07T00:21:32.228035"
}
{
  "prior_works": [
    {
      "title": "Strategic Classification",
      "authors": [
        "Moritz Hardt",
        "Nimrod Megiddo",
        "Christos Papadimitriou",
        "Mary Wootters"
      ],
      "year": 2016,
      "role": "Foundational model of strategic manipulation in classification",
      "relationship_sentence": "The paper builds directly on the Hardt et al. Stackelberg-style model where agents best-respond to a posted classifier with feature manipulation costs, and seeks the principal\u2019s optimal classifier under such strategic behavior."
    },
    {
      "title": "Stackelberg Games for Adversarial Prediction",
      "authors": [
        "Michal Br\u00fcckner",
        "Tobias Scheffer"
      ],
      "year": 2011,
      "role": "Leader\u2013follower optimization framework for manipulated inputs",
      "relationship_sentence": "This work\u2019s bi-level formulation and anticipation of adversarial/strategic feature manipulation inform the paper\u2019s modeling of the principal as leader and agents as followers when analyzing optimal linear separators under manipulation."
    },
    {
      "title": "Performative Prediction",
      "authors": [
        "Perdomo",
        "Zrnic",
        "Mendler-D\u00fcnner",
        "Hardt"
      ],
      "year": 2020,
      "role": "Distribution shift induced by deployed models and online adaptation",
      "relationship_sentence": "The concept that the deployed predictor changes the data distribution directly underpins the paper\u2019s online objective (convergence to a clairvoyant optimum) when agents adapt features in response to the current classifier."
    },
    {
      "title": "Efficiently Learning Halfspaces with Massart Noise",
      "authors": [
        "Ilias Diakonikolas",
        "Daniel M. Kane",
        "Alistair Stewart"
      ],
      "year": 2019,
      "role": "Algorithmic/analytic tools for Massart-noise robust halfspace learning",
      "relationship_sentence": "The analysis leverages Massart-noise assumptions and techniques such as margin-based reasoning from this line to obtain consistency guarantees despite feature-dependent bounded label noise."
    },
    {
      "title": "The Power of Localization for Efficiently Learning Linear Separators with Noise",
      "authors": [
        "Pranjal Awasthi",
        "Maria-Florina Balcan",
        "Philip M. Long"
      ],
      "year": 2014,
      "role": "Noise-robust learning of linear separators via localization",
      "relationship_sentence": "Their localization and margin-based arguments for learning with Massart/Tsybakov noise inform the paper\u2019s design of update rules and confidence regions that remain stable under bounded noise and manipulation."
    },
    {
      "title": "Counterfactual Risk Minimization: Learning from Logged Bandit Feedback",
      "authors": [
        "Adith Swaminathan",
        "Thorsten Joachims"
      ],
      "year": 2015,
      "role": "Unbiased risk estimation from partial-feedback data",
      "relationship_sentence": "The paper adapts importance-weighted estimators from CRM to cope with selective label feedback (observing labels only for positive classifications), enabling consistent online updates under partial observability."
    },
    {
      "title": "Efficient Bandit Algorithms for Online Multiclass Prediction (Banditron)",
      "authors": [
        "Sham M. Kakade",
        "Shai Shalev-Shwartz",
        "Ambuj Tewari"
      ],
      "year": 2008,
      "role": "Online linear classification with bandit/partial feedback and regret guarantees",
      "relationship_sentence": "Banditron-style importance-weighted gradient updates and O(sqrt(T)) regret analysis techniques guide the paper\u2019s online algorithm and regret proof under one-sided label observations."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014an online algorithm for strategic binary linear classification with only positive-label feedback under Massart noise, achieving O(sqrt(T)) regret to a clairvoyant optimum\u2014stands at the intersection of three literatures. First, the strategic manipulation modeling follows the Stackelberg framework of Hardt et al. (Strategic Classification) and Br\u00fcckner\u2013Scheffer, where agents best-respond to a posted classifier with costed feature changes. This establishes the leader\u2013follower objective and the notion of an optimal classifier that anticipates manipulation. Second, performative prediction formalizes distributional shifts induced by deployed models; its equilibrium notions and regret-to-performative-optimum perspective motivate convergence to a clairvoyant classifier defined with respect to strategically transformed features. Third, the work addresses partial observability\u2014only labels of positively classified agents are revealed\u2014by importing bandit/partial-feedback methodology. Counterfactual risk minimization provides importance-weighted estimators to debias selective feedback, while Banditron-style analyses supply online update schemes and O(sqrt(T)) regret techniques for linear prediction with bandit feedback. Finally, robustness to feature-dependent bounded noise is grounded in Massart-noise halfspace learning: Diakonikolas\u2013Kane\u2013Stewart and Awasthi\u2013Balcan\u2013Long contribute margin-based and localization tools that stabilize learning and certify consistency under noise. Integrating these strands, the paper proposes an importance-weighted, margin-aware online learner that anticipates strategic best responses, corrects for selective labels, and provably attains O(sqrt(T)) regret to the clairvoyant strategic optimum.",
  "analysis_timestamp": "2026-01-07T00:21:33.140011"
}
{
  "prior_works": [
    {
      "title": "Evolving Neural Networks through Augmenting Topologies (NEAT)",
      "authors": "Kenneth O. Stanley, Risto Miikkulainen",
      "year": 2002,
      "role": "Neuroevolution of topology and weights; dynamic addition/removal of neurons and connections",
      "relationship_sentence": "NEAT pioneered jointly optimizing network structure and parameters with on-the-fly neuron/edge growth, directly informing SAGP\u2019s self-assembling mechanism for adjusting neurons and synapses during training."
    },
    {
      "title": "A Growing Neural Gas Network Learns Topologies",
      "authors": "Bernd Fritzke",
      "year": 1995,
      "role": "Online graph-structured neural growth and edge adaptation",
      "relationship_sentence": "GNG established a graph-based, incremental add/delete regime for units and edges, a clear antecedent to SAGP\u2019s graph-structured plasticity and topology adaptation."
    },
    {
      "title": "The Cascade-Correlation Learning Architecture",
      "authors": "Scott E. Fahlman, Christian Lebiere",
      "year": 1990,
      "role": "Constructive neural networks that add hidden units to reduce residual error",
      "relationship_sentence": "Cascade-Correlation provides the classical constructive-learning template for neuron growth under training signals, which SAGP modernizes within gradient-based optimization."
    },
    {
      "title": "Dynamic Network Surgery for Efficient DNNs",
      "authors": "Tianqi Guo, Anbang Yao, Yurong Chen",
      "year": 2016,
      "role": "On-the-fly pruning with connection splicing",
      "relationship_sentence": "DNS introduced prune-and-splice dynamics to maintain accuracy while sparsifying, a principle SAGP generalizes to continuous synapse reallocation alongside neuron-level changes."
    },
    {
      "title": "Scalable training of artificial neural networks with adaptive sparse connectivity (SET)",
      "authors": "Decebal C. Mocanu, Elena Mocanu, Peter Stone, Phuong H. Nguyen, Mathijs Pechenizkiy, Antonio Liotta",
      "year": 2018,
      "role": "Dynamic sparse training via periodic prune-and-grow of connections",
      "relationship_sentence": "SET showed that iteratively pruning and regrowing edges can learn effective sparse topologies; SAGP extends this idea to learned, task-driven rewiring and to neuron birth/death."
    },
    {
      "title": "Rigging the Lottery: Making All Tickets Winners (RigL)",
      "authors": "M. Evci, T. Gale, J. Menick, P. J. Castro, E. Elsen",
      "year": 2020,
      "role": "Gradient-guided dynamic sparse connectivity during training",
      "relationship_sentence": "RigL\u2019s use of gradients to allocate new connections motivates SAGP\u2019s synapse-level self-assembly using training signals rather than static or random regrowth."
    },
    {
      "title": "Neural Message Passing for Quantum Chemistry",
      "authors": "Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl",
      "year": 2017,
      "role": "Formalization of message-passing graph neural networks",
      "relationship_sentence": "The MPNN framework underpins treating computation as operations over nodes/edges; SAGP\u2019s Graph Perceptron extends MLPs into a graph-based formulation where connectivity is a learnable object."
    }
  ],
  "synthesis_narrative": "Self-Assembling Graph Perceptrons (SAGP) sit at the intersection of constructive neural learning, graph-based computation, and dynamic sparse training. Early constructive methods such as Cascade-Correlation established that networks can grow hidden units during training to reduce residual error, while Growing Neural Gas showed that a graph of units and edges can be adapted online through addition and deletion. NEAT then unified weight learning and structural evolution, demonstrating that neuron and synapse growth can be jointly optimized to discover compact, high-performing topologies.\n\nModern sparsity literature operationalized these ideas within gradient-based deep learning. Dynamic Network Surgery introduced the prune-and-splice paradigm, preserving accuracy while adapting connectivity; SET generalized this into periodic prune-and-grow dynamics that discover sparse topologies from scratch; and RigL further grounded regrowth in gradients, showing that training signals can guide which connections to form. In parallel, the Message Passing Neural Network framework clarified how computation over nodes and edges can be formalized, providing a natural substrate for treating connectivity as a first-class, learnable element.\n\nSAGP synthesizes these strands by recasting an MLP as a Graph Perceptron and embedding constructive growth and sparsification into a single training loop. It autonomously allocates neurons (node-level plasticity) and synapses (edge-level plasticity) with training-driven criteria, avoiding separate architecture search while retaining the efficiency benefits of sparse, adaptive connectivity learned on the fly.",
  "analysis_timestamp": "2026-01-07T00:02:04.962476"
}
{
  "prior_works": [
    {
      "title": "Membership Inference Attacks Against Machine Learning Models",
      "authors": "Reza Shokri et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Established membership inference as a statistical question of whether a point influenced a trained model; this paper elevates that framing to model provenance by testing whether Bob\u2019s model/text depends on Alice\u2019s randomized training run."
    },
    {
      "title": "The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks",
      "authors": "Nicholas Carlini et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Introduced exposure/canary-based measurements for memorization; the present work extends this memorization-testing idea by correlating memorization signals with the randomized training order to build a rigorous, run-level statistical test."
    },
    {
      "title": "Extracting Training Data from Large Language Models",
      "authors": "Nicholas Carlini et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrated black-box extraction of memorized content from LMs, directly motivating the paper\u2019s black-box and text-only provenance settings that exploit observable memorization."
    },
    {
      "title": "Proof-of-Learning: Definitions and Practice",
      "authors": "Jinyuan Jia et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "Proposed using training randomness (e.g., data order/seed) to verify training, but requires logs/white-box access; this paper addresses that gap by turning the same randomness into a black-box, text-only independence test via palimpsestic memorization."
    },
    {
      "title": "A Watermark for Large Language Models",
      "authors": "Johannes Kirchenbauer et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "Provides an attribution mechanism in the observational setting but requires modifying generation and is attackable; the present work offers watermark-free, statistically quantifiable provenance based on training-order\u2013dependent memorization."
    },
    {
      "title": "A Kernel Statistical Test of Independence",
      "authors": "Arthur Gretton et al.",
      "year": 2008,
      "role": "Foundation",
      "relationship_sentence": "Supplies the formal framework for independence testing that underlies casting provenance as testing dependence between Bob\u2019s outputs and Alice\u2019s randomized training order, enabling exact quantification of evidence."
    },
    {
      "title": "An Empirical Study of Example Forgetting During Deep Neural Network Learning",
      "authors": "Nina Toneva et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "Revealed training dynamics and forgetting across epochs, informing the paper\u2019s palimpsestic hypothesis that later-seen examples are preferentially memorized and thus leave order-dependent statistical traces."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core idea\u2014proving model provenance by testing statistical dependence on a randomized training run\u2014sits at the intersection of membership inference, memorization in language models, and formal independence testing. Shokri et al. (2017) provided the foundational lens of membership inference, framing provenance as a question of whether a model\u2019s behavior depends on particular training data. Building on LM-specific memorization, Carlini et al. (2019) introduced exposure and canary-based tools to quantify unintended memorization, which this paper extends by correlating memorization signals with the randomized order of training examples to construct a run-level test. Carlini et al. (2021) showed that memorized content can be surfaced in black-box LMs, directly motivating both the query-based and text-only observational settings. The paper borrows the key insight from Proof-of-Learning (Jia et al., 2021)\u2014that training randomness (e.g., data order) is verifiable\u2014but overcomes PoL\u2019s white-box/log requirements by recasting provenance as an independence test and leveraging palimpsestic memorization. This statistical formulation is grounded in independence testing frameworks such as Gretton et al. (2008), enabling quantifiable evidence against the null of independence. Finally, while watermarking for LLMs (Kirchenbauer et al., 2023) targets attribution in the observational setting, its need for model modification and vulnerability motivates a watermark-free alternative; the paper\u2019s palimpsestic approach fills that gap, with supporting intuition from training dynamics and forgetting (Toneva et al., 2019).",
  "analysis_timestamp": "2026-01-06T23:08:23.944507"
}
{
  "prior_works": [
    {
      "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
      "authors": "Ranjay Krishna et al.",
      "year": 2017,
      "role": "Representation foundation (scene graphs)",
      "relationship_sentence": "Introduced large-scale scene graphs of objects and relations, directly inspiring SIG\u2019s explicit, compositional encoding of layouts and inter-object relations."
    },
    {
      "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering",
      "authors": "Drew A. Hudson, Christopher D. Manning",
      "year": 2019,
      "role": "Evaluation/benchmark for compositional spatial reasoning",
      "relationship_sentence": "Used scene graphs to craft compositional queries and defined balanced metrics, informing SIG\u2019s VSI metrics that aim to measure structure-aware reasoning rather than language shortcuts."
    },
    {
      "title": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering (VQA-CP)",
      "authors": "Aishwarya Agrawal, Dhruv Batra, Devi Parikh, Aniruddha Kembhavi",
      "year": 2018,
      "role": "Bias analysis and evaluation design",
      "relationship_sentence": "Showed how language priors contaminate VQA performance via changing priors, motivating SIG\u2019s complementary structured channel and its metrics that isolate intrinsic spatial skill from answer priors."
    },
    {
      "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
      "authors": "Justin Johnson et al.",
      "year": 2017,
      "role": "Diagnostic evaluation for spatial/compositional reasoning",
      "relationship_sentence": "Established program-grounded, bias-resistant tests of spatial and compositional skills, underpinning SIG\u2019s design of faithful, compositional probes of visual\u2013spatial intelligence."
    },
    {
      "title": "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language",
      "authors": "Kexin Yi et al.",
      "year": 2018,
      "role": "Methodological precursor (structured intermediate representation)",
      "relationship_sentence": "Demonstrated that inserting a structured, symbolic scene representation between perception and reasoning improves faithfulness, mirroring SIG\u2019s use as a structured channel alongside text for LLM reasoning."
    },
    {
      "title": "Lift, Splat, Shoot: Encoding Images from Arbitrary Cameras to Dense 3D Semantic Grids",
      "authors": "Simon Philion, Sanja Fidler",
      "year": 2020,
      "role": "Grid-based spatial encoding in autonomous driving",
      "relationship_sentence": "Validated dense BEV/grid representations that capture object layouts for driving, directly informing SIG\u2019s grid-based schema for spatial structure and its autonomous driving pilot."
    },
    {
      "title": "Interaction Networks for Learning about Objects, Relations and Physics",
      "authors": "Peter W. Battaglia et al.",
      "year": 2016,
      "role": "Physics-informed relational reasoning",
      "relationship_sentence": "Provided a template for object-centric, relation-based physical reasoning, motivating SIG\u2019s physically grounded human priors within its relational schema and evaluation."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core idea\u2014SIG as a structured, grid-based channel that encodes objects, relations, and physics-informed human priors to complement text for foundation-model reasoning\u2014stands on three intertwined lines of work. First, explicit scene structure: Visual Genome established scene graphs linking objects and relationships, while GQA operationalized them for compositional questioning and balanced metrics. These works directly motivated SIG\u2019s explicit relational encoding and its attribution-friendly evaluation of spatial reasoning. Second, diagnostic evaluation to avoid language shortcuts: CLEVR introduced program-grounded, bias-resistant tests of spatial/compositional skills, and VQA-CP exposed how answer priors distort VQA generalization. Together they shaped the paper\u2019s SIG-informed VSI metrics aimed at isolating intrinsic spatial capability from linguistic priors. Third, spatial and physical inductive biases in embodied domains: Lift-Splat-Shoot popularized dense BEV/grid representations for autonomous driving, validating grids as faithful carriers of scene layout; Interaction Networks demonstrated object-centric, relation-based modeling of physics, informing the incorporation of human/physical priors into SIG\u2019s schema and tasks. Complementing these, Neuro-Symbolic VQA showed that inserting a structured intermediate representation between perception and reasoning increases faithfulness and interpretability\u2014an architectural principle mirrored by SIG when paired with multimodal LLMs. Collectively, these works justify a complementary, structured spatial channel and principled metrics to measure visual\u2013spatial intelligence beyond language biases, and they ground the driving pilot in proven grid and physics priors.",
  "analysis_timestamp": "2026-01-07T00:21:32.341229"
}
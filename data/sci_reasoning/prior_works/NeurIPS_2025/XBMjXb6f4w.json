{
  "prior_works": [
    {
      "title": "MLE-Bench: Evaluating Agents on Realistic Machine Learning Engineering Tasks",
      "authors": "Harel Lidar; Louis Makower; Thomas Jodrell; Lauren Robson; Teun van der Weij; et al.",
      "year": 2024,
      "role": "Benchmark foundation",
      "relationship_sentence": "CTRL-ALT-DECEIT directly extends MLE-Bench\u2019s infrastructure and task format, adding sabotage, sandbagging, and oversight-subversion tracks atop the same realistic ML engineering workflows and evaluation harness."
    },
    {
      "title": "BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain",
      "authors": "Tianyu Gu; Brendan Dolan-Gavitt; Siddharth Garg",
      "year": 2017,
      "role": "Attack paradigm for backdoors",
      "relationship_sentence": "The paper\u2019s backdoor implantation tasks operationalize BadNets-style trigger/backdoor mechanics within agent-run ML pipelines, providing concrete sabotage objectives and success criteria."
    },
    {
      "title": "Clean-Label Backdoor Attacks",
      "authors": "Alexander Turner; Dimitris Tsipras; Aleksander Madry",
      "year": 2019,
      "role": "Stealthy sabotage techniques",
      "relationship_sentence": "CTRL-ALT-DECEIT draws on clean-label backdoor methods to design subtle sabotage tasks (e.g., innocuous data/preprocessing changes) that evade naive review, enabling assessment of agents\u2019 ability to hide malicious changes."
    },
    {
      "title": "Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks",
      "authors": "Bolun Wang; Yuanshun Yao; Shawn Shan; Huiying Li; Bimal Viswanath; Haitao Zheng; Ben Y. Zhao",
      "year": 2019,
      "role": "Oversight and detection baseline",
      "relationship_sentence": "By referencing standard backdoor detection concepts from Neural Cleanse, the paper frames oversight mechanisms that agents may attempt to bypass or subvert, enabling explicit evaluation of oversight evasion."
    },
    {
      "title": "Backdoor Attacks on Federated Learning",
      "authors": "Arjun Bagdasaryan; Andreas Veit; Yiqing Hua; Deborah Estrin; Vitaly Shmatikov",
      "year": 2020,
      "role": "Supply-chain/model-replacement sabotage",
      "relationship_sentence": "The federated \u2018model replacement\u2019 paradigm informs sabotage scenarios where agents tamper with training procedures or artifacts to introduce hidden behavior while appearing to meet surface-level metrics."
    },
    {
      "title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training",
      "authors": "Evan Hubinger et al. (Anthropic)",
      "year": 2024,
      "role": "Deception and oversight subversion",
      "relationship_sentence": "CTRL-ALT-DECEIT adapts the sleeper-agent/deceptive-behavior evaluation lens to ML engineering, explicitly testing whether agents can sandbag, hide capabilities, and behave well under scrutiny while planting later-firing sabotage."
    },
    {
      "title": "Concrete Problems in AI Safety",
      "authors": "Dario Amodei; Chris Olah; Jacob Steinhardt; Paul Christiano; John Schulman; Dan Man\u00e9",
      "year": 2016,
      "role": "Safety framing: specification gaming and oversight failures",
      "relationship_sentence": "The benchmark\u2019s sandbagging and subversion tasks instantiate classic specification gaming and reward hacking concerns from Concrete Problems within realistic ML R&D workflows."
    }
  ],
  "synthesis_narrative": "CTRL-ALT-DECEIT\u2019s core contribution\u2014evaluating whether autonomous AI agents can sabotage machine-learning R&D pipelines, sandbag performance, and subvert oversight\u2014stands on three pillars of prior work. First, it builds operationally on MLE-Bench\u2019s realistic ML engineering substrate, reusing its repository-centric workflows, scoring, and agent harness to make sabotage evaluations faithful to real practice. Second, it translates concrete attack mechanics from the backdoor/poisoning literature into agent objectives: BadNets and clean-label backdoors provide canonical templates for implanting triggers and stealthy data/process manipulations, while Neural Cleanse supplies standard detection/oversight baselines the agents might evade. The federated learning \u2018model replacement\u2019 line further broadens the sabotage surface to supply-chain style tampering with training artifacts and procedures, mirroring how real-world ML systems can be compromised despite apparently passing metrics. Third, the paper imports alignment-and-deception evaluation paradigms from Sleeper Agents and the broader safety framing of Concrete Problems in AI Safety, converting abstract concerns about specification gaming, deceptive alignment, and oversight failure into measurable behaviors like sandbagging under evaluation and deferred-activation sabotage.\n\nThe novelty lies in fusing these strands into an end-to-end, agent-centric benchmark: the tasks and success metrics are grounded in security-realistic attacks, but embedded in the day-to-day operations of ML engineering. This makes it possible to quantify frontier agents\u2019 propensity and competence to harm user interests within the same environments where they might soon be deployed to automate AI R&D.",
  "analysis_timestamp": "2026-01-07T00:02:04.941218"
}
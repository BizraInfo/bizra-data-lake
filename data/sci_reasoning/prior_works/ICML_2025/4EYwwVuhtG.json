{
  "prior_works": [
    {
      "title": "Optimal Inference After Model Selection",
      "authors": "Will Fithian, Dennis Sun, Jonathan Taylor",
      "year": 2014,
      "role": "Foundational theory of selective inference",
      "relationship_sentence": "The paper\u2019s pipeline-level tests rely on the selective inference principle\u2014conditioning on the selection event to control selective Type I error\u2014introduced and formalized in Fithian et al. (2014)."
    },
    {
      "title": "Exact Post-Selection Inference with the Lasso",
      "authors": "Jason D. Lee, Dennis L. Sun, Yuekai Sun, Jonathan E. Taylor",
      "year": 2016,
      "role": "Algorithm-specific selective inference (lasso) via polyhedral conditioning",
      "relationship_sentence": "The authors extend the polyhedral selective inference machinery of Lee et al. to pipeline-selected features, leveraging truncated normal pivots and conditioning sets that underpin valid post-selection tests."
    },
    {
      "title": "Exact Post-Selection Inference for Forward Stepwise and Least Angle Regression",
      "authors": "Jonathan E. Taylor, Richard J. Lockhart, Ryan J. Tibshirani, Robert Tibshirani",
      "year": 2014,
      "role": "Selective inference for sequential feature selection procedures",
      "relationship_sentence": "Because the proposed framework targets pipelines that can include forward/greedy feature selectors, it builds on Taylor et al.\u2019s characterization of selection events and exact inference for stepwise/LAR procedures."
    },
    {
      "title": "Selective Inference After Cross-Validation",
      "authors": "Joshua R. Loftus, Jonathan E. Taylor",
      "year": 2015,
      "role": "Selective inference accommodating tuning and pipeline steps",
      "relationship_sentence": "Loftus and Taylor showed how to condition on cross-validation\u2013driven choices; the present work generalizes this idea by conditioning on multiple predefined components (imputation, outlier detection, selection) within a pipeline."
    },
    {
      "title": "Controlling the False Discovery Rate via Knockoffs",
      "authors": "Rina Foygel Barber, Emmanuel J. Cand\u00e8s",
      "year": 2015,
      "role": "Alternative framework for error control in feature selection",
      "relationship_sentence": "Knockoffs established rigorous error control for feature selection, motivating the need for validity guarantees; the new test provides complementary selective Type I error control specifically for pipeline-driven selections."
    },
    {
      "title": "High Dimensional Variable Selection",
      "authors": "Larry Wasserman, Kathryn Roeder",
      "year": 2009,
      "role": "Early post-selection validity via sample splitting (screen-and-clean)",
      "relationship_sentence": "The work highlights the necessity of adjusting inference for data-driven selection; the proposed selective-inference-based pipeline test advances beyond sample splitting by conditioning on rich selection events."
    }
  ],
  "synthesis_narrative": "This paper\u2019s key contribution\u2014valid statistical testing for feature selection pipelines composed of predefined components\u2014stands on the selective inference paradigm that conditions on the data-dependent selection. Fithian, Sun, and Taylor (2014) provided the unifying theoretical foundation: define and condition on the selection event to guarantee selective Type I error control. The concrete realization of this principle for widely used selectors came through the polyhedral approach of Lee et al. (2016) for the lasso, which yields tractable truncated-normal pivots after characterizing the selection region. Taylor et al. (2014) extended these ideas to sequential procedures like forward stepwise and LAR, giving tools the present work can invoke when a pipeline\u2019s final selector is not the lasso.\n\nPipelines often integrate tuning steps such as cross-validation, creating additional, nontrivial selection events. Loftus and Taylor (2015) demonstrated how to incorporate cross-validation into selective conditioning; the current paper generalizes this to broader, multi-stage preprocessing (missing-value imputation, outlier detection) and selection, composing their selection constraints to deliver valid inference for the final chosen features. Earlier approaches like Wasserman and Roeder (2009) validated post-selection claims via sample splitting, foreshadowing the need to adjust for adaptivity but at a cost in efficiency; selective conditioning recovers power by using all data with precise conditioning. Finally, Barber and Cand\u00e8s (2015) established an alternative, influential path\u2014knockoffs\u2014for rigorous error control in feature selection. While targeting a different criterion (FDR vs. selective Type I error), that line of work set benchmarks and use cases the present pipeline-aware selective tests directly speak to, positioning this paper as a principled, general solution for inference across complex feature selection pipelines.",
  "analysis_timestamp": "2026-01-07T00:29:42.075100"
}
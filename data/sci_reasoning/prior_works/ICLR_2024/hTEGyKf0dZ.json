{
  "prior_works": [
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "arxiv_id": "2203.02155",
      "role": "Foundation",
      "relationship_sentence": "This work established RLHF-aligned assistants with refusal behaviors, providing the alignment baseline whose robustness to downstream fine-tuning the current paper directly probes and undermines."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "arxiv_id": "2212.08073",
      "role": "Foundation",
      "relationship_sentence": "By codifying harmlessness principles and training refusal-centric guardrails, this paper defined the safety alignment regime that the current work shows can be erased with minimal adversarial fine-tuning."
    },
    {
      "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
      "authors": "Hugo Touvron et al.",
      "year": 2023,
      "arxiv_id": "2307.09288",
      "role": "Baseline",
      "relationship_sentence": "Llama 2\u2019s safety-aligned chat models and documented RLHF/safety tuning pipeline serve as the primary aligned baselines that the authors fine-tune and jailbreak with only a handful of examples."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "authors": "Andy Zou et al.",
      "year": 2023,
      "arxiv_id": "2307.15043",
      "role": "Gap Identification",
      "relationship_sentence": "This work showed inference-time prompt-based jailbreaks but left open whether training-time fine-tuning could systematically and cheaply neutralize guardrails, which the current paper demonstrates."
    },
    {
      "title": "Red Teaming Language Models with Language Models",
      "authors": "Ethan Perez et al.",
      "year": 2022,
      "arxiv_id": "2202.03286",
      "role": "Foundation",
      "relationship_sentence": "It introduced LLM-driven red teaming to elicit safety failures, a methodology the present work adopts to construct/evaluate harmful prompts while assessing post\u2013fine-tuning safety degradation."
    },
    {
      "title": "Backdoor Attacks on Pretrained Language Models",
      "authors": "Keita Kurita et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "By showing fine-tuning can implant trigger-based backdoors, this paper highlighted training-time vulnerability that the current work generalizes by removing triggers and revealing broad safety collapse\u2014even from benign fine-tuning."
    }
  ],
  "synthesis_narrative": "Instruction-following alignment via human feedback established assistants that refuse unsafe queries and adhere to user intent, with InstructGPT demonstrating RLHF as a practical recipe for steering outputs. Constitutional AI further codified harmlessness through explicit principles and AI feedback, strengthening refusal behavior as a core safety mechanism. Llama 2 documented an end-to-end safety pipeline\u2014combining supervised instruction tuning, RLHF, and safety evaluations\u2014resulting in widely used, safety-aligned chat models. Concurrently, universal jailbreak work showed that carefully engineered prompts could bypass these guardrails at inference time, revealing fragility in alignment but focusing solely on prompt-level attacks. LLM-driven red teaming introduced systematic methods to generate adversarial prompts and uncover safety failure modes, enabling scalable evaluation of harms. Earlier in NLP, backdoor research established that small amounts of targeted fine-tuning data can reliably override pretrained behavior, albeit typically via triggers and narrow behaviors rather than general safety collapse.\nTaken together, these works expose a vulnerability frontier: alignment methods cultivate refusal but do not guarantee robustness under downstream modification; jailbreaks show evasion at inference time; and backdoors reveal that training-time changes can powerfully redirect model behavior. The natural next step is to test whether minimal user-controlled fine-tuning can broadly neutralize refusal safeguards\u2014and whether even well-intentioned fine-tuning erodes safety. By combining red teaming with practical fine-tuning setups on aligned baselines, the present work shows that a few adversarial examples (and even benign task data) can systematically undo safety alignment, revealing a critical gap in current alignment and deployment practices.",
  "target_paper": {
    "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
    "authors": "Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, Peter Henderson",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "AI Safety, Large Language Models, Fine-tuning, Jailbreaking, AI Alignment",
    "abstract": "Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open-source release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on customized datasets accelerate this trend. But, what are the safety costs associated with such customized fine-tuning? While existing safety alignment techniques restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply",
    "openreview_id": "hTEGyKf0dZ",
    "forum_id": "hTEGyKf0dZ"
  },
  "analysis_timestamp": "2026-01-06T07:44:39.687627"
}
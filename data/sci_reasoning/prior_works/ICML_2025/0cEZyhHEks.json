{
  "prior_works": [
    {
      "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
      "authors": "Mor Geva et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "Established the notion that parametric factual knowledge is stored and retrieved via MLP \u2018memory\u2019 mechanisms, providing the conceptual groundwork for distinguishing parametric memory from contextual information that JuICE explicitly steers between."
    },
    {
      "title": "In-context Learning and Induction Heads",
      "authors": "Catherine Olsson et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "Identified induction (context) heads and head-level circuit roles in transformers, directly informing the memory-head vs context-head framing that JuICE revisits and refines with its superposition finding."
    },
    {
      "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
      "authors": "Elena Voita et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "Argued for head specialization and pruning, an assumption of functional exclusivity that the present work challenges by revealing superpositioned heads serving both memory and context roles."
    },
    {
      "title": "Toy Models of Superposition in Neural Networks",
      "authors": "Nelson Elhage et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "Introduced and analyzed superposition as a fundamental representational phenomenon, directly inspiring JuICE\u2019s hypothesis and empirical discovery that influential attention heads can carry both contextual and parametric signals simultaneously."
    },
    {
      "title": "Locating and Editing Factual Knowledge in GPT",
      "authors": "Kevin Meng et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "Demonstrated parametric knowledge localization and editing but requires model modification; JuICE addresses this gap by providing a test-time, no-finetuning intervention that selectively favors parametric memory or context."
    },
    {
      "title": "DoLa: Decoding by Contrasting Language Models Improves Factuality",
      "authors": "Luo et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "Pioneered a dual-run, test-time decoding strategy to improve factuality; JuICE builds on the dual-run idea but targets attention-head-level interventions to resolve memory\u2013context conflicts."
    },
    {
      "title": "Are Sixteen Heads Really Better than One?",
      "authors": "Paul Michel et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Showed head importance can be probed via ablation, a technique JuICE extends by identifying a set of reliable heads whose targeted intervention drives the model toward either parametric beliefs or contextual knowledge."
    }
  ],
  "synthesis_narrative": "The core of Taming Knowledge Conflicts in Language Models rests on re-examining how transformers balance in-weights (parametric) memory with in-context information and intervening at test time to steer that balance. Foundationally, Geva et al. established that MLP layers function as key\u2013value memories, formalizing the parametric memory side of the dichotomy. Complementing this, Olsson et al. characterized induction (context) heads, grounding the view that specific attention heads propagate contextual evidence. Earlier interpretability work\u2014especially Voita et al.\u2014popularized the assumption of head specialization, often treating heads as single-purpose and pruneable, an assumption the present paper directly challenges. Anthropic\u2019s theory of superposition (Elhage et al.) provided the conceptual lens for polysemantic feature sharing; this paper translates that insight to attention heads, uncovering superposition between contextual and memory signals within the very heads thought to be exclusive. On the intervention front, ROME (Meng et al.) localized and edited parametric knowledge but required model modification, motivating a no-finetuning, test-time approach. DoLa\u2019s dual-run decoding demonstrated the efficacy of contrastive, test-time strategies, which JuICE adapts to the attention mechanism to disambiguate superposed signals. Finally, methods for identifying important heads (Michel et al.) inform JuICE\u2019s selection of reliable heads, enabling principled head-level interventions. Together, these works directly shape JuICE\u2019s key innovation: a dual-run, attention-head-targeted test-time method that resolves memory\u2013context conflicts by exploiting and counteracting head-level superposition.",
  "analysis_timestamp": "2026-01-06T23:07:19.621315"
}
{
  "prior_works": [
    {
      "title": "Learning to summarize with human feedback",
      "authors": "Nisan Stiennon et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "Established the modern RLHF pipeline (preference data -> reward model -> RL fine-tuning) that this paper scrutinizes for its downstream impact on trustworthiness."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "role": "Baseline",
      "relationship_sentence": "Provides the canonical general-purpose preference alignment setup and models that the paper takes as the primary baseline for evaluating whether RLHF reliably improves trustworthiness."
    },
    {
      "title": "Understanding Black-box Predictions via Influence Functions",
      "authors": "Pang Wei Koh et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Introduces influence functions for data attribution, whose core idea this work adapts to the RLHF setting to trace how specific preference data points affect trustworthiness outcomes."
    },
    {
      "title": "Estimating Training Data Influence by Tracing Gradient Descent (TracIn)",
      "authors": "Danish Pruthi et al.",
      "year": 2020,
      "role": "Extension",
      "relationship_sentence": "Provides an efficient influence-estimation method that the paper leverages and adapts to make influence-based data attribution tractable for RLHF-aligned LLMs."
    },
    {
      "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
      "authors": "Stephanie Lin et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "Defines the truthfulness evaluation paradigm used as one of the paper\u2019s five trustworthiness verticals to assess the effect of RLHF."
    },
    {
      "title": "Extracting Training Data from Large Language Models",
      "authors": "Nicholas Carlini et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "Provides the core privacy leakage problem formulation and measurements that underpin the paper\u2019s privacy vertical when assessing RLHF\u2019s trustworthiness impact."
    },
    {
      "title": "Discovering Language Model Behaviors with Model-Written Evaluations",
      "authors": "Ethan Perez et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "Documents undesirable behaviors (e.g., sycophancy) in RLHF-trained models, motivating the paper\u2019s systematic examination of how general-purpose preference alignment can have adverse trustworthiness effects."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contributions\u2014systematically auditing trustworthiness under general-purpose RLHF and attributing trust outcomes to specific preference data\u2014stand on two pillars: the RLHF pipeline and data attribution via influence functions. Stiennon et al. (2020) and Ouyang et al. (2022) established and popularized the modern RLHF recipe (preference data, reward modeling, and RL fine-tuning on general tasks), creating both the methodological foundation and the central baseline this work interrogates. Concurrently, Koh and Liang (2017) introduced influence functions for tracing the impact of individual training points, and Pruthi et al. (2020) proposed TracIn to make such attribution efficient at modern scales\u2014ideas this paper directly adapts to the RLHF regime to explain how particular preference samples shape trust-related behaviors.\nTo evaluate trustworthiness, the study relies on established, domain-defining benchmarks for key verticals: TruthfulQA (Lin et al., 2021) for truthfulness and Carlini et al. (2021) for privacy leakage, grounding its assessments in widely accepted formulations. Finally, Perez et al. (2022) identified that RLHF can induce undesirable behaviors such as sycophancy, highlighting a critical gap: alignment via general-purpose preferences may not uniformly improve trust and can even harm it. This gap motivates the paper\u2019s comprehensive empirical audit and its data attribution technique, which together reveal when and why RLHF fails to guarantee toxicity reduction, bias mitigation, ethical consistency, truthfulness, and privacy preservation.",
  "analysis_timestamp": "2026-01-06T23:09:26.591063"
}
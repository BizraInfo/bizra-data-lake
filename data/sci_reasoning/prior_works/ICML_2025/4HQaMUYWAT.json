{
  "prior_works": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "The paper\u2019s theory explicitly leverages the small-initialization (linearized/NTK) training dynamics to explain why tiny scales bias models toward structured, algorithmic solutions rather than rote memorization."
    },
    {
      "title": "On the Global Convergence of Gradient Descent for Over-Parameterized Models: A Dynamical System Viewpoint",
      "authors": "L\u00e9na\u00efc Chizat et al.",
      "year": 2018,
      "role": "Extension",
      "relationship_sentence": "This work formalized the lazy-vs-feature-learning regimes governed by initialization scale; the current paper extends this lens to transformers, linking small-scale (lazy-like) training to reasoning bias and large-scale to memorization."
    },
    {
      "title": "Training Behavior of Deep Neural Networks: The Frequency Principle (F-Principle)",
      "authors": "Zhi-Qin John Xu et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "The frequency-principle framework underpins the paper\u2019s training-dynamics explanation, connecting small-norm initialization to early learning of low-frequency/global structure that manifests as reasoning behavior."
    },
    {
      "title": "On the Spectral Bias of Neural Networks",
      "authors": "Nazmul Karim Rahaman et al.",
      "year": 2019,
      "role": "Inspiration",
      "relationship_sentence": "By showing that neural nets learn low-frequency components before high-frequency ones, this work directly motivates the paper\u2019s use of controlled \u2018anchor\u2019 target functions to probe reasoning versus memorization under different init scales."
    },
    {
      "title": "Transformer Feed-Forward Layers Are Key-Value Memories",
      "authors": "Mor Geva et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "Identifying FFNs as a learned associative memory directly informs the paper\u2019s claim that larger initializations favor memorization pathways (FFN/embedding-driven) over reasoning."
    },
    {
      "title": "In-Context Learning and Induction Heads",
      "authors": "Catherine Olsson et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "The discovery of induction-head circuits provides the mechanism the paper tests for: small initialization expedites the emergence of attention-based algorithmic reasoning relative to memory-based solutions."
    },
    {
      "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
      "authors": "A. Power et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "Grokking exposed a memorization-to-reasoning transition but did not explain the role of initialization; the present work fills this gap by showing initialization scale systematically controls that preference and dynamics in LLMs."
    }
  ],
  "synthesis_narrative": "The core innovation\u2014showing that initialization scale steers LLMs toward reasoning versus memorization and explaining it via early training dynamics\u2014rests on two intertwined theoretical lineages and a mechanistic account of transformer internals. From the theory side, the NTK framework (Jacot et al.) and the lazy/feature-learning dichotomy governed by initialization (Chizat & Bach) provide the central lens: small initialization keeps training near the linearized regime, whereas larger scales encourage stronger feature evolution. This connects directly to frequency-based accounts of learning (Xu\u2019s F-Principle and Rahaman\u2019s spectral bias), which predict that networks initialized with small norms preferentially learn low-frequency, global structure before high-frequency details, mirroring the paper\u2019s definition of \u201creasoning\u201d versus \u201cmemorization.\u201d On the mechanistic side, transformer interpretability works anchor the pathways: Geva et al. show FFNs and embeddings act as key\u2013value memories, while Olsson et al. identify induction heads as attention circuits for algorithmic pattern-following. These map naturally onto the paper\u2019s component-wise analysis, attributing memorization to FFN/embedding mechanisms and reasoning to self-attention circuits. Finally, the grokking literature (Power et al.) highlighted a memorization-to-generalization transition but left the role of initialization unresolved; this paper resolves that gap by demonstrating, with anchor functions and real tasks, that initialization scale is a primary control knob and by giving a training-dynamics theory that ties component behavior to the observed bias.",
  "analysis_timestamp": "2026-01-06T23:07:19.633569"
}
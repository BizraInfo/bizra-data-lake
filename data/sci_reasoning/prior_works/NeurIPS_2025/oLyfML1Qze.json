{
  "prior_works": [
    {
      "title": "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering (ChebNet)",
      "authors": "Micha\u00ebl Defferrard, Xavier Bresson, Pierre Vandergheynst",
      "year": 2016,
      "role": "Foundational spectral GNN using Chebyshev polynomial filters",
      "relationship_sentence": "Return of ChebNet directly revisits and builds upon ChebNet\u2019s Chebyshev polynomial parameterization of Laplacian filters, re-evaluating its long-range modeling capacity and analyzing/stabilizing its high-order polynomial behavior."
    },
    {
      "title": "Neural Message Passing for Quantum Chemistry",
      "authors": "Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl",
      "year": 2017,
      "role": "Defines the Message Passing Neural Network (MPNN) paradigm",
      "relationship_sentence": "The paper positions ChebNet as an alternative to the locally constrained MPNN framework formalized here, targeting the long-range dependency limitations that MPNNs inherently face."
    },
    {
      "title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank (APPNP)",
      "authors": "Johannes Klicpera, Aleksandar Bojchevski, Stephan G\u00fcnnemann",
      "year": 2019,
      "role": "High-order diffusion for long-range information with stable propagation weights",
      "relationship_sentence": "APPNP\u2019s decoupled, high-order propagation via PPR informs the paper\u2019s emphasis that polynomial/diffusion-based propagation can capture long-range effects efficiently and highlights design choices for stability in deep propagation."
    },
    {
      "title": "On the Bottleneck of Graph Neural Networks and its Practical Implications",
      "authors": "Uri Alon, Eran Yahav",
      "year": 2021,
      "role": "Diagnoses oversquashing as a core cause of long-range information loss in MPNNs",
      "relationship_sentence": "This work motivates the paper\u2019s focus on architectures (like ChebNet) that mitigate long-range information bottlenecks without expensive rewiring or transformers."
    },
    {
      "title": "Adaptive Universal Generalized PageRank Graph Neural Network (GPR-GNN)",
      "authors": "Eli Chien, Pan Li, Olgica Milenkovic",
      "year": 2021,
      "role": "Learnable polynomial filters over the Laplacian for flexible long-range mixing",
      "relationship_sentence": "GPR-GNN\u2019s learnable polynomial viewpoint directly relates to ChebNet\u2019s filter design, reinforcing that polynomial spectra can be tuned for distant interactions and underscoring the need to manage numerical/stability issues in high orders."
    },
    {
      "title": "Understanding Over-Squashing in Graph Neural Networks Through the Lens of Graph Curvature (Rewiring)",
      "authors": "Jake Topping, Francesco Di Giovanni, Benjamin Rosman Chamberlain, Federico Monti, Michael M. Bronstein",
      "year": 2022,
      "role": "Rewiring-based remedy for long-range bottlenecks",
      "relationship_sentence": "By proposing structural rewiring to alleviate oversquashing, this line of work provides a key baseline and contrast for the paper\u2019s claim that ChebNet achieves long-range communication without modifying the graph topology or incurring heavy costs."
    },
    {
      "title": "Long Range Graph Benchmark (LRGB)",
      "authors": "Vijay Prakash Dwivedi, Chaitanya K. Joshi, Martin Rampasek",
      "year": 2022,
      "role": "Benchmark suite emphasizing long-range dependencies",
      "relationship_sentence": "LRGB shapes the paper\u2019s experimental focus and evidences that ChebNet\u2019s polynomial filters yield competitive performance on tasks explicitly designed to stress long-range reasoning."
    }
  ],
  "synthesis_narrative": "The core innovation of Return of ChebNet is to re-establish and improve a classical spectral GNN\u2014ChebNet\u2014as an efficient, scalable solution for long-range graph reasoning, while diagnosing and stabilizing its high-order polynomial behavior. This trajectory starts with Defferrard et al. (ChebNet), which introduced Chebyshev polynomial filters for localized, fast spectral convolutions. Gilmer et al.\u2019s MPNN formalism subsequently drove widespread adoption of local message passing, whose locality inspired the present paper\u2019s critique regarding long-range limitations. Alon and Yahav\u2019s bottleneck analysis provided a precise lens\u2014oversquashing\u2014to understand why MPNNs struggle with distant interactions, catalyzing interest in alternatives that mix information globally.\n\nTwo strands demonstrated that polynomial/diffusion propagation can systematically extend receptive fields. APPNP showed that decoupled, high-order Personalized PageRank diffusion can propagate signals stably and efficiently, while GPR-GNN framed long-range mixing as learning polynomial filters of the Laplacian, closely aligned with ChebNet\u2019s spectral design. Both highlight the promise of polynomial approaches and surface practical issues (e.g., coefficient design, numerical stability) that this paper tackles for high-order Chebyshev expansions.\n\nAs a contrasting remedy, curvature-based rewiring (Topping et al.) alleviates bottlenecks by modifying graph topology, setting a baseline the paper aims to match or surpass without structural changes or transformer-level costs. Finally, the Long Range Graph Benchmark (LRGB) crystallizes the evaluation setting, enabling the authors to demonstrate that a stabilized, revisited ChebNet provides competitive long-range performance with strong scalability, thereby bridging a gap between spectral elegance and practical long-range efficacy.",
  "analysis_timestamp": "2026-01-07T00:02:04.931993"
}
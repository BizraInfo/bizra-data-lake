{
  "prior_works": [
    {
      "title": "DenseCap: Fully Convolutional Localization Networks for Dense Captioning",
      "authors": "Justin Johnson et al.",
      "year": 2016,
      "arxiv_id": "1511.07571",
      "role": "Foundation",
      "relationship_sentence": "Introduced region-level dense captioning, providing the core box-to-text paradigm that is extended here from static regions to spatio-temporal object trajectories."
    },
    {
      "title": "Dense-Captioning Events in Videos",
      "authors": "Ranjay Krishna et al.",
      "year": 2017,
      "arxiv_id": "1705.00754",
      "role": "Foundation",
      "relationship_sentence": "Defined dense video captioning by localizing and describing events, establishing the video-level dense captioning framework whose event-centric scope is refined here to object trajectories and trajectory-aware evaluation."
    },
    {
      "title": "Tracking Objects as Points (CenterTrack)",
      "authors": "Xingyi Zhou et al.",
      "year": 2020,
      "arxiv_id": "2004.01177",
      "role": "Extension",
      "relationship_sentence": "Provides a unified detection-and-tracking architecture that the current work adapts to produce temporally coherent object tracks while adding a captioning head for language generation."
    },
    {
      "title": "MDETR: Modulated Detection for End-to-End Multi-Modal Understanding",
      "authors": "Rohit Goyal et al.",
      "year": 2021,
      "arxiv_id": "2104.12763",
      "role": "Inspiration",
      "relationship_sentence": "Pioneered training detectors with a mixture of disjoint language\u2013region supervision tasks, directly inspiring the paper\u2019s disjoint-task pretraining to align objects and text."
    },
    {
      "title": "GLIP: Grounded Language-Image Pre-training",
      "authors": "Li et al.",
      "year": 2022,
      "arxiv_id": "2112.03857",
      "role": "Related Problem",
      "relationship_sentence": "Demonstrated large-scale grounded pretraining across heterogeneous datasets for open-vocabulary detection, motivating the use of diverse, complementary supervision to learn object\u2013language alignment."
    },
    {
      "title": "OVTrack: Open-Vocabulary Multiple Object Tracking",
      "authors": "Mier et al.",
      "year": 2023,
      "arxiv_id": "unknown",
      "role": "Gap Identification",
      "relationship_sentence": "Showed language-aware MOT using CLIP semantics but is limited to category prompts and lacks generative descriptions, highlighting the need for per-trajectory caption generation and end-to-end learning."
    },
    {
      "title": "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection",
      "authors": "Shilong Liu et al.",
      "year": 2023,
      "arxiv_id": "unknown",
      "role": "Baseline",
      "relationship_sentence": "Serves as the core open-set detector in strong multi-stage detect\u2013track\u2013caption pipelines, whose lack of joint temporal\u2013linguistic modeling this paper\u2019s unified approach is designed to surpass."
    }
  ],
  "synthesis_narrative": "Region-level dense captioning established that localized visual content can be directly translated into natural language, with DenseCap introducing a box-to-text formulation that tied region proposals to descriptive captions. Dense video captioning then extended dense captioning to the temporal domain by localizing and describing events, but centered its scope on event segments rather than persistent object entities. In parallel, CenterTrack unified detection and tracking by treating objects as points and regressing associations across frames, yielding simple and temporally coherent track formation. For grounding language to regions, MDETR showed that mixing disjoint supervision\u2014referring expressions, phrase grounding, and related tasks\u2014can train a detector to align vision and language end-to-end, while GLIP scaled grounded pretraining across heterogeneous datasets to enable open-vocabulary object recognition from captions. Language-aware tracking like OVTrack connected textual semantics to multi-object tracking, but operated with prompt-like category text rather than producing rich descriptions. Strong open-vocabulary detectors such as Grounding DINO became the de facto backbone of multi-stage detect\u2013track\u2013caption pipelines, yet these pipelines lacked a single model that jointly reasons over time and language. Together, these works revealed a gap: object-centric, temporally consistent trajectories with natural-language descriptions trained from complementary, disjoint supervision. The present work emerges as a natural synthesis, adapting unified detection\u2013tracking architectures to generate captions per trajectory and adopting MDETR/GLIP-style mixed supervision at scale, thereby surpassing multi-stage pipelines with an end-to-end, temporally coherent, language-grounded model.",
  "target_paper": {
    "title": "Dense Video Object Captioning from Disjoint Supervision",
    "authors": "Xingyi Zhou, Anurag Arnab, Chen Sun, Cordelia Schmid",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "object captioning, video, tracking",
    "abstract": "We propose a new task and model for dense video object captioning -- detecting, tracking and captioning trajectories of objects in a video. This task unifies spatial and temporal localization in video, whilst also requiring fine-grained visual understanding that is best described by natural language. We propose a unified model, and demonstrate how our end-to-end approach is more accurate and temporally coherent than a multi-stage pipeline combining state-of-the-art detection, tracking, and captioning models. Moreover, we propose a training strategy based on a mixture of disjoint tasks, which allows us to leverage diverse, large-scale datasets which supervise different parts of our model. Although each pretraining task only provides weak supervision, they are complementary and, when combined, result in noteworthy zero-shot ability and serve as strong initialization for additional finetuning to further improve accuracy. We carefully design new metrics capturing all components of our task",
    "openreview_id": "auZZ2gN0ZN",
    "forum_id": "auZZ2gN0ZN"
  },
  "analysis_timestamp": "2026-01-06T19:54:38.816014"
}
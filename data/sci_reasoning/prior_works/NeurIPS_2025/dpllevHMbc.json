{
  "prior_works": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Kernel/linearization foundation for training dynamics",
      "relationship_sentence": "By reducing wide-network training to kernel regression with mode-wise exponential dynamics, NTK provided the linear-dynamics template that the paper leverages to express loss trajectories and to isolate how learning-rate schedules reparameterize time."
    },
    {
      "title": "Spectrum-dependent learning curves in kernel regression",
      "authors": "Charles H. Bordelon, Deniz Canatar, Cengiz Pehlevan",
      "year": 2020,
      "role": "Spectral decomposition linking eigenvalue decay to learning curves",
      "relationship_sentence": "This work\u2019s mode-wise analysis showed how power-law kernel spectra control convergence across frequencies, directly enabling the paper\u2019s assumption of power-law kernels and its derivation of scaling exponents for the full loss trajectory."
    },
    {
      "title": "Nonparametric stochastic approximation with large step-sizes",
      "authors": "Aymeric Dieuleveut, Francis R. Bach",
      "year": 2016,
      "role": "SGD-in-RKHS theory with explicit dependence on step-size schedules",
      "relationship_sentence": "Their RKHS analysis expresses bias/variance and convergence in terms of cumulative sums of step sizes, which underpins the paper\u2019s intrinsic-time variable (effective cumulative learning rate) and the clean separation of schedule effects from spectral effects."
    },
    {
      "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
      "authors": "Martin Mandt, Matthew D. Hoffman, David M. Blei",
      "year": 2017,
      "role": "Continuous-time/SDE viewpoint for SGD and role of LR schedules",
      "relationship_sentence": "By casting SGD as a continuous-time process where learning rates set the effective time/temperature, this work motivates the paper\u2019s reparameterization of training progress via intrinsic time and clarifies how schedules shape the dynamics functionally."
    },
    {
      "title": "Don\u2019t Decay the Learning Rate, Increase the Batch Size",
      "authors": "Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, Quoc V. Le",
      "year": 2018,
      "role": "Equivalence principles linking learning rate, batch size, and progress",
      "relationship_sentence": "Their notion of an effective noise/step scale suggests that training progress accumulates through the integrated step size, aligning with the paper\u2019s intrinsic-time view and its schedule-agnostic functional law."
    },
    {
      "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",
      "authors": "Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, et al.",
      "year": 2017,
      "role": "Practice-defining learning-rate schedules (warmup and decay)",
      "relationship_sentence": "The warmup\u2013stable\u2013decay schedule popularized here directly motivates one of the paper\u2019s instantiated LRS cases and the need to predict full loss trajectories under such piecewise schedules."
    },
    {
      "title": "Scaling Laws for Neural Language Models",
      "authors": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, et al.",
      "year": 2020,
      "role": "Empirical scaling laws that the paper generalizes to trajectories",
      "relationship_sentence": "Providing the canonical final-loss scaling laws, this work motivates extending scaling from endpoints to entire loss dynamics and probing how learning-rate schedules modulate those laws."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014a Functional Scaling Law (FSL) describing the entire loss trajectory and isolating the effect of arbitrary learning-rate schedules via an intrinsic-time reparameterization\u2014emerges by fusing three strands of prior work. First, the NTK framework (Jacot et al.) establishes that wide-network training follows linear kernel dynamics with mode-wise contraction rates set by kernel eigenvalues. Building on this, spectral analyses in kernel regression (Bordelon, Canatar, Pehlevan) show that power-law eigenvalue decay produces predictable convergence profiles across modes, furnishing the spectral backbone and scaling exponents that the present paper aggregates into a functional law. Second, SGD-in-RKHS theory (Dieuleveut & Bach) explicitly characterizes how step-size sequences enter convergence through cumulative sums and related functionals, directly inspiring the paper\u2019s intrinsic-time variable that collapses disparate schedules into a unified progress measure and enables a schedule-separable representation of the loss. Complementary continuous-time viewpoints of SGD (Mandt, Hoffman, Blei) reinforce this time-reparameterization perspective by linking learning rate to effective temperature and temporal scaling. Third, practical scheduling advances\u2014large-batch training with warmup (Goyal et al.) and LR\u2013batch-size equivalences (Smith & Le)\u2014motivate analyzing realistic piecewise schedules such as warmup\u2013stable\u2013decay within the same functional framework. Finally, empirical scaling laws for final losses in language models (Kaplan et al.) supply the impetus to move beyond endpoints to a trajectory-level law, with the new FSL unifying spectral structure and schedule design into a single predictive description of loss dynamics.",
  "analysis_timestamp": "2026-01-07T00:29:42.052405"
}
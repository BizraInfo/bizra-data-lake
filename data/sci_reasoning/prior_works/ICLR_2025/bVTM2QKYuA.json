{
  "prior_works": [
    {
      "title": "Toy Models of Superposition in Neural Networks",
      "authors": "Nelson Elhage et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "This paper articulated the linear representation hypothesis and showed how features appear as (approximately) linear directions subject to superposition, which the current work formalizes and extends to non-contrastive features by representing them as vectors and categories as polytopes."
    },
    {
      "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
      "authors": "Been Kim et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "TCAV operationalized concepts as vectors learned from examples, directly inspiring this paper\u2019s formal treatment of features-as-vectors and its move from contrastive directions to concept vectors with no natural negative class."
    },
    {
      "title": "Null It Out: Guarding Protected Attributes in Text Classifiers by Iterative Nullspace Projection",
      "authors": "Shauli Ravfogel et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "INLP demonstrated that binary attributes reside in linear subspaces but relies on natural contrasts and linear nullification; the present work addresses this limitation by defining feature vectors for non-contrastive concepts and extending the geometry to categorical polytopes."
    },
    {
      "title": "Causal Mediation Analysis for Interpreting Neural NLP Models",
      "authors": "Jesse Vig et al.",
      "year": 2020,
      "role": "Inspiration",
      "relationship_sentence": "By linking model internals to outputs via intervention-based causal mediation, this work motivated the current paper\u2019s \u2018causal inner product\u2019 that ties geometric feature vectors to causal influence on model behavior."
    },
    {
      "title": "Locating and Editing Factual Knowledge in GPT",
      "authors": "Kevin Meng et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "The causal tracing/activation-patching methodology from this paper directly informs how the current work estimates the causal effect of traversing a feature vector in representation space, operationalizing its causal inner product."
    },
    {
      "title": "Poincar\u00e9 Embeddings for Learning Hierarchical Representations",
      "authors": "Maximilian Nickel et al.",
      "year": 2017,
      "role": "Related Problem",
      "relationship_sentence": "This work established a precise geometry\u2013hierarchy correspondence by embedding trees in hyperbolic space; the current paper proves an analogous hierarchy\u2013geometry link that emerges in LLM representations, but within a linear-vector/polytope framework."
    },
    {
      "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
      "authors": "Tolga Bolukbasi et al.",
      "year": 2016,
      "role": "Baseline",
      "relationship_sentence": "By identifying binary attributes (e.g., gender) as linear directions in embedding spaces, this paper provided the canonical contrastive-direction baseline that the present work generalizes beyond to handle non-contrastive features and categorical structure."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014formalizing the linear representation hypothesis beyond contrastive directions to features-as-vectors, categorical concepts as polytopes, and a provable link between hierarchy and geometry\u2014rests on a precise lineage. The linear-feature view and superposition picture crystallized by Elhage et al. provided the foundational hypothesis that semantic content is linearly encoded, but left open how to treat non-contrastive features. Kim et al.\u2019s TCAV showed how to instantiate concepts as vectors from examples, directly inspiring the move from directions to feature vectors that need no natural negative. Prior linear approaches like INLP (Ravfogel et al.) demonstrated that protected attributes live in linear subspaces, yet their dependence on binary contrasts highlighted the exact gap this paper addresses: representing features without opposites and modeling multiway categories. To connect geometry to behavior, the paper builds on interventionist causal methods: Vig et al.\u2019s causal mediation analysis motivates a causal lens on internal representations, while Meng et al.\u2019s causal tracing/activation patching provides the concrete intervention machinery that the authors adapt into a \u2018causal inner product\u2019 aligning geometric vectors with causal influence. Finally, work on hierarchical geometries such as Poincar\u00e9 embeddings (Nickel & Kiela) established that hierarchical structure has a characteristic geometric signature; the present paper proves that an analogous geometry\u2013hierarchy correspondence emerges in LLM representation spaces within a linear/polytope framework, thereby unifying concept vectors, category geometry, and causal effects.",
  "analysis_timestamp": "2026-01-06T23:09:26.624032"
}
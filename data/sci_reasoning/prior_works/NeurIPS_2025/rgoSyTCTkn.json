{
  "prior_works": [
    {
      "title": "Constrained Markov Decision Processes",
      "authors": "Eitan Altman",
      "year": 1999,
      "role": "Foundational CMDP theory",
      "relationship_sentence": "Provides the formal CMDP framework (occupancy measures, Lagrangian/LP formulations, episodic constraints) that the paper builds upon before introducing linear function approximation and regret guarantees."
    },
    {
      "title": "Exploration\u2013Exploitation in Constrained MDPs",
      "authors": "Yehoram Efroni, Shie Mannor, Matteo Pirotta",
      "year": 2020,
      "role": "Tabular zero-violation CMDP learning",
      "relationship_sentence": "Establishes optimistic exploration with per-episode zero constraint violation and near-optimal regret in the tabular case; the new paper lifts this episode-wise safety paradigm to the linear function approximation setting."
    },
    {
      "title": "Minimax Regret Bounds for Reinforcement Learning",
      "authors": "Mohammad Gheshlaghi Azar, Ian Osband, R\u00e9mi Munos",
      "year": 2017,
      "role": "Optimism/UCBVI template for episodic RL",
      "relationship_sentence": "Introduces the UCBVI optimism framework and confidence-bonus dynamic programming that the new work adapts to handle safety constraints while preserving sublinear regret."
    },
    {
      "title": "Provably Efficient Reinforcement Learning with Linear Function Approximation",
      "authors": "Chi Jin, Zhuoran Yang, Zhaoran Wang",
      "year": 2020,
      "role": "LSVI-UCB and linear MDP methodology",
      "relationship_sentence": "Develops least-squares value iteration with UCB bonuses for linear MDPs, providing the estimation and optimism machinery the paper extends to constrained (safe) settings with zero per-episode violation."
    },
    {
      "title": "Sample-Efficient Reinforcement Learning in Constrained Markov Decision Processes",
      "authors": "Rakesh Kalagarla, Aditya Mahajan Nayyar, Rahul Jain",
      "year": 2021,
      "role": "Tabular CMDP optimism and feasibility certificates",
      "relationship_sentence": "Analyzes optimistic learning in tabular CMDPs and techniques for handling feasibility/constraint satisfaction, which the new work generalizes to feature-based models while keeping safety per episode."
    },
    {
      "title": "Lyapunov-based Safe Policy Optimization for Continuous Control",
      "authors": "Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, Mohammad Ghavamzadeh",
      "year": 2018,
      "role": "Safety mechanisms in constrained RL",
      "relationship_sentence": "Introduces Lyapunov-based constraint handling for safe policy updates, informing the design of safety-preserving updates; the new work attains stronger, episode-wise zero-violation guarantees with theoretical regret in linear CMDPs."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014provably efficient learning in constrained MDPs with linear function approximation under episode-wise zero constraint violations\u2014sits at the intersection of CMDP safety, optimism-based exploration, and linear MDP estimation. Altman\u2019s monograph established the CMDP formalism and solution structure (occupancy measures and Lagrangian/LP views), providing the bedrock upon which safety constraints are defined and analyzed. In the tabular regime, Efroni\u2013Mannor\u2013Pirotta showed that optimistic exploration can achieve near-optimal regret with strict per-episode feasibility, crystallizing the notion of zero-violation safety that this work aims to retain beyond the tabular case. The optimism and dynamic-programming backbone is inherited from Azar\u2013Osband\u2013Munos (UCBVI), whose confidence-bonus value iteration template underlies many modern regret analyses.\n\nOn the function-approximation side, Jin\u2013Yang\u2013Wang\u2019s LSVI-UCB for linear MDPs supplies the statistical machinery\u2014least-squares value estimation, linear confidence sets, and optimistic planning\u2014that enables regret scaling with feature dimension rather than state space size. Kalagarla\u2013Nayyar\u2013Jain further refine optimistic learning and feasibility handling in tabular CMDPs, offering tools to reason about feasibility sets and certificates that are adapted here to the linear setting. Finally, Lyapunov-based safe policy optimization (Chow et al.) informs mechanisms for maintaining safety during learning; the present work strengthens these ideas by guaranteeing episode-wise zero violation alongside \u00d5(\u221aK) regret and computational efficiency polynomial in problem-dependent parameters. Collectively, these works furnish the safety specification, optimistic exploration framework, and linear approximation toolkit that the paper integrates to close the gap for CMDPs with function approximation.",
  "analysis_timestamp": "2026-01-07T00:21:32.302583"
}
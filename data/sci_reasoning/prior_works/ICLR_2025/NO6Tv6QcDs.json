{
  "prior_works": [
    {
      "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
      "authors": "Liu et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work established the LLM-as-a-judge paradigm for open-ended generation by showing high human correlation from a strong model judge, directly motivating the setting where a model replaces costly ground-truth labels."
    },
    {
      "title": "Large Language Models Are State-of-the-Art Evaluators of Translation Quality",
      "authors": "Kocmi and Federmann",
      "year": 2023,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "By demonstrating that LLMs can serve as competitive automatic evaluators in MT, this paper concretized the promise of scalable evaluation with a model judge that the present work scrutinizes theoretically."
    },
    {
      "title": "MT-Bench and Chatbot Arena: Evaluating Large Language Models with Pairwise Comparisons",
      "authors": "Lianmin Zheng et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This framework popularized using a strong LLM as a pairwise judge at the LLM frontier and surfaced practical concerns like self-preference, defining the high-stakes evaluation regime analyzed in this paper."
    },
    {
      "title": "Maximum likelihood estimation of observer error-rates using the EM algorithm",
      "authors": "A. P. Dawid and A. M. Skene",
      "year": 1979,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "Their annotator-noise model underpins modern \"debiasing with a few gold labels\" by estimating a judge\u2019s confusion matrix, the very family of debiasing tools whose fundamental limits this paper proves."
    },
    {
      "title": "Learning from Crowds",
      "authors": "Vikas C. Raykar et al.",
      "year": 2010,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "This work formalized learning and calibration with noisy annotators, providing the methodological template\u2014estimating and correcting evaluator bias with limited ground truth\u2014that the present paper shows cannot beat a 2x data bound under frontier conditions."
    },
    {
      "title": "The Alignment Ceiling: When Can AI Feedback Replace Human Feedback?",
      "authors": "Welleck et al.",
      "year": 2024,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By proving that learning from AI feedback is capped by teacher quality, this paper directly inspired the core insight here: evaluation via an AI judge faces inherent limits when the judge is no better than the evaluated model."
    }
  ],
  "synthesis_narrative": "Work on automatic evaluation with strong models first showed that a capable LLM could act as a reliable rater for open-ended generation, with G-Eval demonstrating high human correlation when GPT-4 is prompted as a judge. In machine translation, Kocmi and Federmann provided concrete evidence that LLMs can rival or surpass traditional metrics as evaluators, further validating the feasibility of model-based assessment. MT-Bench and Chatbot Arena then operationalized this idea at scale, using pairwise comparisons and Elo-style aggregation to compare frontier systems, while also revealing practical issues like self-preference when a judge evaluates its own family. Longstanding crowdsourcing research supplies the technical backbone for \u201cdebiasing with a few gold labels\u201d: Dawid\u2013Skene introduces the confusion-matrix view of annotator errors, and Raykar et al. formalize learning and calibration from noisy annotators with limited expert labels, a template many LLM-judge debiasing methods now adopt. More recently, theoretical work on AI feedback such as the Alignment Ceiling argues that learning from an AI teacher cannot surpass the teacher\u2019s quality. Together, these strands created a natural question at the evaluation frontier: if we depend on a strong but imperfect LLM judge and try to calibrate it using a small set of trusted labels, how far can that actually go? The current paper synthesizes the model-as-judge paradigm with crowdsourced-noise calibration and the ceiling intuition from AI feedback to show a sharp limitation: when the judge is no more accurate than the system under test, no debiasing strategy can cut human labeling needs by more than half, delineating the fundamental boundary of scalable evaluation via LLM judges.",
  "target_paper": {
    "title": "Limits to scalable evaluation at the frontier: LLM as judge won\u2019t beat twice the data",
    "authors": "Florian E. Dorner, Vivian Yvonne Nastl, Moritz Hardt",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Evaluation, Benchmarking, Model-as-a-judge, Theory",
    "abstract": "High quality annotations are increasingly a bottleneck in the explosively growing machine learning ecosystem. Scalable evaluation methods that avoid costly annotation have therefore become an important research ambition. Many hope to use strong existing models in lieu of costly labels to provide cheap model evaluations. Unfortunately, this method of using models as judges introduces biases, such as self-preferencing, that can distort model comparisons. An emerging family of debiasing tools promises to fix these issues by using a few high quality labels to debias a large number of model judgments. In this paper, we study how far such debiasing methods, in principle, can go. Our main result shows that when the judge is no more accurate than the evaluated model, no debiasing method can decrease the required amount of ground truth labels by more than half. Our result speaks to the severe limitations of the LLM-as-a-judge paradigm at the evaluation frontier where the goal is to assess newly",
    "openreview_id": "NO6Tv6QcDs",
    "forum_id": "NO6Tv6QcDs"
  },
  "analysis_timestamp": "2026-01-06T09:33:56.238025"
}
{
  "prior_works": [
    {
      "title": "The hippocampus as a predictive map",
      "authors": "M. M. Stachenfeld et al.",
      "year": 2017,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work advanced the predictive-map view that hippocampal representations implement SR-like predictive codes, directly motivating the paper\u2019s comparison between auxiliary predictive RL representations and hippocampal neural changes."
    },
    {
      "title": "Improving Generalization for Temporal-Difference Learning: The Successor Representation",
      "authors": "Peter Dayan",
      "year": 1993,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "It introduced the successor representation and its discounted predictive horizon, providing the formal mechanism the paper manipulates to study how prediction length shapes representational transfer and brain-like structure."
    },
    {
      "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks",
      "authors": "Max Jaderberg et al.",
      "year": 2017,
      "arxiv_id": "1611.05397",
      "role": "Inspiration",
      "relationship_sentence": "This paper established that predictive auxiliary tasks (e.g., pixel control, reward prediction) stabilize and accelerate deep RL, inspiring the paper\u2019s systematic placement of predictive objectives across agent modules."
    },
    {
      "title": "Data-Efficient Reinforcement Learning with Self-Predictive Representations",
      "authors": "Max Schwarzer et al.",
      "year": 2021,
      "arxiv_id": "2007.05929",
      "role": "Extension",
      "relationship_sentence": "By showing multi-step latent self-prediction improves RL, it directly motivates the paper\u2019s analysis of longer predictive horizons and their effects on representation transfer."
    },
    {
      "title": "Dreamer: Reinforcement Learning by Latent Imagination",
      "authors": "Danijar Hafner et al.",
      "year": 2020,
      "arxiv_id": "1912.01603",
      "role": "Related Problem",
      "relationship_sentence": "Demonstrating that learning a predictive latent dynamics model yields stable, data-efficient control supports the paper\u2019s claim that prediction particularly benefits resource-limited architectures."
    },
    {
      "title": "Successor Features for Transfer in Reinforcement Learning",
      "authors": "Andr\u00e9 Barreto et al.",
      "year": 2017,
      "arxiv_id": "1606.05312",
      "role": "Foundation",
      "relationship_sentence": "It showed that predictive successor features enable transfer across tasks, which the paper builds upon by identifying when longer predictive horizons most enhance representational transfer."
    }
  ],
  "synthesis_narrative": "A predictive account of hippocampal coding proposed that neural representations embody future-occupancy structure, formalized via the successor representation (SR) and its discount-controlled horizon. The SR framework established how predictive horizons shape representational geometry, while successor features extended this idea to practical transfer: predictive features can be reused to generalize across tasks. In deep reinforcement learning, auxiliary prediction objectives were shown to stabilize and accelerate representation learning, with UNREAL demonstrating that self-supervised predictive signals (e.g., pixel control, reward prediction) can strengthen state encodings. Moving from one-step to multi-step, self-predictive latent objectives such as SPR highlighted that explicitly predicting future embeddings over several steps further improves data efficiency and robustness. In parallel, model-based approaches like Dreamer demonstrated that learning a latent predictive world model produces more stable, compact, and useful representations for control, especially under limited data or capacity. Together, these works established that prediction\u2014its horizon and where it is imposed in the architecture\u2014strongly shapes learned representations. What remained unclear was how predictive objectives placed across specific agent modules alter representational structure under resource constraints, which horizons best support transfer, and whether these induced changes mirror hippocampal dynamics. By systematically varying where and how far ahead prediction is enforced, and by aligning the resulting representational signatures with hippocampal findings, the paper synthesizes SR-inspired theory with auxiliary-task practice to show that longer-horizon predictive objectives in resource-limited settings yield more transferable, brain-like representations.",
  "target_paper": {
    "title": "Predictive auxiliary objectives in deep RL mimic learning in the brain",
    "authors": "Ching Fang, Kim Stachenfeld",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "oral",
    "keywords": "hippocampus, neuroscience, cognitive science, deep reinforcement learning, representation learning, prediction",
    "abstract": "The ability to predict upcoming events has been hypothesized to comprise a key aspect of natural and machine cognition. This is supported by trends in deep reinforcement learning (RL), where self-supervised auxiliary objectives such as prediction are widely used to support representation learning and improve task performance. Here, we study the effects predictive auxiliary objectives have on representation learning across different modules of an RL system and how these mimic representational changes observed in the brain. We find that predictive objectives improve and stabilize learning particularly in resource-limited architectures, and we identify settings where longer predictive horizons better support representational transfer. Furthermore, we find that representational changes in this RL system bear a striking resemblance to changes in neural activity observed in the brain across various experiments. Specifically, we draw a connection between the auxiliary predictive model of the ",
    "openreview_id": "agPpmEgf8C",
    "forum_id": "agPpmEgf8C"
  },
  "analysis_timestamp": "2026-01-06T07:13:34.356994"
}
{
  "prior_works": [
    {
      "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP",
      "authors": "Patrick Lewis et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "Defines the RAG architecture (retriever + generator) that this paper explicitly evaluates and tunes by scoring alternative components via a synthetic, corpus-grounded multiple-choice exam."
    },
    {
      "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks",
      "authors": "Fabio Petroni et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "Established evaluation for knowledge-intensive tasks grounded in a fixed knowledge source, whose static, non-task-specific nature motivates this paper\u2019s automated, corpus-specific exam generation for RAG evaluation."
    },
    {
      "title": "Measuring Massive Multitask Language Understanding",
      "authors": "Dan Hendrycks et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "Demonstrated that interpretable, exam-style multiple-choice testing can quantify model competence; this work adapts that idea by auto-generating domain-specific exams from the target corpus to measure task-specific RAG accuracy."
    },
    {
      "title": "Some Latent Trait Models and Their Use in Inferring an Examinee\u2019s Ability",
      "authors": "Allan Birnbaum",
      "year": 1968,
      "role": "Foundation",
      "relationship_sentence": "Introduces the 2-parameter logistic Item Response Theory model (difficulty and discrimination) that this paper leverages to estimate exam informativeness and iteratively prune uninformative questions."
    },
    {
      "title": "PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them",
      "authors": "Patrick Lewis et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "Showed that large-scale, corpus-driven question generation with language models is feasible; this paper extends that idea to generate multiple-choice items from a task\u2019s document corpus specifically for evaluating RAG."
    },
    {
      "title": "How Much Knowledge Can You Pack Into the Parameters of a Language Model?",
      "authors": "Adam Roberts et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "Framed the distinction between parametric knowledge and external information access, underscoring why evaluating retrieval-augmented systems on corpus-specific knowledge\u2014precisely what this paper\u2019s exams target\u2014is necessary."
    }
  ],
  "synthesis_narrative": "The core contribution\u2014automated, corpus-specific, multiple-choice exam generation calibrated with Item Response Theory (IRT) to evaluate and select RAG components\u2014emerges from converging lines of work. Retrieval-Augmented Generation (Lewis et al., 2020) provided the system paradigm whose retriever and generator choices practitioners must optimize, while Roberts et al. (2020) sharpened the motivation by contrasting parametric knowledge with the need for external information access. KILT (Petroni et al., 2021) formalized knowledge-intensive evaluation tied to a knowledge source, but its static benchmarks left a gap for task- and corpus-specific assessment that this paper directly addresses. On the measurement side, MMLU (Hendrycks et al., 2021) popularized interpretable, exam-style multiple-choice testing for language models; the present work adapts this format to the target corpus, making accuracy directly reflective of deployment data. Critically, the methodological backbone comes from psychometrics: Birnbaum\u2019s (1968) IRT provides item difficulty and discrimination parameters, enabling the authors to estimate exam information about a model\u2019s ability and iteratively prune low-informative items\u2014turning exam creation into a principled, data-efficient process. Finally, PAQ (Lewis et al., 2021) demonstrates that large-scale, corpus-driven question generation with LMs is practical; this paper repurposes that capability to synthesize high-quality multiple-choice items grounded in the deployment corpus. Together, these works directly enable a robust, automated pipeline for task-specific RAG evaluation.",
  "analysis_timestamp": "2026-01-06T23:09:26.436912"
}
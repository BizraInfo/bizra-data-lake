{
  "prior_works": [
    {
      "title": "EEGNet: A Compact Convolutional Neural Network for EEG-based Brain\u2013Computer Interfaces",
      "authors": "Vernon J. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, Stephen M. Gordon, Chou P. Hung, Brent J. Lance",
      "year": 2018,
      "role": "Foundational EEG architecture establishing joint temporal\u2013spatial filtering",
      "relationship_sentence": "EEGNet\u2019s separable temporal and spatial convolutions grounded CSBrain\u2019s motivation to explicitly model distinct temporal rhythms and spatial topographies rather than using scale-agnostic dense encoders."
    },
    {
      "title": "BENDR: Using Transformers and Self-Supervised Learning for EEG Representation Learning",
      "authors": "Dimitrios Kostas, et al.",
      "year": 2021,
      "role": "Early EEG foundation-model-style pretraining with transformers",
      "relationship_sentence": "BENDR demonstrated the viability of large-scale, self-supervised EEG pretraining, which CSBrain extends with a cross-scale spatiotemporal design to overcome the limitations of dense, scale-agnostic transformers."
    },
    {
      "title": "A Transformer-based Framework for Multivariate Time Series Representation Learning",
      "authors": "Theodoros Zerveas, Srikrishna Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, Carsten Eickhoff",
      "year": 2021,
      "role": "Transformer and masked modeling for generic time-series",
      "relationship_sentence": "TST\u2019s masked modeling and sequence transformer backbone informed CSBrain\u2019s pretraining paradigm, while highlighting the need to adapt beyond uniform attention to capture EEG\u2019s multi-scale dynamics."
    },
    {
      "title": "EEG-based Emotion Recognition Using Graph Convolutional Networks",
      "authors": "Tao Song, Wen-Lan Zheng, Bao-Liang Lu, et al.",
      "year": 2018,
      "role": "Graph modeling of EEG electrode topology",
      "relationship_sentence": "Graph-based treatment of electrodes in this line of work motivated CSBrain\u2019s spatial module to capture localized-to-global cortical interactions across scales instead of treating channels as exchangeable."
    },
    {
      "title": "TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis",
      "authors": "Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long",
      "year": 2023,
      "role": "Explicit multi-period/multi-scale temporal representation for time series",
      "relationship_sentence": "TimesNet\u2019s success in capturing multi-periodic structure directly inspired CSBrain\u2019s cross-scale temporal pathways for handling brief transients and slow rhythms within a unified foundation model."
    },
    {
      "title": "Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows",
      "authors": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo",
      "year": 2021,
      "role": "Hierarchical, windowed attention enabling scalable cross-scale modeling",
      "relationship_sentence": "Swin\u2019s hierarchical, locality-aware attention informed CSBrain\u2019s multi-resolution attention design to efficiently handle long EEG sequences while preserving cross-scale interactions."
    },
    {
      "title": "Invariant Scattering Convolution Networks",
      "authors": "Joan Bruna, St\u00e9phane Mallat",
      "year": 2013,
      "role": "Wavelet-based multi-scale time\u2013frequency representation",
      "relationship_sentence": "The scattering transform\u2019s principled multi-scale time\u2013frequency analysis influenced CSBrain\u2019s use of multi-resolution temporal features to capture both fast activations and slow oscillatory patterns."
    }
  ],
  "synthesis_narrative": "CSBrain\u2019s core contribution\u2014explicit cross-scale spatiotemporal modeling within an EEG foundation model\u2014emerges from three converging lines of prior work. First, domain-specific EEG architectures such as EEGNet established that effective decoding requires distinct temporal and spatial operators: temporal filters to isolate rhythms and spatial filters to exploit channel topology. Graph-based EEG methods further showed that modeling electrode geometry and localized-to-global interactions is crucial, motivating CSBrain\u2019s spatial component that respects cortical structure rather than treating channels as exchangeable.\nSecond, the rise of transformer-based pretraining for time series and EEG (e.g., BENDR and TST) validated large-scale self-supervised learning for generalized decoding, but largely adopted scale-agnostic dense attention from NLP/vision. These successes clarified both the promise of foundation models and the gap: dense, uniform attention underutilizes EEG\u2019s inherently multi-scale dynamics.\nThird, multi-scale representation learning from wavelet scattering and modern time-series architectures (TimesNet) demonstrated performance gains when explicitly capturing multiple periodicities and temporal resolutions. Complementarily, hierarchical transformers like Swin introduced efficient, locality-aware attention that preserves cross-scale interactions.\nCSBrain synthesizes these insights by combining self-supervised pretraining with hierarchical, cross-scale temporal pathways and graph-aware spatial modeling, enabling the model to capture brief transients and long rhythms, along with localized and distributed cortical activity. This integration directly addresses the limitations of prior dense, scale-agnostic EEG foundation models and yields stronger generalization across diverse EEG decoding tasks.",
  "analysis_timestamp": "2026-01-07T00:05:12.552208"
}
{
  "prior_works": [
    {
      "title": "CodeRL: Mastering Code Generation through Deep Reinforcement Learning",
      "authors": "Yue Wang et al.",
      "year": 2022,
      "role": "method",
      "relationship_sentence": "Established execution-based rewards (e.g., unit-test pass rates) for RL fine-tuning of code LMs; CURE generalizes this idea by introducing a second agent (the tester) and learning both coder and tester jointly without ground-truth solutions."
    },
    {
      "title": "ReST: Reinforced Self-Training",
      "authors": "Shibo Gou et al.",
      "year": 2024,
      "role": "training framework",
      "relationship_sentence": "Showed that verifier-derived, programmatic rewards can drive RL without gold labels by training on model-generated data; CURE adapts this principle to coding by using unit-test outcomes and co-evolving two interacting agents."
    },
    {
      "title": "CRITIC: Large Language Models Can Self-Correct with External Feedback",
      "authors": "Aman Madaan et al.",
      "year": 2023,
      "role": "method",
      "relationship_sentence": "Demonstrated iterative self-correction using executable feedback and verifiers; CURE operationalizes this idea at training time by rewarding coder\u2013tester interactions so both improve through execution feedback."
    },
    {
      "title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
      "authors": "Noah Shinn et al.",
      "year": 2023,
      "role": "method",
      "relationship_sentence": "Showed that agents can leverage environment signals and self-critiques to improve; CURE formalizes a paired-agent setup where the tester supplies verifiable signals that shape the coder (and vice versa) via RL."
    },
    {
      "title": "EvoSuite: Automatic Test Suite Generation for Object-Oriented Software",
      "authors": "Gordon Fraser and Andrea Arcuri",
      "year": 2011,
      "role": "test generation",
      "relationship_sentence": "Pioneered learning/evolution of unit tests from program failures; CURE\u2019s tester similarly learns from coder mistakes, but does so with an LLM and RL reward coupling that co-evolves with the coder."
    },
    {
      "title": "APPS: A Benchmark for Code Generation with Input-Output Tests",
      "authors": "Dan Hendrycks et al.",
      "year": 2021,
      "role": "benchmark/enabler",
      "relationship_sentence": "Popularized execution-based evaluation for code using unit tests, enabling verifiable rewards; CURE\u2019s reward design builds on this execution-as-supervision paradigm while removing the need for ground-truth solutions."
    },
    {
      "title": "PAL: Program-Aided Language Models",
      "authors": "Luyu Gao et al.",
      "year": 2023,
      "role": "tool/verifier concept",
      "relationship_sentence": "Framed program execution as a reliable external signal for reasoning; CURE leverages execution outcomes similarly, but for dual-agent RL where tests produced by one agent supervise the other."
    }
  ],
  "synthesis_narrative": "CURE\u2019s core contribution\u2014jointly training a coder and a unit tester via reinforcement learning using purely verifiable, execution-based rewards\u2014directly builds on several lines of work that established execution as supervision and verifier-driven improvement. CodeRL first demonstrated that unit-test pass rates can serve as effective RL rewards for code generation, proving the feasibility of execution-based optimization. ReST generalized the idea of training without gold labels by relying on verifier-derived rewards over model-generated data; CURE adapts this philosophy to the coding domain by letting the unit tester itself become the source of programmatic rewards and evolve with the coder. CRITIC and Reflexion highlighted that iterative self-correction with environment feedback can substantially improve model performance; CURE internalizes this loop at training time, transforming interaction outcomes between coder and tester into reciprocal learning signals. The long-standing EvoSuite work on evolutionary unit test generation showed that tests can be learned from program failures; CURE updates this concept to LLMs, enabling the tester to learn from coder mistakes and produce increasingly discriminative tests. Finally, APPS and PAL cemented the role of execution and programmatic checking as reliable verifiers, which CURE leverages to design scalable, label-free rewards. Together, these works converge on CURE\u2019s insight: co-evolving generators and verifiers under verifiable rewards yields robust coding ability beyond one-shot precision.",
  "analysis_timestamp": "2026-01-07T00:05:12.539888"
}
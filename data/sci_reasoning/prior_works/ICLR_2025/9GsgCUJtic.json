{
  "prior_works": [
    {
      "title": "Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation",
      "authors": "Emmanuel Bengio et al.",
      "year": 2021,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This paper introduced GFlowNets, the flow conservation and edge-level detailed balance constraints that define when sampling matches a target reward-proportional distribution\u2014the exact correctness criterion whose violations this work analyzes."
    },
    {
      "title": "Trajectory Balance: Improved Credit Assignment in GFlowNets",
      "authors": "Akhilesh K. Madan et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Trajectory Balance provides the global equality whose satisfaction guarantees correct sampling, and this work directly studies how deviations from the implied per-edge balances affect the learned distribution."
    },
    {
      "title": "Subtrajectory Balance for Compositional Generation",
      "authors": "Anton Malkin et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By enforcing local consistency along subpaths, Subtrajectory Balance focused attention on localized balance constraints, motivating this paper\u2019s edge-level analysis of how local imbalances propagate and unevenly impact correctness."
    },
    {
      "title": "Bayesian Structure Learning with Generative Flow Networks",
      "authors": "Tristan Deleu et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work established GFlowNets for graph-structured distributions (e.g., DAGs) with GNN-parameterized policies, the exact setting in which this paper analyzes when correctness holds and why imbalance may be inevitable."
    },
    {
      "title": "How Powerful are Graph Neural Networks?",
      "authors": "Keyulu Xu et al.",
      "year": 2019,
      "arxiv_id": "arXiv:1810.00826",
      "role": "Gap Identification",
      "relationship_sentence": "Its characterization of message-passing GNNs\u2019 expressivity limits (via 1-WL) underpins this paper\u2019s claim that GNN-parameterized GFlowNets cannot distinguish certain transitions, making exact detailed balance unattainable in some graph families."
    },
    {
      "title": "Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks",
      "authors": "Christopher Morris et al.",
      "year": 2019,
      "arxiv_id": "arXiv:1810.02244",
      "role": "Related Problem",
      "relationship_sentence": "By formalizing when MPNNs collapse distinct structures, this work provides the theoretical lens used here to argue that unavoidable symmetries induce persistent edge imbalances in GNN-parameterized GFlowNets."
    }
  ],
  "synthesis_narrative": "GFlowNets were introduced with a flow conservation framework that enforces detailed balance across edges to ensure sampling matches a reward-proportional target distribution, precisely articulating when a learned policy is correct. Trajectory Balance subsequently provided a global equality whose satisfaction guarantees correctness, connecting trajectory-level credit assignment to per-edge balance constraints. Subtrajectory Balance emphasized local consistency along subpaths, suggesting that the locus of constraint enforcement matters and implicitly raising the question of how local violations influence global sampling accuracy. GFlowNets were then applied to graph-structured distributions such as DAGs using GNN-parameterized forward/backward policies, demonstrating practical success while exposing a setting where the correctness guarantees hinge on the representational capacity of the parameterization. In parallel, results on GNN expressivity showed that message-passing architectures cannot distinguish structures beyond 1-WL, and that symmetries cause different transitions or edges to be indistinguishable, limiting the realizability of exact balance.\nTogether, these works revealed a tension: correctness objectives require exact (local or trajectory-level) balance, yet common GNN parameterizations on graphs cannot separate all necessary transitions, making some violations inevitable. This paper takes the natural next step by quantifying how imbalance at specific edges affects the learned distribution as a function of the flow traversing those edges, showing the effect is unevenly distributed, and by formalizing inevitability under GNN expressivity limits in graph sampling settings.",
  "target_paper": {
    "title": "When do GFlowNets learn the right distribution?",
    "authors": "Tiago Silva, Rodrigo Barreto Alves, Eliezer de Souza da Silva, Amauri H Souza, Vikas Garg, Samuel Kaski, Diego Mesquita",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "GFlowNets",
    "abstract": "Generative Flow Networks (GFlowNets) are an emerging class of sampling methods for distributions over discrete and compositional objects, e.g., graphs. In spite of their remarkable success in problems such as drug discovery and phylogenetic inference, the question of when and whether GFlowNets learn to sample from the target distribution remains underexplored. To tackle this issue, we first assess the extent to which a violation of the detailed balance of the underlying flow network might hamper the correctness of GFlowNet's sampling distribution. In particular, we demonstrate that the impact of an imbalanced edge on the model's accuracy is influenced by the total amount of flow passing through it and, as a consequence, is unevenly distributed across the network. We also argue that, depending on the parameterization, imbalance may be inevitable. In this regard, we consider the problem of sampling from distributions over graphs with GFlowNets parameterized by graph neural networks (GNNs",
    "openreview_id": "9GsgCUJtic",
    "forum_id": "9GsgCUJtic"
  },
  "analysis_timestamp": "2026-01-06T13:17:27.398679"
}
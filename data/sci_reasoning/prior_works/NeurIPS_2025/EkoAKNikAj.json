{
  "prior_works": [
    {
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)",
      "authors": "Alexey Dosovitskiy et al.",
      "year": 2020,
      "role": "Foundation for global self-attention in vision",
      "relationship_sentence": "PPMA retains ViT\u2019s patch-based self-attention as the global dependency backbone and augments it with a structured mask rather than changing the attention mechanism itself."
    },
    {
      "title": "Mamba-2: Scaling Selective State Space Models",
      "authors": "Tri Dao, Albert Gu et al.",
      "year": 2024,
      "role": "Motivation for explicit locality priors via structured constraints",
      "relationship_sentence": "Inspired by Mamba-2\u2019s explicit modeling of adjacency with structured constraints, PPMA introduces a structured attention mask to inject spatial adjacency priors into a ViT."
    },
    {
      "title": "Longformer: The Long-Document Transformer",
      "authors": "Iz Beltagy, Matthew E. Peters, Arman Cohan",
      "year": 2020,
      "role": "Precedent for locality-preserving sparse attention masks",
      "relationship_sentence": "PPMA generalizes the idea of locality-aware masked attention (e.g., sliding windows) to images by deriving a 2D-aware polyline path mask that preserves neighborhood relationships across the sequence."
    },
    {
      "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
      "authors": "Ze Liu et al.",
      "year": 2021,
      "role": "Demonstrated effectiveness of local windows and relative spatial priors in ViTs",
      "relationship_sentence": "PPMA targets the same goal of encoding 2D locality but does so through a structured polyline path mask rather than fixed or shifted windows, enabling global attention with an adjacency-aware constraint."
    },
    {
      "title": "Axial Attention in Multidimensional Transformers",
      "authors": "Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, Tim Salimans",
      "year": 2019,
      "role": "Architectural bias that respects 2D structure via factorized attention",
      "relationship_sentence": "PPMA similarly prioritizes 2D spatial coherence, replacing axis-factorization with a polyline path that induces a structured mask aligning the 1D sequence with 2D neighborhood topology."
    },
    {
      "title": "Image Transformer",
      "authors": "Niki Parmar et al.",
      "year": 2018,
      "role": "Early use of attention with locality-aware patterns for images",
      "relationship_sentence": "PPMA builds on the notion that attention patterns should reflect image locality, replacing block/local attention with a polyline-derived mask that better preserves adjacency during serialization."
    },
    {
      "title": "Pixel Recurrent Neural Networks",
      "authors": "A\u00e4ron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu",
      "year": 2016,
      "role": "Showed the importance of scan order and masking in 2D image modeling",
      "relationship_sentence": "PPMA extends the insight that sequence ordering and masking shape 2D dependencies by moving beyond raster masks to a polyline path that improves neighborhood preservation in attention."
    }
  ],
  "synthesis_narrative": "PPMA\u2019s key contribution is to marry ViT\u2019s global self-attention with an explicitly adjacency-aware structured mask derived from a 2D polyline scanning path. ViT established the paradigm of treating images as token sequences and modeling global dependencies via self-attention, but it lacks an explicit spatial adjacency prior. Mamba-2, though developed for sequence modeling, showed that imposing structured constraints can encode locality effectively; PPMA draws on this idea to inject a spatial prior through a mask rather than altering the attention computation. Prior sparse/structured attention works like Longformer validated that carefully designed masks can scale context while preserving local neighborhoods, a principle PPMA adapts to vision by deriving the mask from a path that respects 2D topology. Vision-specific designs such as Swin and Axial Attention demonstrated that incorporating locality (shifted windows, axis-wise factorization) is crucial for strong vision performance; PPMA pursues the same objective but without confining attention to windows or axes, instead using a polyline path that better preserves adjacency when mapping 2D grids to 1D sequences. Earlier image attention and autoregressive models (Image Transformer, PixelRNN) underscored how sequence order and masking profoundly affect 2D dependency capture; PPMA advances this by replacing raster/local patterns with a polyline-derived mask, achieving a unified mechanism that keeps ViT\u2019s global reach while enforcing a spatially coherent connectivity prior.",
  "analysis_timestamp": "2026-01-07T00:27:38.141968"
}
{
  "prior_works": [
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot et al.",
      "year": 2018,
      "arxiv_id": "1806.07572",
      "role": "Foundation",
      "relationship_sentence": "This work established that gradient descent training of wide networks is equivalent to kernel regression under the NTK, a core modeling step the paper uses to linearize GNN training dynamics for exact certification."
    },
    {
      "title": "Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels",
      "authors": "Qinliang (Edward) Du et al.",
      "year": 2019,
      "arxiv_id": "1905.13192",
      "role": "Extension",
      "relationship_sentence": "By providing a concrete NTK for GNN architectures (GNTK), this paper supplies the graph-specific kernel machinery that the current work leverages to represent wide GNN training in its MILP-based certification framework."
    },
    {
      "title": "Poisoning Attacks against Support Vector Machines",
      "authors": "Battista Biggio et al.",
      "year": 2012,
      "arxiv_id": "1206.6389",
      "role": "Foundation",
      "relationship_sentence": "This paper formulated data poisoning as a bilevel optimization problem for margin-based learners, a formulation the current work adopts and then exactly reformulates (under NTK) into a mixed-integer linear program for label flipping."
    },
    {
      "title": "Poisoning Attacks with Back-gradient Optimization",
      "authors": "Luis Mu\u00f1oz-Gonz\u00e1lez et al.",
      "year": 2017,
      "arxiv_id": "",
      "role": "Gap Identification",
      "relationship_sentence": "By demonstrating effective bilevel poisoning of deep models via back-gradient methods yet without guarantees, this work highlights the need for provable, exact certification that the current paper provides."
    },
    {
      "title": "Certified Defenses for Data Poisoning Attacks",
      "authors": "Jacob Steinhardt et al.",
      "year": 2017,
      "arxiv_id": "1706.03691",
      "role": "Gap Identification",
      "relationship_sentence": "This paper introduced the idea of certifying worst-case poisoning effects for convex learners but does not handle neural networks or label flipping in GNNs, a key limitation the current work overcomes using NTK and MILP."
    },
    {
      "title": "Understanding Black-box Predictions via Influence Functions",
      "authors": "Pang Wei Koh et al.",
      "year": 2017,
      "arxiv_id": "1703.04730",
      "role": "Related Problem",
      "relationship_sentence": "Influence functions provide an approximate sensitivity analysis of predictions to training labels, a perspective the current paper replaces with exact, worst-case certificates derived via an NTK-based MILP."
    }
  ],
  "synthesis_narrative": "The neural tangent kernel (NTK) formalism showed that gradient descent on wide neural networks is equivalent to kernel regression, making training dynamics tractable through linearization. Building on this, the graph neural tangent kernel (GNTK) instantiated NTK for message-passing architectures, giving a concrete kernel representation that reflects GNN inductive biases. In parallel, early data poisoning research cast training-time attacks as bilevel optimization, with poisoning against SVMs providing a precise margin-based formulation and demonstrating the centrality of discrete label and training-set decisions. Subsequent work introduced back-gradient techniques to carry bilevel attacks to deep networks, underscoring the practical vulnerability of complex models but stopping short of provable guarantees. Complementing these attacks, certified defenses framed worst-case guarantees for poisoning in convex settings, highlighting what can be certified but not addressing neural-network or graph-specific learners. Influence functions further quantified how small label perturbations affect predictions, offering a sensitivity lens that, while insightful, remained approximate and local.\nTogether, these strands pointed to a natural path: represent GNN training in the wide-limit via (G)NTK to obtain a convex, kernelized view; express adversarial label flips within the bilevel poisoning paradigm; and then replace approximation with exactness by encoding the resulting problem as a mixed-integer linear program. This synthesis closes the gap between empirical poisoning attacks and limited convex certifications by delivering exact, sample-wise and collective certificates for GNNs under label flipping, enabling principled robustness comparisons across architectures.",
  "target_paper": {
    "title": "Exact Certification of (Graph) Neural Networks Against Label Poisoning",
    "authors": "Mahalakshmi Sabanayagam, Lukas Gosch, Stephan G\u00fcnnemann, Debarghya Ghoshdastidar",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "graph neural networks, robustness, certificates, provable robustness, neural networks, label poisoning, label flipping, poisoning, mixed-integer linear programming, neural tangent kernel, support vector machines",
    "abstract": "Machine learning models are highly vulnerable to label flipping, i.e., the adversarial modification (poisoning) of training labels to compromise performance. Thus, deriving robustness certificates is important to guarantee that test predictions remain unaffected and to understand worst-case robustness behavior. However, for Graph Neural Networks (GNNs), the problem of certifying label flipping has so far been unsolved. We change this by introducing an exact certification method, deriving both sample-wise and collective certificates. Our method leverages the Neural Tangent Kernel (NTK) to capture the training dynamics of wide networks enabling us to reformulate the bilevel optimization problem representing label flipping into a Mixed-Integer Linear Program (MILP). We apply our method to certify a broad range of GNN architectures in node classification tasks. Thereby, concerning the worst-case robustness to label flipping: $(i)$ we establish hierarchies of GNNs on different benchmark gra",
    "openreview_id": "d9aWa875kj",
    "forum_id": "d9aWa875kj"
  },
  "analysis_timestamp": "2026-01-06T09:50:59.318225"
}
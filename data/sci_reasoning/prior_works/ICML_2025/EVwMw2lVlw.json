{
  "prior_works": [
    {
      "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge",
      "authors": "Kenneth Marino et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "SK-VQA adopts the KB-VQA formulation introduced by OK-VQA\u2014answering image-grounded questions that require outside knowledge\u2014while providing large-scale, explicitly evidence-linked supervision to train context-augmented generation."
    },
    {
      "title": "FVQA: Fact-based Visual Question Answering",
      "authors": "Peng Wang et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Building on FVQA\u2019s core idea of grounding VQA answers in external knowledge facts, SK-VQA generalizes the evidence from KB triples to heterogeneous external sources and scales supervision to millions of items."
    },
    {
      "title": "A-OKVQA: A Benchmark for Visual Question Answering Using External Knowledge",
      "authors": "Adam Schwenk et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "A-OKVQA highlighted the need for higher-quality, knowledge-requiring VQA but remained limited in scale and domain breadth; SK-VQA directly addresses these constraints with 2M+ synthetic, diverse, and evidence-grounded QA pairs."
    },
    {
      "title": "REVEAL: Retrieval-Augmented Visual-Language Pre-Training",
      "authors": "Tanmay Gupta et al.",
      "year": 2022,
      "role": "Inspiration",
      "relationship_sentence": "REVEAL demonstrated that coupling retrieval with V-L pretraining improves knowledge-intensive reasoning, motivating SK-VQA to supply large, clean, retrieval-style supervision explicitly pairing questions, images, and evidence."
    },
    {
      "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
      "authors": "Kelvin Guu et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "REALM established retrieve-then-read training signals; SK-VQA extends this paradigm to multimodal inputs by structuring each example as (image, question, retrieved external knowledge) \u2192 answer."
    },
    {
      "title": "LLaVA: Visual Instruction Tuning",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "LLaVA popularized large-scale synthetic instruction data for aligning VLMs but lacked explicit external-knowledge conditioning; SK-VQA directly fills this gap by generating evidence-linked, knowledge-grounded multimodal QA at scale."
    },
    {
      "title": "PICa: Prompting Language Models with Image Captions for Zero-shot VQA",
      "authors": "Jieyu Yang et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "PICa showed that adding retrieved textual context boosts VQA with LMs; SK-VQA converts this insight into train-time supervision by providing paired evidence with each example rather than relying on inference-time prompting."
    }
  ],
  "synthesis_narrative": "SK-VQA\u2019s core contribution\u2014scaling synthetic, evidence-linked multimodal supervision for context-augmented generation\u2014stands on the problem framing of knowledge-based VQA introduced by OK-VQA and anticipated by FVQA\u2019s explicit grounding to external facts. A-OKVQA sharpened the need for high-quality, knowledge-requiring benchmarks but remained limited in size, diversity, and explicit evidence pairing, directly motivating SK-VQA\u2019s emphasis on scale, domain coverage, and per-example knowledge sources. On the modeling side, REALM crystallized the retrieve-then-read training signal for language models, while REVEAL extended this idea to vision-language pretraining, demonstrating that retrieval materially improves knowledge-intensive reasoning. SK-VQA operationalizes these principles by constructing multimodal examples that pair images and questions with the external knowledge necessary to answer, providing the supervision that retrieval-augmented VLMs have lacked. Finally, the synthetic data paradigm popularized by LLaVA showed that large, curated instruction data can align VLMs but did not target knowledge-grounded generation; SK-VQA fills that gap by generating evidence-conditioned Q/A at unprecedented scale. PICa\u2019s success with inference-time caption retrieval further underscored the value of external context, which SK-VQA transforms into supervised training data, moving beyond prompt-only solutions to teach models to integrate retrieved knowledge during generation.",
  "analysis_timestamp": "2026-01-06T23:07:19.593166"
}
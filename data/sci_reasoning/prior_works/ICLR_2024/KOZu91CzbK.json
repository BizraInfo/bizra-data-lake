{
  "prior_works": [
    {
      "title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
      "authors": "Shinn et al.",
      "year": 2023,
      "arxiv_id": "2303.11366",
      "role": "Gap Identification",
      "relationship_sentence": "Reflexion showed that verbal self-reflection can improve agent performance but lacks a gradient-compatible learning mechanism, directly motivating Retroformer\u2019s learned retrospective model that turns reflections into policy-gradient updates for prompt tuning."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Yao et al.",
      "year": 2022,
      "arxiv_id": "2210.03629",
      "role": "Foundation",
      "relationship_sentence": "ReAct established the reasoning\u2013acting trajectory format and agent loop that Retroformer optimizes over, providing the interaction scaffold and state\u2013action traces that its policy gradient uses."
    },
    {
      "title": "Self-Refine: Iterative Refinement with Self-Feedback",
      "authors": "Madaan et al.",
      "year": 2023,
      "arxiv_id": "2303.17651",
      "role": "Inspiration",
      "relationship_sentence": "Self-Refine\u2019s technique of converting prior mistakes into textual feedback to guide subsequent attempts inspired Retroformer\u2019s retrospective model, which learns to summarize root causes and use them to update the agent prompt via reward-driven learning."
    },
    {
      "title": "RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning",
      "authors": "Deng et al.",
      "year": 2022,
      "arxiv_id": null,
      "role": "Extension",
      "relationship_sentence": "RLPrompt\u2019s use of policy gradient to optimize discrete prompts informed Retroformer\u2019s extension from single-turn LM prompting to multi-step agent prompt optimization using environment rewards across tasks."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Yao et al.",
      "year": 2023,
      "arxiv_id": "2305.10601",
      "role": "Related Problem",
      "relationship_sentence": "Tree of Thoughts improved reasoning via search without learning from task rewards, highlighting a limitation that Retroformer addresses by making agent improvement reward-driven and gradient-based rather than purely search-based."
    },
    {
      "title": "Hindsight Experience Replay",
      "authors": "Andrychowicz et al.",
      "year": 2017,
      "arxiv_id": "1707.01495",
      "role": "Inspiration",
      "relationship_sentence": "HER\u2019s idea of extracting learning signal from past (even failed) trajectories inspired Retroformer\u2019s use of retrospective summaries that transform prior rollouts into effective gradient-bearing updates for the agent prompt."
    }
  ],
  "synthesis_narrative": "ReAct introduced a combined reasoning\u2013acting paradigm that structures agent behavior as interleaved thoughts, actions, and observations, yielding rich trajectories suitable for learning. Reflexion demonstrated that agents can improve by generating self-reflective notes about their past mistakes, but its updates are purely verbal and not integrated into a gradient-based learning process. Self-Refine further showed that converting errors into textual feedback can iteratively steer future outputs, emphasizing the value of concise, actionable retrospectives. RLPrompt established that discrete text prompts can be optimized with reinforcement learning, providing a policy-gradient pathway to adjust prompts based on rewards, albeit for mostly single-turn tasks. Tree of Thoughts highlighted gains from deliberate search over reasoning traces, yet it remains a procedural enhancement without leveraging environment-specific rewards for learning. Hindsight Experience Replay revealed that failed trajectories hold latent supervisory signal when reinterpreted retrospectively, enabling effective learning despite sparse rewards.\nTogether, these works expose an opportunity: use the ReAct-style agent loop to collect trajectories, distill them into concise, error-focused retrospectives (as in Reflexion/Self-Refine), and convert those retrospectives into learnable, reward-aligned prompt updates (as in RLPrompt), while drawing on HER\u2019s hindsight principle to extract signal from failures. Synthesizing these insights, the natural next step is a learned retrospective model that produces gradient-compatible feedback and a policy-gradient procedure that tunes the agent\u2019s prompt across environments, closing the gap between verbal reflection/search and reward-driven optimization.",
  "target_paper": {
    "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
    "authors": "Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh R N, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil L Mui, Huan Wang, Caiming Xiong, Silvio Savarese",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Language Agent, AI Agent, Reinforcement Learning",
    "abstract": "Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior",
    "openreview_id": "KOZu91CzbK",
    "forum_id": "KOZu91CzbK"
  },
  "analysis_timestamp": "2026-01-06T19:41:42.158682"
}
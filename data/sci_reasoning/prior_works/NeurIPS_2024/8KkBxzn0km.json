{
  "prior_works": [
    {
      "title": "Dark Experience for General Continual Learning",
      "authors": "Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, Simone Calderara",
      "year": 2020,
      "role": "Replay-based continual learning backbone (logit-matching + memory)",
      "relationship_sentence": "SER is designed to plug into and strengthen replay-based pipelines like DER/DER++, using saliency-driven modulation to stabilize updates during memory replay and yield larger retention and transfer gains."
    },
    {
      "title": "Tiny Episodic Memories in Continual Learning",
      "authors": "Arslan Chaudhry, Marc\u2019Aurelio Ranzato, Marcus Rohrbach, Mohamed Elhoseiny",
      "year": 2019,
      "role": "Foundational evidence for simple experience replay with small buffers",
      "relationship_sentence": "By establishing that straightforward experience replay is a strong, scalable baseline, this work motivates SER\u2019s choice to augment replay with an auxiliary saliency signal rather than redesign the CL objective from scratch."
    },
    {
      "title": "iCaRL: Incremental Classifier and Representation Learning",
      "authors": "Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, Christoph H. Lampert",
      "year": 2017,
      "role": "Exemplar-based rehearsal for class-incremental learning",
      "relationship_sentence": "iCaRL introduced memory-based rehearsal in class-incremental settings; SER builds on this paradigm by injecting human-like saliency as a modulation signal during rehearsal to better preserve and shape representations."
    },
    {
      "title": "A Model of Saliency-Based Visual Attention for Rapid Scene Analysis",
      "authors": "Laurent Itti, Christof Koch, Ernst Niebur",
      "year": 1998,
      "role": "Foundational computational model of bottom-up visual saliency",
      "relationship_sentence": "SER operationalizes the classic notion that bottom-up saliency guides human visual processing, using predicted saliency maps as a biologically-inspired signal to drive and stabilize learning across task sequences."
    },
    {
      "title": "DeepGaze II: Reading fixations from deep features trained on object recognition",
      "authors": "Matthias K\u00fcmmerer, Thomas S. A. Wallis, Matthias Bethge",
      "year": 2016,
      "role": "Modern deep saliency prediction providing human-like fixation maps",
      "relationship_sentence": "SER relies on accurate, pretrained saliency predictors; DeepGaze II exemplifies the deep saliency models whose features or outputs can serve as the auxiliary modulation signal SER uses during replay."
    },
    {
      "title": "CBAM: Convolutional Block Attention Module",
      "authors": "Sanghyun Woo, Jongchan Park, Joon-Young Lee, In So Kweon",
      "year": 2018,
      "role": "Attention-based feature modulation (spatial/channel gating) in CNNs",
      "relationship_sentence": "CBAM showed that spatial attention maps can gate feature activations for improved recognition; SER extends this idea by using externally predicted human-saliency maps to modulate representations during continual learning and replay."
    },
    {
      "title": "Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations",
      "authors": "Andrew Slavin Ross, Michael C. Hughes, Finale Doshi-Velez",
      "year": 2017,
      "role": "Using explanation/saliency constraints to reduce spurious reliance and improve robustness",
      "relationship_sentence": "SER\u2019s saliency-driven modulation echoes the principle of aligning model focus with human-relevant regions, yielding robustness to spurious features and adversarial perturbations analogous to explanation-regularized training."
    }
  ],
  "synthesis_narrative": "Saliency-driven Experience Replay (SER) fuses two mature lines of work: replay-centric continual learning and human visual saliency. Replay has proven to be a simple, strong baseline for class- and task-incremental learning\u2014first via exemplar rehearsal as in iCaRL and subsequently through tiny episodic memory studies showing that even small buffers can stabilize learning. DER/DER++ refined this paradigm with logit matching atop memory replay, establishing a versatile backbone into which auxiliary signals can be injected. SER\u2019s core idea is to inject such a signal by leveraging bottom-up visual saliency.\n\nThe saliency component stems from classic computational attention (Itti\u2013Koch\u2013Niebur), advanced by modern deep predictors like DeepGaze II that produce human-like fixation maps. Independently, the CNN literature has shown that attention maps can modulate features to improve recognition, as in CBAM\u2019s spatial gating. SER bridges these insights: it treats accurate, pretrained saliency maps as an external, biologically grounded modulation signal applied during experience replay to guide gradient flow and representation updates in non-i.i.d. streams.\n\nFinally, explanation-regularization work (Right for the Right Reasons) demonstrated that aligning model focus with human-relevant regions reduces reliance on spurious features and improves robustness. SER generalizes this to the continual setting: saliency-guided modulation during replay not only mitigates forgetting but also biases representations toward semantically meaningful, human-attended regions, yielding improved resistance to spurious correlations and adversarial attacks while enhancing state-of-the-art replay-based CL methods.",
  "analysis_timestamp": "2026-01-06T23:33:35.534341"
}
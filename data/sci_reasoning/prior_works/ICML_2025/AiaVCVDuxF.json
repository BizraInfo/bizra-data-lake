{
  "prior_works": [
    {
      "title": "Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness",
      "authors": "Michael Kearns et al.",
      "year": 2018,
      "role": "Baseline",
      "relationship_sentence": "This paper provides the core auditing formalism for subgroup fairness that the current work treats as the primary baseline and extends by modeling an adversarial platform that can adapt its answers to the auditor and by adding prior-based, manipulation-proof guarantees."
    },
    {
      "title": "Equality of Opportunity in Supervised Learning",
      "authors": "Moritz Hardt et al.",
      "year": 2016,
      "role": "Foundation",
      "relationship_sentence": "The fairness criteria (e.g., equal opportunity/equalized odds) used to quantify and bound the \"maximum unfairness a platform can hide\" in the new paper originate here, supplying the problem formulation and metrics the audit must enforce."
    },
    {
      "title": "Fairwashing: The Risk of Rationalization",
      "authors": "A\u00efvodji et al.",
      "year": 2019,
      "role": "Gap Identification",
      "relationship_sentence": "By demonstrating that entities can manipulate audits via rationalized, seemingly fair explanations, this work exposes the exact vulnerability\u2014audit manipulation\u2014that the new paper formalizes and overcomes using prior knowledge."
    },
    {
      "title": "The Reusable Holdout: Preserving Validity in Adaptive Data Analysis",
      "authors": "Cynthia Dwork et al.",
      "year": 2015,
      "role": "Inspiration",
      "relationship_sentence": "This work shows that public evaluation data are susceptible to adaptive overfitting/gaming, directly motivating the new paper\u2019s result that relying on public priors (e.g., public datasets) enables easy manipulation and that protected/secret information is needed for robust audits."
    },
    {
      "title": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions",
      "authors": "Avrim Blum et al.",
      "year": 2015,
      "role": "Related Problem",
      "relationship_sentence": "By designing evaluation mechanisms resilient to gaming of public leaderboards, this paper informs the new work\u2019s core insight that auditors must hide or privatize evaluative information (priors) to prevent platforms from tailoring responses to pass audits."
    },
    {
      "title": "Strategic Classification",
      "authors": "Moritz Hardt et al.",
      "year": 2016,
      "role": "Related Problem",
      "relationship_sentence": "Modeling decision-making under strategic manipulation, this work provides the game-theoretic lens the new paper adapts to the auditor\u2013platform interaction, where the platform strategically changes outputs to the auditor and the auditor counters via prior-based tests."
    },
    {
      "title": "A Bayesian Truth Serum for Subjective Judgments",
      "authors": "Drazen Prelec",
      "year": 2004,
      "role": "Inspiration",
      "relationship_sentence": "This paper introduces the idea that a privately held prior can enable manipulation-resistant elicitation of truthful reports without direct verification, which directly inspires the new paper\u2019s use of the auditor\u2019s private prior to detect and deter audit manipulation."
    }
  ],
  "synthesis_narrative": "The core innovation of Robust ML Auditing using Prior Knowledge is to make fairness audits manipulation-proof by leveraging an auditor\u2019s private prior about ground truth. This builds directly on the fairness auditing paradigm of Kearns et al., which formalized audits via subgroup constraints; the new work treats that framework as a baseline but goes further by modeling an adversarial platform that adapts to the auditor\u2019s queries. The unfairness notions that the auditor seeks to certify\u2014such as equal opportunity/equalized odds\u2014come from Hardt, Price, and Srebro, providing the metrics whose hidden violations the paper quantifies and bounds. The immediate motivation is the fairwashing risk identified by A\u00efvodji et al., which showed that actors can rationalize and pass audits via tailored explanations; the present paper tackles this vulnerability head-on by proving conditions under which such manipulation is detectable. A second crucial strand is robustness to gaming of evaluations: Dwork et al.\u2019s reusable holdout and Blum & Hardt\u2019s Ladder both demonstrate that public evaluation artifacts invite adaptive overfitting, directly supporting the paper\u2019s result that public priors or datasets let platforms easily fool audits, hence the need for protected auditor knowledge. Finally, the strategic behavior perspective from Hardt et al.\u2019s Strategic Classification underpins the adversarial modeling of the platform, while Prelec\u2019s Bayesian Truth Serum provides the conceptual template that private priors can render dishonest reporting detectable. Together, these works culminate in a theory and practice of prior-informed, manipulation-proof ML auditing.",
  "analysis_timestamp": "2026-01-06T23:07:19.577845"
}
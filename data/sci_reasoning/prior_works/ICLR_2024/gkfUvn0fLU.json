{
  "prior_works": [
    {
      "title": "Deep Reinforcement Learning from Human Preferences",
      "authors": "Paul Christiano et al.",
      "year": 2017,
      "arxiv_id": "1706.03741",
      "role": "Foundation",
      "relationship_sentence": "Introduced the core RL-from-preferences pipeline of fitting a learned reward model from human feedback and optimizing a policy against it, which this paper adopts as the base alignment framework that becomes vulnerable to overoptimization."
    },
    {
      "title": "Fine-Tuning Language Models from Human Preferences",
      "authors": "Daniel M. Ziegler et al.",
      "year": 2019,
      "arxiv_id": "1909.08593",
      "role": "Foundation",
      "relationship_sentence": "Established the LM RLHF training objective with PPO and a KL penalty to a reference policy, which the present work modifies by augmenting the objective with per-component constraints rather than relying solely on scalarized rewards."
    },
    {
      "title": "Training language models to follow instructions with human feedback",
      "authors": "Long Ouyang et al.",
      "year": 2022,
      "arxiv_id": "2203.02155",
      "role": "Baseline",
      "relationship_sentence": "Serves as the canonical PPO+RM baseline whose scalar reward optimization the current paper improves upon by preventing reward model overoptimization via explicit constraints."
    },
    {
      "title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback",
      "authors": "Yuntao Bai et al.",
      "year": 2022,
      "arxiv_id": "2204.05862",
      "role": "Foundation",
      "relationship_sentence": "Demonstrated practical composite objectives by combining separate helpfulness and harmlessness reward models via a weighted sum, the exact setup whose weight-tuning fragility this paper replaces with constraint-based control."
    },
    {
      "title": "Constrained Policy Optimization",
      "authors": "Joshua Achiam et al.",
      "year": 2017,
      "arxiv_id": "1705.10528",
      "role": "Extension",
      "relationship_sentence": "Provides the constrained MDP formulation and policy-gradient machinery that this paper extends to RLHF by enforcing per-reward-model-component constraints during LM policy optimization."
    },
    {
      "title": "The Inverse Reward Design Problem",
      "authors": "Dylan Hadfield-Menell et al.",
      "year": 2017,
      "arxiv_id": "1711.02827",
      "role": "Gap Identification",
      "relationship_sentence": "Explicitly showed that learned reward proxies can be mis-specified and lead to undesirable behavior when optimized, motivating this paper\u2019s focus on curbing overoptimization of RM proxies."
    },
    {
      "title": "Categorizing Variants of Goodhart\u2019s Law",
      "authors": "David Manheim et al.",
      "year": 2019,
      "arxiv_id": "1803.04585",
      "role": "Inspiration",
      "relationship_sentence": "Identified regressional and extremal Goodhart effects where proxy\u2013goal correlations determine failure modes, inspiring this paper\u2019s empirical analysis of how correlations among component RMs shift overoptimization points."
    }
  ],
  "synthesis_narrative": "Human-in-the-loop reinforcement learning matured with the insight that a policy can be aligned by learning a reward model from pairwise human preferences and optimizing the policy against it, establishing the core preference-learning and policy-optimization loop. In language modeling, PPO-based RLHF with a KL penalty to a reference policy operationalized this idea at scale, defining the practical objective many systems use today. As alignment objectives became multi-faceted, practitioners began training multiple reward models\u2014for helpfulness and harmlessness\u2014and combining them by a weighted sum, thereby turning alignment into a composition problem whose behavior depends acutely on the chosen weights. Theoretical work on reward misspecification cautioned that learned reward proxies can be systematically wrong in parts of the state space; and Goodhart\u2019s taxonomy clarified that when proxies correlate imperfectly with the true objective, optimization can drive behavior into regions where the proxy breaks down, particularly under extremal optimization. Constrained policy optimization, in parallel, provided a principled way to optimize a primary objective while keeping auxiliary quantities within specified limits via a constrained MDP formulation.\nTogether, these strands reveal a gap: composite RM scalarization is brittle and prone to Goodhart-type overoptimization, especially as component correlations and optimization pressure interact, while constrained optimization offers a natural alternative to weight-tuning. The current work synthesizes these insights by diagnosing how component-RM correlations determine overoptimization thresholds and by recasting RLHF as a constrained optimization problem that enforces per-component limits during policy updates, thereby preserving human-rated quality while pursuing improvement.",
  "target_paper": {
    "title": "Confronting Reward Model Overoptimization with Constrained RLHF",
    "authors": "Ted Moskovitz, Aaditya K Singh, DJ Strouse, Tuomas Sandholm, Ruslan Salakhutdinov, Anca Dragan, Stephen Marcus McAleer",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "rlhf, overoptimization, constrained RL",
    "abstract": "Large language models are typically aligned with human preferences by optimizing reward models (RMs) fitted to human feedback. However, human preferences are multi-faceted, and it is increasingly common to derive reward from a composition of simpler reward models which each capture a different aspect of language quality. This itself presents a challenge, as it is difficult to appropriately weight these component RMs when combining them. Compounding this difficulty, because any RM is only a proxy for human evaluation, this process is vulnerable to *overoptimization*, wherein past a certain point, accumulating higher reward is associated with worse human ratings. In this paper, we perform the first study on overoptimization in composite RMs, showing that correlation between component RMs has a significant effect on the locations of these points. We then introduce an approach to solve this issue using constrained reinforcement learning as a means of preventing the agent from exceeding eac",
    "openreview_id": "gkfUvn0fLU",
    "forum_id": "gkfUvn0fLU"
  },
  "analysis_timestamp": "2026-01-06T06:51:49.184732"
}
{
  "prior_works": [
    {
      "title": "DSPy: Compiling Declarative Language Model Programs",
      "authors": "Khattab et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "DSPy formalized LLM pipelines as compositional programs and introduced automatic node-level prompt tuning, a formulation GPTSwarm adopts and generalizes to multi-agent computational graphs with added structural (edge) optimization."
    },
    {
      "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
      "authors": "Wu et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "AutoGen provides a widely used multi-agent conversation framework with hand-designed roles and routing; GPTSwarm uses this style of agent orchestration as a baseline and improves it by automatically optimizing graph connectivity among agents."
    },
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": "Yao et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "ReAct\u2019s interleaving of \u2018thought\u2019 and \u2018act\u2019 tool calls effectively defines a computation graph over LLM/tool nodes, which GPTSwarm generalizes into an explicit graph abstraction spanning multiple agents."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": "Yao et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "Tree-of-Thoughts framed reasoning as search over a tree of intermediate states; GPTSwarm extends this idea by representing agent workflows as general graphs and optimizing the topology (edges) automatically."
    },
    {
      "title": "Graph of Thoughts: Solving Complex Tasks with Language Models",
      "authors": "Besta et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "Graph of Thoughts explicitly casts LLM reasoning as graphs; GPTSwarm builds on this representational insight and contributes automatic graph optimization across agents and prompts."
    },
    {
      "title": "Self-Refine: Iterative Refinement with Self-Feedback",
      "authors": "Madaan et al.",
      "year": 2023,
      "role": "Extension",
      "relationship_sentence": "Self-Refine showed that LLMs can iteratively refine outputs via self-feedback; GPTSwarm extends this principle to systematic node-level prompt refinement within its computation graph."
    },
    {
      "title": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace",
      "authors": "Shen et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "HuggingGPT connects LLMs to tool/model DAGs but relies on largely manual orchestration; GPTSwarm addresses this gap by automatically optimizing both node prompts and the DAG connectivity between agents."
    }
  ],
  "synthesis_narrative": "GPTSwarm\u2019s core innovation is to treat language-agent systems as explicit computational graphs and to optimize them at two levels: node prompts and inter-agent connectivity. This view crystallizes ideas that emerged separately across several lines of work. ReAct established a basic computation loop of reasoning and acting via tool calls\u2014implicitly a graph over LLM/tool nodes. Tree of Thoughts and Graph of Thoughts elevated this to structured search over intermediate reasoning states, making clear that non-linear structures (trees/graphs) can improve performance. In parallel, practical agent frameworks like AutoGen and HuggingGPT demonstrated multi-agent and tool orchestration, but left topology and routing largely hand-designed, revealing a gap that GPTSwarm targets: automated, principled orchestration.\nDSPy supplied the missing learning lens for such systems by compiling LLM pipelines into programs and optimizing node prompts with data-driven objectives. GPTSwarm adopts this program/graph abstraction and generalizes it to hierarchies of collaborating agents, introducing a second optimizer that searches and rewires edges to improve orchestration. For node optimization, GPTSwarm draws on iterative self-improvement methods like Self-Refine to refine prompts using feedback, but embeds this capability natively at each node of the graph.\nBy unifying these strands\u2014graph-structured reasoning (ToT/GoT), practical multi-agent/tool orchestration (AutoGen/HuggingGPT), and automatic prompt tuning of programmatic LLM pipelines (DSPy/Self-Refine)\u2014GPTSwarm delivers an optimizable graph framework that both encompasses prior prompt-engineered agents and automates their design.",
  "analysis_timestamp": "2026-01-06T23:09:26.505102"
}
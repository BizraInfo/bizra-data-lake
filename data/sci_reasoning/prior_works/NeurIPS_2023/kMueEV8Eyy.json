{
  "prior_works": [
    {
      "title": "Orbits of families of vector fields and integrability of distributions",
      "authors": "H. J. Sussmann",
      "year": 1973,
      "role": "Geometric control theorem (orbit/foliation characterization)",
      "relationship_sentence": "The paper\u2019s core method\u2014deriving all data- and loss-independent invariants from the Lie algebra generated by Jacobian vector fields\u2014relies on Sussmann\u2019s orbit theorem, which characterizes functions conserved along all flows generated by a family of vector fields as those constant on Lie\u2013algebraic orbits."
    },
    {
      "title": "Geometric Control Theory",
      "authors": "Velimir Jurdjevic",
      "year": 1997,
      "role": "Foundational toolkit for Lie algebra rank conditions and invariants",
      "relationship_sentence": "The computation of the maximal number of independent conservation laws via Lie brackets of model Jacobians echoes controllability/accessibility results and invariant characterizations summarized in Jurdjevic\u2019s text (building on Rashevsky\u2013Chow/Nagano-type results)."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot, Franck Gabriel, Cl\u00e9ment Hongler",
      "year": 2018,
      "role": "Gradient flow/Jacobian viewpoint for training dynamics",
      "relationship_sentence": "By casting training as gradient flow with dynamics driven by the model\u2019s Jacobian, NTK provides the differential-geometric framing that this paper leverages to define conservation laws tied to the Jacobian-generated vector fields."
    },
    {
      "title": "Path-SGD: Path-normalized optimization in deep neural networks",
      "authors": "Sham Kakade Neyshabur, Ruslan Salakhutdinov, Nathan Srebro",
      "year": 2015,
      "role": "Identification of rescaling symmetries in ReLU networks",
      "relationship_sentence": "Path-SGD formalized node-wise rescaling invariances of ReLU networks; the present work generalizes such architecture-induced symmetries into systematic conservation laws that hold for any data and loss."
    },
    {
      "title": "Sharp Minima Can Generalize For Deep Nets",
      "authors": "Laurent Dinh, Razvan Pascanu, Samy Bengio, Yoshua Bengio",
      "year": 2017,
      "role": "Reparameterization symmetries in deep networks",
      "relationship_sentence": "By showing how rescaling symmetries create equivalent optima, this work motivates architecture-level invariances; the NeurIPS 2023 paper turns these symmetries into explicit conserved quantities along gradient flows."
    },
    {
      "title": "Gradient Descent Maximizes the Margin of Homogeneous Neural Networks",
      "authors": "Kaifeng Lyu, Jian Li",
      "year": 2019,
      "role": "Implicit bias of gradient flow in positively homogeneous models",
      "relationship_sentence": "Results on margin maximization in homogeneous networks highlight data/loss-robust training biases; the current paper explains such robustness via conservation laws derived purely from model architecture."
    },
    {
      "title": "The Implicit Bias of Gradient Descent on Separable Data",
      "authors": "Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, Nathan Srebro",
      "year": 2018,
      "role": "Canonical implicit bias result for gradient descent",
      "relationship_sentence": "This work\u2019s demonstration that gradient descent converges to max-margin solutions on separable data motivates architecture-intrinsic, data-agnostic characterizations; the present paper provides such a characterization via conserved quantities common to all losses."
    }
  ],
  "synthesis_narrative": "The paper\u2019s central contribution\u2014defining and computing conservation laws that hold for gradient flows of a model irrespective of data or loss\u2014rests on two pillars: geometric control of vector fields and the gradient-flow/Jacobian view of learning. On the control side, Sussmann\u2019s orbit theorem and the broader geometric control toolkit consolidated by Jurdjevic connect the Lie algebra generated by a family of vector fields to the structure of their orbits and the functions invariant along all such flows. This provides exactly the mechanism the authors exploit: treat the columns of the model\u2019s Jacobian as a generating family of vector fields, close them under Lie brackets, and read off the maximal number of independent conservation laws from the rank/foliation implied by the Lie algebra.\nOn the learning side, NTK formalized training as gradient flow whose velocity is governed by the Jacobian, making Jacobian-based vector-field manipulations natural for neural networks. Prior observations about architectural symmetries in ReLU networks\u2014rescaling invariances highlighted by Path-SGD and by Dinh et al.\u2014identified concrete symmetry groups; the present work generalizes these into invariants conserved along any gradient flow induced by any dataset or loss. Finally, implicit-bias results for homogeneous and separable settings (Lyu & Li; Soudry et al.) showed striking, data-robust tendencies of gradient descent; conservation laws offer a principled, architecture-only explanation for such robustness. Together, these works directly enable the paper\u2019s algorithmic program: compute Lie-closure of Jacobian fields and extract polynomial conserved quantities, yielding a maximal, loss-agnostic inventory of invariants for gradient flows.",
  "analysis_timestamp": "2026-01-07T00:02:04.854998"
}
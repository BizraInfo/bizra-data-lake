{
  "prior_works": [
    {
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": [
        "Shunyu Yao",
        "Jeffrey Zhao",
        "Dian Yu",
        "Nan Du",
        "Izhak Shafran",
        "Karthik Narasimhan"
      ],
      "year": 2023,
      "role": "Reason+act paradigm for tool-augmented LMs",
      "relationship_sentence": "AceSearcher operationalizes ReAct\u2019s interleaving of reasoning with tool use by training a single policy to alternate between query decomposition (issuing searches) and solution synthesis, moving from prompt-only control to SFT+RL optimization for end-task accuracy."
    },
    {
      "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
      "authors": [
        "Denny Zhou",
        "Nathanael Sch\u00e4rli",
        "Jason Wei",
        "Le Hou",
        "Xuezhi Wang"
      ],
      "year": 2022,
      "role": "Problem decomposition strategy",
      "relationship_sentence": "AceSearcher\u2019s decomposer role instantiates the least-to-most principle, but learns decomposition via supervised mixtures and outcome-based reinforcement rather than relying solely on prompting templates."
    },
    {
      "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
      "authors": [
        "Shunyu Yao",
        "Dian Yu",
        "Jeffrey Zhao",
        "Izhak Shafran",
        "Thomas L. Griffiths",
        "Karthik Narasimhan"
      ],
      "year": 2023,
      "role": "Structured multi-step reasoning and exploration",
      "relationship_sentence": "AceSearcher\u2019s alternating decomposer\u2013solver cycles emulate structured exploration akin to Tree-of-Thoughts, but it trains this behavior into a single model and optimizes for final-answer reward instead of sampling-time search alone."
    },
    {
      "title": "Self-RAG: Learning to Retrieve, Generate, and Critique for Language Models",
      "authors": [
        "Akari Asai",
        "Sewon Min",
        "Yejin Choi",
        "Hannaneh Hajishirzi"
      ],
      "year": 2023,
      "role": "Self-reflective retrieval-augmented generation",
      "relationship_sentence": "AceSearcher extends Self-RAG\u2019s idea of integrating retrieval with model-driven control by unifying query decomposition and answer integration within one policy and optimizing only the final exact-match signal, avoiding intermediate supervision."
    },
    {
      "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
      "authors": [
        "Timo Schick",
        "Jane Dwivedi-Yu",
        "Roberta Raileanu"
      ],
      "year": 2023,
      "role": "Learning tool-use behaviors",
      "relationship_sentence": "AceSearcher builds on the notion of endowing LMs with learned tool-use (search) but advances it via cooperative self-play between roles and reinforcement fine-tuning tied to task outcomes rather than self-supervised proxy signals alone."
    },
    {
      "title": "STaR: Bootstrapping Reasoning With Rationales",
      "authors": [
        "Ethan Zelikman",
        "Yuhuai (Tony) Wu",
        "Noah D. Goodman"
      ],
      "year": 2022,
      "role": "Outcome-supervised bootstrapping of reasoning",
      "relationship_sentence": "AceSearcher adopts STaR\u2019s core insight\u2014bootstrapping reasoning without manual step annotations\u2014by optimizing for verifiable final answers; it generalizes the idea to retrieval-augmented, multi-hop settings with role alternation."
    },
    {
      "title": "AI Safety via Debate",
      "authors": [
        "Geoffrey Irving",
        "Paul Christiano",
        "Dario Amodei"
      ],
      "year": 2018,
      "role": "Self-play as a training paradigm",
      "relationship_sentence": "AceSearcher adapts the self-play philosophy from debate-style training to a cooperative setting: a single model alternates roles (decomposer/solver) to generate learning signals that improve multi-hop search and reasoning without intermediate labels."
    }
  ],
  "synthesis_narrative": "AceSearcher\u2019s core contribution\u2014training a single LLM to alternate between a decomposer that structures multi-hop search and a solver that integrates retrieved evidence, optimized end-to-end with only final-answer rewards\u2014sits at the intersection of reasoning+acting, decomposition, retrieval control, and self-play learning. ReAct provided the operational template for interleaving reasoning with tool use; AceSearcher internalizes this behavior through supervised and reinforcement fine-tuning rather than relying purely on prompting. Least-to-Most Prompting and Tree-of-Thoughts supplied the blueprint for decomposing complex problems and exploring intermediate solution states; AceSearcher encodes these strategies into a unified policy that learns to decompose, search, and integrate, replacing inference-time search heuristics with trainable behavior. On the retrieval side, Self-RAG showed that LMs can steer and critique their own retrieval; AceSearcher extends this by unifying decomposition and synthesis within one model and optimizing solely for end-task exact match, removing the need for step-level annotations. Toolformer demonstrated that tool-use can be learned rather than hard-coded; AceSearcher leverages this to learn effective search issuing and context integration under outcome-based reinforcement. Finally, drawing on self-play traditions exemplified by AI Safety via Debate, AceSearcher\u2019s cooperative role alternation generates training signals that bootstrap both search and reasoning. Together, these works directly inform AceSearcher\u2019s design: a trainable, cooperative reason-and-search framework that scales multi-hop retrieval and reasoning via reinforced self-play without intermediate supervision.",
  "analysis_timestamp": "2026-01-06T23:42:48.108812"
}
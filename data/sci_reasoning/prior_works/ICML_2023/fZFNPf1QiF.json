{
  "prior_works": [
    {
      "title": "A Universal Law of Robustness via Isoperimetry",
      "authors": "S\u00e9bastien Bubeck et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "This paper posed the universal (necessary) lower bound linking robustness to over-parameterization and conjectured tightness for certain models; the ICML\u201923 work directly sharpens this by proving model-specific laws\u2014disproving robustness for random features and showing NTK with even activations meets the universal bound, thereby addressing that conjecture."
    },
    {
      "title": "Random Features for Large-Scale Kernel Machines",
      "authors": "Ali Rahimi et al.",
      "year": 2008,
      "role": "Foundation",
      "relationship_sentence": "The random features model analyzed in the ICML\u201923 paper is precisely the Rahimi\u2013Recht construction, and the new non-robustness result is established for ERM in this canonical RF setting."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": "Arthur Jacot et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "The NTK framework provides the exact model class the paper studies; the main positive result proves that, for even activations, the NTK interpolator achieves the universal robustness lower bound."
    },
    {
      "title": "The Spectrum of Random Kernel Matrices",
      "authors": "Noureddine El Karoui",
      "year": 2010,
      "role": "Foundation",
      "relationship_sentence": "Spectral characterizations and Hermite expansions of kernel matrices under Gaussian inputs from this work underpin the precise kernel/RF analyses used to derive the paper\u2019s sharp robustness laws."
    },
    {
      "title": "Surprises in High-Dimensional Ridgeless Least Squares Interpolation",
      "authors": "Trevor Hastie et al.",
      "year": 2019,
      "role": "Related Problem",
      "relationship_sentence": "This work\u2019s characterization of interpolating ERM solutions in over-parameterized regimes informs the paper\u2019s focus on interpolation and over-parameterization as the lens for understanding robustness thresholds."
    },
    {
      "title": "The Generalization Error of Random Features Regression in High Dimensions",
      "authors": "Song Mei et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "High-dimensional RF analysis techniques from this line of work are extended to the adversarial setting, enabling the ICML\u201923 paper\u2019s proof that RF ERM lacks robustness regardless of over-parameterization."
    }
  ],
  "synthesis_narrative": "The core innovation of Beyond the Universal Law of Robustness is to turn a model-agnostic necessary condition into sharp, model-specific robustness laws for two canonical over-parameterized learners: random features and neural tangent kernels. The immediate intellectual spark is Bubeck and Sellke\u2019s universal isoperimetric law, which formalized how over-parameterization constrains robustness yet left open whether realistic models can meet this bound and which ones fail; the present paper answers both, and resolves the conjectured tightness for a concrete class (NTK with even activations).\n\nTo do so, the authors work squarely within two foundational model classes: Rahimi\u2013Recht random features and the Jacot\u2013Gabriel\u2013Hongler NTK regime. Their analysis relies on precise properties of kernel matrices under Gaussian inputs\u2014classic results by El Karoui on spectra and Hermite expansions\u2014which enable translating interpolation and smoothness considerations into adversarial margin statements. Prior understanding of interpolating ERM in high dimensions (Hastie\u2013Montanari\u2013Rosset\u2013Tibshirani) and the detailed asymptotics of random features regression (Mei\u2013Montanari and collaborators) provide the methodological backbone for characterizing the behavior of RF and NTK interpolators at and beyond the interpolation threshold.\n\nCombining these strands, the paper proves a dichotomy: RF ERM is never robust, even when the universal necessary condition is met, while NTK with even activations exactly attains the universal lower bound\u2014thereby sharpening the universal law into concrete, discriminating predictions for two prototypical learners.",
  "analysis_timestamp": "2026-01-06T23:09:26.529316"
}
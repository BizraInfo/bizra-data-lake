{
  "prior_works": [
    {
      "title": "The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning",
      "authors": "Rubinstein and Kroese",
      "year": 2004,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "The paper\u2019s importance-sampling estimator and adaptive proposal search directly instantiate the cross-entropy method\u2019s rare-event simulation recipe to concentrate sampling on failure-inducing inputs while maintaining unbiased likelihood-ratio estimates of tiny probabilities."
    },
    {
      "title": "Adaptive Stress Testing: Finding Likely Failure Events with Reinforcement Learning",
      "authors": "Lee et al.",
      "year": 2018,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "Adaptive Stress Testing\u2019s core idea\u2014searching input space to uncover rare failures and using importance sampling to estimate their probabilities in black-box systems\u2014directly motivates translating the same rare-event estimation paradigm to language models and prompt distributions."
    },
    {
      "title": "Red Teaming Language Models",
      "authors": "Perez et al.",
      "year": 2022,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work established systematic adversarial prompt search to elicit undesirable LM behaviors, which the current paper formalizes as an importance-sampling procedure to quantify the probability of such behaviors under a specified input distribution rather than merely discovering them."
    },
    {
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": "Bai et al.",
      "year": 2022,
      "arxiv_id": "2212.08073",
      "role": "Related Problem",
      "relationship_sentence": "By demonstrating that adversarially sourced data and feedback reduce harmful outputs, this paper provides the adversarial-training paradigm that the present work generalizes by directly minimizing an estimated rare-event probability instead of training on a fixed adversarial set."
    },
    {
      "title": "Towards Open Set Deep Networks (OpenMax) using Extreme Value Theory",
      "authors": "Bendale and Boult",
      "year": 2016,
      "arxiv_id": "1511.06233",
      "role": "Inspiration",
      "relationship_sentence": "OpenMax\u2019s EVT-based tail modeling of logits motivates the paper\u2019s activation-extrapolation approach of fitting a distribution to model logits and extrapolating to estimate tail probabilities for rare outputs under argmax decoding."
    },
    {
      "title": "On Calibration of Modern Neural Networks",
      "authors": "Guo et al.",
      "year": 2017,
      "arxiv_id": "1706.04599",
      "role": "Related Problem",
      "relationship_sentence": "The finding that simple parametric transforms of logits (e.g., temperature scaling) calibrate predicted probabilities underpins the paper\u2019s idea of modeling logit distributions and extrapolating them to estimate very small probabilities."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models (Jailbroken)",
      "authors": "Zou et al.",
      "year": 2023,
      "arxiv_id": "2307.15043",
      "role": "Gap Identification",
      "relationship_sentence": "By showing that safety-tuned models can still be reliably jailbroken with synthesized prompts, this work exposes the gap of not quantifying how likely such failures are, which the paper addresses by estimating their probabilities under a formal input distribution."
    }
  ],
  "synthesis_narrative": "Rare-event simulation offers a blueprint for quantifying tiny probabilities: the cross-entropy method prescribes adaptively reshaping sampling distributions toward failure regions while correcting with likelihood ratios to retain unbiased estimates, and Adaptive Stress Testing converts this idea into black-box search that locates likely failures and estimates their probabilities. In language models, red teaming introduced systematic adversarial prompt search to elicit undesirable behaviors, and Constitutional AI demonstrated how adversarially sourced data and feedback reduce harmful outputs, establishing adversarial training as a practical mitigation. On the modeling side, OpenMax applied extreme value theory to the tails of logit-like scores, showing that fitted tail distributions can extrapolate rare outcomes, while calibration work demonstrated that simple parametric transformations of logits can accurately reflect probabilities, suggesting that the shape of logit distributions carries usable probabilistic information. Jailbreak studies further revealed that safety-tuned models still succumb to carefully constructed prompts, underscoring the need to quantify\u2014not just find\u2014failures.\nTaken together, these strands suggest a natural next step: treat adversarial prompt search as a principled importance-sampling procedure to estimate extremely small failure probabilities under a specified input distribution, and complement it with tail modeling of logits to extrapolate beyond feasible sampling regimes. Building on rare-event IS and stress-testing, the paper adapts search-and-reweight techniques to LM prompts; drawing from EVT and calibration, it fits distributions to logits for activation extrapolation. Finally, by optimizing the estimated probability itself, it generalizes adversarial training from minimizing observed failures on curated sets to directly minimizing the modeled prevalence of undesirable behaviors.",
  "target_paper": {
    "title": "Estimating the Probabilities of Rare Outputs in Language Models",
    "authors": "Gabriel Wu, Jacob Hilton",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "low probabilities, adversarial training, importance sampling",
    "abstract": "We consider the problem of *low probability estimation*: given a machine learning model and a formally-specified input distribution, how can we estimate the probability of a binary property of the model's output, even when that probability is too small to estimate by random sampling? This problem is motivated by the need to improve worst-case performance, which distribution shift can make much more likely. We study low probability estimation in the context of argmax sampling from small transformer language models. We compare two types of methods: importance sampling, which involves searching for inputs giving rise to the rare output, and activation extrapolation, which involves extrapolating a probability distribution fit to the model's logits. We find that importance sampling outperforms activation extrapolation, but both outperform naive sampling. Finally, we explain how minimizing the probability estimate of an undesirable behavior generalizes adversarial training, and argue that ne",
    "openreview_id": "DC8bsa9bzY",
    "forum_id": "DC8bsa9bzY"
  },
  "analysis_timestamp": "2026-01-06T07:09:22.887290"
}
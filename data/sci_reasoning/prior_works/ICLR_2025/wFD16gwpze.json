{
  "prior_works": [
    {
      "title": "On-line learning in Soft Committee Machines",
      "authors": "David Saad and Sara A. Solla",
      "year": 1995,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "This work established the student\u2013teacher framework and order-parameter dynamics for one-pass SGD in two-layer (committee) networks, the analytical template that the current paper directly extends to structured (power-law) data spectra and nonlinear feature learning."
    },
    {
      "title": "High-dimensional dynamics of generalization error in neural networks",
      "authors": "M. S. Advani and Andrew M. Saxe",
      "year": 2017,
      "arxiv_id": "1710.03667",
      "role": "Inspiration",
      "relationship_sentence": "By showing how data covariance anisotropy controls learning dynamics and generalization in linear models, this paper provided the specific spectral perspective that is adopted and pushed to the power-law regime for two-layer networks trained by one-pass SGD."
    },
    {
      "title": "Gaussian Processes for Regression: Learning Curves",
      "authors": "Peter Sollich",
      "year": 2002,
      "arxiv_id": null,
      "role": "Foundation",
      "relationship_sentence": "Sollich\u2019s learning-curve theory linked eigenvalue spectra of data (or kernels) to power-law generalization rates, an insight the present work translates from kernel/GP settings to SGD-trained two-layer neural networks."
    },
    {
      "title": "Spectrum-dependent learning curves in kernel regression",
      "authors": "Blake Bordelon, Kerem Canatar, and Cengiz Pehlevan",
      "year": 2020,
      "arxiv_id": "2009.01870",
      "role": "Gap Identification",
      "relationship_sentence": "This paper quantified how power-law eigenvalue decay yields power-law learning curves in kernel regression, highlighting a gap\u2014the absence of analogous spectral\u2013scaling theory for feature-learning networks trained with SGD\u2014that the current work fills."
    },
    {
      "title": "Deep learning scaling is predictable, empirically",
      "authors": "Joel Hestness et al.",
      "year": 2017,
      "arxiv_id": "1712.00409",
      "role": "Foundation",
      "relationship_sentence": "It documented robust power-law scaling with data and model size across tasks, defining the empirical phenomenon that motivates deriving exact scaling conditions from first principles in two-layer networks."
    },
    {
      "title": "Explaining Neural Scaling Laws",
      "authors": "Yasaman Bahri et al.",
      "year": 2021,
      "arxiv_id": "2102.06701",
      "role": "Gap Identification",
      "relationship_sentence": "Offering a phenomenological account that ties power-law performance to broad difficulty spectra, this work underscored the need for a mechanistic SGD-based derivation\u2014precisely what the present analysis provides in a student\u2013teacher two-layer setting."
    }
  ],
  "synthesis_narrative": "Empirical studies first established that performance often follows power laws with data and model size, with Hestness et al. showing predictable scaling across deep learning benchmarks. Bahri et al. proposed a phenomenological explanation grounded in broad distributions of task difficulty, suggesting that heavy-tailed spectra can induce power-law learning curves. In parallel, theory from kernel and Gaussian process regression connected data or kernel eigenspectra to learning curves: Sollich demonstrated that eigenvalue decay controls generalization rates, and Bordelon, Canatar, and Pehlevan derived spectrum-dependent formulas that yield power-law regimes under power-law eigenvalue decay. Complementing these, Advani and Saxe analyzed gradient-based learning in high dimensions, revealing how anisotropic data covariance shapes generalization dynamics in linear models. Crucially, Saad and Solla introduced the student\u2013teacher framework and order-parameter dynamics for one-pass SGD in two-layer (soft committee) networks, enabling exact analytical tracking of generalization through statistical mechanics.\nTogether, these works exposed a clear opportunity: while spectral theories predict power-law learning in kernelized or linear settings, and empirical works document scaling in neural networks, there lacked a mechanistic derivation for feature-learning two-layer networks trained with one-pass SGD under structured (power-law) data. By fusing the student\u2013teacher dynamical machinery with the spectral lens from kernel/GP theory and high-dimensional linear dynamics, the present study naturally advances the field\u2014deriving explicit generalization curves, identifying when power laws emerge from power-law covariances, and extending the analysis from linear to nonlinear activation regimes.",
  "target_paper": {
    "title": "Analyzing Neural Scaling Laws in Two-Layer Networks with Power-Law Data Spectra",
    "authors": "Roman Worschech, Bernd Rosenow",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "Statistical mechanics, neural scaling laws",
    "abstract": "Neural scaling laws describe how the performance of deep neural networks scales with key factors such as training data size, model complexity, and training time, often following power-law behaviors over multiple orders of magnitude. Despite their empirical observation, the theoretical understanding of these scaling laws remains limited. In this work, we employ techniques from statistical mechanics to analyze one-pass stochastic gradient descent within a student-teacher framework, where both the student and teacher are two-layer neural networks. Our study primarily focuses on the generalization error and its behavior in response to data covariance matrices that exhibit power-law spectra.\nFor linear activation functions, we derive analytical expressions for the generalization error, exploring different learning regimes and identifying conditions under which power-law scaling emerges. Additionally, we extend our analysis to non-linear activation functions in the feature learning regime, i",
    "openreview_id": "wFD16gwpze",
    "forum_id": "wFD16gwpze"
  },
  "analysis_timestamp": "2026-01-06T08:29:59.839654"
}
{
  "prior_works": [
    {
      "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima",
      "authors": "Nitish S. Keskar et al.",
      "year": 2017,
      "role": "Gap Identification",
      "relationship_sentence": "This work argued that small-batch SGD\u2019s noise steers training away from sharp minima to improve generalization; the present paper directly revisits this claim in the single-epoch (online) regime and shows that the purported implicit-bias benefit of small batches disappears there."
    },
    {
      "title": "Stochastic Gradient Descent as Approximate Bayesian Inference",
      "authors": "Stephan Mandt et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "By modeling SGD as a stochastic process whose noise (set by learning rate and batch size) induces implicit regularization, this paper provides the core theoretical lens that the current work tests and ultimately finds insignificant in the online regime."
    },
    {
      "title": "A Bayesian Perspective on Generalization and Stochastic Gradient Descent",
      "authors": "Samuel L. Smith et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "This paper\u2019s SDE/\u2018temperature\u2019 view links batch size and learning rate to an implicit bias mechanism; the current work builds on this formulation to design online-learning experiments that isolate noise effects and show they do not yield generalization gains."
    },
    {
      "title": "Three Factors Influencing Minima in SGD",
      "authors": "Stanislaw Jastrzebski et al.",
      "year": 2017,
      "role": "Extension",
      "relationship_sentence": "Their noise-scale characterization (learning rate\u2013to\u2013batch-size ratio) is directly used and extended to the online single-pass setting, where the authors demonstrate that varying noise scale does not alter implicit bias, contrary to the offline story."
    },
    {
      "title": "An Empirical Model of Large-Batch Training",
      "authors": "Sam McCandlish et al.",
      "year": 2018,
      "role": "Inspiration",
      "relationship_sentence": "By quantifying the gradient noise scale and compute\u2013batch-size trade-offs, this work motivates the paper\u2019s central claim that in online training small batches confer strictly computational (not implicit-bias) benefits."
    },
    {
      "title": "Train longer, generalize better: closing the generalization gap in large-batch training of neural networks",
      "authors": "Elad Hoffer et al.",
      "year": 2017,
      "role": "Related Problem",
      "relationship_sentence": "Showing that generalization gaps can vanish when matching the number of updates, this work informs the present paper\u2019s design and interpretation that, in single-epoch training, batch-size effects are about compute (update counts), not bias."
    },
    {
      "title": "The Implicit Bias of Gradient Descent on Separable Data",
      "authors": "Daniel Soudry et al.",
      "year": 2018,
      "role": "Foundation",
      "relationship_sentence": "Establishing deterministic GD\u2019s implicit bias (e.g., max-margin), this paper provides the theoretical anchor for the current work\u2019s \u2018golden path\u2019 hypothesis that online SGD takes noisy steps near the noiseless GD trajectory."
    }
  ],
  "synthesis_narrative": "The paper challenges the prevailing view that the stochasticity of small-batch SGD provides a beneficial implicit bias, arguing that in online (single-epoch) learning, this noise is insignificant for generalization and is only computationally useful. This conclusion is framed against foundational stochastic-process accounts of SGD\u2019s implicit regularization (Mandt et al.; Smith et al.) and empirical claims that small batches find flatter, better-generalizing minima (Keskar et al.). The authors directly operationalize the noise-scale framework from Jastrzebski et al. and Smith et al., varying batch size and learning rate to hold the effective noise constant while controlling compute, and find that in single-pass training the expected implicit-bias advantages do not materialize. Instead, insights from compute-centric studies of batch scaling (McCandlish et al.) and from the observation that large-batch \u2018gaps\u2019 shrink when matching update counts (Hoffer et al.) point to a purely computational role for small batches in online regimes\u2014cheaper steps and more updates per unit compute. To explain the dynamics, the paper adopts GD\u2019s established implicit-bias perspective (Soudry et al.) as a reference trajectory, presenting evidence that online SGD behaves like noisy steps taken along this \u2018golden path\u2019 of noiseless gradient descent in both loss and function space. Together, these works directly motivate the problem, provide the theoretical and empirical baselines the authors test, and supply the compute-aware lens that underpins the paper\u2019s central finding: in online learning, SGD noise does not confer implicit-bias benefits.",
  "analysis_timestamp": "2026-01-06T23:09:26.450642"
}
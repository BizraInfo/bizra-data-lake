{
  "prior_works": [
    {
      "title": "Predicting many properties of a quantum system from classical shadows",
      "authors": "H.-Y. Huang et al.",
      "year": 2020,
      "arxiv_id": "2002.08953",
      "role": "Foundation",
      "relationship_sentence": "This work introduced the randomized-measurement (classical shadows) paradigm that turns raw bitstring snapshots into general-purpose property estimates, providing the input representation and problem formulation that LLM4QPE pretrains on while aiming to surpass linear estimators\u2019 sample complexity."
    },
    {
      "title": "Neural-network quantum state tomography",
      "authors": "Giacomo Torlai et al.",
      "year": 2018,
      "arxiv_id": null,
      "role": "Baseline",
      "relationship_sentence": "This paper established supervised, task- and system-specific neural estimators from measurement data, a paradigm LLM4QPE explicitly seeks to replace with a task-agnostic pretrain-then-finetune approach to reduce labeled data and per-task retraining."
    },
    {
      "title": "Deep Autoregressive Models of Many-Body Quantum States",
      "authors": "Or Sharir et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Inspiration",
      "relationship_sentence": "By treating spin/measurement configurations as sequences modeled with autoregressive Transformers, this work provided the concrete sequence-modeling insight LLM4QPE leverages to encode quantum measurement bitstrings during unsupervised pretraining."
    },
    {
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": "Jacob Devlin et al.",
      "year": 2019,
      "arxiv_id": "1810.04805",
      "role": "Inspiration",
      "relationship_sentence": "BERT\u2019s masked-token pretraining and subsequent task-specific finetuning directly motivate LLM4QPE\u2019s LLM-style, task-agnostic pretraining objective and transfer to diverse quantum property estimation tasks with limited labeled data."
    },
    {
      "title": "Masked Autoencoders Are Scalable Vision Learners",
      "authors": "Kaiming He et al.",
      "year": 2022,
      "arxiv_id": "2111.06377",
      "role": "Inspiration",
      "relationship_sentence": "The masked reconstruction principle in MAE informs LLM4QPE\u2019s unsupervised objective of reconstructing or predicting masked parts of quantum measurement sequences to learn robust, transferable representations."
    },
    {
      "title": "GROVER: Self-Supervised Graph Transformer on Large-Scale Molecular Data",
      "authors": "Kaiyuan Rong et al.",
      "year": 2020,
      "arxiv_id": null,
      "role": "Related Problem",
      "relationship_sentence": "GROVER demonstrated that self-supervised Transformer pretraining on domain-specific structures yields strong low-data transfer for property prediction, an approach LLM4QPE adapts from molecules to quantum measurement data for property estimation."
    }
  ],
  "synthesis_narrative": "Randomized measurements via classical shadows demonstrated that short bitstring snapshots can encode a wide range of quantum observables, and provided linear estimators for property prediction that scale broadly across tasks. Neural-network quantum state tomography showed that deep models trained on measurement data can map to states or properties, but typically require task- and system-specific supervised training. Deep autoregressive models of many-body quantum states took a critical step by representing spin or measurement configurations as sequences modeled by Transformers, capturing long-range correlations directly from bitstrings. In parallel, BERT established masked-token pretraining with finetuning as a general recipe for learning transferable representations from unlabeled data, while Masked Autoencoders reinforced the efficacy of masked reconstruction to learn strong encoders in an unsupervised fashion. In the scientific domain, GROVER showed how self-supervised pretraining on structured data can markedly improve downstream property prediction in low-data regimes.\nTogether, these works reveal a gap: although randomized measurement bitstrings and neural sequence models exist, and pretraining-finetuning has proven transformative elsewhere, quantum property estimation still relied on task-specific supervised learners or linear shadow estimators. The natural next step is to combine sequence modeling of measurement bitstrings with a BERT/MAE-style unsupervised objective to learn a task-agnostic representation, then finetune for specific quantum properties. LLM4QPE synthesizes these ingredients\u2014classical shadows\u2019 input formulation, Transformer sequence modeling of measurements, and self-supervised pretraining\u2014to deliver data-efficient, transferable estimators across diverse quantum systems.",
  "target_paper": {
    "title": "Towards LLM4QPE: Unsupervised Pretraining of Quantum Property Estimation and A Benchmark",
    "authors": "Yehui Tang, Hao Xiong, Nianzu Yang, Tailong Xiao, Junchi Yan",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "quantum property estimation, pretraining, finetuning",
    "abstract": "Estimating the properties of quantum systems such as quantum phase has been critical in addressing the essential quantum many-body problems in physics and chemistry. Deep learning models have been recently introduced to property estimation, surpassing  conventional statistical approaches. However, these methods are tailored to the specific task and quantum data at hand. It remains an open and attractive question for devising a more universal task-agnostic pretraining model for quantum property estimation. In this paper, we propose LLM4QPE, a large language model style quantum task-agnostic pretraining and finetuning paradigm that 1) performs unsupervised pretraining on diverse quantum systems with different physical conditions; 2) uses the pretrained model for supervised finetuning and delivers high performance with limited training data, on downstream tasks. It mitigates the cost for quantum data collection and speeds up convergence. Extensive experiments show the promising efficacy o",
    "openreview_id": "vrBVFXwAmi",
    "forum_id": "vrBVFXwAmi"
  },
  "analysis_timestamp": "2026-01-06T11:53:14.253208"
}
{
  "prior_works": [
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": "Alec Radford et al.",
      "year": 2021,
      "role": "Foundational vision\u2013language pretraining",
      "relationship_sentence": "PerceptionLM inherits CLIP-style contrastive pretraining and encoder choices as the base visual representation on which its open, reproducible PLM stack is built."
    },
    {
      "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "authors": "Jean-Baptiste Alayrac et al.",
      "year": 2022,
      "role": "Architectural template for multimodal LM",
      "relationship_sentence": "Flamingo\u2019s modality connector and sequence-level fusion for image/video informed PerceptionLM\u2019s PLM design for unified image\u2013video understanding without relying on closed components."
    },
    {
      "title": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models",
      "authors": "Isaac Awadalla et al.",
      "year": 2023,
      "role": "Open, reproducible VLM training recipe",
      "relationship_sentence": "OpenFlamingo demonstrated that Flamingo-like models can be trained end-to-end on open data, directly motivating PerceptionLM\u2019s fully open pipeline and influencing data mixtures and training procedures."
    },
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": "Junnan Li et al.",
      "year": 2023,
      "role": "Efficient vision\u2013LLM bridging without proprietary teachers",
      "relationship_sentence": "BLIP-2\u2019s strategy of coupling frozen vision encoders with an LLM via a lightweight connector guided PerceptionLM\u2019s non-distilled training paradigm and scalable alignment design."
    },
    {
      "title": "Visual Instruction Tuning (LLaVA)",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "role": "Distillation-based instruction tuning baseline",
      "relationship_sentence": "LLaVA popularized GPT-4(V)-driven distillation for instruction tuning; PerceptionLM explicitly avoids this closed-teacher route, using LLaVA as a comparison point to show the value of transparent data and training."
    },
    {
      "title": "Ego4D: Around the World in 3,000 Hours of Egocentric Video",
      "authors": "Kristen Grauman et al.",
      "year": 2022,
      "role": "Large-scale video benchmarks emphasizing fine-grained, grounded understanding",
      "relationship_sentence": "Ego4D\u2019s long-form, grounded tasks exposed gaps in detailed spatio-temporal reasoning, directly informing PerceptionLM\u2019s focus on fine-grained video QA and grounded annotations."
    },
    {
      "title": "TVQA: Localized, Compositional Video Question Answering",
      "authors": "Jie Lei et al.",
      "year": 2018,
      "role": "Precedent for temporally localized video QA",
      "relationship_sentence": "TVQA established the need for temporally localized evidence in video QA, shaping PerceptionLM\u2019s design of spatio-temporally grounded question\u2013answer pairs and evaluation protocols."
    }
  ],
  "synthesis_narrative": "PerceptionLM\u2019s central contribution\u2014an open, fully reproducible perception language model for detailed image and video understanding, supported by large-scale human-labeled spatio-temporally grounded video QA\u2014emerges from two converging lines of prior work. First, architectural and training foundations from CLIP and Flamingo defined how to couple visual and linguistic representations at scale. CLIP provided the robust vision backbone and contrastive pretraining regime, while Flamingo established a practical recipe for sequence-level multimodal fusion across images and videos. OpenFlamingo then proved these ideas could be realized transparently with open data and code, directly inspiring PerceptionLM\u2019s insistence on open pipelines and revealing practical choices in data mixtures and training procedures. BLIP-2 further showed how to efficiently bridge frozen vision encoders and LLMs without proprietary teachers, shaping PerceptionLM\u2019s non-distillation alignment strategy.\nSecond, the limitations of distillation-centric instruction tuning, epitomized by LLaVA, motivated PerceptionLM to avoid closed teachers and instead confront data gaps head-on. Benchmarks like Ego4D and TVQA highlighted that existing video QA often lacks fine-grained, spatio-temporally grounded supervision\u2014especially for long, complex video. This directly drove PerceptionLM to construct and release millions of human-labeled, fine-grained video QA pairs with explicit spatio-temporal grounding, addressing the precise failure modes exposed by these datasets. Together, these works supplied the architectural blueprint, open training ethos, and the problem framing that PerceptionLM advances by delivering transparent data, models, and a rigorous evaluation path for detailed visual understanding.",
  "analysis_timestamp": "2026-01-07T00:05:12.559344"
}
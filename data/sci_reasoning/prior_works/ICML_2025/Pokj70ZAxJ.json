{
  "prior_works": [
    {
      "title": "Parameter-Efficient Transfer Learning for NLP",
      "authors": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly",
      "year": 2019,
      "role": "Foundational adapter design enabling parameter-efficient fine-tuning by inserting small bottleneck modules into a frozen backbone",
      "relationship_sentence": "This paper\u2019s core insight\u2014that adapters can act as a domain-information decoupler\u2014directly builds on Houlsby-style adapters, leveraging their modular, add-on capacity to isolate domain-specific adaptation from shared backbone features."
    },
    {
      "title": "Simple, Scalable Adaptation for Neural Machine Translation",
      "authors": "Ankur Bapna, Orhan Firat",
      "year": 2019,
      "role": "Early demonstration of domain/language adaptation via per-domain adapters while sharing the main model",
      "relationship_sentence": "By showing that adapters can specialize to domains without altering shared parameters, this work provides a clear precedent for viewing adapters as structural decouplers of domain information, a perspective the ICML\u201925 paper formalizes and exploits for CD-FSS."
    },
    {
      "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning",
      "authors": "Jonas Pfeiffer, Andreas R\u00fcckl\u00e9, Clifton Poth, Aishwarya Kamath, Ivan Vuli\u0107, Sebastian Ruder, Iryna Gurevych",
      "year": 2020,
      "role": "Demonstrated modularity and composability of adapters that capture task/domain-specific representations",
      "relationship_sentence": "AdapterFusion\u2019s evidence that adapters encapsulate separable, composable knowledge supports the paper\u2019s thesis that adapter pathways naturally decouple domain-specific signals from backbone representations, motivating a structure-first decoupling strategy."
    },
    {
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": "Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Weizhu Chen",
      "year": 2022,
      "role": "Parameter-efficient adaptation via low-rank updates to frozen backbones",
      "relationship_sentence": "LoRA reinforces the broader principle that lightweight, add-on modules can house most domain/task-specific changes while leaving the backbone intact\u2014an architectural decoupling idea mirrored by using adapters as domain feature carriers in CD-FSS."
    },
    {
      "title": "Domain-Adversarial Training of Neural Networks (DANN)",
      "authors": "Yaroslav Ganin, Victor Lempitsky",
      "year": 2015,
      "role": "Canonical loss-based approach to enforce domain-invariant features via adversarial objectives",
      "relationship_sentence": "As a prototypical loss-based decoupler, DANN provides the contrast the paper targets: rather than enforcing invariance with auxiliary losses, the proposed DFN leverages architectural pathways (adapters) to structurally separate domain information."
    },
    {
      "title": "Domain Separation Networks",
      "authors": "Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, Dumitru Erhan",
      "year": 2016,
      "role": "Loss-based disentanglement of shared vs. private (domain-specific) representations",
      "relationship_sentence": "DSN crystallizes the goal of decoupling domain-specific from domain-invariant features; the ICML\u201925 paper pursues the same objective but replaces explicit disentanglement losses with a structure-driven decoupler built on adapters (DFN)."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution reframes adapters as intrinsic domain-information decouplers and instantiates this via a structure-based Domain Feature Navigator (DFN) for cross-domain few-shot semantic segmentation. Houlsby et al. introduced adapter modules as parameter-efficient add-ons to frozen backbones, establishing the architectural lever this paper exploits. Bapna and Firat extended adapters specifically for domain and language adaptation, empirically demonstrating that domain specialization can be cleanly isolated in small, per-domain modules\u2014an early form of structural decoupling that underpins the authors\u2019 core insight. Pfeiffer et al.\u2019s AdapterFusion further showed that adapters encapsulate modular, composable knowledge, reinforcing the idea that adapters can carry domain-specific signals without entangling the shared backbone.\n\nIn contrast, classic domain adaptation methods such as DANN and Domain Separation Networks achieve decoupling through auxiliary objectives\u2014adversarial or disentanglement losses\u2014to enforce invariance or partition private/shared subspaces. The present work departs from these loss-based strategies, arguing that the model\u2019s inherent architecture with adapters already yields a natural separation, which DFN explicitly harnesses to capture domain-specific cues and guide fine-tuning under scarce target data. LoRA\u2019s success with low-rank, add-on updates provides convergent evidence that lightweight structural adapters can shoulder most domain/task adaptation, supporting the paper\u2019s claim that structural decoupling can be both effective and data-efficient. Together, these works shape a trajectory from loss-driven invariance toward architecture-native, modular decoupling for robust CD-FSS.",
  "analysis_timestamp": "2026-01-07T00:21:32.402300"
}
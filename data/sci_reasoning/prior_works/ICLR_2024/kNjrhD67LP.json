{
  "prior_works": [
    {
      "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
      "authors": "Jun-Yan Zhu et al.",
      "year": 2017,
      "arxiv_id": "1703.10593",
      "role": "Inspiration",
      "relationship_sentence": "This work introduced the cycle-consistency training principle for learning from unpaired data, which ITIT directly generalizes from intra-visual domains to the cross-modal image\u2194text setting to enable round-trip supervision without paired examples."
    },
    {
      "title": "Dual Learning for Machine Translation",
      "authors": "Di He et al.",
      "year": 2016,
      "arxiv_id": "1611.00179",
      "role": "Inspiration",
      "relationship_sentence": "Dual learning\u2019s bidirectional loop (source\u2192target\u2192source) for exploiting monolingual corpora concretely motivated ITIT\u2019s dual image-to-text and text-to-image training cycles to leverage unpaired modality-specific datasets."
    },
    {
      "title": "Unsupervised Machine Translation Using Monolingual Corpora Only",
      "authors": "Guillaume Lample et al.",
      "year": 2018,
      "arxiv_id": "1711.00043",
      "role": "Extension",
      "relationship_sentence": "Back-translation and iterative refinement for unsupervised MT are extended in ITIT to cross-modal back-translation (image\u2192text\u2192image and text\u2192image\u2192text) using generative decoders and a shared encoder to learn from unpaired data."
    },
    {
      "title": "Multimodal Generative Models for Scalable Weakly-Supervised Learning",
      "authors": "Mike Wu et al.",
      "year": 2018,
      "arxiv_id": "1802.05335",
      "role": "Foundation",
      "relationship_sentence": "This MVAE work established a joint latent with modality-specific decoders and principled training with missing modalities, a structural template that ITIT adopts via a joint image\u2013text encoder with disjoint image and text decoders to support cross-modal generation."
    },
    {
      "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation",
      "authors": "Junnan Li et al.",
      "year": 2022,
      "arxiv_id": "2201.12086",
      "role": "Gap Identification",
      "relationship_sentence": "BLIP\u2019s reliance on pseudo-paired captions to mine noisy web images highlights the limitations of synthetic pairing, which ITIT addresses by training directly on unpaired image and text via cycle consistency rather than generating pseudo pairs."
    },
    {
      "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
      "authors": "Jiahui Yu et al.",
      "year": 2022,
      "arxiv_id": "2205.01917",
      "role": "Baseline",
      "relationship_sentence": "CoCa\u2019s joint image\u2013text encoder with a generative text decoder serves as the baseline architecture for the language pathway that ITIT extends by adding an image decoder and introducing cycle-consistent training to exploit unpaired data."
    }
  ],
  "synthesis_narrative": "Cycle consistency was first operationalized for unpaired supervision in image-to-image translation, where a mapping and its inverse are jointly learned to ensure round-trip fidelity between domains, enabling training without aligned pairs. In sequence domains, dual learning formalized bidirectional loops for machine translation to harness monolingual corpora, and subsequent unsupervised MT advanced this with back-translation and iterative refinement to stabilize and improve performance from unpaired data alone. In multimodal generative modeling, multimodal VAEs introduced a joint latent variable with modality-specific decoders and principled objectives that can train when some modalities are missing, establishing that cross-modal generation is feasible without complete pairing. At scale, vision-language pretraining methods such as contrastive captioners showed the effectiveness of joint encoders with generative text decoders for paired data, while bootstrapped pretraining demonstrated that resorting to pseudo-pairs from captioners can alleviate data scarcity but introduces noise and bias tied to the captioner.\nTogether these strands reveal an opportunity: combine bidirectional, round-trip training from unpaired corpora with a multimodal architecture that supports generation in both directions, avoiding noisy pseudo-pairing while retaining strong supervision signals. The natural next step is to couple a joint image\u2013text encoder with disjoint image and text decoders and train with cycle-consistency losses anchored by a small seed of genuine pairs, effectively extending dual/back-translation ideas to the image\u2013text modality pair and leveraging multimodal generative structures to learn from large unpaired datasets.",
  "target_paper": {
    "title": "Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency",
    "authors": "Tianhong Li, Sangnie Bhardwaj, Yonglong Tian, Han Zhang, Jarred Barber, Dina Katabi, Guillaume Lajoie, Huiwen Chang, Dilip Krishnan",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "vision-language generative model, cycle consistency",
    "abstract": "Current vision-language generative models rely on expansive corpora of $\\textit{paired}$ image-text data to attain optimal performance and generalization capabilities. However, automatically collecting such data (e.g. via large-scale web scraping) leads to low quality and poor image-text correlation, while human annotation is more accurate but requires significant manual effort and expense. We introduce $\\textbf{ITIT}$ ($\\textbf{I}$n$\\textbf{T}$egrating $\\textbf{I}$mage $\\textbf{T}$ext): an innovative training paradigm grounded in the concept of cycle consistency which allows vision-language training on $\\textit{unpaired}$ image and text data. ITIT is comprised of a joint image-text encoder with disjoint image and text decoders that enable bidirectional image-to-text and text-to-image generation in a single framework. During training, ITIT leverages a small set of paired image-text data to ensure its output matches the input reasonably well in both directions. Simultaneously, the model",
    "openreview_id": "kNjrhD67LP",
    "forum_id": "kNjrhD67LP"
  },
  "analysis_timestamp": "2026-01-07T00:17:55.730521"
}
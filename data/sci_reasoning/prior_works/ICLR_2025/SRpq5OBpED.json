{
  "prior_works": [
    {
      "title": "Gaussian-Process Factor Analysis for low-dimensional single-trial analysis of neural population activity",
      "authors": "Byron M. Yu et al.",
      "year": 2009,
      "arxiv_id": "",
      "role": "Foundation",
      "relationship_sentence": "This work established the core problem of extracting low-dimensional latent trajectories from population activity on a per-recording basis, motivating a need to leverage shared structure across related datasets that the current paper addresses."
    },
    {
      "title": "Inferring single-trial neural population dynamics using sequential autoencoders",
      "authors": "Chethan Pandarinath et al.",
      "year": 2018,
      "arxiv_id": "1709.01915",
      "role": "Baseline",
      "relationship_sentence": "LFADS introduced a powerful sequential VAE to infer nonlinear latent dynamics from a single recording, which the present work generalizes by meta-learning a low-dimensional solution manifold that enables rapid adaptation to new recordings."
    },
    {
      "title": "Recurrent switching linear dynamical systems",
      "authors": "Scott W. Linderman et al.",
      "year": 2017,
      "arxiv_id": "1610.08466",
      "role": "Related Problem",
      "relationship_sentence": "rSLDS provided an interpretable state-space framework for neural dynamics but assumes per-dataset parameter estimation, a limitation the current paper overcomes by learning recording-specific dynamics via a shared low-dimensional parameterization."
    },
    {
      "title": "Long-term stability of cortical population dynamics underlying consistent behavior",
      "authors": "Juan A. Gallego et al.",
      "year": 2020,
      "arxiv_id": "",
      "role": "Inspiration",
      "relationship_sentence": "By showing that neural population activity evolves on a stable low-dimensional manifold across days, this paper directly motivates representing inter-recording variability as coordinates on a manifold that parameterizes a family of latent dynamics."
    },
    {
      "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks",
      "authors": "Chelsea Finn et al.",
      "year": 2017,
      "arxiv_id": "1703.03400",
      "role": "Inspiration",
      "relationship_sentence": "MAML introduced the principle of learning initializations that enable rapid task-specific adaptation, which the present work adopts to quickly specialize latent dynamics for a new neural recording using few data."
    },
    {
      "title": "HyperNetworks",
      "authors": "David Ha et al.",
      "year": 2016,
      "arxiv_id": "1609.09106",
      "role": "Extension",
      "relationship_sentence": "HyperNetworks\u2019 idea of mapping a low-dimensional code into full model parameters is directly leveraged to generate recording-specific state-space dynamics from coordinates on the learned manifold."
    }
  ],
  "synthesis_narrative": "Gaussian-Process Factor Analysis framed neural population analysis as recovering low-dimensional latent trajectories from single trials, demonstrating that much variance can be captured by a compact state representation but restricting inference to per-recording linear-Gaussian models. Sequential autoencoders in LFADS advanced this to nonlinear latent dynamics with powerful amortized inference, yet still fit each dataset independently. Recurrent switching linear dynamical systems offered interpretable, piecewise-linear latent dynamics with discrete mode structure, again assuming per-dataset parameter estimation rather than exploiting cross-recording commonalities. Concurrently, systems neuroscience showed that population activity often resides on stable, low-dimensional manifolds across days and contexts, with Gallego and colleagues demonstrating long-term stability that suggests shared structure spanning recordings. In machine learning, MAML formalized meta-learning for rapid task adaptation from limited data, while HyperNetworks introduced conditioning full model parameters on low-dimensional embeddings, enabling families of models to be compactly parameterized.\nTogether these works expose a gap: powerful neural state-space models exist, and stable low-dimensional structure persists across recordings, but existing methods do not learn and exploit a shared solution space for fast adaptation. The present paper synthesizes these insights by meta-learning a low-dimensional manifold that parameterizes a family of state-space dynamics, using HyperNetwork-style conditioning to generate recording-specific parameters and MAML-style rapid adaptation to novel recordings, thereby operationalizing manifold stability within a flexible latent dynamical modeling framework.",
  "target_paper": {
    "title": "Meta-Dynamical State Space Models for Integrative Neural Data Analysis",
    "authors": "Ayesha Vermani, Josue Nassar, Hyungju Jeon, Matthew Dowling, Il Memming Park",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "spotlight",
    "keywords": "neural dynamics, state-space model, meta learning",
    "abstract": "Learning shared structure across environments facilitates rapid learning and adaptive behavior in neural systems. This has been widely demonstrated and applied in machine learning to train models that are capable of generalizing to novel settings. However, there has been limited work exploiting the shared structure in neural activity during similar tasks for learning latent dynamics from neural recordings.\nExisting approaches are designed to infer dynamics from a single dataset and cannot be readily adapted to account for statistical heterogeneities across recordings. In this work, we hypothesize that similar tasks admit a corresponding family of\nrelated solutions and propose a novel approach for meta-learning this solution space from task-related neural activity of trained animals. Specifically, we capture the variabilities across recordings on a low-dimensional manifold which concisely parametrizes this family of dynamics, thereby facilitating rapid learning of latent dynamics given ",
    "openreview_id": "SRpq5OBpED",
    "forum_id": "SRpq5OBpED"
  },
  "analysis_timestamp": "2026-01-06T08:47:39.091784"
}
{
  "prior_works": [
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": "Andrew M. Saxe, James L. McClelland, Surya Ganguli",
      "year": 2014,
      "role": "Foundational analysis of gradient-descent learning dynamics in linear networks via SVD modes",
      "relationship_sentence": "This paper establishes that gradient descent learns data singular modes in an ordered fashion, providing the analytical template the ICML 2025 work extends from static deep linear nets to temporally structured linear RNNs, adding the novel dimension of temporal precedence in mode learning."
    },
    {
      "title": "On the difficulty of training recurrent neural networks",
      "authors": "Razvan Pascanu, Tomas Mikolov, Yoshua Bengio",
      "year": 2013,
      "role": "Characterization of vanishing/exploding gradients and stability in RNN training",
      "relationship_sentence": "By tying optimization behavior to the spectrum of recurrent dynamics, this work motivates the new paper\u2019s analysis of how task dynamics interact with recurrence to determine solution stability and extrapolation properties in linear RNNs."
    },
    {
      "title": "Unitary Evolution Recurrent Neural Networks",
      "authors": "Martin Arjovsky, Amar Shah, Yoshua Bengio",
      "year": 2016,
      "role": "Stability-oriented recurrent parameterization via unitary/orthogonal dynamics",
      "relationship_sentence": "The emphasis on spectral control of the recurrent operator in Unitary RNNs directly informs the ICML 2025 paper\u2019s result that task dynamics shape stable solutions and the tradeoff between recurrent and feedforward pathways through an effective regularizer."
    },
    {
      "title": "Gradient Descent Learns Linear Dynamical Systems",
      "authors": "Moritz Hardt, Tengyu Ma, Benjamin Recht",
      "year": 2018,
      "role": "Convergence and identifiability of gradient methods for linear dynamical systems",
      "relationship_sentence": "This work shows how gradient descent behaves when fitting LDS, underpinning the new analysis that derives exact learning dynamics in linear RNNs and connects convergence/stability to the temporal structure of the task."
    },
    {
      "title": "Learning Linear Dynamical Systems via Spectral Filtering",
      "authors": "Elad Hazan, Karan Singh, Cyril Zhang",
      "year": 2017,
      "role": "Spectral/Hankel viewpoint for exploiting temporal structure in system identification",
      "relationship_sentence": "By framing temporal tasks through spectral (Hankel) structure, this paper provides the mathematical lens the ICML 2025 work leverages to show that singular components associated with later temporal structure can be learned faster in LRNNs."
    },
    {
      "title": "Implicit Regularization in Matrix Factorization",
      "authors": "Suriya Gunasekar, Jason D. Lee, Daniel Soudry, Nati Srebro (alt. versions with Woodworth, Bhojanapalli, Neyshabur, Srebro)",
      "year": 2017,
      "role": "Theory of implicit bias/regularization induced by gradient descent in linear factorizations",
      "relationship_sentence": "The new paper\u2019s identification of an effective regularization term that biases toward small weights and mediates a recurrent\u2013feedforward tradeoff builds directly on the implicit regularization principles established for linear (factored) models."
    },
    {
      "title": "Principal component analysis in linear systems: Controllability, observability, and model reduction",
      "authors": "Bruce C. Moore",
      "year": 1981,
      "role": "Balanced truncation and Hankel singular values for LTI systems",
      "relationship_sentence": "The connection between task dynamics, stability, and dominant Hankel/singular directions in linear systems informs the ICML 2025 result that temporal structure governs which modes are learned and how solutions extrapolate in LRNNs."
    }
  ],
  "synthesis_narrative": "The ICML 2025 paper builds a precise learning-dynamics theory for linear RNNs by synthesizing insights from deep linear learning, system identification, and RNN stability. Saxe et al. established that gradient descent learns data singular modes in a predictable order in deep linear networks; this work generalizes that paradigm to temporally structured settings, revealing a new interaction between mode scale and temporal precedence. The spectral/Hankel viewpoint from control and identification\u2014via Moore\u2019s balanced truncation and Hazan et al.\u2019s spectral filtering\u2014grounds the treatment of task dynamics, clarifying why later-occurring temporal components can be preferentially learned and how dominant Hankel/singular directions shape extrapolation. Concurrently, the training stability and gradient pathologies characterized by Pascanu et al., and the spectral control strategies exemplified by Unitary RNNs, motivate and contextualize the paper\u2019s stability and extrapolation results for LRNNs in terms of the recurrent operator\u2019s spectrum.\n\nOn the optimization side, Hardt, Ma, and Recht\u2019s analysis of gradient descent for linear dynamical systems provides direct precedent for studying convergence in recurrent linear models under gradient-based training. Finally, the discovery of an effective regularization term mediating a tradeoff between recurrent and feedforward computation extends the implicit bias principles known from linear factorization (Gunasekar et al.) to the recurrent, temporally structured regime. Together, these works directly scaffold the paper\u2019s core contributions: ordered learning of temporal singular components, task-dependent stability and extrapolation, and an implicit regularizer that allocates computation across recurrent and feedforward pathways.",
  "analysis_timestamp": "2026-01-07T00:21:32.387895"
}
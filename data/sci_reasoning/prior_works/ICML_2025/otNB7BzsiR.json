{
  "prior_works": [
    {
      "title": "SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot",
      "authors": "Frantar et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "This work establishes one-shot, layer-wise Hessian-guided pruning for LLMs via local output reconstruction, but implicitly suffers from cumulative reconstruction error across layers\u2014precisely the phenomenon the present paper formalizes and mitigates with a depth-dependent sparsity schedule."
    },
    {
      "title": "Wanda: Pruning by Weights and Activations for Large Language Models",
      "authors": "Sun et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "Wanda provides a simple post-training pruning baseline using weight\u2013activation magnitudes and typically uniform/global sparsity, which lacks a principled layer-wise allocation and therefore exhibits the reconstruction-error accumulation that the new arithmetic-progression allocation is designed to avoid."
    },
    {
      "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers",
      "authors": "Frantar et al.",
      "year": 2023,
      "role": "Inspiration",
      "relationship_sentence": "GPTQ formalizes sequential, layer-wise output reconstruction for transformers and highlights error accumulation during post-training compression, directly inspiring the paper\u2019s theoretical treatment of reconstruction-error propagation and its translation to sparsity allocation."
    },
    {
      "title": "Net-Trim: Convex pruning of deep neural networks with provable guarantees",
      "authors": "Aghasi et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "Net-Trim introduces a layer-wise reconstruction framework with explicit cumulative error bounds across depth, providing the theoretical foundation on which this paper builds to derive a monotone, closed-form (arithmetic progression) allocation of pruning tolerances."
    },
    {
      "title": "AMC: AutoML for Model Compression and Acceleration on Mobile Devices",
      "authors": "He et al.",
      "year": 2018,
      "role": "Gap Identification",
      "relationship_sentence": "AMC demonstrates that per-layer sparsity allocation is a costly multi-parameter search problem under global constraints, a practical gap the new method closes by reducing allocation to a single common-difference hyperparameter."
    },
    {
      "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning",
      "authors": "Sanh et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "Movement Pruning shows depth-dependent sensitivity in transformers via training-time sparsification, informing the intuition that layers should receive different sparsity levels; the present work achieves this adaptivity post-training with a theoretically justified increasing schedule."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core innovation\u2014a theoretically derived, monotonically increasing arithmetic progression for layer-wise sparsity\u2014emerges from a direct line of work on local reconstruction and sequential compression. Net-Trim provided the foundational perspective that pruning can be posed as layer-wise output reconstruction with explicit control of error propagation across depth. GPTQ then popularized a similar sequential reconstruction view for transformers in post-training quantization, empirically revealing how local errors accumulate when layers are processed in order. Transferring this lens to sparsification, the authors identify the same reconstruction-error explosion as the key bottleneck.\n\nOn the LLM pruning front, SparseGPT and Wanda serve as primary baselines: both operate layer-wise and one-shot, yet typically rely on uniform or ad hoc sparsity settings that do not account for cross-layer error amplification. The new theory explains why these choices degrade downstream performance and prescribes a simple, depth-increasing allocation that preserves early layers (where errors would propagate) while safely pruning later layers more aggressively. Practically, this collapses the high-dimensional allocation search highlighted by AMC to a single parameter, eliminating expensive tuning while improving reliability. Movement Pruning, though training-based, corroborates the notion of heterogeneous layer sensitivity in transformers; the present work distills that intuition into a post-training, provably motivated allocation rule. Together, these prior works directly shape the paper\u2019s identification of error explosion and its elegant, one-parameter solution for layer-wise sparsity in LLMs.",
  "analysis_timestamp": "2026-01-06T23:07:19.644499"
}
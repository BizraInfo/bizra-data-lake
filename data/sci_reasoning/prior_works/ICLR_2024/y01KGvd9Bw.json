{
  "prior_works": [
    {
      "title": "Flamingo: a Visual Language Model for Few-Shot Learning",
      "authors": "Jean-Baptiste Alayrac et al.",
      "year": 2022,
      "arxiv_id": "2204.14198",
      "role": "Gap Identification",
      "relationship_sentence": "Flamingo established interleaved image\u2013text prompting with a frozen visual encoder but could not generate images, directly motivating DreamLLM\u2019s unified autoregressive modeling over raw image and text tokens to enable both understanding and free-form interleaved generation."
    },
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": "Junnan Li et al.",
      "year": 2023,
      "arxiv_id": "2301.12597",
      "role": "Gap Identification",
      "relationship_sentence": "BLIP-2\u2019s reliance on external CLIP-style visual encoders and a Q-Former creates a feature bottleneck and precludes image synthesis, which DreamLLM addresses by directly sampling in the raw multimodal token space to avoid information loss while enabling image generation."
    },
    {
      "title": "OFA: Unifying Architectures, Tasks, and Modalities through a Simple Sequence-to-Sequence Learning Framework",
      "authors": "Peng Wang et al.",
      "year": 2022,
      "arxiv_id": "2202.03052",
      "role": "Inspiration",
      "relationship_sentence": "OFA showed a single seq2seq model can perform both multimodal understanding and image synthesis via a unified token interface, an idea DreamLLM adopts and extends to free-form interleaved documents and full joint distribution modeling."
    },
    {
      "title": "NUWA: Visual Synthesis Pre-training for Unified Image and Video Generation",
      "authors": "Huaishao Luo et al.",
      "year": 2021,
      "arxiv_id": "2111.12417",
      "role": "Foundation",
      "relationship_sentence": "NUWA framed visual synthesis as autoregressive generation over quantized visual tokens conditioned on text, a discrete-token generative view that DreamLLM generalizes to jointly model text and images in arbitrary interleavings."
    },
    {
      "title": "Taming Transformers for High-Resolution Image Synthesis",
      "authors": "Patrick Esser et al.",
      "year": 2021,
      "arxiv_id": "2012.09841",
      "role": "Foundation",
      "relationship_sentence": "VQGAN provided the discrete image codebook and transformer recipe enabling images to be modeled as token sequences, a prerequisite for DreamLLM\u2019s direct sampling in the raw multimodal space."
    },
    {
      "title": "CogView: Mastering Text-to-Image Generation via Transformers",
      "authors": "Ming Ding et al.",
      "year": 2021,
      "arxiv_id": "2105.13290",
      "role": "Inspiration",
      "relationship_sentence": "CogView demonstrated a single transformer can jointly model text and VQ image tokens for both text-to-image and captioning, directly inspiring DreamLLM\u2019s single-model synergy between multimodal comprehension and creation."
    },
    {
      "title": "LLaVA: Large Language and Vision Assistant",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "arxiv_id": "2304.08485",
      "role": "Baseline",
      "relationship_sentence": "As a representative MLLM that aligns an LLM with CLIP-derived visual features for understanding but not image generation, LLaVA serves as a primary baseline that DreamLLM surpasses by eliminating feature encoders and generating free-form interleaved content."
    }
  ],
  "synthesis_narrative": "Discrete image tokenization via VQGAN established that pixels could be represented as code sequences, making transformer-based autoregressive modeling in the image domain practical. Building on this, CogView showed that a single transformer trained jointly on text and visual code tokens could perform text-to-image generation and image captioning within one model, demonstrating early synergy between comprehension and creation through shared generative modeling. NUWA further cast visual synthesis as autoregressive generation over quantized visual tokens conditioned on text, reinforcing the discrete-token generative formulation for cross-modal modeling. OFA unified a broad set of multimodal tasks under a single sequence-to-sequence interface, including both understanding and image synthesis through a common tokenized representation, pointing to the power of a unified modeling space. In contrast, Flamingo introduced interleaved image\u2013text prompting for strong comprehension but relied on frozen image encoders and could not generate images, while BLIP-2 (and follow-ups like LLaVA) strengthened understanding by coupling frozen vision encoders to LLMs, at the cost of a feature bottleneck and no raw-space generation.\nCollectively, these works revealed a clear opportunity: unify the discrete-token generative paradigm (VQGAN, CogView, NUWA, OFA) with the interleaved multimodal reasoning setup popularized by Flamingo, but without frozen feature bottlenecks. DreamLLM takes the natural next step by modeling text and image tokens jointly in a single autoregressive space and training on raw, interleaved documents, thereby learning conditional, marginal, and joint distributions and enabling free-form interleaved generation alongside strong multimodal understanding.",
  "target_paper": {
    "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation",
    "authors": "Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, Li Yi",
    "conference": "ICLR",
    "year": 2024,
    "presentation_type": "spotlight",
    "keywords": "Multimodal Large Language Models, Large Language Models, Generative Models, Vision Language, Representation Learning, GPT",
    "abstract": "This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal",
    "openreview_id": "y01KGvd9Bw",
    "forum_id": "y01KGvd9Bw"
  },
  "analysis_timestamp": "2026-01-06T19:48:50.654846"
}
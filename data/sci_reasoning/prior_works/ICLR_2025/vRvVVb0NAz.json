{
  "prior_works": [
    {
      "title": "Editing Models with Task Arithmetic",
      "authors": "Gabriel Ilharco et al.",
      "year": 2023,
      "arxiv_id": "2212.04089",
      "role": "Foundation",
      "relationship_sentence": "This work introduced task vectors as weight updates from a pretrained model and empirically showed addition and negation can compose or erase capabilities, which this paper rigorously characterizes with generalization guarantees for nonlinear Transformers."
    },
    {
      "title": "Model Soups: Averaging weights of multiple fine-tuned models improves generalization",
      "authors": "Mitchell Wortsman et al.",
      "year": 2022,
      "arxiv_id": "2203.05482",
      "role": "Inspiration",
      "relationship_sentence": "By demonstrating that simple weight-space averaging of fine-tuned models can improve multi-task generalization, this paper motivated analyzing when unweighted addition of task-specific updates is provably effective in attention-based nonlinear networks."
    },
    {
      "title": "Merging Models with Fisher-Weighted Averaging",
      "authors": "Zachary Matena et al.",
      "year": 2021,
      "arxiv_id": "2111.09832",
      "role": "Related Problem",
      "relationship_sentence": "This work proposed Fisher-weighted model merging to mitigate interference between tasks, and the present analysis pinpoints conditions (aligned or irrelevant discriminative patterns) under which plain task-vector addition already generalizes without Fisher reweighting."
    },
    {
      "title": "Git Re-Basin: Merging Models modulo Permutation Symmetries",
      "authors": "Evan C. Ainsworth et al.",
      "year": 2023,
      "arxiv_id": "2209.04836",
      "role": "Gap Identification",
      "relationship_sentence": "By revealing that parameter alignment is critical for successful weight-space merging, this paper exposed the need to formalize notions of alignment, which here are instantiated as aligned discriminative patterns enabling provably effective task-vector addition."
    },
    {
      "title": "TIES-Merging: Resolving Interference when Merging Models",
      "authors": "Yadav et al.",
      "year": 2023,
      "arxiv_id": null,
      "role": "Gap Identification",
      "relationship_sentence": "This study showed naive summation of fine-tuned weights can fail due to interference and proposed structured merging, motivating this work\u2019s theoretical delineation of regimes (irrelevant vs. aligned tasks) where interference is negligible or constructive for task-vector addition/negation."
    },
    {
      "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning",
      "authors": "Jonas Pfeiffer et al.",
      "year": 2021,
      "arxiv_id": "2005.00247",
      "role": "Related Problem",
      "relationship_sentence": "AdapterFusion demonstrated that composing task-specific parameter updates can yield multi-task behavior, directly inspiring a formal study of when additive composition of full-parameter task deltas succeeds in nonlinear Transformer architectures."
    }
  ],
  "synthesis_narrative": "Task-vector editing emerged from the observation that weight deltas between a pretrained model and fine-tuned variants encode task-specific capabilities that can be additively composed or negated; Editing Models with Task Arithmetic crystallized this idea and documented robust empirical effects such as task addition and forgetting via simple weight-space operations. Model Soups further showed that naive averaging of independently fine-tuned weights often improves generalization, suggesting surprising linearity in weight space that might extend to composing task-specific updates. Complementing these findings, Merging Models with Fisher-Weighted Averaging argued that interference across tasks can derail naive merging and proposed curvature-aware reweighting, while Git Re-Basin highlighted that parameter alignment (modulo permutation symmetries) is crucial to making any merging succeed. TIES-Merging explicitly identified interference as a core failure mode and introduced structure to reduce destructive interactions during merging, underscoring the need to characterize when simple addition is safe. AdapterFusion provided converging evidence that composing task-specific parameter changes can yield multi-task competence without retraining, albeit through adapters rather than full weights. Together these works raised a precise opportunity: isolate when the apparent linearity in weight space is principled versus fragile. The present paper seizes this by formalizing a conceptual learning setting for nonlinear Transformers and proving generalization guarantees for task-vector addition and negation under explicit task relationships (aligned or irrelevant discriminative patterns), thereby theoretically grounding when simple, training-free model editing is provably effective.",
  "target_paper": {
    "title": "When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers",
    "authors": "Hongkang Li, Yihua Zhang, Shuai Zhang, Pin-Yu Chen, Sijia Liu, Meng Wang",
    "conference": "ICLR",
    "year": 2025,
    "presentation_type": "oral",
    "keywords": "Task arithmetic, generalization, nonlinear Transformers, deep learning theory, machine unlearning",
    "abstract": "Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned ta",
    "openreview_id": "vRvVVb0NAz",
    "forum_id": "vRvVVb0NAz"
  },
  "analysis_timestamp": "2026-01-06T14:00:03.827096"
}
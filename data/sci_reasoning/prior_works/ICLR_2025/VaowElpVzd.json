{
  "prior_works": [
    {
      "title": "Learning Individual Styles of Conversational Gesture",
      "authors": "Shiry Ginosar et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "Introduced the modern problem formulation of mapping raw speech to human gestures and established core alignment principles that Co^3Gesture retains while generalizing from a single speaker to concurrent two-person interactions."
    },
    {
      "title": "Gesticulator: A framework for semantically-aware speech-driven gesture generation",
      "authors": "Taras Kucherenko et al.",
      "year": 2020,
      "role": "Gap Identification",
      "relationship_sentence": "A leading single-speaker co-speech gesture system whose inability to model two interlocutors\u2019 concurrent, coordinated gestures directly motivates Co^3Gesture\u2019s interactive, dual-branch design."
    },
    {
      "title": "Style-Controllable Speech-Driven Gesture Synthesis Using Normalizing Flows",
      "authors": "Simon Alexanderson et al.",
      "year": 2020,
      "role": "Baseline",
      "relationship_sentence": "Provides a strong speech-to-gesture baseline (single-speaker, style-controllable) that Co^3Gesture surpasses by introducing concurrent, cross-person coherence and interaction-aware generation."
    },
    {
      "title": "Human Motion Diffusion Model",
      "authors": "Guy Tevet et al.",
      "year": 2022,
      "role": "Extension",
      "relationship_sentence": "Supplies the diffusion-based motion synthesis paradigm that Co^3Gesture extends into an interactive setting with two cooperative diffusion branches that exchange information to maintain inter-speaker coherence."
    },
    {
      "title": "SepFormer: Speech Separation with Transformers",
      "authors": "D. S. Subakan et al.",
      "year": 2021,
      "role": "Foundation",
      "relationship_sentence": "Provides a practical method for decomposing overlapped two-speaker audio streams; Co^3Gesture relies on such per-speaker separation to condition each branch on the correct speaker signal."
    },
    {
      "title": "End-to-End Neural Speaker Diarization",
      "authors": "Yusuke Fujita et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "Enables assigning time-varying speech activity to specific speakers in multi-party audio; Co^3Gesture leverages this diarization-style decomposition to synchronize each agent\u2019s gestures with its corresponding speech in concurrent conversation."
    }
  ],
  "synthesis_narrative": "The lineage of Co^3Gesture\u2019s core idea begins with Ginosar et al., who crystallized the modern formulation of mapping speech to body gestures and demonstrated that acoustic prosody can predict gesture timing and form. Building on this, single-speaker systems such as Gesticulator and style-controllable flow-based methods (Alexanderson et al.) advanced semantic and stylistic fidelity but remained fundamentally monologic, exposing a key gap: they could not model concurrent, coordinated gestures in a two-person dialogue. Co^3Gesture addresses this gap by adopting diffusion as the generative backbone\u2014specifically extending the Human Motion Diffusion Model into a two-branch, interactive diffusion architecture in which branches exchange information to ensure inter-person coherence and responsiveness. Crucially, this interaction-aware design depends on clean, per-speaker conditioning signals in overlapping speech. Here, speech processing advances are foundational: SepFormer supplies high-quality source separation to yield distinct speaker waveforms, while end-to-end neural diarization provides time-resolved speaker attribution, allowing the model to align each agent\u2019s gestures to the correct speaker stream. Together, these works directly enable Co^3Gesture\u2019s key innovation\u2014coherent, concurrent co-speech gesture synthesis with explicit inter-speaker coordination\u2014and motivate the creation of a large-scale dyadic dataset to overcome the limitations of prior single-speaker corpora.",
  "analysis_timestamp": "2026-01-06T23:09:26.596780"
}
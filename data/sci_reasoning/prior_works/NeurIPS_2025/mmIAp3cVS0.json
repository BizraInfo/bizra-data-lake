{
  "prior_works": [
    {
      "title": "Generative Agents: Interactive Simulacra of Human Behavior",
      "authors": "Joon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, Michael S. Bernstein",
      "year": 2023,
      "role": "Single-agent long-term memory and reflection",
      "relationship_sentence": "G-Memory generalizes Generative Agents\u2019 memory stream, importance-weighted retrieval, and reflective \u201cinsight\u201d formation to a multi-agent setting by elevating those reflections into an explicit insight graph that conditions and is conditioned by agent interactions."
    },
    {
      "title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Collaboration",
      "authors": "Wu et al.",
      "year": 2023,
      "role": "LLM multi-agent collaboration framework",
      "relationship_sentence": "AutoGen\u2019s conversational interaction traces and role-based multi-agent exchanges motivate G-Memory\u2019s interaction and query graphs, which explicitly model inter-agent collaboration trajectories and task contexts across sessions."
    },
    {
      "title": "CAMEL: Communicative Agents for \u201cMind\u201d Exploration",
      "authors": "Li et al.",
      "year": 2023,
      "role": "Role-playing multi-agent communication",
      "relationship_sentence": "CAMEL\u2019s role-specialized agents highlight the need for agent-specific and cross-trial memory; G-Memory operationalizes this with agent-tailored nodes and edges that preserve role-dependent experiences and coordination patterns."
    },
    {
      "title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
      "authors": "Shinn et al.",
      "year": 2023,
      "role": "Self-reflective lessons to improve future trials",
      "relationship_sentence": "Reflexion\u2019s episode-level \u201clessons\u201d directly inspire G-Memory\u2019s high-level, generalizable insight layer, extending reflection from single agents to team-level insights that guide future multi-agent behaviors."
    },
    {
      "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
      "authors": "Guanzhi Wang et al.",
      "year": 2023,
      "role": "Lifelong skill library and cross-episode accumulation",
      "relationship_sentence": "Voyager\u2019s persistent skill and experience accumulation informs G-Memory\u2019s cross-trial retention and retrieval, while G-Memory extends this idea by structuring memories across agents and linking skills to collaboration traces."
    },
    {
      "title": "Graph of Thoughts: Solving Elaborate Problems with LLMs",
      "authors": "Maciej Besta et al.",
      "year": 2023,
      "role": "Graph-structured reasoning and traversal",
      "relationship_sentence": "G-Memory adapts the idea of graph-structured organization and traversal from reasoning to memory, introducing three graph tiers and bi-directional traversal to bridge granular interactions with abstract insights."
    },
    {
      "title": "Organizational Memory",
      "authors": "James P. Walsh, Gerardo R. Ungson",
      "year": 1991,
      "role": "Theory of hierarchical, distributed organizational memory",
      "relationship_sentence": "Walsh and Ungson\u2019s framework for storage bins and retrieval in organizations directly shapes G-Memory\u2019s hierarchical design and the mapping of agent- and team-level memories into structured repositories."
    }
  ],
  "synthesis_narrative": "G-Memory\u2019s core contribution\u2014a hierarchical, graph-structured memory that captures inter-agent collaboration trajectories and supports cross-trial, agent-specific retrieval\u2014emerges at the intersection of multi-agent LLM frameworks, single-agent long-term memory, and graph-structured reasoning, grounded in organizational memory theory. AutoGen and CAMEL established the power of role-based, conversational multi-agent systems while exposing a gap: interaction logs were retained only as flat transcripts with little structure for reuse. In parallel, single-agent memory advances like Generative Agents and Reflexion showed that persistent episodic traces can be distilled into higher-level insights that improve future performance, and Voyager demonstrated how cross-episode accumulation enables persistent competence. G-Memory lifts these ideas into the multi-agent domain by explicitly modeling three tiers\u2014interaction, query, and insight graphs\u2014so that low-level exchanges, task contexts, and distilled lessons are separately maintained yet linked. The design borrows from Graph of Thoughts the benefits of representing knowledge as graphs and navigating them via structured traversal; G-Memory\u2019s bi-directional traversal retrieves abstract insights top-down while also grounding them bottom-up in concrete interaction subgraphs. Underpinning this is Walsh and Ungson\u2019s organizational memory theory, which informs the hierarchical separation of storage and retrieval across agents (individual memory) and the team (organizational memory). Together, these works directly shaped G-Memory\u2019s architecture for scalable, personalized, and collaboration-aware memory in multi-agent systems.",
  "analysis_timestamp": "2026-01-07T00:21:32.347189"
}
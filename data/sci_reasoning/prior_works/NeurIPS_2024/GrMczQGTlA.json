{
  "prior_works": [
    {
      "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
      "authors": "Lili Chen et al.",
      "year": 2021,
      "role": "Conceptual/methodological precursor",
      "relationship_sentence": "Established that control can be cast as autoregressive next-token prediction over trajectories, directly inspiring the paper\u2019s framing of humanoid locomotion as causal sequence modeling."
    },
    {
      "title": "Trajectory Transformer",
      "authors": "Michael Janner et al.",
      "year": 2021,
      "role": "Method for autoregressive modeling of continuous control",
      "relationship_sentence": "Showed how to discretize and model state\u2013action trajectories with a transformer, informing tokenization and autoregressive prediction choices for sensorimotor streams."
    },
    {
      "title": "Gato: A Generalist Agent",
      "authors": "Scott Reed et al.",
      "year": 2022,
      "role": "Multimodal generalist architecture precedent",
      "relationship_sentence": "Demonstrated a single causal transformer operating on interleaved, modality-tagged tokens for vision, language, and control, motivating the modality-aligned next-token training used to handle heterogeneous sensorimotor inputs."
    },
    {
      "title": "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control",
      "authors": "Anthony Brohan et al.",
      "year": 2023,
      "role": "Large-scale multimodal robot learning precedent",
      "relationship_sentence": "Showed that internet-scale visual-language data can be fused with robot action tokens to improve real-world control and generalization, supporting this paper\u2019s use of YouTube human videos alongside policy/controller data."
    },
    {
      "title": "DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills",
      "authors": "Xue Bin (Jason) Peng et al.",
      "year": 2018,
      "role": "Humanoid motion imitation precedent",
      "relationship_sentence": "Established motion capture as a powerful supervision signal for humanoid skills, directly motivating inclusion of mocap sequences as a training source for the token prediction model."
    },
    {
      "title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos",
      "authors": "Bowen Baker et al.",
      "year": 2022,
      "role": "Using web video with missing actions",
      "relationship_sentence": "Demonstrated how to leverage YouTube videos with absent action labels for policy learning, conceptually underpinning the paper\u2019s modality-aligned objective that accommodates missing modalities (e.g., video-only data)."
    },
    {
      "title": "Rapid Motor Adaptation for Legged Robots",
      "authors": "A. Kumar et al.",
      "year": 2021,
      "role": "Real-world locomotion robustness precedent",
      "relationship_sentence": "Showed zero-shot robustness and adaptation for locomotion in the real world, informing the goal of robust real-world transfer and generalization from limited data in humanoid walking."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core idea\u2014casting humanoid locomotion as next-token prediction with a causal transformer over modality-aligned sensorimotor sequences\u2014stands on a clear lineage of sequence modeling for control and multimodal generalist agents. Decision Transformer and Trajectory Transformer provided the decisive methodological shift: treat control as autoregressive sequence modeling, with discretization/tokenization strategies that make continuous sensorimotor streams amenable to next-token prediction. Building on this, Gato established that a single transformer can operate on interleaved, modality-tagged tokens across vision, language, and action, directly informing the paper\u2019s modality-aligned autoregression and its ability to train on heterogeneous inputs.\nRT-2 showed that augmenting robot data with internet-scale visual-language corpora and action vocabularies materially improves real-world generalization, an insight mirrored here by mixing prior policy/controller rollouts, mocap, and YouTube human videos. DeepMimic contributed the blueprint for exploiting motion capture as a rich supervision source for humanoid skills, legitimizing mocap as a key component of the training set. VPT demonstrated a practical path to leverage web videos where action labels are absent, conceptually supporting the paper\u2019s objective that predicts the next token within each modality and thus tolerates missing modalities. Finally, Rapid Motor Adaptation highlighted strategies for robust real-world locomotion and zero-shot transfer, shaping the paper\u2019s emphasis on deployment in the wild and generalization from limited hours of data. Together, these works converge on a unified, autoregressive, multimodal paradigm that this paper extends to real humanoid walking.",
  "analysis_timestamp": "2026-01-06T23:39:42.952631"
}
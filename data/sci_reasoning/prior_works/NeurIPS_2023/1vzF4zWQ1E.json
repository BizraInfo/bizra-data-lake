{
  "prior_works": [
    {
      "title": "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification",
      "authors": "Joy Buolamwini, Timnit Gebru",
      "year": 2018,
      "role": "Empirical foundation on demographic bias in face analysis",
      "relationship_sentence": "Provided pivotal evidence that face analysis systems exhibit large demographic disparities, motivating the paper\u2019s focus on fairness in face recognition and the need to look beyond data-only explanations."
    },
    {
      "title": "Face Recognition Vendor Test (FRVT) Part 3: Demographic Effects",
      "authors": "Patrick Grother, Mei Ngan, Kayee Hanaoka",
      "year": 2019,
      "role": "Benchmarking and metrics for demographic differentials in face recognition",
      "relationship_sentence": "Established rigorous, large-scale measurements of demographic performance gaps in face recognition, shaping the paper\u2019s evaluation protocol and fairness metrics (e.g., FMR/FRR gaps) against which new architectures are judged."
    },
    {
      "title": "Racial Faces in the Wild: Reducing Racial Bias by Deep Unsupervised Domain Adaptation",
      "authors": "Mei Wang, Weihong Deng",
      "year": 2019,
      "role": "Fairness benchmark and data-centric mitigation baseline for face recognition",
      "relationship_sentence": "Introduced the RFW benchmark and a data/process-centric mitigation approach, giving the authors a standard evaluation setting and a class of baselines that their architecture-centric approach aims to surpass."
    },
    {
      "title": "ArcFace: Additive Angular Margin Loss for Deep Face Recognition",
      "authors": "Jiankang Deng, Jia Guo, Niannan Xue, Stefanos Zafeiriou",
      "year": 2019,
      "role": "State-of-the-art face recognition training objective and backbone baseline",
      "relationship_sentence": "Served as the dominant FR training recipe and architectural choice that the paper interrogates for inherent bias, and against which the fairness\u2013accuracy Pareto improvements of searched architectures are compared."
    },
    {
      "title": "A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II",
      "authors": "Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, T. Meyarivan",
      "year": 2002,
      "role": "Foundational framework for Pareto multi-objective optimization",
      "relationship_sentence": "Provided the conceptual backbone for optimizing competing objectives\u2014here, fairness and accuracy\u2014enabling the paper\u2019s fairness-aware search to find architectures on superior Pareto fronts."
    },
    {
      "title": "BOHB: Robust and Efficient Hyperparameter Optimization at Scale",
      "authors": "Stefan Falkner, Aaron Klein, Frank Hutter",
      "year": 2018,
      "role": "Scalable joint hyperparameter and architecture search methodology",
      "relationship_sentence": "Influenced the paper\u2019s design of a joint search over architectures and hyperparameters, a key technical element for uncovering that architectural choices themselves drive fairness outcomes."
    },
    {
      "title": "Mitigating Unwanted Biases with Adversarial Learning",
      "authors": "Brian Hu Zhang, Blake Lemoine, Margaret Mitchell",
      "year": 2018,
      "role": "Representative in-processing debiasing paradigm",
      "relationship_sentence": "Embodies the prevailing model-centric debiasing approach the paper contrasts with, highlighting that despite such interventions, biases persist\u2014motivating the shift to architecture-level mitigation via search."
    }
  ],
  "synthesis_narrative": "This paper reframes fairness in face recognition by arguing that biases can be intrinsic to neural architectures, not only to data or training procedures. Early empirical works\u2014Gender Shades and NIST\u2019s FRVT Part 3\u2014documented substantial demographic disparities and established rigorous evaluation metrics (e.g., FMR/FRR gaps), sharpening the community\u2019s understanding of what fairness in face recognition must quantify. In response, many mitigation efforts focused on data balancing and domain adaptation, exemplified by RFW\u2019s benchmark and methodology, as well as in-processing strategies like adversarial debiasing. Yet, these approaches often struggled to deliver fairness at high-accuracy operating points typical of operational face recognition.\n\nAgainst this backdrop, the paper\u2019s key contribution is to treat fairness as a property that can be optimized by selecting better architectures and hyperparameters. Methodologically, it builds on multi-objective optimization foundations such as NSGA-II to reason about Pareto trade-offs\u2014here, fairness versus accuracy\u2014and on scalable joint hyperparameter/architecture search ideas exemplified by BOHB. Practically, it interrogates prevailing face recognition training recipes epitomized by ArcFace, revealing that alternative architectures discovered via fairness-aware search can Pareto-dominate standard baselines and prior debiasing techniques. Together, these prior works collectively motivate the need for, provide the metrics to evaluate, and supply the optimization tools to realize the paper\u2019s central insight: fairer architectures can materially improve the fairness\u2013accuracy frontier in face recognition.",
  "analysis_timestamp": "2026-01-07T00:02:04.870550"
}
{
  "prior_works": [
    {
      "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
      "authors": [
        "Aleksander Madry",
        "Aleksandar Makelov",
        "Ludwig Schmidt",
        "Dimitris Tsipras",
        "Adrian Vladu"
      ],
      "year": 2018,
      "role": "Theoretical foundation for minimax robust optimization",
      "relationship_sentence": "RPO\u2019s core objective\u2014explicitly modeling an adaptive adversary and optimizing against worst-case attacks\u2014directly instantiates the Madry-style adversarial training (min\u2013max) paradigm, but applies it to system-level prompts instead of model parameters."
    },
    {
      "title": "Universal Adversarial Triggers for NLP",
      "authors": [
        "Eric Wallace",
        "Shi Feng",
        "Nikhil Kandpal",
        "Matt Gardner",
        "Sameer Singh"
      ],
      "year": 2019,
      "role": "Demonstrated universal, transferable token triggers",
      "relationship_sentence": "RPO\u2019s idea of learning a compact, transferable suffix is inspired by universal trigger work showing that short token sequences can reliably control model behavior across inputs."
    },
    {
      "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
      "authors": [
        "Andy Zou et al."
      ],
      "year": 2023,
      "role": "Jailbreak attacks via optimized adversarial suffixes (GCG)",
      "relationship_sentence": "RPO is explicitly designed to defend against suffix-based jailbreaks like GCG by embedding such adaptive attackers in the inner loop and optimizing a counter-suffix that remains robust to both seen and unseen jailbreak variants."
    },
    {
      "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
      "authors": [
        "Taylor Shin",
        "Yasaman Razeghi",
        "Robert L. Logan IV",
        "Eric Wallace",
        "Sameer Singh"
      ],
      "year": 2020,
      "role": "Discrete prompt token optimization methodology",
      "relationship_sentence": "RPO\u2019s search over discrete tokenized prompts builds on the insight that token-level optimization can systematically steer LMs, adapting this idea from capability elicitation to safety defense."
    },
    {
      "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
      "authors": [
        "Xiang Lisa Li",
        "Percy Liang"
      ],
      "year": 2021,
      "role": "Lightweight, parameter-efficient control via small prompts",
      "relationship_sentence": "RPO adopts the principle of modifying a small, model-agnostic prompt (rather than fine-tuning weights) to achieve substantial behavioral control, aligning with the lightweight control ethos of prefix/prompt-tuning."
    },
    {
      "title": "More than you\u2019ve asked for: A Comprehensive Analysis of Prompt Injection Attacks",
      "authors": [
        "Kai Greshake",
        "Ahmed Salem",
        "Michael Backes",
        "Ben Stock",
        "Yang Zhang"
      ],
      "year": 2023,
      "role": "Threat modeling for prompt/jailbreak attacks in LLMs",
      "relationship_sentence": "By cataloging diverse prompt injection strategies and demonstrating adaptive attack behaviors, this work motivates RPO\u2019s need to optimize against a broad, evolving adversary set and to generalize beyond seen attack templates."
    }
  ],
  "synthesis_narrative": "Robust Prompt Optimization (RPO) marries the minimax philosophy of adversarial training with the mechanics of prompt-based control. The Madry et al. framework provides the foundational insight that robustness emerges from optimizing against an explicit inner-loop adversary; RPO instantiates this at the prompt level, treating the system suffix as the optimizable defense object rather than the model\u2019s weights. Wallace et al.\u2019s universal adversarial triggers and Zou et al.\u2019s GCG jailbreaks demonstrate that short, token-level suffixes can universally and transferably manipulate LLM behavior\u2014precisely the attack surface RPO targets by learning a robust counter-suffix in a worst-case setting. Methodologically, RPO draws on discrete prompt optimization traditions: AutoPrompt shows that token search can reliably steer models without modifying parameters, while prefix/prompt-tuning underscores that small, lightweight prompts can deliver strong control and transfer across tasks and models. Finally, the broad threat taxonomy and adaptive scenarios outlined by Greshake et al. motivate RPO\u2019s emphasis on defenses that generalize beyond seen jailbreak templates, ensuring robustness against evolving and indirect prompt manipulations. Together, these works directly shape RPO\u2019s key contribution: a principled minimax objective and practical algorithm for optimizing a compact, transferable system-level suffix that measurably hardens LLMs against both known and adaptive jailbreak attacks.",
  "analysis_timestamp": "2026-01-07T00:02:04.744345"
}
{
  "prior_works": [
    {
      "title": "Learning to Branch in Mixed-Integer Programming with Graph Convolutional Neural Networks",
      "authors": "Maxime Gasse, Didier Ch\u00e9telat, Nicola Ferroni, Laurent Charlin, Andrea Lodi",
      "year": 2019,
      "role": "Foundational L2O with MPNNs for branching decisions",
      "relationship_sentence": "Provides the MPNN-based representation and imitation-learning framework for replacing strong branching, which the paper extends to QP settings and targets with augmentation to overcome data scarcity."
    },
    {
      "title": "Learning to Branch in Mixed Integer Programs",
      "authors": "Elias B. Khalil, Pierre Le Bodic, Le Song, George Nemhauser, Bistra Dilkina",
      "year": 2016,
      "role": "Pioneering learning-to-branch surrogate for strong branching",
      "relationship_sentence": "Establishes the learning-to-branch paradigm that motivates building data-driven surrogates for strong branching scores, the exact target task whose robustness the proposed augmentations aim to improve."
    },
    {
      "title": "Learning Combinatorial Optimization Algorithms over Graphs",
      "authors": "Hanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, Le Song",
      "year": 2017,
      "role": "MPNN blueprint for learning heuristics on graph-structured optimization problems",
      "relationship_sentence": "Demonstrates the efficacy of message passing for learning optimization heuristics, directly informing the choice of MPNNs as the backbone that benefits from the paper\u2019s augmentation scheme."
    },
    {
      "title": "Symmetry in Integer Linear Programming",
      "authors": "Fran\u00e7ois Margot",
      "year": 2010,
      "role": "Symmetry theory for optimization models",
      "relationship_sentence": "Supplies the symmetry group perspective (e.g., variable/constraint permutations) that underpins optimality-preserving transformations used as principled data augmentations in the paper."
    },
    {
      "title": "Convex Optimization",
      "authors": "Stephen Boyd, Lieven Vandenberghe",
      "year": 2004,
      "role": "QP fundamentals and equivalence transformations",
      "relationship_sentence": "Provides the theoretical basis for equivalence-preserving operations in QPs (e.g., scaling, redundant constraints, congruence under variable reparameterizations) used to justify augmentations that preserve optimal solutions."
    },
    {
      "title": "Group Equivariant Convolutional Networks",
      "authors": "Taco S. Cohen, Max Welling",
      "year": 2016,
      "role": "Principled use of symmetry groups for invariance/equivariance and augmentation",
      "relationship_sentence": "Inspires framing data augmentation as sampling from problem symmetries; the paper adapts this idea to QP symmetries to generate label-consistent training instances for MPNNs."
    },
    {
      "title": "Graph Contrastive Learning with Augmentations (GraphCL)",
      "authors": "Yunsheng You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang",
      "year": 2020,
      "role": "Graph data augmentation as a learning signal",
      "relationship_sentence": "Highlights the power and pitfalls of graph augmentations, motivating the need for domain-consistent, optimality-preserving augmentations that the paper formalizes for QPs."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core contribution\u2014principled, theoretically justified data augmentation for training MPNNs to solve quadratic programs\u2014sits at the intersection of learning-to-optimize for mathematical programming and symmetry-based augmentation. Gasse et al. (2019) established an MPNN formulation for mixed-integer optimization states and demonstrated that a learned policy can mimic strong branching; Khalil et al. (2016) pioneered this learning-to-branch objective, defining the target signal that the present work seeks to approximate robustly in data-scarce regimes. Dai et al. (2017) further validated message passing as a vehicle for learning heuristics over graph-structured optimization problems, solidifying MPNNs as the architectural choice.\n\nThe augmentation strategy draws on two theoretical pillars. First, convex optimization theory (Boyd & Vandenberghe, 2004) provides equivalence-preserving transformations for QPs\u2014such as positive rescalings, redundant constraints, and congruent variable reparameterizations\u2014that maintain optimality and labels. Second, symmetry in integer/linear programming (Margot, 2010) frames permutations of variables/constraints as group actions that preserve solution structure, offering a rigorous lens for generating additional training instances.\n\nFinally, the general paradigm of leveraging symmetry groups for invariance/equivariance and data augmentation (Cohen & Welling, 2016) and the practical lessons from graph augmentation in representation learning (You et al., 2020) inform the design choices: augment only along symmetries that guarantee label consistency. Together, these works directly enable the paper\u2019s main advance: a symmetry- and equivalence-driven augmentation pipeline that yields diverse yet optimality-preserving QP instances, improving robustness and generalization of L2O MPNNs.",
  "analysis_timestamp": "2026-01-07T00:21:32.330546"
}
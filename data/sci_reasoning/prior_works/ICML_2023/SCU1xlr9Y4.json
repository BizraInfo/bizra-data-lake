{
  "prior_works": [
    {
      "title": "Deep Sets",
      "authors": "Manzil Zaheer et al.",
      "year": 2017,
      "role": "Baseline",
      "relationship_sentence": "The proposed layers generalize the permutation-invariant/equivariant set-processing principle of Deep Sets from single-set symmetry to the coupled product-of-permutations acting on neurons across MLP layers, providing a stronger baseline they directly improve upon."
    },
    {
      "title": "Equivariance Through Parameter-Sharing",
      "authors": "Siamak Ravanbakhsh et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "This work established that group equivariance can be enforced via parameter sharing; the paper adopts this framework to derive and characterize all affine layers equivariant to neuron-permutation symmetries in deep weight spaces."
    },
    {
      "title": "Invariant and Equivariant Graph Networks",
      "authors": "Haggai Maron et al.",
      "year": 2019,
      "role": "Extension",
      "relationship_sentence": "Building on Maron et al.\u2019s complete characterization of linear S_n-equivariant maps for graph-structured tensors, the paper extends the characterization to the product of symmetric groups that act on MLP weights and biases and derives the corresponding affine equivariant/invariant layers."
    },
    {
      "title": "Git Re-Basin: Merging Models modulo Permutation Symmetries",
      "authors": "Samuel Ainsworth et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "By showing that neuron permutations are a fundamental symmetry that hinders direct comparison or merging of networks unless aligned, this work motivates the need for architectures that are intrinsically equivariant to such permutations, which the paper provides."
    },
    {
      "title": "Implicit Neural Representations with Periodic Activation Functions (SIREN)",
      "authors": "Vincent Sitzmann et al.",
      "year": 2020,
      "role": "Foundation",
      "relationship_sentence": "SIREN popularized representing signals as MLP weights (INRs), directly motivating the paper\u2019s problem setting of learning on raw weight vectors of pre-trained MLPs representing functions."
    },
    {
      "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
      "authors": "Ben Mildenhall et al.",
      "year": 2020,
      "role": "Related Problem",
      "relationship_sentence": "NeRF demonstrates real-world objects represented as MLP weights, providing a central application domain for the paper\u2019s equivariant architecture to process and edit neural fields directly in weight space."
    },
    {
      "title": "Learning to learn by gradient descent by gradient descent",
      "authors": "Marcin Andrychowicz et al.",
      "year": 2016,
      "role": "Related Problem",
      "relationship_sentence": "This meta-optimization work operates directly on network parameters but lacks neuron-permutation equivariance; the paper addresses this gap by designing and characterizing layers that respect the natural permutation symmetries of deep weight spaces."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core idea\u2014designing architectures that process neural networks in weight space while respecting neuron-permutation symmetries\u2014draws directly from the theory and practice of permutation-equivariant learning. Deep Sets provided the essential blueprint for permutation-invariant/equivariant processing on unordered structures, serving as the natural baseline that the paper generalizes beyond single-set symmetry. Ravanbakhsh et al.\u2019s parameter-sharing view of equivariance supplied the foundational mechanism for constructing group-equivariant layers, which the authors adopt to systematically derive layers tied to the specific product of symmetric groups acting on neurons in each MLP layer. Maron et al. advanced this line by characterizing all linear equivariant maps for S_n actions in graph networks; the present work extends that characterization to the coupled, multi-layer permutation group of deep weight spaces and lifts it to the affine case, yielding a complete description of admissible equivariant and invariant layers.\nSimultaneously, application-driven works on implicit representations\u2014SIREN and NeRF\u2014framed signals and scenes as MLPs, making the raw weights themselves meaningful data objects. This motivates learning directly in weight space for tasks like editing INRs or adapting pre-trained networks. Finally, recent observations that neuron permutations obstruct direct model comparison and merging, epitomized by Git Re-Basin, crystallize the practical need for permutation-aware architectures; the paper answers this need with a principled, symmetry-respecting design. Earlier meta-learning on parameters (Andrychowicz et al.) hinted at operating in weight space, but without symmetry guarantees\u2014precisely the gap the present work closes.",
  "analysis_timestamp": "2026-01-06T23:09:26.560648"
}
{
  "prior_works": [
    {
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": "Edward J. Hu et al.",
      "year": 2021,
      "role": "Extension",
      "relationship_sentence": "HealthGPT\u2019s H-LoRA directly extends LoRA by introducing heterogeneous low-rank adapters to inject distinct comprehension and generation knowledge streams into a frozen LLM while preserving base parameters."
    },
    {
      "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning",
      "authors": "Jonas Pfeiffer et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "The idea of composing multiple task-specific adapters to aggregate heterogeneous knowledge in HealthGPT is conceptually inspired by AdapterFusion, which demonstrated non-destructive integration of separate adapters."
    },
    {
      "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
      "authors": "Junnan Li et al.",
      "year": 2023,
      "role": "Foundation",
      "relationship_sentence": "HealthGPT\u2019s bootstrapping philosophy and stage-wise alignment of visual features to a frozen LLM follow BLIP-2\u2019s blueprint of leveraging a powerful LLM with lightweight cross-modal adaptation."
    },
    {
      "title": "LLaVA: Large Language and Vision Assistant",
      "authors": "Haotian Liu et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "HealthGPT adopts and advances the LLaVA-style visual instruction tuning pipeline, addressing LLaVA\u2019s limitations on fine-grained medical perception with its hierarchical visual perception (HVP) and three-stage learning (TLS)."
    },
    {
      "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
      "authors": "Deyao Zhu et al.",
      "year": 2023,
      "role": "Gap Identification",
      "relationship_sentence": "HealthGPT targets MiniGPT-4\u2019s two-stage alignment limitations\u2014hallucinations and weak domain generation\u2014by proposing H-LoRA and TLS to robustly fuse medical comprehension and report-style generation."
    },
    {
      "title": "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework",
      "authors": "Peng Wang et al.",
      "year": 2022,
      "role": "Foundation",
      "relationship_sentence": "HealthGPT\u2019s unified autoregressive formulation for both visual comprehension (e.g., VQA) and generation (e.g., reporting) directly builds on OFA\u2019s principle of casting diverse multimodal tasks into a single sequence-to-sequence interface."
    },
    {
      "title": "Med-Flamingo: a Multimodal Medical Few-Shot Learner",
      "authors": "Michael Moor et al.",
      "year": 2023,
      "role": "Baseline",
      "relationship_sentence": "As a leading Med-LVLM baseline, Med-Flamingo\u2019s limitations in scaling and unified training motivate HealthGPT\u2019s heterogeneous adaptation (H-LoRA) and tailored hierarchical perception for clinical images."
    }
  ],
  "synthesis_narrative": "HealthGPT\u2019s core contribution\u2014unifying medical visual comprehension and generation under a single autoregressive paradigm via heterogeneous low-rank adaptation\u2014is rooted in three strands of prior work. First, the unified sequence modeling lineage (OFA) established that diverse multimodal tasks can be cast as a single sequence-to-sequence problem, a principle HealthGPT adopts to jointly handle VQA-style comprehension and long-form report generation. Second, the LVLM bootstrapping line (BLIP-2, LLaVA, MiniGPT-4) showed how to harness powerful frozen LLMs with lightweight cross-modal adapters and instruction tuning; HealthGPT follows this stagewise philosophy but customizes it to the medical domain with a three-stage learning strategy and a hierarchical visual perception module to capture fine-grained, multi-scale clinical cues. Third, the parameter-efficient adaptation line (LoRA, AdapterFusion) demonstrated how modular, low-rank adapters can inject new skills without overwriting base knowledge; HealthGPT\u2019s H-LoRA advances this idea by explicitly separating and integrating heterogeneous comprehension versus generation knowledge streams within the same LLM, enabling stable co-learning of discriminative and generative medical capabilities. Med-Flamingo and LLaVA serve as principal baselines in medical LVLMs, and their limitations\u2014few-shot fragility, weaker fine-grained perception, and incomplete unification of generation and comprehension\u2014are directly addressed by HealthGPT\u2019s H-LoRA, HVP, and TLS. Together, these works form the direct intellectual scaffold that HealthGPT extends to deliver a scalable, unified Med-LVLM.",
  "analysis_timestamp": "2026-01-06T23:07:19.625112"
}
{
  "prior_works": [
    {
      "title": "Min-K%: A Simple and Strong Baseline for Pre-Training Data Detection in LLMs",
      "authors": "Shi et al.",
      "year": 2024,
      "role": "Baseline",
      "relationship_sentence": "Min-K%++ directly formalizes and extends the Min-K% heuristic\u2014aggregating the lowest K% token log-probabilities\u2014by providing a theoretical local-mode criterion under LLM conditional distributions and turning the heuristic into a principled test."
    },
    {
      "title": "Membership Inference Attacks against Machine Learning Models",
      "authors": "Reza Shokri et al.",
      "year": 2017,
      "role": "Foundation",
      "relationship_sentence": "This work established the membership inference problem framing\u2014distinguishing training versus non-training samples\u2014which Min-K%++ adopts specifically for pre-training data detection in LLMs."
    },
    {
      "title": "The Secret Sharer: Measuring Unintended Memorization in Neural Networks",
      "authors": "Nicholas Carlini et al.",
      "year": 2019,
      "role": "Foundation",
      "relationship_sentence": "By introducing exposure and likelihood-based measures of memorization, this paper directly motivates using token-level probabilities as signals for detecting whether sequences were seen in training, a core premise underlying Min-K%++."
    },
    {
      "title": "Extracting Training Data from Large Language Models",
      "authors": "Nicholas Carlini et al.",
      "year": 2021,
      "role": "Gap Identification",
      "relationship_sentence": "This work demonstrated concrete risks from LLM memorization and data regurgitation, highlighting the need for principled pre-training data detection beyond ad-hoc heuristics that Min-K%++ addresses."
    },
    {
      "title": "Label-Only Membership Inference Attacks",
      "authors": "Frederik Tram\u00e8r et al.",
      "year": 2021,
      "role": "Inspiration",
      "relationship_sentence": "Showing that membership can be inferred from ranks/thresholds of the true label\u2019s score without access to losses inspired Min-K%++\u2019s token-wise test of whether the observed token is a local mode under the conditional categorical distribution."
    },
    {
      "title": "Membership Inference Attacks From First Principles",
      "authors": "Nicholas Carlini et al.",
      "year": 2022,
      "role": "Related Problem",
      "relationship_sentence": "Its likelihood-ratio perspective informed Min-K%++\u2019s shift from heuristic thresholds to a principled decision rule comparing observed-token probability to the conditional alternative distribution."
    },
    {
      "title": "Deduplicating Training Data Makes Language Models Better",
      "authors": "Saurabh Kandpal et al.",
      "year": 2022,
      "role": "Gap Identification",
      "relationship_sentence": "By showing the prevalence and impact of duplicates/contamination in LLM corpora, this paper underscored the necessity of reliable pre-training data detection that Min-K%++ aims to provide."
    }
  ],
  "synthesis_narrative": "Min-K%++ sits at the confluence of three lines of work: the formal membership-inference lens, likelihood-based memorization signals in language models, and a practical but heuristic baseline it replaces with theory. Shokri et al. established the general membership-inference framework, which directly maps to pre-training data detection: decide if a sequence was in the training set. Carlini et al. then showed that language models memorize and can regurgitate training data, and introduced exposure/likelihood signals as practical indicators of memorization\u2014cementing token-level probabilities as key evidence for detection. Building on these foundations, the Min-K% heuristic emerged as a strong baseline that aggregates a sequence\u2019s lowest K% token log-probabilities to score membership, but it lacked a principled grounding. Two membership-inference advances nudged the field toward more principled criteria: label-only MIAs revealed that ranks/thresholds of the true label\u2019s score can suffice for membership, and LiRA formalized likelihood-ratio decision rules. Min-K%++ fuses these insights: it proves that, under maximum-likelihood training, genuine training samples are local maxima along each input (token) dimension of the model\u2019s conditional distributions, and then operationalizes this as a discrete local-mode test\u2014transforming Min-K%\u2019s heuristic into a theoretically motivated decision rule. Finally, the urgency of reliable detection is reinforced by work on data duplication/contamination in LLM corpora, which Min-K%++ is designed to address with stronger foundations and performance.",
  "analysis_timestamp": "2026-01-06T23:08:23.924372"
}
{
  "prior_works": [
    {
      "title": "Using goal-driven deep learning models to understand sensory cortex",
      "authors": "Daniel L.K. Yamins, James J. DiCarlo",
      "year": 2016,
      "role": "Modeling paradigm",
      "relationship_sentence": "Established the stimulus-computable, task-optimized framework for building vision models that the paper assumes as the base system from which a reaction-time metric is derived."
    },
    {
      "title": "Recurrence is required to capture the representational dynamics of the human visual system",
      "authors": "Tim C. Kietzmann, Courtney J. Spoerer, Lynn K. McClure, Robert A. Kriegeskorte",
      "year": 2019,
      "role": "Empirical motivation for recurrence",
      "relationship_sentence": "Showed that recurrent computations are necessary to match human temporal dynamics, motivating the use of recurrent vision models whose iterative states can be mapped onto reaction times."
    },
    {
      "title": "Recurrent Convolutional Neural Networks: A Better Model of Biological Object Recognition",
      "authors": "Courtney J. Spoerer, Patrick McClure, Nikolaus Kriegeskorte",
      "year": 2017,
      "role": "Architectural precedent",
      "relationship_sentence": "Demonstrated concrete recurrent CNN architectures whose time-unfolded processing can be interpreted as evolving evidence, a prerequisite for defining an accumulation-based RT metric."
    },
    {
      "title": "The diffusion decision model: Theory and data for two-choice decision tasks",
      "authors": "Roger Ratcliff, Gail McKoon",
      "year": 2008,
      "role": "Foundational decision-time theory",
      "relationship_sentence": "Provided the core evidence-accumulation account linking decision thresholds to reaction time, which the paper operationalizes over recurrent model iterations."
    },
    {
      "title": "Subjective Logic: A Formalism for Reasoning Under Uncertainty",
      "authors": "Audun J\u00f8sang",
      "year": 2016,
      "role": "Theoretical framework for evidential representation",
      "relationship_sentence": "Supplied the principled mapping from categorical probabilities to belief, uncertainty, and Dirichlet evidence that the paper uses to summarize and accumulate model evidence over time."
    },
    {
      "title": "Evidential Deep Learning to Quantify Predictive Uncertainty",
      "authors": "Murat Sensoy, Lance Kaplan, Melih Kandemir",
      "year": 2018,
      "role": "Methodological precedent for extracting evidence from DNNs",
      "relationship_sentence": "Introduced a practical way to convert network logits into Dirichlet evidence under subjective logic, directly informing the paper\u2019s per-iteration evidence metric in recurrent models."
    },
    {
      "title": "Deep Equilibrium Models",
      "authors": "Shaojie Bai, J. Zico Kolter, Vladlen Koltun",
      "year": 2019,
      "role": "Stability and implicit recurrence framework",
      "relationship_sentence": "Provided a stable, convergent notion of recurrent inference and computation-time that underpins treating iterative model dynamics as evidence accumulation until a stopping criterion, analogous to reaction time."
    }
  ],
  "synthesis_narrative": "The paper\u2019s key contribution\u2014deriving a human-like reaction time (RT) metric from stimulus-computable, task-optimized recurrent vision models\u2014sits at the intersection of three threads. First, goal-driven modeling (Yamins & DiCarlo) established that high-performing, stimulus-computable networks are viable scientific models of vision, while work on recurrence in vision (Kietzmann et al.; Spoerer et al.) showed that iterative processing is needed to capture temporal dynamics observed in humans and brains. This motivates using recurrent models whose unfolding computations can be treated as time-varying internal states.\nSecond, classic decision-making theory (Ratcliff & McKoon) links RTs to the time required for noisy evidence to reach a decision threshold. To instantiate such accumulation within neural networks, the paper needs a principled way to quantify and aggregate evidence from model outputs across iterations.\nThird, subjective logic (J\u00f8sang) and its practical instantiation in evidential deep learning (Sensoy et al.) provide exactly this: a mapping from categorical predictions to Dirichlet-based evidence and uncertainty. This lets the authors summarize each recurrent step as incremental evidence and define threshold-crossing dynamics consistent with sequential sampling theories. Finally, stable implicit recurrence (Bai et al., Deep Equilibrium Models) offers a framework for well-behaved iterative inference and natural stopping criteria, aligning model compute time with RT. Together, these works directly enable the paper\u2019s novel RT metric: accumulate subjective-logic evidence over recurrent iterations of a task-optimized vision model and read out reaction time from when evidence meets decision criteria, yielding alignment with human RT patterns across diverse tasks.",
  "analysis_timestamp": "2026-01-07T00:02:04.802824"
}
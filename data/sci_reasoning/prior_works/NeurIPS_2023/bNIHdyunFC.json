{
  "prior_works": [
    {
      "title": "Group Equivariant Convolutional Networks",
      "authors": "Taco S. Cohen, Max Welling",
      "year": 2016,
      "role": "Foundational hard equivariant architectures via group convolutions",
      "relationship_sentence": "The paper relaxes the fixed, hard symmetry constraints of G-CNNs by learning the degree of equivariance per layer while keeping their general benefits."
    },
    {
      "title": "Equivariance Through Parameter-Sharing",
      "authors": "Siamak Ravanbakhsh, Jeff Schneider, Barnab\u00e1s P\u00f3czos",
      "year": 2017,
      "role": "Formal link between group actions and weight-tying/connectivity patterns",
      "relationship_sentence": "This work\u2019s view of equivariance as induced by connectivity/weight-sharing directly motivates the paper\u2019s learnable, layer-wise parameterization of symmetry structure."
    },
    {
      "title": "A General E(2)-Equivariant Steerable CNN",
      "authors": "Maurice Weiler, Gabriele Cesa",
      "year": 2019,
      "role": "Practical parameterizations of equivariant filters for Euclidean symmetries",
      "relationship_sentence": "The proposed soft equivariance parameterizations build on steerable representations\u2019 flexible filter bases, but make the strength of equivariance learnable rather than fixed."
    },
    {
      "title": "Augerino: Search for Invariances in Neural Networks",
      "authors": "Gregory Benton, Y. Daniel (D.) Finzi, Pavel Izmailov, Andrew Gordon Wilson",
      "year": 2020,
      "role": "Gradient-based learning of symmetries (invariances) via differentiable augmentation and regularization",
      "relationship_sentence": "Augerino\u2019s paradigm of learning symmetries from data directly inspires this paper\u2019s goal of automatically discovering and tuning layer-wise equivariances."
    },
    {
      "title": "Learning Invariances Using the Marginal Likelihood",
      "authors": "Mark van der Wilk, Vincent Dutordoir, Carl Edward Rasmussen, James Hensman",
      "year": 2018,
      "role": "Evidence-based discovery of symmetries in probabilistic models",
      "relationship_sentence": "This work motivates using the marginal likelihood to balance data fit and model complexity for symmetry discovery, which the paper extends to deep nets and layer-wise equivariance."
    },
    {
      "title": "Laplace Redux \u2014 Effortless Bayesian Deep Learning",
      "authors": "Christian Daxberger, A. Kristiadi, Alexander Immer, Matthias Eschenhagen, Philipp Hennig Bauer, Neil Nickisch",
      "year": 2021,
      "role": "Differentiable Laplace approximation enabling marginal likelihood optimization in deep nets",
      "relationship_sentence": "Provides the differentiable marginal-likelihood machinery the paper leverages to learn the amount of equivariance per layer."
    },
    {
      "title": "A Scalable Laplace Approximation for Neural Networks",
      "authors": "Hippolyt Ritter, Aleksandar Botev, David Barber",
      "year": 2018,
      "role": "Scalable curvature approximations (K-FAC) for Laplace in deep networks",
      "relationship_sentence": "Underpins practical evidence estimation in deep models, supporting the paper\u2019s use of Laplace-based marginal likelihood for symmetry learning."
    }
  ],
  "synthesis_narrative": "The paper\u2019s core idea\u2014learning the amount of equivariance in each layer via gradients and marginal likelihood\u2014sits at the intersection of equivariant architectures and Bayesian evidence optimization. Group Equivariant CNNs established the benefits of encoding symmetry via hard constraints, while Equivariance Through Parameter-Sharing clarified that such symmetries arise from weight-tying/connectivity patterns. Steerable CNN frameworks generalized these constructions with flexible filter bases for Euclidean groups, informing how layer-wise equivariances can be parameterized. However, these approaches fix symmetries a priori.\n\nAugerino shifted the paradigm by learning symmetries directly from data through differentiable objectives, demonstrating that invariances need not be prescribed. In probabilistic modeling, Learning Invariances Using the Marginal Likelihood showed that the evidence can balance data fit and model complexity to infer invariances automatically. Bringing this insight to deep networks requires scalable, differentiable approximations to the marginal likelihood; Laplace Redux provided exactly this tool, while earlier scalable Laplace methods (e.g., K-FAC-based Laplace) made evidence estimation practical for large networks.\n\nCombining these strands, the paper introduces improved soft parameterizations of layer-wise equivariance (informed by steerable/parameter-sharing views) and learns their strength via differentiable Laplace-approximated marginal likelihood. This unifies symmetry discovery with Bayesian model selection, enabling data-driven, layer-specific equivariance that adapts beyond fixed group assumptions while preserving computational tractability.",
  "analysis_timestamp": "2026-01-07T00:02:04.781971"
}
{
  "prior_works": [
    {
      "title": "MT-Bench and Chatbot Arena: Evaluating LLMs via Pairwise Judgments and Crowdsourced Battles",
      "authors": [
        "Lianmin Zheng",
        "Wei-Lin Chiang",
        "Siyuan Zhuang",
        "Ying Sheng",
        "et al."
      ],
      "year": 2023,
      "role": "Benchmark and reference signal",
      "relationship_sentence": "This work popularized GPT-4-as-a-judge for automatic pairwise evaluation (MT-Bench) and introduced human pairwise Arena ratings used as a gold-standard reference; the present paper measures reliability by correlating its proposed rankings against Arena and adopts the pairwise-comparison setup scrutinized for non-transitivity."
    },
    {
      "title": "AlpacaEval: Automatic Evaluation of Instruction-Following Models with Pairwise Comparisons",
      "authors": [
        "Tatsu Hashimoto lab (Stanford)",
        "et al."
      ],
      "year": 2023,
      "role": "Evaluation framework under study",
      "relationship_sentence": "AlpacaEval\u2019s baseline-referenced pairwise judging protocol is the exact setting in which the paper diagnoses non-transitive LLM-judge preferences and baseline sensitivity, motivating the shift to round-robin tournaments and model-based ranking."
    },
    {
      "title": "G-Eval: NLG Evaluation Using GPT-4 with Better Reliability",
      "authors": [
        "Liu",
        "Xu",
        "Zhang",
        "et al."
      ],
      "year": 2023,
      "role": "Evidence for LLM-as-a-judge viability",
      "relationship_sentence": "By demonstrating that GPT-4 can approximate human judgments in open-ended evaluation, G\u2011Eval underpins the LLM-as-a-judge paradigm whose hidden assumption of transitivity this paper interrogates."
    },
    {
      "title": "Rank Analysis of Incomplete Block Designs: The Method of Paired Comparisons (Bradley\u2013Terry Model)",
      "authors": [
        "R. A. Bradley",
        "M. E. Terry"
      ],
      "year": 1952,
      "role": "Core statistical model",
      "relationship_sentence": "The paper\u2019s mitigation strategy explicitly fits Bradley\u2013Terry preference models to round-robin outcomes to recover transitive latent scores from potentially non-transitive observed LLM judgments."
    },
    {
      "title": "On Extending the Bradley\u2013Terry Model to Allow for Ties in Paired Comparisons",
      "authors": [
        "R. R. Davidson"
      ],
      "year": 1970,
      "role": "Model extension for realistic comparisons",
      "relationship_sentence": "Allowing ties is critical in LLM judging; Davidson\u2019s extension informs how to statistically handle win\u2013loss\u2013tie outcomes when fitting Bradley\u2013Terry\u2013type models to tournament data."
    },
    {
      "title": "The Rating of Chessplayers, Past and Present (Elo rating system)",
      "authors": [
        "Arpad E. Elo"
      ],
      "year": 1978,
      "role": "Pairwise-comparison ranking baseline",
      "relationship_sentence": "Arena-style pairwise human battles commonly use Elo; the paper contrasts baseline-sensitive pairwise judging with more reliable tournament-plus-model approaches that conceptually relate to Elo\u2019s pairwise skill inference."
    },
    {
      "title": "TrueSkill: A Bayesian Skill Rating System",
      "authors": [
        "Ralf Herbrich",
        "Tom Minka",
        "Thore Graepel"
      ],
      "year": 2007,
      "role": "Probabilistic ranking from tournaments",
      "relationship_sentence": "TrueSkill exemplifies principled inference of latent abilities from round-robin match data; it directly motivates using tournament designs paired with probabilistic preference models for robust LLM rankings."
    }
  ],
  "synthesis_narrative": "The paper interrogates a core assumption behind LLM-as-a-judge evaluation\u2014transitive preferences\u2014within a widely used automatic framework and proposes a principled fix grounded in classic pairwise-ranking theory. The immediate precursors are MT-Bench and Chatbot Arena, which popularized GPT-4-as-a-judge for automatic pairwise comparisons and provided human pairwise preferences with Elo ratings as a public reference. AlpacaEval operationalizes a baseline-referenced pairwise protocol that is convenient but implicitly assumes transitivity; this work directly targets that setting, showing how non-transitive judge preferences induce baseline-sensitive rankings. Earlier evidence such as G\u2011Eval established that LLMs can approximate human evaluators, making it essential to understand their failure modes and the statistical structure of their preferences.\nTo remedy non-transitivity, the authors leverage tournament design and model-based inference from the paired-comparison literature. The Bradley\u2013Terry model (and its Davidson extension for ties) provides a probabilistic framework that recovers a transitive latent score even when observed pairwise outcomes contain cycles. Complementary systems like Elo and TrueSkill demonstrate how round-robin or match-based data can be converted into robust skill estimates, inspiring the paper\u2019s shift from baseline comparisons to round-robin tournaments fitted with Bradley\u2013Terry models. By aligning the evaluation protocol with these principled ranking methods and validating against Chatbot Arena, the paper connects modern LLM judging practice to established statistical tools, thereby improving reliability and external validity of LLM leaderboards.",
  "analysis_timestamp": "2026-01-07T00:04:09.152049"
}
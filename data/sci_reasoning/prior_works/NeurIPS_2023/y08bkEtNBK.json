{
  "prior_works": [
    {
      "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting",
      "authors": "Haoyi Zhou et al.",
      "year": 2021,
      "role": "Efficiency baseline for long-range TS forecasting via sparse attention",
      "relationship_sentence": "WITRAN\u2019s Recurrent Acceleration Network (RAN) is positioned as a more generic, lower-complexity alternative to Informer\u2019s sparsified self-attention for handling very long sequences."
    },
    {
      "title": "Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting",
      "authors": "Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long",
      "year": 2021,
      "role": "Periodic-pattern discovery and decomposition for long-term forecasting",
      "relationship_sentence": "Autoformer\u2019s emphasis on discovering series-wise periodicity motivates WITRAN\u2019s bi-granular information transmission to capture both long- and short-term repetitive patterns within a unified framework."
    },
    {
      "title": "FEDformer: Frequency Enhanced Decomposed Transformer for Long-Term Series Forecasting",
      "authors": "Tian Zhou et al.",
      "year": 2022,
      "role": "Frequency-domain modeling to capture global periodic structures efficiently",
      "relationship_sentence": "FEDformer\u2019s frequency-enhanced components underscore the value of modeling global periodic correlations, which WITRAN complements by simultaneously addressing local correlations and short-term repetitions."
    },
    {
      "title": "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting",
      "authors": "Bryan Lim, Sercan O. Arik, Nicolas Loeff, Tomas Pfister",
      "year": 2021,
      "role": "Gated selective fusion and variable (feature) selection across temporal and feature axes",
      "relationship_sentence": "TFT\u2019s gated residual and variable selection ideas directly inform WITRAN\u2019s Horizontal-Vertical Gated Selective Unit (HVGSU) for recursively fusing and selecting information along time and feature dimensions."
    },
    {
      "title": "TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis",
      "authors": "Yue Wu et al.",
      "year": 2023,
      "role": "Multi-periodicity discovery and local-global semantic modeling via 2D temporal patterns",
      "relationship_sentence": "TimesNet\u2019s explicit modeling of multi-scale temporal variations inspires WITRAN\u2019s bi-granular design to transmit and aggregate both short- and long-term repetitive patterns."
    },
    {
      "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling (Temporal Convolutional Networks)",
      "authors": "Shaojie Bai, J. Zico Kolter, Vladlen Koltun",
      "year": 2018,
      "role": "Multi-scale, dilated causal convolutions enabling efficient long-range dependency capture",
      "relationship_sentence": "The wave-like, multi-scale receptive fields of TCNs inform WITRAN\u2019s \u2018water-wave\u2019 information transmission concept for efficient propagation of local and global context."
    }
  ],
  "synthesis_narrative": "WITRAN\u2019s core contribution is to unify multi-scale semantic capture (global/local correlations and long/short-term repetitions) with low time\u2013memory complexity. Informer established the importance of efficiency for long-sequence forecasting via sparse attention, setting a baseline that WITRAN\u2019s Recurrent Acceleration Network seeks to surpass through a generic recurrent speedup mechanism. Autoformer and FEDformer demonstrated that discovering repetitive, long-term periodic structures\u2014via auto-correlation and frequency-enhanced decomposition\u2014is key to long-range accuracy; WITRAN generalizes this insight with bi-granular information transmission that simultaneously targets both long- and short-term repetitive patterns rather than favoring only global periodicity. TimesNet further highlighted the benefits of modeling multi-periodicity and local-global semantics with multi-scale temporal variations, which aligns with WITRAN\u2019s design to propagate information across granularities.\nIn parallel, Temporal Fusion Transformers introduced gated residual and variable selection modules to selectively fuse signals across time and feature dimensions. WITRAN operationalizes a similar philosophy in its Horizontal-Vertical Gated Selective Unit (HVGSU), recursively fusing and selecting information along temporal (horizontal) and feature (vertical) axes to capture global and local correlations. Finally, the intuition behind water-wave-style propagation draws on the success of Temporal Convolutional Networks, whose dilated, multi-scale receptive fields efficiently spread contextual information over long horizons. Together, these strands\u2014periodicity discovery, selective fusion across axes, multi-scale semantics, and efficiency\u2014converge in WITRAN\u2019s WIT + HVGSU + RAN architecture to deliver accurate and scalable long-range time series forecasting.",
  "analysis_timestamp": "2026-01-07T00:02:04.779764"
}
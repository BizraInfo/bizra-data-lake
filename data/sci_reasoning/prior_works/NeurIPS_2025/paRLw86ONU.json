{
  "prior_works": [
    {
      "title": "Concept Bottleneck Models",
      "authors": [
        "Koh et al."
      ],
      "year": 2020,
      "role": "Core antecedent: ante-hoc interpretability via concept supervision",
      "relationship_sentence": "DANCE adopts an ante-hoc concept bottleneck and extends CBMs to video by structuring the bottleneck into motion, object, and scene concept types and enforcing prediction solely through them."
    },
    {
      "title": "Two-Stream Convolutional Networks for Action Recognition",
      "authors": [
        "Karen Simonyan",
        "Andrew Zisserman"
      ],
      "year": 2014,
      "role": "Methodological inspiration: disentangling motion and appearance",
      "relationship_sentence": "DANCE operationalizes the two-stream intuition at the concept level, replacing entangled feature streams with interpretable motion (pose) and spatial (object/scene) concepts."
    },
    {
      "title": "Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition (ST-GCN)",
      "authors": [
        "Sijie Yan",
        "Yuanjun Xiong",
        "Dahua Lin"
      ],
      "year": 2018,
      "role": "Representation choice: pose sequences for motion dynamics",
      "relationship_sentence": "ST-GCN\u2019s success with skeleton sequences motivates DANCE\u2019s design of motion dynamics concepts explicitly as human pose sequences feeding the concept bottleneck."
    },
    {
      "title": "Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields (OpenPose)",
      "authors": [
        "Zhe Cao",
        "Tomas Simon",
        "Shih-En Wei",
        "Yaser Sheikh"
      ],
      "year": 2017,
      "role": "Enabling tool: robust pose keypoint extraction",
      "relationship_sentence": "OpenPose provides the practical mechanism to extract human keypoints, enabling DANCE to instantiate motion concepts as pose trajectories."
    },
    {
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
      "authors": [
        "Ramprasaath R. Selvaraju",
        "Michael Cogswell",
        "Abhishek Das",
        "Ramakrishna Vedantam",
        "Devi Parikh",
        "Dhruv Batra"
      ],
      "year": 2017,
      "role": "Motivating baseline: saliency explanations",
      "relationship_sentence": "DANCE addresses the entanglement of post-hoc saliency like Grad-CAM by enforcing disentangled, concept-level reasoning ante-hoc."
    },
    {
      "title": "Learning Transferable Visual Models From Natural Language Supervision (CLIP)",
      "authors": [
        "Alec Radford",
        "Jong Wook Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh"
      ],
      "year": 2021,
      "role": "Semantic bridge: language-guided concept enumeration",
      "relationship_sentence": "CLIP popularized leveraging language to structure vision; DANCE similarly employs LLMs to automatically enumerate object and scene concepts per action class."
    },
    {
      "title": "Generating Visual Explanations",
      "authors": [
        "Lisa Anne Hendricks",
        "Zeynep Akata",
        "Marcus Rohrbach",
        "Jeff Donahue",
        "Bernt Schiele",
        "Trevor Darrell"
      ],
      "year": 2016,
      "role": "Motivating prior: language-based justifications",
      "relationship_sentence": "Textual justification work highlights limits of language for tacit motion; DANCE departs by encoding motion as pose-sequence concepts rather than prose."
    }
  ],
  "synthesis_narrative": "DANCE\u2019s core innovation\u2014ante-hoc, disentangled concept-based reasoning for video actions\u2014sits at the intersection of concept supervision, motion\u2013appearance separation, and language-driven semantics. Concept Bottleneck Models provided the central blueprint for enforcing predictions through human-aligned concepts, which DANCE extends to the video domain by explicitly partitioning the bottleneck into motion dynamics, objects, and scenes. The longstanding two-stream paradigm crystallized the need to separate motion from spatial context; DANCE pushes this further by elevating the separation from feature streams to interpretable concept types. For motion, the skeleton-action literature, epitomized by ST-GCN, demonstrated that pose sequences effectively capture human dynamics. DANCE adopts this representation directly as its motion concepts, operationalized in practice by robust keypoint extraction methods such as OpenPose.\nAt the same time, DANCE recognizes that spatial context\u2014objects and scenes\u2014is best structured via language priors. Inspired by language-supervised vision like CLIP and the broader trend of LLM-guided semantic structuring, DANCE uses an LLM to automatically enumerate object and scene concepts tied to each action class. This stands in contrast to prior explanatory paradigms: saliency methods such as Grad-CAM provide post-hoc, entangled heatmaps, while textual justification methods show that language alone often struggles to express tacit motion. By unifying these threads, DANCE converts the two-stream intuition into a concept bottleneck with explicit motion (pose), object, and scene concepts, yielding faithful, disentangled explanations and competitive recognition performance.",
  "analysis_timestamp": "2026-01-07T00:21:32.312927"
}
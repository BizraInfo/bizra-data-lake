{
  "prior_works": [
    {
      "title": "Mean-field theory of two-layer neural networks: dimension-free bounds and kernel limit",
      "authors": [
        "Song Mei",
        "Andrea Montanari",
        "Phan-Minh Nguyen"
      ],
      "year": 2019,
      "role": "methodological precedent",
      "relationship_sentence": "Established the mean-field (measure-valued) description of training dynamics for wide two-layer networks, providing the mathematical scaffold this paper extends to capture time-scale separation and complexity growth."
    },
    {
      "title": "On Lazy Training in Differentiable Programming",
      "authors": [
        "L\u00e9na\u00efc Chizat",
        "Edouard Oyallon",
        "Francis Bach"
      ],
      "year": 2019,
      "role": "foundational theory",
      "relationship_sentence": "Clarified the lazy/NTK regime versus feature-learning regime as a function of initialization scale, directly informing the paper\u2019s result on inductive bias toward small complexity from low-complexity initialization."
    },
    {
      "title": "Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
      "authors": [
        "Arthur Jacot",
        "Franck Gabriel",
        "Cl\u00e9ment Hongler"
      ],
      "year": 2018,
      "role": "contrast baseline",
      "relationship_sentence": "Provided the kernel (no feature-learning) baseline used implicitly to contrast the paper\u2019s dynamical decoupling between feature learning and overfitting and to delineate when dynamics depart from NTK behavior."
    },
    {
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "authors": [
        "Andrew M. Saxe",
        "James L. McClelland",
        "Surya Ganguli"
      ],
      "year": 2014,
      "role": "methodological precedent",
      "relationship_sentence": "Showed analytic training-time dynamics and separation of time scales across modes, inspiring the present DMFT-based analysis of slow variables and non-monotone test error in nonlinear two-layer nets."
    },
    {
      "title": "On-line learning in soft committee machines",
      "authors": [
        "David Saad",
        "Sara A. Solla"
      ],
      "year": 1995,
      "role": "foundational theory",
      "relationship_sentence": "Pioneered statistical-mechanics/mean-field analyses of two-layer networks\u2019 learning dynamics via macroscopic order parameters, a direct intellectual precursor to the paper\u2019s DMFT treatment."
    },
    {
      "title": "Rademacher and Gaussian complexities: Risk bounds and structural results",
      "authors": [
        "Peter L. Bartlett",
        "Shahar Mendelson"
      ],
      "year": 2002,
      "role": "foundational theory",
      "relationship_sentence": "Introduced the Gaussian/Rademacher complexity framework the paper elevates to dynamical state variables, linking their training-time growth to generalization behavior."
    },
    {
      "title": "Reconciling modern machine-learning practice and the classical bias\u2013variance trade-off",
      "authors": [
        "Mikhail Belkin",
        "Daniel Hsu",
        "Siyuan Ma",
        "Soumik Mandal"
      ],
      "year": 2019,
      "role": "empirical phenomenon",
      "relationship_sentence": "Documented non-monotonic generalization (double descent), motivating the paper\u2019s dynamical explanation of non-monotone test error via feature unlearning at late times."
    }
  ],
  "synthesis_narrative": "This paper\u2019s core contribution\u2014a dynamical mean-field theory (DMFT) that reveals a separation of time scales and a decoupling between feature learning and overfitting in large two-layer networks\u2014sits at the intersection of mean-field training dynamics, implicit bias, and generalization complexity. The mean-field program for wide two-layer networks (Mei\u2013Montanari\u2013Nguyen) established the measure-valued dynamics that this work extends to explicitly track slow variables tied to model complexity. The lazy-vs-feature-learning dichotomy (Chizat\u2013Oyallon\u2013Bach) and the NTK baseline (Jacot\u2013Gabriel\u2013Hongler) provide the conceptual and mathematical contrasts for when the network remains kernel-like versus when features evolve; the present paper leverages initialization-controlled regimes to formalize an inductive bias toward low complexity when initialized with small complexity. Classic statistical-mechanics analyses of two-layer networks (Saad\u2013Solla) demonstrated how macroscopic order parameters can capture generalization during learning; this work updates that viewpoint with DMFT in modern overparameterized limits. Saxe\u2013McClelland\u2013Ganguli\u2019s exact dynamics in deep linear networks showed mode-dependent time scales and non-monotonic generalization, presaging the slow-growth and late-phase phenomena that here emerge in nonlinear networks as complexity-driven slow dynamics. Finally, the Gaussian/Rademacher complexity framework (Bartlett\u2013Mendelson) is made dynamical: the paper identifies their gradual increase as the source of a slow time scale, which, together with empirical insights from double descent (Belkin et al.), yields a principled mechanism for non-monotone test error and a feature-unlearning phase at late training times.",
  "analysis_timestamp": "2026-01-07T00:21:32.246812"
}